2018-12-03 06:01:55,403 INFO: Logging to /home/stack/.instack/install-undercloud.log
2018-12-03 06:01:55,471 INFO: Checking for a FQDN hostname...
2018-12-03 06:01:55,544 INFO: Static hostname detected as undercloud.example.com
2018-12-03 06:01:55,573 INFO: Transient hostname detected as undercloud.example.com
2018-12-03 06:01:55,575 WARNING: Option "undercloud_public_vip" from group "DEFAULT" is deprecated. Use option "undercloud_public_host" from group "DEFAULT".
2018-12-03 06:01:55,575 WARNING: Option "undercloud_admin_vip" from group "DEFAULT" is deprecated. Use option "undercloud_admin_host" from group "DEFAULT".
2018-12-03 06:01:55,584 INFO: Generated new password for undercloud_db_password
2018-12-03 06:01:55,584 INFO: Generated new password for undercloud_admin_token
2018-12-03 06:01:55,584 INFO: Generated new password for undercloud_admin_password
2018-12-03 06:01:55,585 INFO: Generated new password for undercloud_glance_password
2018-12-03 06:01:55,585 INFO: Generated new password for undercloud_heat_encryption_key
2018-12-03 06:01:55,585 INFO: Generated new password for undercloud_heat_password
2018-12-03 06:01:55,585 INFO: Generated new password for undercloud_heat_cfn_password
2018-12-03 06:01:55,586 INFO: Generated new password for undercloud_neutron_password
2018-12-03 06:01:55,586 INFO: Generated new password for undercloud_nova_password
2018-12-03 06:01:55,586 INFO: Generated new password for undercloud_ironic_password
2018-12-03 06:01:55,587 INFO: Generated new password for undercloud_aodh_password
2018-12-03 06:01:55,587 INFO: Generated new password for undercloud_gnocchi_password
2018-12-03 06:01:55,587 INFO: Generated new password for undercloud_ceilometer_password
2018-12-03 06:01:55,587 INFO: Generated new password for undercloud_panko_password
2018-12-03 06:01:55,588 INFO: Generated new password for undercloud_ceilometer_metering_secret
2018-12-03 06:01:55,588 INFO: Generated new password for undercloud_ceilometer_snmpd_password
2018-12-03 06:01:55,588 INFO: Generated new password for undercloud_swift_password
2018-12-03 06:01:55,589 INFO: Generated new password for undercloud_mistral_password
2018-12-03 06:01:55,589 INFO: Generated new password for undercloud_rabbit_cookie
2018-12-03 06:01:55,590 INFO: Generated new password for undercloud_rabbit_password
2018-12-03 06:01:55,590 INFO: Generated new password for undercloud_rabbit_username
2018-12-03 06:01:55,590 INFO: Generated new password for undercloud_heat_stack_domain_admin_password
2018-12-03 06:01:55,590 INFO: Generated new password for undercloud_swift_hash_suffix
2018-12-03 06:01:55,591 INFO: Generated new password for undercloud_haproxy_stats_password
2018-12-03 06:01:55,591 INFO: Generated new password for undercloud_zaqar_password
2018-12-03 06:01:55,591 INFO: Generated new password for undercloud_horizon_secret_key
2018-12-03 06:01:55,592 INFO: Generated new password for undercloud_cinder_password
2018-12-03 06:01:55,592 INFO: Generated new password for undercloud_novajoin_password
2018-12-03 06:01:55,651 INFO: Running yum clean all
2018-12-03 06:01:55,913 INFO: Loaded plugins: search-disabled-repos
2018-12-03 06:01:55,937 INFO: Cleaning repos: rhel-7-server-extras-rpms
2018-12-03 06:01:55,937 INFO:               : rhel-7-server-openstack-13-devtools-rpms
2018-12-03 06:01:55,937 INFO:               : rhel-7-server-openstack-13-optools-rpms
2018-12-03 06:01:55,937 INFO:               : rhel-7-server-openstack-13-rpms rhel-7-server-rh-common-rpms
2018-12-03 06:01:55,938 INFO:               : rhel-7-server-rpms rhel-ha-for-rhel-7-server-rpms
2018-12-03 06:01:55,978 INFO: yum-clean-all completed successfully
2018-12-03 06:01:55,978 INFO: Running yum update
2018-12-03 06:01:56,242 INFO: Loaded plugins: search-disabled-repos
2018-12-03 06:01:58,311 INFO: No packages marked for update
2018-12-03 06:01:58,336 INFO: yum-update completed successfully
2018-12-03 06:01:58,387 INFO: Running instack
2018-12-03 06:01:58,579 INFO: INFO: 2018-12-03 06:01:58,579 -- Starting run of instack
2018-12-03 06:01:58,592 INFO: INFO: 2018-12-03 06:01:58,591 -- Using json file: /usr/share/instack-undercloud/json-files/rhel-7-undercloud-packages.json
2018-12-03 06:01:58,592 INFO: INFO: 2018-12-03 06:01:58,592 -- Running Installation
2018-12-03 06:01:58,593 INFO: INFO: 2018-12-03 06:01:58,593 -- Initialized with elements path: /usr/share/tripleo-puppet-elements /usr/share/instack-undercloud /usr/share/tripleo-image-elements /usr/share/diskimage-builder/elements
2018-12-03 06:01:58,617 INFO: WARNING: 2018-12-03 06:01:58,617 -- expand_dependencies() deprecated, use get_elements
2018-12-03 06:01:58,651 INFO: INFO: 2018-12-03 06:01:58,650 -- List of all elements and dependencies: epel undercloud-install dib-python source-repositories install-types puppet-modules install-bin pip-manifest puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url pkg-map enable-packages-install puppet os-apply-config hiera package-installs
2018-12-03 06:01:58,651 INFO: INFO: 2018-12-03 06:01:58,650 -- Excluding element pip-and-virtualenv
2018-12-03 06:01:58,651 INFO: INFO: 2018-12-03 06:01:58,650 -- Excluding element epel
2018-12-03 06:01:58,651 INFO: INFO: 2018-12-03 06:01:58,650 -- Excluding element pip-manifest
2018-12-03 06:01:58,652 INFO: INFO: 2018-12-03 06:01:58,651 -- Excluding element package-installs
2018-12-03 06:01:58,652 INFO: INFO: 2018-12-03 06:01:58,651 -- Excluding element pkg-map
2018-12-03 06:01:58,652 INFO: INFO: 2018-12-03 06:01:58,651 -- Excluding element puppet
2018-12-03 06:01:58,652 INFO: INFO: 2018-12-03 06:01:58,651 -- Excluding element cache-url
2018-12-03 06:01:58,653 INFO: INFO: 2018-12-03 06:01:58,651 -- Excluding element dib-python
2018-12-03 06:01:58,653 INFO: INFO: 2018-12-03 06:01:58,651 -- Excluding element install-bin
2018-12-03 06:01:58,653 INFO: INFO: 2018-12-03 06:01:58,652 -- List of all elements and dependencies after excludes: undercloud-install source-repositories install-types puppet-modules puppet-stack-config os-refresh-config element-manifest manifests enable-packages-install os-apply-config hiera
2018-12-03 06:01:58,881 INFO: INFO: 2018-12-03 06:01:58,880 --   Running hook extra-data
2018-12-03 06:01:58,881 INFO: INFO: 2018-12-03 06:01:58,880 -- ############### Begin stdout/stderr logging ###############
2018-12-03 06:01:58,898 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/../environment.d/00-dib-v2-env
2018-12-03 06:01:58,901 INFO: + source /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/../environment.d/00-dib-v2-env
2018-12-03 06:01:58,901 INFO: ++ export 'IMAGE_ELEMENT=epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs'
2018-12-03 06:01:58,902 INFO: ++ IMAGE_ELEMENT='epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs'
2018-12-03 06:01:58,902 INFO: ++ export 'IMAGE_ELEMENT_YAML={cache-url: /usr/share/diskimage-builder/elements/cache-url, dib-python: /usr/share/diskimage-builder/elements/dib-python,
2018-12-03 06:01:58,903 INFO:   element-manifest: /usr/share/diskimage-builder/elements/element-manifest, enable-packages-install: /usr/share/tripleo-image-elements/enable-packages-install,
2018-12-03 06:01:58,903 INFO:   epel: /usr/share/diskimage-builder/elements/epel, hiera: /usr/share/tripleo-puppet-elements/hiera,
2018-12-03 06:01:58,903 INFO:   install-bin: /usr/share/diskimage-builder/elements/install-bin, install-types: /usr/share/diskimage-builder/elements/install-types,
2018-12-03 06:01:58,903 INFO:   manifests: /usr/share/diskimage-builder/elements/manifests, os-apply-config: /usr/share/tripleo-image-elements/os-apply-config,
2018-12-03 06:01:58,904 INFO:   os-refresh-config: /usr/share/tripleo-image-elements/os-refresh-config, package-installs: /usr/share/diskimage-builder/elements/package-installs,
2018-12-03 06:01:58,904 INFO:   pip-and-virtualenv: /usr/share/diskimage-builder/elements/pip-and-virtualenv, pip-manifest: /usr/share/tripleo-image-elements/pip-manifest,
2018-12-03 06:01:58,904 INFO:   pkg-map: /usr/share/diskimage-builder/elements/pkg-map, puppet: /usr/share/tripleo-puppet-elements/puppet,
2018-12-03 06:01:58,905 INFO:   puppet-modules: /usr/share/tripleo-puppet-elements/puppet-modules, puppet-stack-config: /usr/share/instack-undercloud/puppet-stack-config,
2018-12-03 06:01:58,905 INFO:   source-repositories: /usr/share/diskimage-builder/elements/source-repositories,
2018-12-03 06:01:58,905 INFO:   undercloud-install: /usr/share/instack-undercloud/undercloud-install}
2018-12-03 06:01:58,905 INFO: '
2018-12-03 06:01:58,906 INFO: ++ IMAGE_ELEMENT_YAML='{cache-url: /usr/share/diskimage-builder/elements/cache-url, dib-python: /usr/share/diskimage-builder/elements/dib-python,
2018-12-03 06:01:58,906 INFO:   element-manifest: /usr/share/diskimage-builder/elements/element-manifest, enable-packages-install: /usr/share/tripleo-image-elements/enable-packages-install,
2018-12-03 06:01:58,906 INFO:   epel: /usr/share/diskimage-builder/elements/epel, hiera: /usr/share/tripleo-puppet-elements/hiera,
2018-12-03 06:01:58,907 INFO:   install-bin: /usr/share/diskimage-builder/elements/install-bin, install-types: /usr/share/diskimage-builder/elements/install-types,
2018-12-03 06:01:58,907 INFO:   manifests: /usr/share/diskimage-builder/elements/manifests, os-apply-config: /usr/share/tripleo-image-elements/os-apply-config,
2018-12-03 06:01:58,907 INFO:   os-refresh-config: /usr/share/tripleo-image-elements/os-refresh-config, package-installs: /usr/share/diskimage-builder/elements/package-installs,
2018-12-03 06:01:58,908 INFO:   pip-and-virtualenv: /usr/share/diskimage-builder/elements/pip-and-virtualenv, pip-manifest: /usr/share/tripleo-image-elements/pip-manifest,
2018-12-03 06:01:58,908 INFO:   pkg-map: /usr/share/diskimage-builder/elements/pkg-map, puppet: /usr/share/tripleo-puppet-elements/puppet,
2018-12-03 06:01:58,908 INFO:   puppet-modules: /usr/share/tripleo-puppet-elements/puppet-modules, puppet-stack-config: /usr/share/instack-undercloud/puppet-stack-config,
2018-12-03 06:01:58,909 INFO:   source-repositories: /usr/share/diskimage-builder/elements/source-repositories,
2018-12-03 06:01:58,909 INFO:   undercloud-install: /usr/share/instack-undercloud/undercloud-install}
2018-12-03 06:01:58,909 INFO: '
2018-12-03 06:01:58,909 INFO: ++ export -f get_image_element_array
2018-12-03 06:01:58,909 INFO: + set +o xtrace
2018-12-03 06:01:58,910 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/../environment.d/01-export-install-types.bash
2018-12-03 06:01:58,910 INFO: + source /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/../environment.d/01-export-install-types.bash
2018-12-03 06:01:58,910 INFO: ++ export DIB_DEFAULT_INSTALLTYPE=package
2018-12-03 06:01:58,911 INFO: ++ DIB_DEFAULT_INSTALLTYPE=package
2018-12-03 06:01:58,911 INFO: + set +o xtrace
2018-12-03 06:01:58,911 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/../environment.d/01-puppet-module-pins.sh
2018-12-03 06:01:58,911 INFO: + source /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/../environment.d/01-puppet-module-pins.sh
2018-12-03 06:01:58,912 INFO: ++ export DIB_REPOREF_puppetlabs_ntp=4.2.x
2018-12-03 06:01:58,912 INFO: ++ DIB_REPOREF_puppetlabs_ntp=4.2.x
2018-12-03 06:01:58,912 INFO: + set +o xtrace
2018-12-03 06:01:58,912 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/../environment.d/02-puppet-modules-install-types.sh
2018-12-03 06:01:58,913 INFO: + source /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/../environment.d/02-puppet-modules-install-types.sh
2018-12-03 06:01:58,913 INFO: ++ DIB_DEFAULT_INSTALLTYPE=package
2018-12-03 06:01:58,913 INFO: ++ DIB_INSTALLTYPE_puppet_modules=package
2018-12-03 06:01:58,913 INFO: ++ '[' package = source ']'
2018-12-03 06:01:58,913 INFO: + set +o xtrace
2018-12-03 06:01:58,914 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/../environment.d/10-os-apply-config-venv-dir.bash
2018-12-03 06:01:58,915 INFO: + source /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/../environment.d/10-os-apply-config-venv-dir.bash
2018-12-03 06:01:58,915 INFO: ++ '[' -z '' ']'
2018-12-03 06:01:58,915 INFO: ++ export OS_APPLY_CONFIG_VENV_DIR=/opt/stack/venvs/os-apply-config
2018-12-03 06:01:58,915 INFO: ++ OS_APPLY_CONFIG_VENV_DIR=/opt/stack/venvs/os-apply-config
2018-12-03 06:01:58,915 INFO: + set +o xtrace
2018-12-03 06:01:58,916 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/../environment.d/14-manifests
2018-12-03 06:01:58,918 INFO: + source /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/../environment.d/14-manifests
2018-12-03 06:01:58,918 INFO: ++ export DIB_MANIFEST_IMAGE_DIR=/etc/dib-manifests
2018-12-03 06:01:58,919 INFO: ++ DIB_MANIFEST_IMAGE_DIR=/etc/dib-manifests
2018-12-03 06:01:58,919 INFO: ++ export DIB_MANIFEST_SAVE_DIR=instack.d/
2018-12-03 06:01:58,919 INFO: ++ DIB_MANIFEST_SAVE_DIR=instack.d/
2018-12-03 06:01:58,919 INFO: + set +o xtrace
2018-12-03 06:01:58,920 INFO: dib-run-parts Running /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/10-install-git
2018-12-03 06:01:58,923 INFO: + yum -y install git
2018-12-03 06:01:59,165 INFO: Loaded plugins: search-disabled-repos
2018-12-03 06:01:59,330 INFO: Package git-1.8.3.1-20.el7.x86_64 already installed and latest version
2018-12-03 06:01:59,330 INFO: Nothing to do
2018-12-03 06:01:59,355 INFO: dib-run-parts 10-install-git completed
2018-12-03 06:01:59,356 INFO: dib-run-parts Running /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/20-manifest-dir
2018-12-03 06:01:59,360 INFO: + set -eu
2018-12-03 06:01:59,361 INFO: + set -o pipefail
2018-12-03 06:01:59,361 INFO: + sudo mkdir -p /root/.instack/tmp/instack.hV_fWV/mnt//etc/dib-manifests
2018-12-03 06:01:59,386 INFO: dib-run-parts 20-manifest-dir completed
2018-12-03 06:01:59,387 INFO: dib-run-parts Running /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/75-inject-element-manifest
2018-12-03 06:01:59,391 INFO: + set -eu
2018-12-03 06:01:59,391 INFO: + set -o pipefail
2018-12-03 06:01:59,392 INFO: + DIB_ELEMENT_MANIFEST_PATH=/etc/dib-manifests/dib-element-manifest
2018-12-03 06:01:59,392 INFO: ++ dirname /etc/dib-manifests/dib-element-manifest
2018-12-03 06:01:59,393 INFO: + sudo mkdir -p /root/.instack/tmp/instack.hV_fWV/mnt//etc/dib-manifests
2018-12-03 06:01:59,416 INFO: + sudo /bin/bash -c 'echo epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs | tr '\'' '\'' '\''\n'\'' > /root/.instack/tmp/instack.hV_fWV/mnt//etc/dib-manifests/dib-element-manifest'
2018-12-03 06:01:59,442 INFO: dib-run-parts 75-inject-element-manifest completed
2018-12-03 06:01:59,443 INFO: dib-run-parts Running /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/98-source-repositories
2018-12-03 06:01:59,462 INFO: Getting /root/.cache/image-create/source-repositories/repositories_flock: Mon Dec  3 06:01:59 EST 2018 for /root/.instack/tmp/instack.hV_fWV/hook5_VopT/source-repository-puppet-modules
2018-12-03 06:01:59,467 INFO: (0001 / 0081)
2018-12-03 06:01:59,474 INFO: puppetlabs-apache install type not set to source
2018-12-03 06:01:59,475 INFO: (0002 / 0081)
2018-12-03 06:01:59,481 INFO: puppet-aodh install type not set to source
2018-12-03 06:01:59,482 INFO: (0003 / 0081)
2018-12-03 06:01:59,487 INFO: puppet-auditd install type not set to source
2018-12-03 06:01:59,489 INFO: (0004 / 0081)
2018-12-03 06:01:59,494 INFO: puppet-barbican install type not set to source
2018-12-03 06:01:59,495 INFO: (0005 / 0081)
2018-12-03 06:01:59,500 INFO: puppet-cassandra install type not set to source
2018-12-03 06:01:59,502 INFO: (0006 / 0081)
2018-12-03 06:01:59,507 INFO: puppet-ceph install type not set to source
2018-12-03 06:01:59,508 INFO: (0007 / 0081)
2018-12-03 06:01:59,514 INFO: puppet-ceilometer install type not set to source
2018-12-03 06:01:59,517 INFO: (0008 / 0081)
2018-12-03 06:01:59,523 INFO: puppet-congress install type not set to source
2018-12-03 06:01:59,524 INFO: (0009 / 0081)
2018-12-03 06:01:59,530 INFO: puppet-gnocchi install type not set to source
2018-12-03 06:01:59,531 INFO: (0010 / 0081)
2018-12-03 06:01:59,537 INFO: puppet-certmonger install type not set to source
2018-12-03 06:01:59,538 INFO: (0011 / 0081)
2018-12-03 06:01:59,543 INFO: puppet-cinder install type not set to source
2018-12-03 06:01:59,545 INFO: (0012 / 0081)
2018-12-03 06:01:59,550 INFO: puppet-common install type not set to source
2018-12-03 06:01:59,551 INFO: (0013 / 0081)
2018-12-03 06:01:59,556 INFO: puppet-contrail install type not set to source
2018-12-03 06:01:59,558 INFO: (0014 / 0081)
2018-12-03 06:01:59,563 INFO: puppetlabs-concat install type not set to source
2018-12-03 06:01:59,564 INFO: (0015 / 0081)
2018-12-03 06:01:59,570 INFO: puppetlabs-firewall install type not set to source
2018-12-03 06:01:59,571 INFO: (0016 / 0081)
2018-12-03 06:01:59,576 INFO: puppet-glance install type not set to source
2018-12-03 06:01:59,578 INFO: (0017 / 0081)
2018-12-03 06:01:59,583 INFO: puppet-gluster install type not set to source
2018-12-03 06:01:59,584 INFO: (0018 / 0081)
2018-12-03 06:01:59,590 INFO: puppetlabs-haproxy install type not set to source
2018-12-03 06:01:59,591 INFO: (0019 / 0081)
2018-12-03 06:01:59,596 INFO: puppet-heat install type not set to source
2018-12-03 06:01:59,598 INFO: (0020 / 0081)
2018-12-03 06:01:59,603 INFO: puppet-healthcheck install type not set to source
2018-12-03 06:01:59,604 INFO: (0021 / 0081)
2018-12-03 06:01:59,609 INFO: puppet-horizon install type not set to source
2018-12-03 06:01:59,611 INFO: (0022 / 0081)
2018-12-03 06:01:59,616 INFO: puppetlabs-inifile install type not set to source
2018-12-03 06:01:59,617 INFO: (0023 / 0081)
2018-12-03 06:01:59,623 INFO: puppet-kafka install type not set to source
2018-12-03 06:01:59,624 INFO: (0024 / 0081)
2018-12-03 06:01:59,629 INFO: puppet-keystone install type not set to source
2018-12-03 06:01:59,631 INFO: (0025 / 0081)
2018-12-03 06:01:59,636 INFO: puppet-manila install type not set to source
2018-12-03 06:01:59,637 INFO: (0026 / 0081)
2018-12-03 06:01:59,643 INFO: puppet-memcached install type not set to source
2018-12-03 06:01:59,644 INFO: (0027 / 0081)
2018-12-03 06:01:59,649 INFO: puppet-mistral install type not set to source
2018-12-03 06:01:59,651 INFO: (0028 / 0081)
2018-12-03 06:01:59,656 INFO: puppetlabs-mongodb install type not set to source
2018-12-03 06:01:59,657 INFO: (0029 / 0081)
2018-12-03 06:01:59,663 INFO: puppetlabs-mysql install type not set to source
2018-12-03 06:01:59,664 INFO: (0030 / 0081)
2018-12-03 06:01:59,669 INFO: puppet-neutron install type not set to source
2018-12-03 06:01:59,671 INFO: (0031 / 0081)
2018-12-03 06:01:59,676 INFO: puppet-nova install type not set to source
2018-12-03 06:01:59,677 INFO: (0032 / 0081)
2018-12-03 06:01:59,682 INFO: puppet-octavia install type not set to source
2018-12-03 06:01:59,684 INFO: (0033 / 0081)
2018-12-03 06:01:59,689 INFO: puppet-oslo install type not set to source
2018-12-03 06:01:59,690 INFO: (0034 / 0081)
2018-12-03 06:01:59,696 INFO: puppet-nssdb install type not set to source
2018-12-03 06:01:59,697 INFO: (0035 / 0081)
2018-12-03 06:01:59,702 INFO: puppet-opendaylight install type not set to source
2018-12-03 06:01:59,704 INFO: (0036 / 0081)
2018-12-03 06:01:59,709 INFO: puppet-ovn install type not set to source
2018-12-03 06:01:59,710 INFO: (0037 / 0081)
2018-12-03 06:01:59,716 INFO: puppet-panko install type not set to source
2018-12-03 06:01:59,717 INFO: (0038 / 0081)
2018-12-03 06:01:59,722 INFO: puppet-puppet install type not set to source
2018-12-03 06:01:59,724 INFO: (0039 / 0081)
2018-12-03 06:01:59,729 INFO: puppetlabs-rabbitmq install type not set to source
2018-12-03 06:01:59,730 INFO: (0040 / 0081)
2018-12-03 06:01:59,736 INFO: puppet-redis install type not set to source
2018-12-03 06:01:59,737 INFO: (0041 / 0081)
2018-12-03 06:01:59,742 INFO: puppetlabs-rsync install type not set to source
2018-12-03 06:01:59,744 INFO: (0042 / 0081)
2018-12-03 06:01:59,749 INFO: puppet-sahara install type not set to source
2018-12-03 06:01:59,750 INFO: (0043 / 0081)
2018-12-03 06:01:59,755 INFO: sensu-puppet install type not set to source
2018-12-03 06:01:59,757 INFO: (0044 / 0081)
2018-12-03 06:01:59,762 INFO: puppet-tacker install type not set to source
2018-12-03 06:01:59,763 INFO: (0045 / 0081)
2018-12-03 06:01:59,769 INFO: puppet-trove install type not set to source
2018-12-03 06:01:59,770 INFO: (0046 / 0081)
2018-12-03 06:01:59,775 INFO: puppet-ssh install type not set to source
2018-12-03 06:01:59,777 INFO: (0047 / 0081)
2018-12-03 06:01:59,782 INFO: puppet-staging install type not set to source
2018-12-03 06:01:59,783 INFO: (0048 / 0081)
2018-12-03 06:01:59,788 INFO: puppetlabs-stdlib install type not set to source
2018-12-03 06:01:59,790 INFO: (0049 / 0081)
2018-12-03 06:01:59,795 INFO: puppet-swift install type not set to source
2018-12-03 06:01:59,796 INFO: (0050 / 0081)
2018-12-03 06:01:59,802 INFO: puppetlabs-sysctl install type not set to source
2018-12-03 06:01:59,803 INFO: (0051 / 0081)
2018-12-03 06:01:59,808 INFO: puppet-timezone install type not set to source
2018-12-03 06:01:59,810 INFO: (0052 / 0081)
2018-12-03 06:01:59,815 INFO: puppet-uchiwa install type not set to source
2018-12-03 06:01:59,816 INFO: (0053 / 0081)
2018-12-03 06:01:59,822 INFO: puppetlabs-vcsrepo install type not set to source
2018-12-03 06:01:59,823 INFO: (0054 / 0081)
2018-12-03 06:01:59,828 INFO: puppet-vlan install type not set to source
2018-12-03 06:01:59,830 INFO: (0055 / 0081)
2018-12-03 06:01:59,835 INFO: puppet-vswitch install type not set to source
2018-12-03 06:01:59,836 INFO: (0056 / 0081)
2018-12-03 06:01:59,841 INFO: puppetlabs-xinetd install type not set to source
2018-12-03 06:01:59,843 INFO: (0057 / 0081)
2018-12-03 06:01:59,848 INFO: puppet-zookeeper install type not set to source
2018-12-03 06:01:59,849 INFO: (0058 / 0081)
2018-12-03 06:01:59,855 INFO: puppet-openstacklib install type not set to source
2018-12-03 06:01:59,856 INFO: (0059 / 0081)
2018-12-03 06:01:59,862 INFO: puppet-module-keepalived install type not set to source
2018-12-03 06:01:59,863 INFO: (0060 / 0081)
2018-12-03 06:01:59,868 INFO: puppetlabs-ntp install type not set to source
2018-12-03 06:01:59,870 INFO: (0061 / 0081)
2018-12-03 06:01:59,875 INFO: puppet-snmp install type not set to source
2018-12-03 06:01:59,876 INFO: (0062 / 0081)
2018-12-03 06:01:59,881 INFO: puppet-tripleo install type not set to source
2018-12-03 06:01:59,883 INFO: (0063 / 0081)
2018-12-03 06:01:59,888 INFO: puppet-ironic install type not set to source
2018-12-03 06:01:59,889 INFO: (0064 / 0081)
2018-12-03 06:01:59,895 INFO: puppet-ipaclient install type not set to source
2018-12-03 06:01:59,896 INFO: (0065 / 0081)
2018-12-03 06:01:59,901 INFO: puppetlabs-corosync install type not set to source
2018-12-03 06:01:59,903 INFO: (0066 / 0081)
2018-12-03 06:01:59,908 INFO: puppet-pacemaker install type not set to source
2018-12-03 06:01:59,909 INFO: (0067 / 0081)
2018-12-03 06:01:59,915 INFO: puppet_aviator install type not set to source
2018-12-03 06:01:59,916 INFO: (0068 / 0081)
2018-12-03 06:01:59,921 INFO: puppet-openstack_extras install type not set to source
2018-12-03 06:01:59,923 INFO: (0069 / 0081)
2018-12-03 06:01:59,928 INFO: konstantin-fluentd install type not set to source
2018-12-03 06:01:59,929 INFO: (0070 / 0081)
2018-12-03 06:01:59,935 INFO: puppet-elasticsearch install type not set to source
2018-12-03 06:01:59,936 INFO: (0071 / 0081)
2018-12-03 06:01:59,941 INFO: puppet-kibana3 install type not set to source
2018-12-03 06:01:59,943 INFO: (0072 / 0081)
2018-12-03 06:01:59,948 INFO: puppetlabs-git install type not set to source
2018-12-03 06:01:59,949 INFO: (0073 / 0081)
2018-12-03 06:01:59,955 INFO: puppet-datacat install type not set to source
2018-12-03 06:01:59,956 INFO: (0074 / 0081)
2018-12-03 06:01:59,961 INFO: puppet-kmod install type not set to source
2018-12-03 06:01:59,963 INFO: (0075 / 0081)
2018-12-03 06:01:59,968 INFO: puppet-zaqar install type not set to source
2018-12-03 06:01:59,969 INFO: (0076 / 0081)
2018-12-03 06:01:59,975 INFO: puppet-ec2api install type not set to source
2018-12-03 06:01:59,976 INFO: (0077 / 0081)
2018-12-03 06:01:59,981 INFO: puppet-qdr install type not set to source
2018-12-03 06:01:59,983 INFO: (0078 / 0081)
2018-12-03 06:01:59,988 INFO: puppet-systemd install type not set to source
2018-12-03 06:01:59,989 INFO: (0079 / 0081)
2018-12-03 06:01:59,994 INFO: puppet-etcd install type not set to source
2018-12-03 06:01:59,996 INFO: (0080 / 0081)
2018-12-03 06:02:00,001 INFO: puppet-veritas_hyperscale install type not set to source
2018-12-03 06:02:00,002 INFO: (0081 / 0081)
2018-12-03 06:02:00,008 INFO: puppet-ptp install type not set to source
2018-12-03 06:02:00,010 INFO: dib-run-parts 98-source-repositories completed
2018-12-03 06:02:00,011 INFO: dib-run-parts Running /root/.instack/tmp/instack.hV_fWV/hook5_VopT/extra-data.d/99-enable-install-types
2018-12-03 06:02:00,015 INFO: + set -eu
2018-12-03 06:02:00,015 INFO: + set -o pipefail
2018-12-03 06:02:00,016 INFO: + declare -a SPECIFIED_ELEMS
2018-12-03 06:02:00,016 INFO: + SPECIFIED_ELEMS[0]=
2018-12-03 06:02:00,016 INFO: + PREFIX=DIB_INSTALLTYPE_
2018-12-03 06:02:00,016 INFO: ++ env
2018-12-03 06:02:00,016 INFO: ++ grep '^DIB_INSTALLTYPE_'
2018-12-03 06:02:00,017 INFO: ++ cut -d= -f1
2018-12-03 06:02:00,018 INFO: ++ echo ''
2018-12-03 06:02:00,019 INFO: + INSTALL_TYPE_VARS=
2018-12-03 06:02:00,020 INFO: ++ find /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d -maxdepth 1 -name '*-package-install' -type d
2018-12-03 06:02:00,022 INFO: + default_install_type_dirs=/root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/puppet-modules-package-install
2018-12-03 06:02:00,022 INFO: + for _install_dir in '$default_install_type_dirs'
2018-12-03 06:02:00,023 INFO: + SUFFIX=-package-install
2018-12-03 06:02:00,023 INFO: ++ basename /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/puppet-modules-package-install
2018-12-03 06:02:00,024 INFO: + _install_dir=puppet-modules-package-install
2018-12-03 06:02:00,024 INFO: + INSTALLDIRPREFIX=puppet-modules
2018-12-03 06:02:00,024 INFO: + found=0
2018-12-03 06:02:00,025 INFO: + '[' 0 = 0 ']'
2018-12-03 06:02:00,025 INFO: + pushd /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d
2018-12-03 06:02:00,025 INFO: ~/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d /home/stack
2018-12-03 06:02:00,025 INFO: + ln -sf puppet-modules-package-install/75-puppet-modules-package .
2018-12-03 06:02:00,026 INFO: + popd
2018-12-03 06:02:00,026 INFO: /home/stack
2018-12-03 06:02:00,027 INFO: dib-run-parts 99-enable-install-types completed
2018-12-03 06:02:00,028 INFO: dib-run-parts ----------------------- PROFILING -----------------------
2018-12-03 06:02:00,028 INFO: dib-run-parts
2018-12-03 06:02:00,030 INFO: dib-run-parts Target: extra-data.d
2018-12-03 06:02:00,030 INFO: dib-run-parts
2018-12-03 06:02:00,030 INFO: dib-run-parts Script                                     Seconds
2018-12-03 06:02:00,030 INFO: dib-run-parts ---------------------------------------  ----------
2018-12-03 06:02:00,031 INFO: dib-run-parts
2018-12-03 06:02:00,041 INFO: dib-run-parts 10-install-git                                0.435
2018-12-03 06:02:00,047 INFO: dib-run-parts 20-manifest-dir                               0.029
2018-12-03 06:02:00,054 INFO: dib-run-parts 75-inject-element-manifest                    0.054
2018-12-03 06:02:00,061 INFO: dib-run-parts 98-source-repositories                        0.566
2018-12-03 06:02:00,068 INFO: dib-run-parts 99-enable-install-types                       0.015
2018-12-03 06:02:00,069 INFO: dib-run-parts
2018-12-03 06:02:00,070 INFO: dib-run-parts --------------------- END PROFILING ---------------------
2018-12-03 06:02:00,070 INFO: INFO: 2018-12-03 06:02:00,070 -- ############### End stdout/stderr logging ###############
2018-12-03 06:02:00,070 INFO: INFO: 2018-12-03 06:02:00,070 --   Running hook pre-install
2018-12-03 06:02:00,071 INFO: INFO: 2018-12-03 06:02:00,070 --     Skipping hook pre-install, the hook directory doesn't exist at /root/.instack/tmp/instack.hV_fWV/hook5_VopT/pre-install.d
2018-12-03 06:02:00,071 INFO: INFO: 2018-12-03 06:02:00,071 --   Running hook install
2018-12-03 06:02:00,071 INFO: INFO: 2018-12-03 06:02:00,071 -- ############### Begin stdout/stderr logging ###############
2018-12-03 06:02:00,088 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../environment.d/00-dib-v2-env
2018-12-03 06:02:00,091 INFO: + source /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../environment.d/00-dib-v2-env
2018-12-03 06:02:00,091 INFO: ++ export 'IMAGE_ELEMENT=epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs'
2018-12-03 06:02:00,092 INFO: ++ IMAGE_ELEMENT='epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs'
2018-12-03 06:02:00,092 INFO: ++ export 'IMAGE_ELEMENT_YAML={cache-url: /usr/share/diskimage-builder/elements/cache-url, dib-python: /usr/share/diskimage-builder/elements/dib-python,
2018-12-03 06:02:00,093 INFO:   element-manifest: /usr/share/diskimage-builder/elements/element-manifest, enable-packages-install: /usr/share/tripleo-image-elements/enable-packages-install,
2018-12-03 06:02:00,093 INFO:   epel: /usr/share/diskimage-builder/elements/epel, hiera: /usr/share/tripleo-puppet-elements/hiera,
2018-12-03 06:02:00,093 INFO:   install-bin: /usr/share/diskimage-builder/elements/install-bin, install-types: /usr/share/diskimage-builder/elements/install-types,
2018-12-03 06:02:00,094 INFO:   manifests: /usr/share/diskimage-builder/elements/manifests, os-apply-config: /usr/share/tripleo-image-elements/os-apply-config,
2018-12-03 06:02:00,094 INFO:   os-refresh-config: /usr/share/tripleo-image-elements/os-refresh-config, package-installs: /usr/share/diskimage-builder/elements/package-installs,
2018-12-03 06:02:00,094 INFO:   pip-and-virtualenv: /usr/share/diskimage-builder/elements/pip-and-virtualenv, pip-manifest: /usr/share/tripleo-image-elements/pip-manifest,
2018-12-03 06:02:00,095 INFO:   pkg-map: /usr/share/diskimage-builder/elements/pkg-map, puppet: /usr/share/tripleo-puppet-elements/puppet,
2018-12-03 06:02:00,095 INFO:   puppet-modules: /usr/share/tripleo-puppet-elements/puppet-modules, puppet-stack-config: /usr/share/instack-undercloud/puppet-stack-config,
2018-12-03 06:02:00,095 INFO:   source-repositories: /usr/share/diskimage-builder/elements/source-repositories,
2018-12-03 06:02:00,095 INFO:   undercloud-install: /usr/share/instack-undercloud/undercloud-install}
2018-12-03 06:02:00,096 INFO: '
2018-12-03 06:02:00,096 INFO: ++ IMAGE_ELEMENT_YAML='{cache-url: /usr/share/diskimage-builder/elements/cache-url, dib-python: /usr/share/diskimage-builder/elements/dib-python,
2018-12-03 06:02:00,096 INFO:   element-manifest: /usr/share/diskimage-builder/elements/element-manifest, enable-packages-install: /usr/share/tripleo-image-elements/enable-packages-install,
2018-12-03 06:02:00,097 INFO:   epel: /usr/share/diskimage-builder/elements/epel, hiera: /usr/share/tripleo-puppet-elements/hiera,
2018-12-03 06:02:00,097 INFO:   install-bin: /usr/share/diskimage-builder/elements/install-bin, install-types: /usr/share/diskimage-builder/elements/install-types,
2018-12-03 06:02:00,097 INFO:   manifests: /usr/share/diskimage-builder/elements/manifests, os-apply-config: /usr/share/tripleo-image-elements/os-apply-config,
2018-12-03 06:02:00,098 INFO:   os-refresh-config: /usr/share/tripleo-image-elements/os-refresh-config, package-installs: /usr/share/diskimage-builder/elements/package-installs,
2018-12-03 06:02:00,098 INFO:   pip-and-virtualenv: /usr/share/diskimage-builder/elements/pip-and-virtualenv, pip-manifest: /usr/share/tripleo-image-elements/pip-manifest,
2018-12-03 06:02:00,098 INFO:   pkg-map: /usr/share/diskimage-builder/elements/pkg-map, puppet: /usr/share/tripleo-puppet-elements/puppet,
2018-12-03 06:02:00,098 INFO:   puppet-modules: /usr/share/tripleo-puppet-elements/puppet-modules, puppet-stack-config: /usr/share/instack-undercloud/puppet-stack-config,
2018-12-03 06:02:00,099 INFO:   source-repositories: /usr/share/diskimage-builder/elements/source-repositories,
2018-12-03 06:02:00,099 INFO:   undercloud-install: /usr/share/instack-undercloud/undercloud-install}
2018-12-03 06:02:00,099 INFO: '
2018-12-03 06:02:00,099 INFO: ++ export -f get_image_element_array
2018-12-03 06:02:00,100 INFO: + set +o xtrace
2018-12-03 06:02:00,100 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../environment.d/01-export-install-types.bash
2018-12-03 06:02:00,100 INFO: + source /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../environment.d/01-export-install-types.bash
2018-12-03 06:02:00,100 INFO: ++ export DIB_DEFAULT_INSTALLTYPE=package
2018-12-03 06:02:00,101 INFO: ++ DIB_DEFAULT_INSTALLTYPE=package
2018-12-03 06:02:00,101 INFO: + set +o xtrace
2018-12-03 06:02:00,101 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../environment.d/01-puppet-module-pins.sh
2018-12-03 06:02:00,102 INFO: + source /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../environment.d/01-puppet-module-pins.sh
2018-12-03 06:02:00,102 INFO: ++ export DIB_REPOREF_puppetlabs_ntp=4.2.x
2018-12-03 06:02:00,102 INFO: ++ DIB_REPOREF_puppetlabs_ntp=4.2.x
2018-12-03 06:02:00,102 INFO: + set +o xtrace
2018-12-03 06:02:00,103 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../environment.d/02-puppet-modules-install-types.sh
2018-12-03 06:02:00,103 INFO: + source /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../environment.d/02-puppet-modules-install-types.sh
2018-12-03 06:02:00,103 INFO: ++ DIB_DEFAULT_INSTALLTYPE=package
2018-12-03 06:02:00,103 INFO: ++ DIB_INSTALLTYPE_puppet_modules=package
2018-12-03 06:02:00,103 INFO: ++ '[' package = source ']'
2018-12-03 06:02:00,104 INFO: + set +o xtrace
2018-12-03 06:02:00,104 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../environment.d/10-os-apply-config-venv-dir.bash
2018-12-03 06:02:00,104 INFO: + source /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../environment.d/10-os-apply-config-venv-dir.bash
2018-12-03 06:02:00,104 INFO: ++ '[' -z '' ']'
2018-12-03 06:02:00,105 INFO: ++ export OS_APPLY_CONFIG_VENV_DIR=/opt/stack/venvs/os-apply-config
2018-12-03 06:02:00,105 INFO: ++ OS_APPLY_CONFIG_VENV_DIR=/opt/stack/venvs/os-apply-config
2018-12-03 06:02:00,105 INFO: + set +o xtrace
2018-12-03 06:02:00,105 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../environment.d/14-manifests
2018-12-03 06:02:00,108 INFO: + source /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../environment.d/14-manifests
2018-12-03 06:02:00,108 INFO: ++ export DIB_MANIFEST_IMAGE_DIR=/etc/dib-manifests
2018-12-03 06:02:00,109 INFO: ++ DIB_MANIFEST_IMAGE_DIR=/etc/dib-manifests
2018-12-03 06:02:00,109 INFO: ++ export DIB_MANIFEST_SAVE_DIR=instack.d/
2018-12-03 06:02:00,109 INFO: ++ DIB_MANIFEST_SAVE_DIR=instack.d/
2018-12-03 06:02:00,109 INFO: + set +o xtrace
2018-12-03 06:02:00,110 INFO: dib-run-parts Running /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/02-puppet-stack-config
2018-12-03 06:02:01,058 INFO: dib-run-parts 02-puppet-stack-config completed
2018-12-03 06:02:01,059 INFO: dib-run-parts Running /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/10-hiera-yaml-symlink
2018-12-03 06:02:01,063 INFO: + set -o pipefail
2018-12-03 06:02:01,063 INFO: + ln -f -s /etc/puppet/hiera.yaml /etc/hiera.yaml
2018-12-03 06:02:01,067 INFO: dib-run-parts 10-hiera-yaml-symlink completed
2018-12-03 06:02:01,068 INFO: dib-run-parts Running /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/10-puppet-stack-config-puppet-module
2018-12-03 06:02:01,072 INFO: + set -o pipefail
2018-12-03 06:02:01,072 INFO: + mkdir -p /etc/puppet/manifests
2018-12-03 06:02:01,074 INFO: ++ dirname /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/10-puppet-stack-config-puppet-module
2018-12-03 06:02:01,076 INFO: + cp /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../puppet-stack-config.pp /etc/puppet/manifests/puppet-stack-config.pp
2018-12-03 06:02:01,080 INFO: dib-run-parts 10-puppet-stack-config-puppet-module completed
2018-12-03 06:02:01,080 INFO: dib-run-parts Running /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/11-create-template-root
2018-12-03 06:02:01,085 INFO: ++ os-apply-config --print-templates
2018-12-03 06:02:01,252 INFO: + TEMPLATE_ROOT=/usr/libexec/os-apply-config/templates
2018-12-03 06:02:01,252 INFO: + mkdir -p /usr/libexec/os-apply-config/templates
2018-12-03 06:02:01,256 INFO: dib-run-parts 11-create-template-root completed
2018-12-03 06:02:01,256 INFO: dib-run-parts Running /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/11-hiera-orc-install
2018-12-03 06:02:01,261 INFO: + set -o pipefail
2018-12-03 06:02:01,262 INFO: + mkdir -p /usr/libexec/os-refresh-config/configure.d/
2018-12-03 06:02:01,264 INFO: ++ dirname /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/11-hiera-orc-install
2018-12-03 06:02:01,265 INFO: + install -m 0755 -o root -g root /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../10-hiera-disable /usr/libexec/os-refresh-config/configure.d/10-hiera-disable
2018-12-03 06:02:01,274 INFO: ++ dirname /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/11-hiera-orc-install
2018-12-03 06:02:01,275 INFO: + install -m 0755 -o root -g root /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../40-hiera-datafiles /usr/libexec/os-refresh-config/configure.d/40-hiera-datafiles
2018-12-03 06:02:01,284 INFO: dib-run-parts 11-hiera-orc-install completed
2018-12-03 06:02:01,284 INFO: dib-run-parts Running /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/75-puppet-modules-package
2018-12-03 06:02:01,289 INFO: + find /opt/stack/puppet-modules/ -mindepth 1
2018-12-03 06:02:01,289 INFO: + read
2018-12-03 06:02:01,296 INFO: + ln -f -s /usr/share/openstack-puppet/modules/aodh /usr/share/openstack-puppet/modules/apache /usr/share/openstack-puppet/modules/archive /usr/share/openstack-puppet/modules/auditd /usr/share/openstack-puppet/modules/barbican /usr/share/openstack-puppet/modules/cassandra /usr/share/openstack-puppet/modules/ceilometer /usr/share/openstack-puppet/modules/ceph /usr/share/openstack-puppet/modules/certmonger /usr/share/openstack-puppet/modules/cinder /usr/share/openstack-puppet/modules/collectd /usr/share/openstack-puppet/modules/concat /usr/share/openstack-puppet/modules/contrail /usr/share/openstack-puppet/modules/corosync /usr/share/openstack-puppet/modules/datacat /usr/share/openstack-puppet/modules/designate /usr/share/openstack-puppet/modules/dns /usr/share/openstack-puppet/modules/ec2api /usr/share/openstack-puppet/modules/elasticsearch /usr/share/openstack-puppet/modules/fdio /usr/share/openstack-puppet/modules/firewall /usr/share/openstack-puppet/modules/fluentd /usr/share/openstack-puppet/modules/git /usr/share/openstack-puppet/modules/glance /usr/share/openstack-puppet/modules/gnocchi /usr/share/openstack-puppet/modules/haproxy /usr/share/openstack-puppet/modules/heat /usr/share/openstack-puppet/modules/horizon /usr/share/openstack-puppet/modules/inifile /usr/share/openstack-puppet/modules/ipaclient /usr/share/openstack-puppet/modules/ironic /usr/share/openstack-puppet/modules/java /usr/share/openstack-puppet/modules/kafka /usr/share/openstack-puppet/modules/keepalived /usr/share/openstack-puppet/modules/keystone /usr/share/openstack-puppet/modules/kibana3 /usr/share/openstack-puppet/modules/kmod /usr/share/openstack-puppet/modules/manila /usr/share/openstack-puppet/modules/memcached /usr/share/openstack-puppet/modules/midonet /usr/share/openstack-puppet/modules/mistral /usr/share/openstack-puppet/modules/module-data /usr/share/openstack-puppet/modules/mysql /usr/share/openstack-puppet/modules/n1k_vsm /usr/share/openstack-puppet/modules/neutron /usr/share/openstack-puppet/modules/nova /usr/share/openstack-puppet/modules/nssdb /usr/share/openstack-puppet/modules/ntp /usr/share/openstack-puppet/modules/octavia /usr/share/openstack-puppet/modules/opendaylight /usr/share/openstack-puppet/modules/openstack_extras /usr/share/openstack-puppet/modules/openstacklib /usr/share/openstack-puppet/modules/oslo /usr/share/openstack-puppet/modules/ovn /usr/share/openstack-puppet/modules/pacemaker /usr/share/openstack-puppet/modules/panko /usr/share/openstack-puppet/modules/rabbitmq /usr/share/openstack-puppet/modules/redis /usr/share/openstack-puppet/modules/remote /usr/share/openstack-puppet/modules/rsync /usr/share/openstack-puppet/modules/sahara /usr/share/openstack-puppet/modules/sensu /usr/share/openstack-puppet/modules/snmp /usr/share/openstack-puppet/modules/ssh /usr/share/openstack-puppet/modules/staging /usr/share/openstack-puppet/modules/stdlib /usr/share/openstack-puppet/modules/swift /usr/share/openstack-puppet/modules/sysctl /usr/share/openstack-puppet/modules/systemd /usr/share/openstack-puppet/modules/timezone /usr/share/openstack-puppet/modules/tomcat /usr/share/openstack-puppet/modules/tripleo /usr/share/openstack-puppet/modules/trove /usr/share/openstack-puppet/modules/uchiwa /usr/share/openstack-puppet/modules/vcsrepo /usr/share/openstack-puppet/modules/veritas_hyperscale /usr/share/openstack-puppet/modules/vswitch /usr/share/openstack-puppet/modules/xinetd /usr/share/openstack-puppet/modules/zaqar /usr/share/openstack-puppet/modules/zookeeper /etc/puppet/modules/
2018-12-03 06:02:01,299 INFO: dib-run-parts 75-puppet-modules-package completed
2018-12-03 06:02:01,299 INFO: dib-run-parts Running /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/99-install-config-templates
2018-12-03 06:02:01,304 INFO: ++ os-apply-config --print-templates
2018-12-03 06:02:01,470 INFO: + TEMPLATE_ROOT=/usr/libexec/os-apply-config/templates
2018-12-03 06:02:01,470 INFO: ++ dirname /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/99-install-config-templates
2018-12-03 06:02:01,472 INFO: + TEMPLATE_SOURCE=/root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../os-apply-config
2018-12-03 06:02:01,472 INFO: + mkdir -p /usr/libexec/os-apply-config/templates
2018-12-03 06:02:01,474 INFO: + '[' -d /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../os-apply-config ']'
2018-12-03 06:02:01,474 INFO: + rsync '--exclude=.*.swp' -Cr /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../os-apply-config/ /usr/libexec/os-apply-config/templates/
2018-12-03 06:02:01,483 INFO: dib-run-parts 99-install-config-templates completed
2018-12-03 06:02:01,483 INFO: dib-run-parts Running /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/99-os-refresh-config-install-scripts
2018-12-03 06:02:01,488 INFO: ++ os-refresh-config --print-base
2018-12-03 06:02:01,565 INFO: + SCRIPT_BASE=/usr/libexec/os-refresh-config
2018-12-03 06:02:01,566 INFO: ++ dirname /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/99-os-refresh-config-install-scripts
2018-12-03 06:02:01,567 INFO: + SCRIPT_SOURCE=/root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../os-refresh-config
2018-12-03 06:02:01,568 INFO: + rsync -r /root/.instack/tmp/instack.hV_fWV/hook5_VopT/install.d/../os-refresh-config/ /usr/libexec/os-refresh-config/
2018-12-03 06:02:01,574 INFO: dib-run-parts 99-os-refresh-config-install-scripts completed
2018-12-03 06:02:01,575 INFO: dib-run-parts ----------------------- PROFILING -----------------------
2018-12-03 06:02:01,575 INFO: dib-run-parts
2018-12-03 06:02:01,577 INFO: dib-run-parts Target: install.d
2018-12-03 06:02:01,577 INFO: dib-run-parts
2018-12-03 06:02:01,577 INFO: dib-run-parts Script                                     Seconds
2018-12-03 06:02:01,577 INFO: dib-run-parts ---------------------------------------  ----------
2018-12-03 06:02:01,578 INFO: dib-run-parts
2018-12-03 06:02:01,587 INFO: dib-run-parts 02-puppet-stack-config                        0.948
2018-12-03 06:02:01,594 INFO: dib-run-parts 10-hiera-yaml-symlink                         0.007
2018-12-03 06:02:01,600 INFO: dib-run-parts 10-puppet-stack-config-puppet-module          0.010
2018-12-03 06:02:01,607 INFO: dib-run-parts 11-create-template-root                       0.174
2018-12-03 06:02:01,614 INFO: dib-run-parts 11-hiera-orc-install                          0.026
2018-12-03 06:02:01,620 INFO: dib-run-parts 75-puppet-modules-package                     0.013
2018-12-03 06:02:01,627 INFO: dib-run-parts 99-install-config-templates                   0.182
2018-12-03 06:02:01,634 INFO: dib-run-parts 99-os-refresh-config-install-scripts          0.089
2018-12-03 06:02:01,636 INFO: dib-run-parts
2018-12-03 06:02:01,636 INFO: dib-run-parts --------------------- END PROFILING ---------------------
2018-12-03 06:02:01,637 INFO: INFO: 2018-12-03 06:02:01,636 -- ############### End stdout/stderr logging ###############
2018-12-03 06:02:01,637 INFO: INFO: 2018-12-03 06:02:01,637 --   Running hook post-install
2018-12-03 06:02:01,637 INFO: INFO: 2018-12-03 06:02:01,637 --     Skipping hook post-install, the hook directory doesn't exist at /root/.instack/tmp/instack.hV_fWV/hook5_VopT/post-install.d
2018-12-03 06:02:01,643 INFO: INFO: 2018-12-03 06:02:01,642 -- Ending run of instack.
2018-12-03 06:02:01,655 INFO: Instack completed successfully
2018-12-03 06:02:01,655 INFO: Running os-refresh-config
2018-12-03 06:02:01,751 INFO: [2018-12-03 06:02:01,750] (os-refresh-config) [INFO] Starting phase configure
2018-12-03 06:02:01,766 INFO: dib-run-parts Mon Dec  3 06:02:01 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/10-hiera-disable
2018-12-03 06:02:01,769 INFO: + '[' -f /etc/puppet/hiera.yaml ']'
2018-12-03 06:02:01,773 INFO: dib-run-parts Mon Dec  3 06:02:01 EST 2018 10-hiera-disable completed
2018-12-03 06:02:01,774 INFO: dib-run-parts Mon Dec  3 06:02:01 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/20-os-apply-config
2018-12-03 06:02:01,937 INFO: [2018/12/03 06:02:01 AM] [WARNING] DEPRECATED: falling back to /var/run/os-collect-config/os_config_files.json
2018-12-03 06:02:01,946 INFO: [2018/12/03 06:02:01 AM] [INFO] writing /etc/os-net-config/config.json
2018-12-03 06:02:01,946 INFO: [2018/12/03 06:02:01 AM] [INFO] writing /root/stackrc
2018-12-03 06:02:01,946 INFO: [2018/12/03 06:02:01 AM] [INFO] writing /root/tripleo-undercloud-passwords
2018-12-03 06:02:01,948 INFO: [2018/12/03 06:02:01 AM] [INFO] writing /etc/puppet/hiera.yaml
2018-12-03 06:02:01,948 INFO: [2018/12/03 06:02:01 AM] [INFO] writing /var/opt/undercloud-stack/masquerade
2018-12-03 06:02:01,949 INFO: [2018/12/03 06:02:01 AM] [INFO] writing /etc/puppet/hieradata/RedHat.yaml
2018-12-03 06:02:01,949 INFO: [2018/12/03 06:02:01 AM] [INFO] writing /etc/puppet/hieradata/CentOS.yaml
2018-12-03 06:02:01,950 INFO: [2018/12/03 06:02:01 AM] [INFO] writing /var/run/heat-config/heat-config
2018-12-03 06:02:01,950 INFO: [2018/12/03 06:02:01 AM] [INFO] writing /etc/os-collect-config.conf
2018-12-03 06:02:01,952 INFO: [2018/12/03 06:02:01 AM] [INFO] success
2018-12-03 06:02:01,962 INFO: dib-run-parts Mon Dec  3 06:02:01 EST 2018 20-os-apply-config completed
2018-12-03 06:02:01,963 INFO: dib-run-parts Mon Dec  3 06:02:01 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/30-reload-keepalived
2018-12-03 06:02:01,967 INFO: + systemctl is-enabled keepalived
2018-12-03 06:02:01,978 INFO: Failed to get unit file state for keepalived.service: No such file or directory
2018-12-03 06:02:01,982 INFO: dib-run-parts Mon Dec  3 06:02:01 EST 2018 30-reload-keepalived completed
2018-12-03 06:02:01,983 INFO: dib-run-parts Mon Dec  3 06:02:01 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/40-hiera-datafiles
2018-12-03 06:02:02,151 INFO: [2018/12/03 06:02:02 AM] [WARNING] DEPRECATED: falling back to /var/run/os-collect-config/os_config_files.json
2018-12-03 06:02:02,169 INFO: dib-run-parts Mon Dec  3 06:02:02 EST 2018 40-hiera-datafiles completed
2018-12-03 06:02:02,171 INFO: dib-run-parts Mon Dec  3 06:02:02 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/50-puppet-stack-config
2018-12-03 06:02:02,175 INFO: + set -o pipefail
2018-12-03 06:02:02,175 INFO: + puppet_apply puppet apply --summarize --detailed-exitcodes /etc/puppet/manifests/puppet-stack-config.pp
2018-12-03 06:02:02,175 INFO: + set +e
2018-12-03 06:02:02,175 INFO: + puppet apply --summarize --detailed-exitcodes /etc/puppet/manifests/puppet-stack-config.pp
2018-12-03 06:02:09,106 INFO: [mNotice: hiera(): Cannot load backend module_data: cannot load such file -- hiera/backend/module_data_backend[0m
2018-12-03 06:02:09,262 INFO: [1;33mWarning: ModuleLoader: module 'openstacklib' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-03 06:02:09,262 INFO:    (file & line not available)[0m
2018-12-03 06:02:09,673 INFO: [mNotice: hiera(): Cannot load backend module_data: cannot load such file -- hiera/backend/module_data_backend[0m
2018-12-03 06:02:09,790 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-03 06:02:09,790 INFO:                     with Stdlib::Compat::Bool. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 54]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-03 06:02:09,791 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-03 06:02:09,798 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-03 06:02:09,798 INFO:                     with Stdlib::Compat::Absolute_Path. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 55]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-03 06:02:09,799 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-03 06:02:09,907 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-03 06:02:09,907 INFO:                     with Stdlib::Compat::String. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 56]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-03 06:02:09,908 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-03 06:02:09,931 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-03 06:02:09,931 INFO:                     with Stdlib::Compat::Array. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 66]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-03 06:02:09,932 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-03 06:02:09,937 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-03 06:02:09,937 INFO:                     with Pattern[]. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 68]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-03 06:02:09,938 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-03 06:02:09,947 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-03 06:02:09,947 INFO:                     with Stdlib::Compat::Numeric. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 76]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-03 06:02:09,948 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-03 06:02:10,447 INFO: [1;33mWarning: This method is deprecated, please use match expressions with Stdlib::Compat::Ipv6 instead. They are described at https://docs.puppet.com/puppet/latest/reference/lang_data_type.html#match-expressions. at ["/etc/puppet/modules/rabbitmq/manifests/install/rabbitmqadmin.pp", 37]:["/etc/puppet/modules/rabbitmq/manifests/init.pp", 316]
2018-12-03 06:02:10,447 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-03 06:02:10,586 INFO: [1;33mWarning: tag is a metaparam; this value will inherit to all contained resources in the tripleo::firewall::rule definition[0m
2018-12-03 06:02:10,700 INFO: [mNotice: Scope(Class[Tripleo::Firewall::Post]): At this stage, all network traffic is blocked.[0m
2018-12-03 06:02:11,934 INFO: [1;33mWarning: Unknown variable: '::domain'. at /etc/puppet/modules/keepalived/manifests/init.pp:43:40[0m
2018-12-03 06:02:11,938 INFO: [1;33mWarning: Unknown variable: '::domain'. at /etc/puppet/modules/keepalived/manifests/init.pp:44:44[0m
2018-12-03 06:02:12,005 INFO: [1;33mWarning: notify is a metaparam; this value will inherit to all contained resources in the keepalived::instance definition[0m
2018-12-03 06:02:12,053 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-03 06:02:12,053 INFO:                     with Stdlib::Compat::Hash. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/tripleo/manifests/profile/base/database/mysql.pp", 103]:["/etc/puppet/manifests/puppet-stack-config.pp", 97]
2018-12-03 06:02:12,054 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-03 06:02:12,180 INFO: [1;33mWarning: ModuleLoader: module 'mysql' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-03 06:02:12,180 INFO:    (file & line not available)[0m
2018-12-03 06:02:12,536 INFO: [1;33mWarning: ModuleLoader: module 'keystone' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-03 06:02:12,536 INFO:    (file & line not available)[0m
2018-12-03 06:02:13,322 INFO: [1;33mWarning: ModuleLoader: module 'glance' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-03 06:02:13,322 INFO:    (file & line not available)[0m
2018-12-03 06:02:13,627 INFO: [1;33mWarning: ModuleLoader: module 'nova' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-03 06:02:13,627 INFO:    (file & line not available)[0m
2018-12-03 06:02:13,820 INFO: [1;33mWarning: Unknown variable: '::nova::db::mysql_api::setup_cell0'. at /etc/puppet/modules/nova/manifests/db/mysql.pp:53:28[0m
2018-12-03 06:02:13,882 INFO: [1;33mWarning: ModuleLoader: module 'neutron' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-03 06:02:13,882 INFO:    (file & line not available)[0m
2018-12-03 06:02:14,803 INFO: [1;33mWarning: ModuleLoader: module 'heat' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-03 06:02:14,803 INFO:    (file & line not available)[0m
2018-12-03 06:02:14,894 INFO: [1;33mWarning: ModuleLoader: module 'ironic' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-03 06:02:14,894 INFO:    (file & line not available)[0m
2018-12-03 06:02:15,101 INFO: [1;33mWarning: ModuleLoader: module 'swift' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-03 06:02:15,101 INFO:    (file & line not available)[0m
2018-12-03 06:02:15,562 INFO: [1;33mWarning: Scope(Class[Keystone]): keystone::rabbit_host, keystone::rabbit_hosts, keystone::rabbit_password, keystone::rabbit_port, keystone::rabbit_userid and keystone::rabbit_virtual_host are deprecated. Please use keystone::default_transport_url instead.[0m
2018-12-03 06:02:17,649 INFO: [1;33mWarning: Scope(Class[Glance::Notify::Rabbitmq]): glance::notify::rabbitmq::rabbit_host, glance::notify::rabbitmq::rabbit_hosts, glance::notify::rabbitmq::rabbit_password, glance::notify::rabbitmq::rabbit_port, glance::notify::rabbitmq::rabbit_userid and glance::notify::rabbitmq::rabbit_virtual_host are deprecated. Please use glance::notify::rabbitmq::default_transport_url instead.[0m
2018-12-03 06:02:17,799 INFO: [1;33mWarning: Scope(Class[Nova::Db]): placement_database_connection has no effect as of pike, and may be removed in a future release[0m
2018-12-03 06:02:17,799 INFO: [1;33mWarning: Scope(Class[Nova::Db]): placement_slave_connection has no effect as of pike, and may be removed in a future release[0m
2018-12-03 06:02:18,233 INFO: [1;33mWarning: ModuleLoader: module 'cinder' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-03 06:02:18,233 INFO:    (file & line not available)[0m
2018-12-03 06:02:18,686 INFO: [1;33mWarning: Unknown variable: 'until_complete_real'. at /etc/puppet/modules/nova/manifests/cron/archive_deleted_rows.pp:91:90[0m
2018-12-03 06:02:18,768 INFO: [1;33mWarning: This method is deprecated, please use match expressions with Stdlib::Compat::Array instead. They are described at https://docs.puppet.com/puppet/latest/reference/lang_data_type.html#match-expressions. at ["/etc/puppet/modules/nova/manifests/scheduler/filter.pp", 147]:["/etc/puppet/manifests/puppet-stack-config.pp", 400]
2018-12-03 06:02:18,768 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-03 06:02:19,116 INFO: [1;33mWarning: Scope(Class[Neutron]): neutron::rabbit_host, neutron::rabbit_hosts, neutron::rabbit_password, neutron::rabbit_port, neutron::rabbit_user, neutron::rabbit_virtual_host and neutron::rpc_backend are deprecated. Please use neutron::default_transport_url instead.[0m
2018-12-03 06:02:20,803 INFO: [1;33mWarning: Unknown variable: 'methods_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:100:56[0m
2018-12-03 06:02:20,803 INFO: [1;33mWarning: Unknown variable: 'incoming_remove_headers_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:101:56[0m
2018-12-03 06:02:20,807 INFO: [1;33mWarning: Unknown variable: 'incoming_allow_headers_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:102:56[0m
2018-12-03 06:02:20,807 INFO: [1;33mWarning: Unknown variable: 'outgoing_remove_headers_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:103:56[0m
2018-12-03 06:02:20,808 INFO: [1;33mWarning: Unknown variable: 'outgoing_allow_headers_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:104:56[0m
2018-12-03 06:02:21,005 INFO: [1;33mWarning: Scope(Class[Swift::Storage::All]): The default port for the object storage server has changed from 6000 to 6200 and will be changed in a later release[0m
2018-12-03 06:02:21,007 INFO: [1;33mWarning: Scope(Class[Swift::Storage::All]): The default port for the container storage server has changed from 6001 to 6201 and will be changed in a later release[0m
2018-12-03 06:02:21,007 INFO: [1;33mWarning: Scope(Class[Swift::Storage::All]): The default port for the account storage server has changed from 6002 to 6202 and will be changed in a later release[0m
2018-12-03 06:02:21,692 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-03 06:02:21,692 INFO:                     with Stdlib::Compat::Integer. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/heat/manifests/wsgi/apache_api_cfn.pp", 125]:["/etc/puppet/manifests/puppet-stack-config.pp", 528]
2018-12-03 06:02:21,693 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-03 06:02:22,308 INFO: [1;33mWarning: Unknown variable: '::ironic::conductor::swift_account'. at /etc/puppet/modules/ironic/manifests/glance.pp:117:30[0m
2018-12-03 06:02:22,310 INFO: [1;33mWarning: Unknown variable: '::ironic::conductor::swift_temp_url_key'. at /etc/puppet/modules/ironic/manifests/glance.pp:118:35[0m
2018-12-03 06:02:22,310 INFO: [1;33mWarning: Unknown variable: '::ironic::conductor::swift_temp_url_duration'. at /etc/puppet/modules/ironic/manifests/glance.pp:119:40[0m
2018-12-03 06:02:22,346 INFO: [1;33mWarning: Unknown variable: '::ironic::api::neutron_url'. at /etc/puppet/modules/ironic/manifests/neutron.pp:58:29[0m
2018-12-03 06:02:23,808 INFO: [1;33mWarning: ModuleLoader: module 'mistral' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-03 06:02:23,808 INFO:    (file & line not available)[0m
2018-12-03 06:02:23,897 INFO: [1;33mWarning: Unknown variable: '::mistral::database_idle_timeout'. at /etc/puppet/modules/mistral/manifests/db.pp:57:40[0m
2018-12-03 06:02:23,897 INFO: [1;33mWarning: Unknown variable: '::mistral::database_min_pool_size'. at /etc/puppet/modules/mistral/manifests/db.pp:58:40[0m
2018-12-03 06:02:23,901 INFO: [1;33mWarning: Unknown variable: '::mistral::database_max_pool_size'. at /etc/puppet/modules/mistral/manifests/db.pp:59:40[0m
2018-12-03 06:02:23,901 INFO: [1;33mWarning: Unknown variable: '::mistral::database_max_retries'. at /etc/puppet/modules/mistral/manifests/db.pp:60:40[0m
2018-12-03 06:02:23,902 INFO: [1;33mWarning: Unknown variable: '::mistral::database_retry_interval'. at /etc/puppet/modules/mistral/manifests/db.pp:61:40[0m
2018-12-03 06:02:23,902 INFO: [1;33mWarning: Unknown variable: '::mistral::database_max_overflow'. at /etc/puppet/modules/mistral/manifests/db.pp:62:40[0m
2018-12-03 06:02:23,994 INFO: [1;33mWarning: Scope(Class[Mistral]): mistral::rabbit_host, mistral::rabbit_hosts, mistral::rabbit_password, mistral::rabbit_port, mistral::rabbit_userid, mistral::rabbit_virtual_host and mistral::rpc_backend are deprecated. Please use mistral::default_transport_url instead.[0m
2018-12-03 06:02:24,431 INFO: [1;33mWarning: ModuleLoader: module 'zaqar' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-03 06:02:24,431 INFO:    (file & line not available)[0m
2018-12-03 06:02:25,667 INFO: [1;33mWarning: Unknown variable: 'ca_pem'. at /etc/puppet/modules/tripleo/manifests/certmonger/haproxy.pp:95:22[0m
2018-12-03 06:02:26,403 INFO: [1;33mWarning: ModuleLoader: module 'oslo' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-03 06:02:26,403 INFO:    (file & line not available)[0m
2018-12-03 06:02:26,536 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[keystone_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-03 06:02:27,799 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[glance_api_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-03 06:02:27,821 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[glance_registry_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-03 06:02:28,047 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[neutron_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-03 06:02:28,141 INFO: [1;33mWarning: Scope(Neutron::Plugins::Ml2::Type_driver[local]): local type_driver is useful only for single-box, because it provides no connectivity between hosts[0m
2018-12-03 06:02:29,166 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[mistral_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-03 06:02:29,963 INFO: [1;33mWarning: Scope(Haproxy::Config[haproxy]): haproxy: The $merge_options parameter will default to true in the next major release. Please review the documentation regarding the implications.[0m
2018-12-03 06:02:35,873 INFO: [mNotice: Compiled catalog for undercloud in environment production in 27.15 seconds[0m
2018-12-03 06:03:07,735 INFO: [mNotice: /Stage[setup]/Vswitch::Ovs/Package[openvswitch]/ensure: created[0m
2018-12-03 06:03:08,430 INFO: [mNotice: /Stage[setup]/Vswitch::Ovs/Service[openvswitch]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:03:14,283 INFO: [mNotice: /Stage[setup]/Tripleo::Network::Os_net_config/Exec[os-net-config]/returns: executed successfully[0m
2018-12-03 06:03:14,297 INFO: [mNotice: /Stage[setup]/Tripleo::Network::Os_net_config/Exec[trigger-keepalived-restart]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:03:14,582 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Time::Ntp/Service[chronyd]/ensure: ensure changed 'running' to 'stopped'[0m
2018-12-03 06:03:17,186 INFO: [mNotice: /Stage[main]/Ntp::Install/Package[ntp]/ensure: created[0m
2018-12-03 06:03:17,454 INFO: [mNotice: /Stage[main]/Ntp::Config/File[/etc/ntp.conf]/content: content changed '{md5}913c85f0fde85f83c2d6c030ecf259e9' to '{md5}e4539afcd4e03217b6bde799c8c0e337'[0m
2018-12-03 06:03:17,810 INFO: [mNotice: /Stage[main]/Ntp::Service/Service[ntp]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:03:19,980 INFO: [mNotice: /Stage[main]/Firewall::Linux::Redhat/Package[iptables-services]/ensure: created[0m
2018-12-03 06:03:20,058 INFO: [mNotice: /Stage[main]/Firewall::Linux::Redhat/Exec[/usr/bin/systemctl daemon-reload]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:03:20,414 INFO: [mNotice: /Stage[main]/Firewall::Linux::Redhat/Service[iptables]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:03:20,758 INFO: [mNotice: /Stage[main]/Firewall::Linux::Redhat/Service[ip6tables]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:03:20,773 INFO: [mNotice: /Stage[main]/Tripleo::Selinux/File[/etc/selinux/config]/content: content changed '{md5}ff8d2663b88f1a20c8489fdf152e6b31' to '{md5}1b476ce188acf89a99c4da6ae6f0e57f'[0m
2018-12-03 06:03:20,773 INFO: [mNotice: /Stage[main]/Tripleo::Selinux/File[/etc/selinux/config]/mode: mode changed '0644' to '0444'[0m
2018-12-03 06:03:23,664 INFO: [mNotice: /Stage[main]/Keepalived::Install/Package[keepalived]/ensure: created[0m
2018-12-03 06:03:23,680 INFO: [mNotice: /Stage[main]/Mysql::Server::Config/File[mysql-config-file]/ensure: defined content as '{md5}4377846377dc6b9d197ab7d4a6311e1d'[0m
2018-12-03 06:03:34,828 INFO: [mNotice: /Stage[main]/Mysql::Server::Install/Package[mysql-server]/ensure: created[0m
2018-12-03 06:03:44,835 INFO: [mNotice: /Stage[main]/Mysql::Server::Installdb/Mysql_datadir[/var/lib/mysql]/ensure: created[0m
2018-12-03 06:03:44,842 INFO: [mNotice: /Stage[main]/Mysql::Server::Installdb/File[/var/log/mariadb/mariadb.log]/seluser: seluser changed 'unconfined_u' to 'system_u'[0m
2018-12-03 06:03:44,854 INFO: [mNotice: /Stage[main]/Main/File[/etc/systemd/system/mariadb.service.d]/ensure: created[0m
2018-12-03 06:03:44,863 INFO: [mNotice: /Stage[main]/Main/File[/etc/systemd/system/mariadb.service.d/limits.conf]/ensure: defined content as '{md5}8eb9ff6c576b9869944215af3a568c2e'[0m
2018-12-03 06:03:44,967 INFO: [mNotice: /Stage[main]/Main/Exec[systemctl-daemon-reload]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:03:45,801 INFO: [mNotice: /Stage[main]/Mysql::Server::Service/Service[mysqld]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:03:45,823 INFO: [mNotice: /Stage[main]/Mysql::Server::Service/Exec[wait_for_mysql_socket_to_open]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:03:45,961 INFO: [mNotice: /Stage[main]/Mysql::Server::Root_password/Mysql_user[root@localhost]/password_hash: defined 'password_hash' as '*E72379D56202A763B4BCB2759774759B406403DC'[0m
2018-12-03 06:03:45,969 INFO: [mNotice: /Stage[main]/Mysql::Server::Root_password/File[/root/.my.cnf]/ensure: defined content as '{md5}acf31fdceadf10502b92e00a64405f88'[0m
2018-12-03 06:03:45,990 INFO: [mNotice: /Stage[main]/Mysql::Server::Account_security/Mysql_user[root@127.0.0.1]/ensure: removed[0m
2018-12-03 06:03:46,008 INFO: [mNotice: /Stage[main]/Mysql::Server::Account_security/Mysql_user[root@::1]/ensure: removed[0m
2018-12-03 06:03:46,027 INFO: [mNotice: /Stage[main]/Mysql::Server::Account_security/Mysql_user[@localhost]/ensure: removed[0m
2018-12-03 06:03:46,128 INFO: [mNotice: /Stage[main]/Mysql::Server::Account_security/Mysql_database[test]/ensure: removed[0m
2018-12-03 06:03:46,132 INFO: [mNotice: /Stage[main]/Main/File[/var/log/journal]/ensure: created[0m
2018-12-03 06:03:46,833 INFO: [mNotice: /Stage[main]/Main/Service[systemd-journald]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:03:50,845 INFO: [mNotice: /Stage[main]/Swift/Package[swift]/ensure: created[0m
2018-12-03 06:03:52,948 INFO: [mNotice: /Stage[main]/Apache::Mod::Mime/Package[mailcap]/ensure: created[0m
2018-12-03 06:04:02,378 INFO: [mNotice: /Stage[main]/Glance/Package[openstack-glance]/ensure: created[0m
2018-12-03 06:04:02,380 INFO: [mNotice: /Stage[main]/Glance::Deps/Anchor[glance::install::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:04:02,426 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[DEFAULT/bind_host]/ensure: created[0m
2018-12-03 06:04:02,460 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[DEFAULT/bind_port]/ensure: created[0m
2018-12-03 06:04:02,512 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[DEFAULT/workers]/ensure: created[0m
2018-12-03 06:04:02,617 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[DEFAULT/image_cache_dir]/ensure: created[0m
2018-12-03 06:04:02,690 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[DEFAULT/enable_v1_api]/ensure: created[0m
2018-12-03 06:04:02,720 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[DEFAULT/enable_v2_api]/ensure: created[0m
2018-12-03 06:04:03,015 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[glance_store/os_region_name]/ensure: created[0m
2018-12-03 06:04:03,133 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[glance_store/stores]/ensure: created[0m
2018-12-03 06:04:03,173 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_cache_config[glance_store/os_region_name]/ensure: created[0m
2018-12-03 06:04:03,206 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[DEFAULT/registry_host]/ensure: created[0m
2018-12-03 06:04:03,251 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_cache_config[DEFAULT/registry_host]/ensure: created[0m
2018-12-03 06:04:03,289 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[paste_deploy/flavor]/ensure: created[0m
2018-12-03 06:04:03,441 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_api_config[glance_store/swift_store_create_container_on_put]/ensure: created[0m
2018-12-03 06:04:03,705 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_api_config[glance_store/swift_store_endpoint_type]/ensure: created[0m
2018-12-03 06:04:03,737 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_api_config[glance_store/swift_store_config_file]/ensure: created[0m
2018-12-03 06:04:03,774 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_api_config[glance_store/default_swift_reference]/ensure: created[0m
2018-12-03 06:04:03,804 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_api_config[glance_store/default_store]/ensure: created[0m
2018-12-03 06:04:03,805 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_swift_config[ref1/user]/ensure: created[0m
2018-12-03 06:04:03,807 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_swift_config[ref1/key]/ensure: created[0m
2018-12-03 06:04:03,808 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_swift_config[ref1/auth_address]/ensure: created[0m
2018-12-03 06:04:03,810 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_swift_config[ref1/auth_version]/ensure: created[0m
2018-12-03 06:04:03,812 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_swift_config[ref1/user_domain_id]/ensure: created[0m
2018-12-03 06:04:03,813 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_swift_config[ref1/project_domain_id]/ensure: created[0m
2018-12-03 06:04:10,502 INFO: [mNotice: /Stage[main]/Nova/Package[python-nova]/ensure: created[0m
2018-12-03 06:04:12,617 INFO: [mNotice: /Stage[main]/Nova/Package[nova-common]/ensure: created[0m
2018-12-03 06:04:23,062 INFO: [mNotice: /Stage[main]/Neutron/Package[neutron]/ensure: created[0m
2018-12-03 06:04:25,600 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Package[neutron-plugin-ml2]/ensure: created[0m
2018-12-03 06:04:27,619 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2::Networking_baremetal/Package[python2-networking-baremetal]/ensure: created[0m
2018-12-03 06:04:29,665 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Package[python2-ironic-neutron-agent]/ensure: created[0m
2018-12-03 06:04:31,812 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Package[neutron-ovs-agent]/ensure: created[0m
2018-12-03 06:04:31,818 INFO: [mNotice: /Stage[main]/Neutron::Deps/Anchor[neutron::install::end]: Triggered 'refresh' from 5 events[0m
2018-12-03 06:04:31,831 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[DEFAULT/bind_host]/ensure: created[0m
2018-12-03 06:04:31,846 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[DEFAULT/auth_strategy]/ensure: created[0m
2018-12-03 06:04:31,856 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[DEFAULT/core_plugin]/ensure: created[0m
2018-12-03 06:04:31,883 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[DEFAULT/dns_domain]/ensure: created[0m
2018-12-03 06:04:31,895 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[DEFAULT/dhcp_agents_per_network]/ensure: created[0m
2018-12-03 06:04:32,162 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[DEFAULT/global_physnet_mtu]/ensure: created[0m
2018-12-03 06:04:32,176 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[agent/root_helper]/ensure: created[0m
2018-12-03 06:04:32,198 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[DEFAULT/service_plugins]/ensure: created[0m
2018-12-03 06:04:32,232 INFO: [mNotice: /Stage[main]/Neutron::Server/Neutron_config[DEFAULT/l3_ha]/ensure: created[0m
2018-12-03 06:04:32,242 INFO: [mNotice: /Stage[main]/Neutron::Server/Neutron_config[DEFAULT/max_l3_agents_per_router]/ensure: created[0m
2018-12-03 06:04:32,257 INFO: [mNotice: /Stage[main]/Neutron::Server/Neutron_config[DEFAULT/api_workers]/ensure: created[0m
2018-12-03 06:04:32,266 INFO: [mNotice: /Stage[main]/Neutron::Server/Neutron_config[DEFAULT/rpc_workers]/ensure: created[0m
2018-12-03 06:04:32,289 INFO: [mNotice: /Stage[main]/Neutron::Server/Neutron_config[DEFAULT/router_scheduler_driver]/ensure: created[0m
2018-12-03 06:04:32,366 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/auth_url]/ensure: created[0m
2018-12-03 06:04:32,374 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/username]/ensure: created[0m
2018-12-03 06:04:32,382 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/password]/ensure: created[0m
2018-12-03 06:04:32,392 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/project_domain_id]/ensure: created[0m
2018-12-03 06:04:32,400 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/project_domain_name]/ensure: created[0m
2018-12-03 06:04:32,409 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/project_name]/ensure: created[0m
2018-12-03 06:04:32,418 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/user_domain_id]/ensure: created[0m
2018-12-03 06:04:32,427 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/user_domain_name]/ensure: created[0m
2018-12-03 06:04:32,652 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/auth_type]/ensure: created[0m
2018-12-03 06:04:32,661 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/tenant_name]/ensure: created[0m
2018-12-03 06:04:32,671 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[DEFAULT/notify_nova_on_port_status_changes]/ensure: created[0m
2018-12-03 06:04:32,681 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[DEFAULT/notify_nova_on_port_data_changes]/ensure: created[0m
2018-12-03 06:04:32,714 INFO: [mNotice: /Stage[main]/Neutron::Quota/Neutron_config[quotas/quota_port]/ensure: created[0m
2018-12-03 06:04:32,766 INFO: [mNotice: /Stage[main]/Neutron::Quota/Neutron_config[quotas/quota_firewall_rule]/ensure: created[0m
2018-12-03 06:04:32,786 INFO: [mNotice: /Stage[main]/Neutron::Quota/Neutron_config[quotas/quota_network_gateway]/ensure: created[0m
2018-12-03 06:04:32,796 INFO: [mNotice: /Stage[main]/Neutron::Quota/Neutron_config[quotas/quota_packet_filter]/ensure: created[0m
2018-12-03 06:04:32,820 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/File[/etc/neutron/plugin.ini]/ensure: created[0m
2018-12-03 06:04:32,825 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/File[/etc/default/neutron-server]/ensure: created[0m
2018-12-03 06:04:32,829 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron_plugin_ml2[ml2/type_drivers]/ensure: created[0m
2018-12-03 06:04:32,831 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron_plugin_ml2[ml2/tenant_network_types]/ensure: created[0m
2018-12-03 06:04:32,836 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron_plugin_ml2[ml2/mechanism_drivers]/ensure: created[0m
2018-12-03 06:04:32,840 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron_plugin_ml2[ml2/path_mtu]/ensure: created[0m
2018-12-03 06:04:32,844 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron_plugin_ml2[ml2/extension_drivers]/ensure: created[0m
2018-12-03 06:04:32,856 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/auth_type]/ensure: created[0m
2018-12-03 06:04:32,857 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/auth_url]/ensure: created[0m
2018-12-03 06:04:32,860 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/username]/ensure: created[0m
2018-12-03 06:04:32,862 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/password]/ensure: created[0m
2018-12-03 06:04:32,864 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/project_domain_id]/ensure: created[0m
2018-12-03 06:04:32,865 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/project_domain_name]/ensure: created[0m
2018-12-03 06:04:32,868 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/project_name]/ensure: created[0m
2018-12-03 06:04:32,870 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/user_domain_id]/ensure: created[0m
2018-12-03 06:04:32,872 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/user_domain_name]/ensure: created[0m
2018-12-03 06:04:32,872 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/region_name]/ensure: created[0m
2018-12-03 06:04:32,878 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Neutron_dhcp_agent_config[DEFAULT/enable_isolated_metadata]/ensure: created[0m
2018-12-03 06:04:32,885 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Neutron_dhcp_agent_config[DEFAULT/enable_metadata_network]/ensure: created[0m
2018-12-03 06:04:32,889 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Neutron_dhcp_agent_config[DEFAULT/state_path]/ensure: created[0m
2018-12-03 06:04:32,895 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Neutron_dhcp_agent_config[DEFAULT/resync_interval]/ensure: created[0m
2018-12-03 06:04:32,898 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Neutron_dhcp_agent_config[DEFAULT/interface_driver]/ensure: created[0m
2018-12-03 06:04:32,905 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Neutron_dhcp_agent_config[DEFAULT/root_helper]/ensure: created[0m
2018-12-03 06:04:32,913 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Neutron_dhcp_agent_config[DEFAULT/dnsmasq_config_file]/ensure: created[0m
2018-12-03 06:04:33,140 INFO: [mNotice: /Stage[main]/Neutron::Agents::L3/Neutron_l3_agent_config[DEFAULT/interface_driver]/ensure: created[0m
2018-12-03 06:04:33,157 INFO: [mNotice: /Stage[main]/Neutron::Agents::L3/Neutron_l3_agent_config[DEFAULT/agent_mode]/ensure: created[0m
2018-12-03 06:04:33,165 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Neutron_agent_ovs[ovs/bridge_mappings]/ensure: created[0m
2018-12-03 06:04:33,177 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Neutron_agent_ovs[agent/drop_flows_on_start]/ensure: created[0m
2018-12-03 06:04:33,185 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Neutron_agent_ovs[ovs/integration_bridge]/ensure: created[0m
2018-12-03 06:04:33,200 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Neutron_agent_ovs[securitygroup/firewall_driver]/ensure: created[0m
2018-12-03 06:04:33,401 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Service[ovs-cleanup-service]/enable: enable changed 'false' to 'true'[0m
2018-12-03 06:04:33,410 INFO: [mNotice: /Stage[main]/Main/Neutron_config[DEFAULT/notification_driver]/ensure: created[0m
2018-12-03 06:04:35,630 INFO: [mNotice: /Stage[main]/Memcached/Package[memcached]/ensure: created[0m
2018-12-03 06:04:35,639 INFO: [mNotice: /Stage[main]/Memcached/File[/etc/sysconfig/memcached]/content: content changed '{md5}a50ed62e82d31fb4cb2de2226650c545' to '{md5}d8425d450cd9f050a2601befed0ce63a'[0m
2018-12-03 06:04:35,998 INFO: [mNotice: /Stage[main]/Memcached/Service[memcached]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:04:38,383 INFO: [mNotice: /Stage[main]/Swift::Proxy/Package[swift-proxy]/ensure: created[0m
2018-12-03 06:04:38,392 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[pipeline:main/pipeline]/value: value changed 'catch_errors cache proxy-server' to 'catch_errors proxy-server'[0m
2018-12-03 06:04:38,395 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/auto_create_account_prefix]/ensure: created[0m
2018-12-03 06:04:38,397 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/concurrency]/ensure: created[0m
2018-12-03 06:04:38,398 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/expiring_objects_account_name]/ensure: created[0m
2018-12-03 06:04:38,400 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/interval]/ensure: created[0m
2018-12-03 06:04:38,402 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/process]/ensure: created[0m
2018-12-03 06:04:38,404 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/processes]/ensure: created[0m
2018-12-03 06:04:38,407 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/reclaim_age]/ensure: created[0m
2018-12-03 06:04:38,409 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/recon_cache_path]/ensure: created[0m
2018-12-03 06:04:38,410 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/report_interval]/ensure: created[0m
2018-12-03 06:04:38,413 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/log_facility]/ensure: created[0m
2018-12-03 06:04:38,416 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/log_level]/ensure: created[0m
2018-12-03 06:04:40,650 INFO: [mNotice: /Stage[main]/Xinetd/Package[xinetd]/ensure: created[0m
2018-12-03 06:04:40,662 INFO: [mNotice: /Stage[main]/Xinetd/File[/etc/xinetd.conf]/content: content changed '{md5}9ff8cc688dd9f0dfc45e5afd25c427a7' to '{md5}7d37008224e71625019cb48768f267e7'[0m
2018-12-03 06:04:40,662 INFO: [mNotice: /Stage[main]/Xinetd/File[/etc/xinetd.conf]/mode: mode changed '0600' to '0644'[0m
2018-12-03 06:04:40,747 INFO: [mNotice: /Stage[main]/Main/Exec[restart rsyslog]/returns: executed successfully[0m
2018-12-03 06:04:46,770 INFO: [mNotice: /Stage[main]/Heat/Package[heat-common]/ensure: created[0m
2018-12-03 06:04:48,848 INFO: [mNotice: /Stage[main]/Heat::Api/Package[heat-api]/ensure: created[0m
2018-12-03 06:04:51,205 INFO: [mNotice: /Stage[main]/Heat::Api_cfn/Package[heat-api-cfn]/ensure: created[0m
2018-12-03 06:04:53,635 INFO: [mNotice: /Stage[main]/Heat::Engine/Package[heat-engine]/ensure: created[0m
2018-12-03 06:04:53,637 INFO: [mNotice: /Stage[main]/Heat::Deps/Anchor[heat::install::end]: Triggered 'refresh' from 4 events[0m
2018-12-03 06:04:53,660 INFO: [mNotice: /Stage[main]/Heat/Heat_config[trustee/auth_type]/ensure: created[0m
2018-12-03 06:04:53,677 INFO: [mNotice: /Stage[main]/Heat/Heat_config[trustee/auth_url]/ensure: created[0m
2018-12-03 06:04:53,697 INFO: [mNotice: /Stage[main]/Heat/Heat_config[trustee/username]/ensure: created[0m
2018-12-03 06:04:53,714 INFO: [mNotice: /Stage[main]/Heat/Heat_config[trustee/password]/ensure: created[0m
2018-12-03 06:04:53,731 INFO: [mNotice: /Stage[main]/Heat/Heat_config[trustee/project_domain_name]/ensure: created[0m
2018-12-03 06:04:53,752 INFO: [mNotice: /Stage[main]/Heat/Heat_config[trustee/user_domain_name]/ensure: created[0m
2018-12-03 06:04:53,770 INFO: [mNotice: /Stage[main]/Heat/Heat_config[clients_keystone/auth_uri]/ensure: created[0m
2018-12-03 06:04:54,030 INFO: [mNotice: /Stage[main]/Heat/Heat_config[clients/endpoint_type]/ensure: created[0m
2018-12-03 06:04:54,065 INFO: [mNotice: /Stage[main]/Heat/Heat_config[DEFAULT/max_json_body_size]/ensure: created[0m
2018-12-03 06:04:54,108 INFO: [mNotice: /Stage[main]/Heat/Heat_config[ec2authtoken/auth_uri]/ensure: created[0m
2018-12-03 06:04:54,133 INFO: [mNotice: /Stage[main]/Heat/Heat_config[yaql/limit_iterators]/ensure: created[0m
2018-12-03 06:04:54,150 INFO: [mNotice: /Stage[main]/Heat/Heat_config[yaql/memory_quota]/ensure: created[0m
2018-12-03 06:04:54,171 INFO: [mNotice: /Stage[main]/Heat::Api/Heat_config[heat_api/bind_host]/ensure: created[0m
2018-12-03 06:04:54,200 INFO: [mNotice: /Stage[main]/Heat::Api/Heat_config[heat_api/workers]/ensure: created[0m
2018-12-03 06:04:54,219 INFO: [mNotice: /Stage[main]/Heat::Api_cfn/Heat_config[heat_api_cfn/bind_host]/ensure: created[0m
2018-12-03 06:04:54,250 INFO: [mNotice: /Stage[main]/Heat::Api_cfn/Heat_config[heat_api_cfn/workers]/ensure: created[0m
2018-12-03 06:04:54,274 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/auth_encryption_key]/ensure: created[0m
2018-12-03 06:04:54,296 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/heat_stack_user_role]/ensure: created[0m
2018-12-03 06:04:54,315 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/heat_metadata_server_url]/ensure: created[0m
2018-12-03 06:04:54,332 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/heat_waitcondition_server_url]/ensure: created[0m
2018-12-03 06:04:54,350 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/heat_watch_server_url]/ensure: created[0m
2018-12-03 06:04:54,421 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/max_resources_per_stack]/ensure: created[0m
2018-12-03 06:04:54,457 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/num_engine_workers]/ensure: created[0m
2018-12-03 06:04:54,475 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/convergence_engine]/ensure: created[0m
2018-12-03 06:04:54,492 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/reauthentication_auth_method]/ensure: created[0m
2018-12-03 06:04:54,765 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/max_nested_stack_depth]/ensure: created[0m
2018-12-03 06:04:54,801 INFO: [mNotice: /Stage[main]/Heat::Keystone::Domain/Heat_config[DEFAULT/stack_domain_admin]/ensure: created[0m
2018-12-03 06:04:54,824 INFO: [mNotice: /Stage[main]/Heat::Keystone::Domain/Heat_config[DEFAULT/stack_domain_admin_password]/ensure: created[0m
2018-12-03 06:04:54,842 INFO: [mNotice: /Stage[main]/Heat::Keystone::Domain/Heat_config[DEFAULT/stack_user_domain_name]/ensure: created[0m
2018-12-03 06:04:54,892 INFO: [mNotice: /Stage[main]/Heat::Cron::Purge_deleted/Cron[heat-manage purge_deleted]/ensure: created[0m
2018-12-03 06:05:02,278 INFO: [mNotice: /Stage[main]/Ironic/Package[ironic-common]/ensure: created[0m
2018-12-03 06:05:02,290 INFO: [mNotice: /Stage[main]/Main/File[dnsmasq-ironic.conf]/ensure: defined content as '{md5}1c23f6b2b9a0910c3e32f02970493f00'[0m
2018-12-03 06:05:04,414 INFO: [mNotice: /Stage[main]/Ironic::Api/Package[ironic-api]/ensure: created[0m
2018-12-03 06:05:06,486 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Package[ironic-conductor]/ensure: created[0m
2018-12-03 06:05:08,559 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Ansible/Package[systemd-python]/ensure: created[0m
2018-12-03 06:05:10,947 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Staging/Package[ironic-staging-drivers]/ensure: created[0m
2018-12-03 06:05:14,120 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Package[ironic-inspector]/ensure: created[0m
2018-12-03 06:05:16,354 INFO: [mNotice: /Stage[main]/Ironic::Pxe/Package[tftp-server]/ensure: created[0m
2018-12-03 06:05:18,562 INFO: [mNotice: /Stage[main]/Ironic::Pxe/Package[ipxe]/ensure: created[0m
2018-12-03 06:05:18,570 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::install::end]: Triggered 'refresh' from 4 events[0m
2018-12-03 06:05:18,574 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic-inspector::install::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:05:18,603 INFO: [mNotice: /Stage[main]/Ironic::Glance/Ironic_config[glance/auth_type]/ensure: created[0m
2018-12-03 06:05:18,631 INFO: [mNotice: /Stage[main]/Ironic::Glance/Ironic_config[glance/username]/ensure: created[0m
2018-12-03 06:05:18,659 INFO: [mNotice: /Stage[main]/Ironic::Glance/Ironic_config[glance/password]/ensure: created[0m
2018-12-03 06:05:18,686 INFO: [mNotice: /Stage[main]/Ironic::Glance/Ironic_config[glance/auth_url]/ensure: created[0m
2018-12-03 06:05:18,720 INFO: [mNotice: /Stage[main]/Ironic::Glance/Ironic_config[glance/project_name]/ensure: created[0m
2018-12-03 06:05:18,748 INFO: [mNotice: /Stage[main]/Ironic::Glance/Ironic_config[glance/user_domain_name]/ensure: created[0m
2018-12-03 06:05:18,786 INFO: [mNotice: /Stage[main]/Ironic::Glance/Ironic_config[glance/project_domain_name]/ensure: created[0m
2018-12-03 06:05:19,175 INFO: [mNotice: /Stage[main]/Ironic::Neutron/Ironic_config[neutron/auth_type]/ensure: created[0m
2018-12-03 06:05:19,205 INFO: [mNotice: /Stage[main]/Ironic::Neutron/Ironic_config[neutron/username]/ensure: created[0m
2018-12-03 06:05:19,232 INFO: [mNotice: /Stage[main]/Ironic::Neutron/Ironic_config[neutron/password]/ensure: created[0m
2018-12-03 06:05:19,259 INFO: [mNotice: /Stage[main]/Ironic::Neutron/Ironic_config[neutron/auth_url]/ensure: created[0m
2018-12-03 06:05:19,287 INFO: [mNotice: /Stage[main]/Ironic::Neutron/Ironic_config[neutron/project_name]/ensure: created[0m
2018-12-03 06:05:19,317 INFO: [mNotice: /Stage[main]/Ironic::Neutron/Ironic_config[neutron/user_domain_name]/ensure: created[0m
2018-12-03 06:05:19,346 INFO: [mNotice: /Stage[main]/Ironic::Neutron/Ironic_config[neutron/project_domain_name]/ensure: created[0m
2018-12-03 06:05:19,374 INFO: [mNotice: /Stage[main]/Ironic/Ironic_config[DEFAULT/auth_strategy]/ensure: created[0m
2018-12-03 06:05:19,402 INFO: [mNotice: /Stage[main]/Ironic/Ironic_config[DEFAULT/my_ip]/ensure: created[0m
2018-12-03 06:05:19,432 INFO: [mNotice: /Stage[main]/Ironic/Ironic_config[DEFAULT/default_resource_class]/ensure: created[0m
2018-12-03 06:05:19,440 INFO: [mNotice: /Stage[main]/Ironic::Db::Sync/File[/var/log/ironic/ironic-dbsync.log]/ensure: created[0m
2018-12-03 06:05:19,472 INFO: [mNotice: /Stage[main]/Ironic::Api/Ironic_config[api/host_ip]/ensure: created[0m
2018-12-03 06:05:19,500 INFO: [mNotice: /Stage[main]/Ironic::Api/Ironic_config[api/port]/ensure: created[0m
2018-12-03 06:05:19,528 INFO: [mNotice: /Stage[main]/Ironic::Api/Ironic_config[api/max_limit]/ensure: created[0m
2018-12-03 06:05:19,559 INFO: [mNotice: /Stage[main]/Ironic::Api/Ironic_config[api/api_workers]/ensure: created[0m
2018-12-03 06:05:19,864 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Agent/Ironic_config[agent/deploy_logs_collect]/ensure: created[0m
2018-12-03 06:05:19,892 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Agent/Ironic_config[agent/deploy_logs_storage_backend]/ensure: created[0m
2018-12-03 06:05:19,928 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Agent/Ironic_config[agent/deploy_logs_local_path]/ensure: created[0m
2018-12-03 06:05:19,988 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[DEFAULT/enabled_drivers]/ensure: created[0m
2018-12-03 06:05:20,023 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[DEFAULT/enabled_hardware_types]/ensure: created[0m
2018-12-03 06:05:20,052 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[conductor/max_time_interval]/ensure: created[0m
2018-12-03 06:05:20,088 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[conductor/force_power_state_during_sync]/ensure: created[0m
2018-12-03 06:05:20,120 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[conductor/automated_clean]/ensure: created[0m
2018-12-03 06:05:20,155 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[conductor/api_url]/ensure: created[0m
2018-12-03 06:05:20,183 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[deploy/http_url]/ensure: created[0m
2018-12-03 06:05:20,225 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[deploy/erase_devices_priority]/ensure: created[0m
2018-12-03 06:05:20,255 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[deploy/erase_devices_metadata_priority]/ensure: created[0m
2018-12-03 06:05:20,339 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[deploy/default_boot_option]/ensure: created[0m
2018-12-03 06:05:20,648 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[neutron/cleaning_network]/ensure: created[0m
2018-12-03 06:05:20,675 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[neutron/provisioning_network]/ensure: created[0m
2018-12-03 06:05:20,905 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Ilo/Ironic_config[ilo/default_boot_mode]/ensure: created[0m
2018-12-03 06:05:20,933 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/enabled]/ensure: created[0m
2018-12-03 06:05:20,977 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/auth_type]/ensure: created[0m
2018-12-03 06:05:21,005 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/username]/ensure: created[0m
2018-12-03 06:05:21,043 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/password]/ensure: created[0m
2018-12-03 06:05:21,072 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/auth_url]/ensure: created[0m
2018-12-03 06:05:21,100 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/project_name]/ensure: created[0m
2018-12-03 06:05:21,129 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/user_domain_name]/ensure: created[0m
2018-12-03 06:05:21,382 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/project_domain_name]/ensure: created[0m
2018-12-03 06:05:21,439 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/ipxe_enabled]/ensure: created[0m
2018-12-03 06:05:21,489 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/pxe_bootfile_name]/ensure: created[0m
2018-12-03 06:05:21,521 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/pxe_config_template]/ensure: created[0m
2018-12-03 06:05:21,564 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/tftp_root]/ensure: created[0m
2018-12-03 06:05:21,615 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/tftp_master_path]/ensure: created[0m
2018-12-03 06:05:21,656 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/uefi_pxe_bootfile_name]/ensure: created[0m
2018-12-03 06:05:21,684 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/uefi_pxe_config_template]/ensure: created[0m
2018-12-03 06:05:21,721 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/ipxe_timeout]/ensure: created[0m
2018-12-03 06:05:21,754 INFO: [mNotice: /Stage[main]/Ironic::Inspector/File[/etc/ironic-inspector/inspector.conf]/owner: owner changed 'root' to 'ironic-inspector'[0m
2018-12-03 06:05:21,766 INFO: [mNotice: /Stage[main]/Ironic::Inspector/File[/etc/ironic-inspector/dnsmasq.conf]/content: content changed '{md5}9eabe6f969928fde6524d0dd00781479' to '{md5}b84675588f83c34e4b0c71e04f3c7bbc'[0m
2018-12-03 06:05:21,776 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[DEFAULT/listen_address]/ensure: created[0m
2018-12-03 06:05:21,785 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[DEFAULT/auth_strategy]/ensure: created[0m
2018-12-03 06:05:21,798 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[capabilities/boot_mode]/ensure: created[0m
2018-12-03 06:05:21,808 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[iptables/dnsmasq_interface]/ensure: created[0m
2018-12-03 06:05:21,820 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[processing/ramdisk_logs_dir]/ensure: created[0m
2018-12-03 06:05:21,838 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[processing/keep_ports]/ensure: created[0m
2018-12-03 06:05:21,848 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[processing/store_data]/ensure: created[0m
2018-12-03 06:05:21,859 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/auth_type]/ensure: created[0m
2018-12-03 06:05:21,868 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/username]/ensure: created[0m
2018-12-03 06:05:21,877 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/password]/ensure: created[0m
2018-12-03 06:05:21,890 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/project_name]/ensure: created[0m
2018-12-03 06:05:21,901 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/project_domain_name]/ensure: created[0m
2018-12-03 06:05:21,911 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/user_domain_name]/ensure: created[0m
2018-12-03 06:05:22,149 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/auth_url]/ensure: created[0m
2018-12-03 06:05:22,159 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/max_retries]/ensure: created[0m
2018-12-03 06:05:22,168 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/retry_interval]/ensure: created[0m
2018-12-03 06:05:22,180 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[swift/auth_type]/ensure: created[0m
2018-12-03 06:05:22,188 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[swift/username]/ensure: created[0m
2018-12-03 06:05:22,197 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[swift/password]/ensure: created[0m
2018-12-03 06:05:22,206 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[swift/project_name]/ensure: created[0m
2018-12-03 06:05:22,215 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[swift/project_domain_name]/ensure: created[0m
2018-12-03 06:05:22,224 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[swift/user_domain_name]/ensure: created[0m
2018-12-03 06:05:22,235 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[swift/auth_url]/ensure: created[0m
2018-12-03 06:05:22,245 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[processing/processing_hooks]/ensure: created[0m
2018-12-03 06:05:22,260 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[discovery/enroll_node_driver]/ensure: created[0m
2018-12-03 06:05:22,269 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Pxe_filter/Ironic_inspector_config[pxe_filter/driver]/ensure: created[0m
2018-12-03 06:05:22,281 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Pxe_filter::Dnsmasq/Ironic_inspector_config[dnsmasq_pxe_filter/dhcp_hostsdir]/ensure: created[0m
2018-12-03 06:05:22,291 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Pxe_filter::Dnsmasq/Ironic_inspector_config[dnsmasq_pxe_filter/dnsmasq_start_command]/ensure: created[0m
2018-12-03 06:05:22,301 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Pxe_filter::Dnsmasq/Ironic_inspector_config[dnsmasq_pxe_filter/dnsmasq_stop_command]/ensure: created[0m
2018-12-03 06:05:22,306 INFO: [mNotice: /Stage[main]/Ironic::Pxe/File[/tftpboot]/ensure: created[0m
2018-12-03 06:05:22,309 INFO: [mNotice: /Stage[main]/Ironic::Pxe/File[/tftpboot/pxelinux.cfg]/ensure: created[0m
2018-12-03 06:05:22,313 INFO: [mNotice: /Stage[main]/Ironic::Pxe/File[/httpboot]/ensure: created[0m
2018-12-03 06:05:22,319 INFO: [mNotice: /Stage[main]/Ironic::Inspector/File[/httpboot/inspector.ipxe]/ensure: defined content as '{md5}5c182f06e5a69809ad971c47da276122'[0m
2018-12-03 06:05:22,324 INFO: [mNotice: /Stage[main]/Ironic::Pxe/File[/tftpboot/map-file]/ensure: defined content as '{md5}1c4343c656b7f7b9de48495fdc2b6c5e'[0m
2018-12-03 06:05:22,380 INFO: [mNotice: /Stage[main]/Ironic::Pxe/File[/tftpboot/undionly.kpxe]/ensure: defined content as '{md5}60d84c8e9035fac59c73ed4cee8dc82c'[0m
2018-12-03 06:05:22,625 INFO: [mNotice: /Stage[main]/Ironic::Pxe/File[/tftpboot/ipxe.efi]/ensure: defined content as '{md5}8f49ea062dadf0290b5f8b7e5f42a9b9'[0m
2018-12-03 06:05:22,656 INFO: [mNotice: /Stage[main]/Ironic::Service_catalog/Ironic_config[service_catalog/auth_type]/ensure: created[0m
2018-12-03 06:05:22,684 INFO: [mNotice: /Stage[main]/Ironic::Service_catalog/Ironic_config[service_catalog/username]/ensure: created[0m
2018-12-03 06:05:22,712 INFO: [mNotice: /Stage[main]/Ironic::Service_catalog/Ironic_config[service_catalog/password]/ensure: created[0m
2018-12-03 06:05:22,744 INFO: [mNotice: /Stage[main]/Ironic::Service_catalog/Ironic_config[service_catalog/auth_url]/ensure: created[0m
2018-12-03 06:05:22,772 INFO: [mNotice: /Stage[main]/Ironic::Service_catalog/Ironic_config[service_catalog/project_name]/ensure: created[0m
2018-12-03 06:05:22,800 INFO: [mNotice: /Stage[main]/Ironic::Service_catalog/Ironic_config[service_catalog/user_domain_name]/ensure: created[0m
2018-12-03 06:05:22,832 INFO: [mNotice: /Stage[main]/Ironic::Service_catalog/Ironic_config[service_catalog/project_domain_name]/ensure: created[0m
2018-12-03 06:05:23,086 INFO: [mNotice: /Stage[main]/Ironic::Swift/Ironic_config[swift/auth_type]/ensure: created[0m
2018-12-03 06:05:23,114 INFO: [mNotice: /Stage[main]/Ironic::Swift/Ironic_config[swift/username]/ensure: created[0m
2018-12-03 06:05:23,152 INFO: [mNotice: /Stage[main]/Ironic::Swift/Ironic_config[swift/password]/ensure: created[0m
2018-12-03 06:05:23,187 INFO: [mNotice: /Stage[main]/Ironic::Swift/Ironic_config[swift/auth_url]/ensure: created[0m
2018-12-03 06:05:23,216 INFO: [mNotice: /Stage[main]/Ironic::Swift/Ironic_config[swift/project_name]/ensure: created[0m
2018-12-03 06:05:23,254 INFO: [mNotice: /Stage[main]/Ironic::Swift/Ironic_config[swift/user_domain_name]/ensure: created[0m
2018-12-03 06:05:23,282 INFO: [mNotice: /Stage[main]/Ironic::Swift/Ironic_config[swift/project_domain_name]/ensure: created[0m
2018-12-03 06:05:28,346 INFO: [mNotice: /Stage[main]/Main/Package[openstack-tempest]/ensure: created[0m
2018-12-03 06:05:30,591 INFO: [mNotice: /Stage[main]/Main/Package[subunit-filters]/ensure: created[0m
2018-12-03 06:05:30,616 INFO: [mNotice: /Stage[main]/Main/Group[docker]/ensure: created[0m
2018-12-03 06:05:30,646 INFO: [mNotice: /Stage[main]/Main/User[docker_user]/groups: groups changed '' to ['docker'][0m
2018-12-03 06:05:33,871 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker_registry/Package[docker-distribution]/ensure: created[0m
2018-12-03 06:05:33,881 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker_registry/File[/etc/docker-distribution/registry/config.yml]/content: content changed '{md5}fcc7b86bd3a8b9b41577e3af434de461' to '{md5}7bec1d7568b3fda481c67c509545f1f4'[0m
2018-12-03 06:05:34,261 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker_registry/Service[docker-distribution]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:05:37,480 INFO: [mNotice: /Stage[main]/Mistral/Package[mistral-common]/ensure: created[0m
2018-12-03 06:05:39,535 INFO: [mNotice: /Stage[main]/Mistral::Api/Package[mistral-api]/ensure: created[0m
2018-12-03 06:05:41,574 INFO: [mNotice: /Stage[main]/Mistral::Engine/Package[mistral-engine]/ensure: created[0m
2018-12-03 06:05:43,605 INFO: [mNotice: /Stage[main]/Mistral::Executor/Package[mistral-executor]/ensure: created[0m
2018-12-03 06:05:43,612 INFO: [mNotice: /Stage[main]/Mistral::Deps/Anchor[mistral::install::end]: Triggered 'refresh' from 4 events[0m
2018-12-03 06:05:43,616 INFO: [mNotice: /Stage[main]/Mistral::Api/Mistral_config[api/api_workers]/ensure: created[0m
2018-12-03 06:05:43,619 INFO: [mNotice: /Stage[main]/Mistral::Api/Mistral_config[api/host]/ensure: created[0m
2018-12-03 06:05:43,622 INFO: [mNotice: /Stage[main]/Mistral::Engine/Mistral_config[engine/execution_field_size_limit_kb]/ensure: created[0m
2018-12-03 06:05:43,624 INFO: [mNotice: /Stage[main]/Mistral::Engine/Mistral_config[execution_expiration_policy/evaluation_interval]/ensure: created[0m
2018-12-03 06:05:43,627 INFO: [mNotice: /Stage[main]/Mistral::Engine/Mistral_config[execution_expiration_policy/older_than]/ensure: created[0m
2018-12-03 06:05:43,632 INFO: [mNotice: /Stage[main]/Mistral::Cron_trigger/Mistral_config[cron_trigger/execution_interval]/ensure: created[0m
2018-12-03 06:05:47,528 INFO: [mNotice: /Stage[main]/Tripleo::Ui/Package[openstack-tripleo-ui]/ensure: created[0m
2018-12-03 06:05:49,725 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Validations/Package[openstack-tripleo-validations]/ensure: created[0m
2018-12-03 06:05:49,762 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Validations/User[validations]/ensure: created[0m
2018-12-03 06:05:57,756 INFO: [mNotice: /Stage[main]/Zaqar/Package[zaqar-common]/ensure: created[0m
2018-12-03 06:05:57,757 INFO: [mNotice: /Stage[main]/Zaqar::Deps/Anchor[zaqar::install::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:05:57,778 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Trust/Zaqar_config[trustee/username]/ensure: created[0m
2018-12-03 06:05:57,801 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Trust/Zaqar_config[trustee/user_domain_name]/ensure: created[0m
2018-12-03 06:05:57,816 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Trust/Zaqar_config[trustee/auth_url]/ensure: created[0m
2018-12-03 06:05:57,843 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Trust/Zaqar_config[trustee/auth_type]/ensure: created[0m
2018-12-03 06:05:58,120 INFO: [mNotice: /Stage[main]/Zaqar/Zaqar_config[DEFAULT/auth_strategy]/ensure: created[0m
2018-12-03 06:05:58,144 INFO: [mNotice: /Stage[main]/Zaqar/Zaqar_config[DEFAULT/unreliable]/ensure: created[0m
2018-12-03 06:05:58,173 INFO: [mNotice: /Stage[main]/Zaqar/Zaqar_config[storage/message_pipeline]/ensure: created[0m
2018-12-03 06:05:58,209 INFO: [mNotice: /Stage[main]/Zaqar/Zaqar_config[transport/max_messages_post_size]/ensure: created[0m
2018-12-03 06:05:58,225 INFO: [mNotice: /Stage[main]/Zaqar/Zaqar_config[drivers/message_store]/ensure: created[0m
2018-12-03 06:05:58,240 INFO: [mNotice: /Stage[main]/Zaqar/Zaqar_config[drivers/management_store]/ensure: created[0m
2018-12-03 06:05:58,256 INFO: [mNotice: /Stage[main]/Zaqar::Management::Sqlalchemy/Zaqar_config[drivers:management_store:sqlalchemy/uri]/ensure: created[0m
2018-12-03 06:05:58,271 INFO: [mNotice: /Stage[main]/Zaqar::Messaging::Swift/Zaqar_config[drivers:message_store:swift/uri]/ensure: created[0m
2018-12-03 06:05:58,287 INFO: [mNotice: /Stage[main]/Zaqar::Messaging::Swift/Zaqar_config[drivers:message_store:swift/auth_url]/ensure: created[0m
2018-12-03 06:05:58,304 INFO: [mNotice: /Stage[main]/Zaqar::Transport::Websocket/Zaqar_config[drivers:transport:websocket/bind]/ensure: created[0m
2018-12-03 06:05:58,334 INFO: [mNotice: /Stage[main]/Zaqar::Transport::Websocket/Zaqar_config[drivers:transport:websocket/notification_bind]/ensure: created[0m
2018-12-03 06:05:58,493 INFO: [mNotice: /Stage[main]/Main/Sysctl::Value[net.ipv4.ip_forward]/Sysctl[net.ipv4.ip_forward]/ensure: created[0m
2018-12-03 06:05:58,650 INFO: [mNotice: /Stage[main]/Main/Sysctl::Value[net.ipv4.ip_forward]/Sysctl_runtime[net.ipv4.ip_forward]/val: val changed '0' to '1'[0m
2018-12-03 06:06:07,934 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Package[docker]/ensure: created[0m
2018-12-03 06:06:07,939 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/File[/etc/systemd/system/docker.service.d]/ensure: created[0m
2018-12-03 06:06:07,947 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/File[/etc/systemd/system/docker.service.d/99-unset-mountflags.conf]/ensure: defined content as '{md5}b984426de0b5978853686a649b64e4b8'[0m
2018-12-03 06:06:08,035 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Exec[systemd daemon-reload]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:06:08,171 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Augeas[docker-sysconfig-options]/returns: executed successfully[0m
2018-12-03 06:06:08,273 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Augeas[docker-sysconfig-registry]/returns: executed successfully[0m
2018-12-03 06:06:08,295 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Augeas[docker-daemon.json-debug]/returns: executed successfully[0m
2018-12-03 06:06:08,386 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Augeas[docker-sysconfig-storage]/returns: executed successfully[0m
2018-12-03 06:06:08,472 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Augeas[docker-sysconfig-network]/returns: executed successfully[0m
2018-12-03 06:06:11,088 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Service[docker]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:06:11,564 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[000 accept related established rules]/Firewall[000 accept related established rules ipv4]/ensure: created[0m
2018-12-03 06:06:12,010 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[000 accept related established rules]/Firewall[000 accept related established rules ipv6]/ensure: created[0m
2018-12-03 06:06:12,227 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[001 accept all icmp]/Firewall[001 accept all icmp ipv4]/ensure: created[0m
2018-12-03 06:06:12,376 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[001 accept all icmp]/Firewall[001 accept all icmp ipv6]/ensure: created[0m
2018-12-03 06:06:12,853 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[002 accept all to lo interface]/Firewall[002 accept all to lo interface ipv4]/ensure: created[0m
2018-12-03 06:06:13,007 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[002 accept all to lo interface]/Firewall[002 accept all to lo interface ipv6]/ensure: created[0m
2018-12-03 06:06:13,497 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[003 accept ssh]/Firewall[003 accept ssh ipv4]/ensure: created[0m
2018-12-03 06:06:13,661 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[003 accept ssh]/Firewall[003 accept ssh ipv6]/ensure: created[0m
2018-12-03 06:06:14,085 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[004 accept ipv6 dhcpv6]/Firewall[004 accept ipv6 dhcpv6 ipv6]/ensure: created[0m
2018-12-03 06:06:14,319 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[105 ntp]/Firewall[105 ntp ipv4]/ensure: created[0m
2018-12-03 06:06:14,758 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[105 ntp]/Firewall[105 ntp ipv6]/ensure: created[0m
2018-12-03 06:06:14,996 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[106 vrrp]/Firewall[106 vrrp ipv4]/ensure: created[0m
2018-12-03 06:06:15,435 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[106 vrrp]/Firewall[106 vrrp ipv6]/ensure: created[0m
2018-12-03 06:06:15,931 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[107 haproxy stats]/Firewall[107 haproxy stats ipv4]/ensure: created[0m
2018-12-03 06:06:16,122 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[107 haproxy stats]/Firewall[107 haproxy stats ipv6]/ensure: created[0m
2018-12-03 06:06:16,625 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[108 redis]/Firewall[108 redis ipv4]/ensure: created[0m
2018-12-03 06:06:16,818 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[108 redis]/Firewall[108 redis ipv6]/ensure: created[0m
2018-12-03 06:06:17,326 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[110 ceph]/Firewall[110 ceph ipv4]/ensure: created[0m
2018-12-03 06:06:17,781 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[110 ceph]/Firewall[110 ceph ipv6]/ensure: created[0m
2018-12-03 06:06:18,302 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[111 keystone]/Firewall[111 keystone ipv4]/ensure: created[0m
2018-12-03 06:06:18,511 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[111 keystone]/Firewall[111 keystone ipv6]/ensure: created[0m
2018-12-03 06:06:19,034 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[112 glance]/Firewall[112 glance ipv4]/ensure: created[0m
2018-12-03 06:06:19,502 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[112 glance]/Firewall[112 glance ipv6]/ensure: created[0m
2018-12-03 06:06:20,030 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[113 nova]/Firewall[113 nova ipv4]/ensure: created[0m
2018-12-03 06:06:20,252 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[113 nova]/Firewall[113 nova ipv6]/ensure: created[0m
2018-12-03 06:06:20,784 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[114 neutron server]/Firewall[114 neutron server ipv4]/ensure: created[0m
2018-12-03 06:06:21,269 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[114 neutron server]/Firewall[114 neutron server ipv6]/ensure: created[0m
2018-12-03 06:06:21,816 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[115 neutron dhcp input]/Firewall[115 neutron dhcp input ipv4]/ensure: created[0m
2018-12-03 06:06:22,050 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[115 neutron dhcp input]/Firewall[115 neutron dhcp input ipv6]/ensure: created[0m
2018-12-03 06:06:23,210 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[116 neutron dhcp output]/Firewall[116 neutron dhcp output ipv4]/ensure: created[0m
2018-12-03 06:06:23,452 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[116 neutron dhcp output]/Firewall[116 neutron dhcp output ipv6]/ensure: created[0m
2018-12-03 06:06:24,010 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[118 neutron vxlan networks]/Firewall[118 neutron vxlan networks ipv4]/ensure: created[0m
2018-12-03 06:06:24,515 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[118 neutron vxlan networks]/Firewall[118 neutron vxlan networks ipv6]/ensure: created[0m
2018-12-03 06:06:25,078 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[119 cinder]/Firewall[119 cinder ipv4]/ensure: created[0m
2018-12-03 06:06:25,588 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[119 cinder]/Firewall[119 cinder ipv6]/ensure: created[0m
2018-12-03 06:06:26,159 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[120 iscsi initiator]/Firewall[120 iscsi initiator ipv4]/ensure: created[0m
2018-12-03 06:06:26,679 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[120 iscsi initiator]/Firewall[120 iscsi initiator ipv6]/ensure: created[0m
2018-12-03 06:06:27,253 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[121 memcached]/Firewall[121 memcached ipv4]/ensure: created[0m
2018-12-03 06:06:27,832 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[122 swift proxy]/Firewall[122 swift proxy ipv4]/ensure: created[0m
2018-12-03 06:06:28,353 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[122 swift proxy]/Firewall[122 swift proxy ipv6]/ensure: created[0m
2018-12-03 06:06:28,946 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[123 swift storage]/Firewall[123 swift storage ipv4]/ensure: created[0m
2018-12-03 06:06:29,472 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[123 swift storage]/Firewall[123 swift storage ipv6]/ensure: created[0m
2018-12-03 06:06:30,071 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[125 heat]/Firewall[125 heat ipv4]/ensure: created[0m
2018-12-03 06:06:30,601 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[125 heat]/Firewall[125 heat ipv6]/ensure: created[0m
2018-12-03 06:06:31,459 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[126 horizon]/Firewall[126 horizon ipv4]/ensure: created[0m
2018-12-03 06:06:31,996 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[126 horizon]/Firewall[126 horizon ipv6]/ensure: created[0m
2018-12-03 06:06:32,603 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[127 snmp]/Firewall[127 snmp ipv4]/ensure: created[0m
2018-12-03 06:06:33,148 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[127 snmp]/Firewall[127 snmp ipv6]/ensure: created[0m
2018-12-03 06:06:33,765 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[128 aodh]/Firewall[128 aodh ipv4]/ensure: created[0m
2018-12-03 06:06:34,318 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[128 aodh]/Firewall[128 aodh ipv6]/ensure: created[0m
2018-12-03 06:06:35,192 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[129 gnocchi-api]/Firewall[129 gnocchi-api ipv4]/ensure: created[0m
2018-12-03 06:06:35,750 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[129 gnocchi-api]/Firewall[129 gnocchi-api ipv6]/ensure: created[0m
2018-12-03 06:06:36,383 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[130 tftp]/Firewall[130 tftp ipv4]/ensure: created[0m
2018-12-03 06:06:36,949 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[130 tftp]/Firewall[130 tftp ipv6]/ensure: created[0m
2018-12-03 06:06:37,832 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[131 novnc]/Firewall[131 novnc ipv4]/ensure: created[0m
2018-12-03 06:06:38,404 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[131 novnc]/Firewall[131 novnc ipv6]/ensure: created[0m
2018-12-03 06:06:39,048 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[132 mistral]/Firewall[132 mistral ipv4]/ensure: created[0m
2018-12-03 06:06:39,624 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[132 mistral]/Firewall[132 mistral ipv6]/ensure: created[0m
2018-12-03 06:06:40,529 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[133 zaqar]/Firewall[133 zaqar ipv4]/ensure: created[0m
2018-12-03 06:06:41,121 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[133 zaqar]/Firewall[133 zaqar ipv6]/ensure: created[0m
2018-12-03 06:06:42,027 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[134 zaqar websockets]/Firewall[134 zaqar websockets ipv4]/ensure: created[0m
2018-12-03 06:06:42,619 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[134 zaqar websockets]/Firewall[134 zaqar websockets ipv6]/ensure: created[0m
2018-12-03 06:06:43,284 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[135 ironic]/Firewall[135 ironic ipv4]/ensure: created[0m
2018-12-03 06:06:44,137 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[135 ironic]/Firewall[135 ironic ipv6]/ensure: created[0m
2018-12-03 06:06:44,817 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[136 trove]/Firewall[136 trove ipv4]/ensure: created[0m
2018-12-03 06:06:45,672 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[136 trove]/Firewall[136 trove ipv6]/ensure: created[0m
2018-12-03 06:06:46,357 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[137 ironic-inspector]/Firewall[137 ironic-inspector ipv4]/ensure: created[0m
2018-12-03 06:06:47,222 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[137 ironic-inspector]/Firewall[137 ironic-inspector ipv6]/ensure: created[0m
2018-12-03 06:06:47,913 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[138 docker registry]/Firewall[138 docker registry ipv4]/ensure: created[0m
2018-12-03 06:06:48,784 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[138 docker registry]/Firewall[138 docker registry ipv6]/ensure: created[0m
2018-12-03 06:06:49,480 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[139 apache vhost]/Firewall[139 apache vhost ipv4]/ensure: created[0m
2018-12-03 06:06:50,362 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[139 apache vhost]/Firewall[139 apache vhost ipv6]/ensure: created[0m
2018-12-03 06:06:51,064 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[140 destination ctlplane-subnet cidr nat]/Firewall[140 destination ctlplane-subnet cidr nat ipv4]/ensure: created[0m
2018-12-03 06:06:52,015 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[140 source ctlplane-subnet cidr nat]/Firewall[140 source ctlplane-subnet cidr nat ipv4]/ensure: created[0m
2018-12-03 06:06:52,973 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[142 tripleo-ui]/Firewall[142 tripleo-ui ipv4]/ensure: created[0m
2018-12-03 06:06:53,603 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[142 tripleo-ui]/Firewall[142 tripleo-ui ipv6]/ensure: created[0m
2018-12-03 06:06:54,569 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[143 panko-api]/Firewall[143 panko-api ipv4]/ensure: created[0m
2018-12-03 06:06:55,461 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[143 panko-api]/Firewall[143 panko-api ipv6]/ensure: created[0m
2018-12-03 06:06:55,526 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Kernel/Sysctl::Value[net.ipv4.ip_nonlocal_bind]/Sysctl[net.ipv4.ip_nonlocal_bind]/ensure: created[0m
2018-12-03 06:06:55,543 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Kernel/Sysctl::Value[net.ipv4.ip_nonlocal_bind]/Sysctl_runtime[net.ipv4.ip_nonlocal_bind]/val: val changed '0' to '1'[0m
2018-12-03 06:06:55,545 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Kernel/Sysctl::Value[net.ipv6.ip_nonlocal_bind]/Sysctl[net.ipv6.ip_nonlocal_bind]/ensure: created[0m
2018-12-03 06:06:55,560 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Kernel/Sysctl::Value[net.ipv6.ip_nonlocal_bind]/Sysctl_runtime[net.ipv6.ip_nonlocal_bind]/val: val changed '0' to '1'[0m
2018-12-03 06:06:58,499 INFO: [mNotice: /Stage[main]/Certmonger/Package[certmonger]/ensure: created[0m
2018-12-03 06:06:58,971 INFO: [mNotice: /Stage[main]/Certmonger/Service[certmonger]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:07:01,005 INFO: [mNotice: /Stage[main]/Tripleo::Certmonger::Ca::Local/Exec[extract-and-trust-ca]/returns: executed successfully[0m
2018-12-03 06:07:01,034 INFO: [mNotice: /Stage[main]/Tripleo::Certmonger::Ca::Local/Exec[extract-and-trust-ca]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:07:01,096 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Certmonger_user/Tripleo::Certmonger::Haproxy[undercloud-haproxy-public]/Certmonger_certificate[undercloud-haproxy-public-cert]/ensure: created[0m
2018-12-03 06:07:01,647 INFO: [mNotice: /Stage[main]/Haproxy/Haproxy::Instance[haproxy]/Anchor[haproxy::haproxy::begin]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:07:01,712 INFO: [mNotice: /Stage[main]/Keepalived::Config/Concat[/etc/keepalived/keepalived.conf]/File[/etc/keepalived/keepalived.conf]/content: content changed '{md5}968d3992f248f7c764c5d91c84db4f3f' to '{md5}12521f81b3ed86e11b151072c65dd33e'[0m
2018-12-03 06:07:01,802 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Db::Mysql/Openstacklib::Db::Mysql[ironic-inspector]/Mysql_database[ironic-inspector]/ensure: created[0m
2018-12-03 06:07:01,899 INFO: [mNotice: /Stage[main]/Glance::Policy/Oslo::Policy[glance_api_config]/Glance_api_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-03 06:07:01,948 INFO: [mNotice: /Stage[main]/Glance::Policy/Oslo::Policy[glance_registry_config]/Glance_registry_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-03 06:07:02,043 INFO: [mNotice: /Stage[main]/Glance::Api::Db/Oslo::Db[glance_api_config]/Glance_api_config[database/connection]/ensure: created[0m
2018-12-03 06:07:02,584 INFO: [mNotice: /Stage[main]/Glance::Api::Logging/Oslo::Log[glance_api_config]/Glance_api_config[DEFAULT/debug]/ensure: created[0m
2018-12-03 06:07:02,647 INFO: [mNotice: /Stage[main]/Glance::Api::Logging/Oslo::Log[glance_api_config]/Glance_api_config[DEFAULT/log_file]/ensure: created[0m
2018-12-03 06:07:02,679 INFO: [mNotice: /Stage[main]/Glance::Api::Logging/Oslo::Log[glance_api_config]/Glance_api_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-03 06:07:03,166 INFO: [mNotice: /Stage[main]/Glance::Cache::Logging/Oslo::Log[glance_cache_config]/Glance_cache_config[DEFAULT/debug]/ensure: created[0m
2018-12-03 06:07:03,200 INFO: [mNotice: /Stage[main]/Glance::Cache::Logging/Oslo::Log[glance_cache_config]/Glance_cache_config[DEFAULT/log_file]/ensure: created[0m
2018-12-03 06:07:03,222 INFO: [mNotice: /Stage[main]/Glance::Cache::Logging/Oslo::Log[glance_cache_config]/Glance_cache_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-03 06:07:03,396 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-03 06:07:03,665 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-03 06:07:04,032 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-03 06:07:04,062 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/username]/ensure: created[0m
2018-12-03 06:07:04,366 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/password]/ensure: created[0m
2018-12-03 06:07:04,396 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-03 06:07:04,425 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-03 06:07:04,454 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-03 06:07:04,514 INFO: [mNotice: /Stage[main]/Glance::Api/Oslo::Middleware[glance_api_config]/Glance_api_config[oslo_middleware/enable_proxy_headers_parsing]/ensure: created[0m
2018-12-03 06:07:04,675 INFO: [mNotice: /Stage[main]/Glance::Notify::Rabbitmq/Oslo::Messaging::Rabbit[glance_api_config]/Glance_api_config[oslo_messaging_rabbit/rabbit_password]/ensure: created[0m
2018-12-03 06:07:04,765 INFO: [mNotice: /Stage[main]/Glance::Notify::Rabbitmq/Oslo::Messaging::Rabbit[glance_api_config]/Glance_api_config[oslo_messaging_rabbit/rabbit_userid]/ensure: created[0m
2018-12-03 06:07:05,113 INFO: [mNotice: /Stage[main]/Glance::Notify::Rabbitmq/Oslo::Messaging::Rabbit[glance_api_config]/Glance_api_config[oslo_messaging_rabbit/rabbit_host]/ensure: created[0m
2018-12-03 06:07:05,272 INFO: [mNotice: /Stage[main]/Glance::Notify::Rabbitmq/Oslo::Messaging::Rabbit[glance_registry_config]/Glance_registry_config[oslo_messaging_rabbit/rabbit_password]/ensure: created[0m
2018-12-03 06:07:05,320 INFO: [mNotice: /Stage[main]/Glance::Notify::Rabbitmq/Oslo::Messaging::Rabbit[glance_registry_config]/Glance_registry_config[oslo_messaging_rabbit/rabbit_userid]/ensure: created[0m
2018-12-03 06:07:05,381 INFO: [mNotice: /Stage[main]/Glance::Notify::Rabbitmq/Oslo::Messaging::Rabbit[glance_registry_config]/Glance_registry_config[oslo_messaging_rabbit/rabbit_host]/ensure: created[0m
2018-12-03 06:07:05,813 INFO: [mNotice: /Stage[main]/Glance::Deps/Anchor[glance::config::end]: Triggered 'refresh' from 47 events[0m
2018-12-03 06:07:05,839 INFO: [mNotice: /Stage[main]/Glance::Db::Mysql/Openstacklib::Db::Mysql[glance]/Mysql_database[glance]/ensure: created[0m
2018-12-03 06:07:07,930 INFO: [mNotice: /Stage[main]/Nova::Api/Nova::Generic_service[api]/Package[nova-api]/ensure: created[0m
2018-12-03 06:07:13,952 INFO: [mNotice: /Stage[main]/Nova::Wsgi::Apache_placement/Nova::Generic_service[placement-api]/Package[nova-placement-api]/ensure: created[0m
2018-12-03 06:07:16,004 INFO: [mNotice: /Stage[main]/Nova::Conductor/Nova::Generic_service[conductor]/Package[nova-conductor]/ensure: created[0m
2018-12-03 06:07:18,053 INFO: [mNotice: /Stage[main]/Nova::Scheduler/Nova::Generic_service[scheduler]/Package[nova-scheduler]/ensure: created[0m
2018-12-03 06:07:21,058 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova::Generic_service[compute]/Package[nova-compute]/ensure: created[0m
2018-12-03 06:07:21,065 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::install::end]: Triggered 'refresh' from 7 events[0m
2018-12-03 06:07:21,127 INFO: [mNotice: /Stage[main]/Nova::Db/Nova_config[api_database/connection]/ensure: created[0m
2018-12-03 06:07:21,189 INFO: [mNotice: /Stage[main]/Nova::Db/Nova_config[placement_database/connection]/ensure: created[0m
2018-12-03 06:07:21,515 INFO: [mNotice: /Stage[main]/Nova/Nova_config[glance/api_servers]/ensure: created[0m
2018-12-03 06:07:21,555 INFO: [mNotice: /Stage[main]/Nova/Nova_config[DEFAULT/ssl_only]/ensure: created[0m
2018-12-03 06:07:21,670 INFO: [mNotice: /Stage[main]/Nova/Nova_config[api/auth_strategy]/ensure: created[0m
2018-12-03 06:07:22,017 INFO: [mNotice: /Stage[main]/Nova/Nova_config[DEFAULT/ram_allocation_ratio]/ensure: created[0m
2018-12-03 06:07:22,495 INFO: [mNotice: /Stage[main]/Nova/Nova_config[notifications/notify_api_faults]/ensure: created[0m
2018-12-03 06:07:22,529 INFO: [mNotice: /Stage[main]/Nova/Nova_config[notifications/notification_format]/ensure: created[0m
2018-12-03 06:07:22,584 INFO: [mNotice: /Stage[main]/Nova/Nova_config[DEFAULT/state_path]/ensure: created[0m
2018-12-03 06:07:22,625 INFO: [mNotice: /Stage[main]/Nova/Nova_config[DEFAULT/service_down_time]/ensure: created[0m
2018-12-03 06:07:22,666 INFO: [mNotice: /Stage[main]/Nova/Nova_config[DEFAULT/rootwrap_config]/ensure: created[0m
2018-12-03 06:07:22,707 INFO: [mNotice: /Stage[main]/Nova/Nova_config[DEFAULT/report_interval]/ensure: created[0m
2018-12-03 06:07:23,056 INFO: [mNotice: /Stage[main]/Nova/Nova_config[notifications/notify_on_state_change]/ensure: created[0m
2018-12-03 06:07:23,649 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[wsgi/api_paste_config]/ensure: created[0m
2018-12-03 06:07:23,690 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/enabled_apis]/ensure: created[0m
2018-12-03 06:07:23,997 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/osapi_compute_listen]/ensure: created[0m
2018-12-03 06:07:24,045 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/metadata_listen]/ensure: created[0m
2018-12-03 06:07:24,088 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/metadata_listen_port]/ensure: created[0m
2018-12-03 06:07:24,129 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/osapi_compute_listen_port]/ensure: created[0m
2018-12-03 06:07:24,170 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/osapi_compute_workers]/ensure: created[0m
2018-12-03 06:07:24,213 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/metadata_workers]/ensure: created[0m
2018-12-03 06:07:24,598 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[api/use_forwarded_for]/ensure: created[0m
2018-12-03 06:07:24,639 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[api/fping_path]/ensure: created[0m
2018-12-03 06:07:25,652 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[vendordata_dynamic_auth/project_domain_name]/ensure: created[0m
2018-12-03 06:07:25,722 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[vendordata_dynamic_auth/user_domain_name]/ensure: created[0m
2018-12-03 06:07:26,057 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[neutron/service_metadata_proxy]/ensure: created[0m
2018-12-03 06:07:26,164 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/allow_resize_to_same_host]/ensure: created[0m
2018-12-03 06:07:26,198 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/auth_type]/ensure: created[0m
2018-12-03 06:07:26,238 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/auth_url]/ensure: created[0m
2018-12-03 06:07:26,279 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/password]/ensure: created[0m
2018-12-03 06:07:26,592 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/project_domain_name]/ensure: created[0m
2018-12-03 06:07:26,626 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/project_name]/ensure: created[0m
2018-12-03 06:07:26,680 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/user_domain_name]/ensure: created[0m
2018-12-03 06:07:26,714 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/username]/ensure: created[0m
2018-12-03 06:07:26,749 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/os_region_name]/ensure: created[0m
2018-12-03 06:07:26,811 INFO: [mNotice: /Stage[main]/Nova::Conductor/Nova_config[conductor/workers]/ensure: created[0m
2018-12-03 06:07:27,156 INFO: [mNotice: /Stage[main]/Nova::Scheduler/Nova_config[scheduler/driver]/ensure: created[0m
2018-12-03 06:07:27,217 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[scheduler/host_manager]/ensure: created[0m
2018-12-03 06:07:27,254 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[scheduler/max_attempts]/ensure: created[0m
2018-12-03 06:07:27,322 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[filter_scheduler/host_subset_size]/ensure: created[0m
2018-12-03 06:07:27,367 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[filter_scheduler/max_io_ops_per_host]/ensure: created[0m
2018-12-03 06:07:27,402 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[filter_scheduler/max_instances_per_host]/ensure: created[0m
2018-12-03 06:07:27,748 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[filter_scheduler/available_filters]/ensure: created[0m
2018-12-03 06:07:27,782 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[filter_scheduler/weight_classes]/ensure: created[0m
2018-12-03 06:07:27,816 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[filter_scheduler/use_baremetal_filters]/ensure: created[0m
2018-12-03 06:07:27,851 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[filter_scheduler/enabled_filters]/ensure: created[0m
2018-12-03 06:07:28,872 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[DEFAULT/reserved_host_memory_mb]/ensure: created[0m
2018-12-03 06:07:28,939 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[DEFAULT/heal_instance_info_cache_interval]/ensure: created[0m
2018-12-03 06:07:29,371 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[key_manager/backend]/ensure: created[0m
2018-12-03 06:07:29,805 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[compute/consecutive_build_service_disable_threshold]/ensure: created[0m
2018-12-03 06:07:29,895 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[vnc/enabled]/ensure: created[0m
2018-12-03 06:07:29,941 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[DEFAULT/force_config_drive]/ensure: created[0m
2018-12-03 06:07:29,979 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[DEFAULT/instance_usage_audit]/ensure: created[0m
2018-12-03 06:07:30,018 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[DEFAULT/instance_usage_audit_period]/ensure: created[0m
2018-12-03 06:07:30,351 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[DEFAULT/force_raw_images]/ensure: created[0m
2018-12-03 06:07:30,444 INFO: [mNotice: /Stage[main]/Main/Nova_config[DEFAULT/sync_power_state_interval]/ensure: created[0m
2018-12-03 06:07:30,478 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/auth_plugin]/ensure: created[0m
2018-12-03 06:07:30,512 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/username]/ensure: created[0m
2018-12-03 06:07:30,547 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/password]/ensure: created[0m
2018-12-03 06:07:30,582 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/auth_url]/ensure: created[0m
2018-12-03 06:07:30,913 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/project_name]/ensure: created[0m
2018-12-03 06:07:30,948 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/api_endpoint]/ensure: created[0m
2018-12-03 06:07:31,047 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/user_domain_name]/ensure: created[0m
2018-12-03 06:07:31,081 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/project_domain_name]/ensure: created[0m
2018-12-03 06:07:31,120 INFO: [mNotice: /Stage[main]/Nova::Compute::Ironic/Nova_config[DEFAULT/compute_driver]/ensure: created[0m
2018-12-03 06:07:31,485 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[DEFAULT/dhcp_domain]/ensure: created[0m
2018-12-03 06:07:31,525 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[DEFAULT/firewall_driver]/ensure: created[0m
2018-12-03 06:07:31,563 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[DEFAULT/vif_plugging_is_fatal]/ensure: created[0m
2018-12-03 06:07:31,600 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[DEFAULT/vif_plugging_timeout]/ensure: created[0m
2018-12-03 06:07:31,633 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/default_floating_pool]/ensure: created[0m
2018-12-03 06:07:31,698 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/url]/ensure: created[0m
2018-12-03 06:07:31,735 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/timeout]/ensure: created[0m
2018-12-03 06:07:32,073 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/project_name]/ensure: created[0m
2018-12-03 06:07:32,107 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/project_domain_name]/ensure: created[0m
2018-12-03 06:07:32,142 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/region_name]/ensure: created[0m
2018-12-03 06:07:32,176 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/username]/ensure: created[0m
2018-12-03 06:07:32,211 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/user_domain_name]/ensure: created[0m
2018-12-03 06:07:32,244 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/password]/ensure: created[0m
2018-12-03 06:07:32,286 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/auth_url]/ensure: created[0m
2018-12-03 06:07:32,321 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/ovs_bridge]/ensure: created[0m
2018-12-03 06:07:32,656 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/extension_sync_interval]/ensure: created[0m
2018-12-03 06:07:32,691 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/auth_type]/ensure: created[0m
2018-12-03 06:07:33,573 INFO: [mNotice: /Stage[main]/Main/Augeas[lvm.conf]/returns: executed successfully[0m
2018-12-03 06:07:33,662 INFO: [mNotice: /Stage[main]/Nova::Db/Oslo::Db[nova_config]/Nova_config[database/connection]/ensure: created[0m
2018-12-03 06:07:34,788 INFO: [mNotice: /Stage[main]/Nova::Logging/Oslo::Log[nova_config]/Nova_config[DEFAULT/debug]/ensure: created[0m
2018-12-03 06:07:35,216 INFO: [mNotice: /Stage[main]/Nova::Logging/Oslo::Log[nova_config]/Nova_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-03 06:07:40,108 INFO: [mNotice: /Stage[main]/Nova/Oslo::Messaging::Default[nova_config]/Nova_config[DEFAULT/rpc_response_timeout]/ensure: created[0m
2018-12-03 06:07:40,149 INFO: [mNotice: /Stage[main]/Nova/Oslo::Messaging::Default[nova_config]/Nova_config[DEFAULT/transport_url]/ensure: created[0m
2018-12-03 06:07:40,215 INFO: [mNotice: /Stage[main]/Nova/Oslo::Messaging::Notifications[nova_config]/Nova_config[oslo_messaging_notifications/driver]/ensure: created[0m
2018-12-03 06:07:40,685 INFO: [mNotice: /Stage[main]/Nova/Oslo::Concurrency[nova_config]/Nova_config[oslo_concurrency/lock_path]/ensure: created[0m
2018-12-03 06:07:40,719 INFO: [mNotice: /Stage[main]/Nova::Policy/Oslo::Policy[nova_config]/Nova_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-03 06:07:41,196 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-03 06:07:41,231 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-03 06:07:42,979 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-03 06:07:43,014 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/username]/ensure: created[0m
2018-12-03 06:07:43,050 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/password]/ensure: created[0m
2018-12-03 06:07:43,086 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-03 06:07:43,122 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-03 06:07:43,158 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-03 06:07:43,621 INFO: [mNotice: /Stage[main]/Nova::Api/Oslo::Middleware[nova_config]/Nova_config[oslo_middleware/enable_proxy_headers_parsing]/ensure: created[0m
2018-12-03 06:07:43,631 INFO: [mNotice: /Stage[main]/Neutron::Logging/Oslo::Log[neutron_config]/Neutron_config[DEFAULT/debug]/ensure: created[0m
2018-12-03 06:07:43,660 INFO: [mNotice: /Stage[main]/Neutron::Logging/Oslo::Log[neutron_config]/Neutron_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-03 06:07:43,784 INFO: [mNotice: /Stage[main]/Neutron/Oslo::Messaging::Default[neutron_config]/Neutron_config[DEFAULT/control_exchange]/ensure: created[0m
2018-12-03 06:07:43,799 INFO: [mNotice: /Stage[main]/Neutron/Oslo::Concurrency[neutron_config]/Neutron_config[oslo_concurrency/lock_path]/ensure: created[0m
2018-12-03 06:07:44,249 INFO: [mNotice: /Stage[main]/Neutron/Oslo::Messaging::Rabbit[neutron_config]/Neutron_config[oslo_messaging_rabbit/rabbit_password]/ensure: created[0m
2018-12-03 06:07:44,283 INFO: [mNotice: /Stage[main]/Neutron/Oslo::Messaging::Rabbit[neutron_config]/Neutron_config[oslo_messaging_rabbit/rabbit_userid]/ensure: created[0m
2018-12-03 06:07:44,298 INFO: [mNotice: /Stage[main]/Neutron/Oslo::Messaging::Rabbit[neutron_config]/Neutron_config[oslo_messaging_rabbit/rabbit_hosts]/ensure: created[0m
2018-12-03 06:07:44,926 INFO: [mNotice: /Stage[main]/Neutron::Db/Oslo::Db[neutron_config]/Neutron_config[database/connection]/ensure: created[0m
2018-12-03 06:07:45,047 INFO: [mNotice: /Stage[main]/Neutron::Policy/Oslo::Policy[neutron_config]/Neutron_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-03 06:07:45,076 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-03 06:07:45,085 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-03 06:07:45,611 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-03 06:07:45,621 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/username]/ensure: created[0m
2018-12-03 06:07:45,630 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/password]/ensure: created[0m
2018-12-03 06:07:45,639 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-03 06:07:45,648 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-03 06:07:45,659 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-03 06:07:45,683 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron::Plugins::Ml2::Type_driver[flat]/Neutron_plugin_ml2[ml2_type_flat/flat_networks]/ensure: created[0m
2018-12-03 06:07:45,688 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron::Plugins::Ml2::Type_driver[vlan]/Neutron_plugin_ml2[ml2_type_vlan/network_vlan_ranges]/ensure: created[0m
2018-12-03 06:07:45,692 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron::Plugins::Ml2::Type_driver[gre]/Neutron_plugin_ml2[ml2_type_gre/tunnel_id_ranges]/ensure: created[0m
2018-12-03 06:07:45,696 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron::Plugins::Ml2::Type_driver[vxlan]/Neutron_plugin_ml2[ml2_type_vxlan/vxlan_group]/ensure: created[0m
2018-12-03 06:07:45,705 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron::Plugins::Ml2::Type_driver[vxlan]/Neutron_plugin_ml2[ml2_type_vxlan/vni_ranges]/ensure: created[0m
2018-12-03 06:07:45,711 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron::Plugins::Ml2::Type_driver[geneve]/Neutron_plugin_ml2[ml2_type_geneve/vni_ranges]/ensure: created[0m
2018-12-03 06:07:45,714 INFO: [mNotice: /Stage[main]/Neutron::Deps/Anchor[neutron::config::end]: Triggered 'refresh' from 83 events[0m
2018-12-03 06:07:45,749 INFO: [mNotice: /Stage[main]/Neutron::Db::Mysql/Openstacklib::Db::Mysql[neutron]/Mysql_database[neutron]/ensure: created[0m
2018-12-03 06:07:45,810 INFO: [mNotice: /Stage[main]/Rsync::Server/Xinetd::Service[rsync]/File[/etc/xinetd.d/rsync]/ensure: defined content as '{md5}07f7d21614492b12e647b7140939f2c0'[0m
2018-12-03 06:07:45,837 INFO: [mNotice: /Stage[main]/Rsync::Server/Concat[/etc/rsyncd.conf]/File[/etc/rsyncd.conf]/content: content changed '{md5}c63fccb45c0dcbbbe17d0f4bdba920ec' to '{md5}bb435847238b233af8de6f9c8b9b7f5b'[0m
2018-12-03 06:07:45,865 INFO: [mNotice: /Stage[main]/Heat::Logging/Oslo::Log[heat_config]/Heat_config[DEFAULT/debug]/ensure: created[0m
2018-12-03 06:07:46,308 INFO: [mNotice: /Stage[main]/Heat::Logging/Oslo::Log[heat_config]/Heat_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-03 06:07:46,490 INFO: [mNotice: /Stage[main]/Heat::Db/Oslo::Db[heat_config]/Heat_config[database/connection]/ensure: created[0m
2018-12-03 06:07:46,674 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-03 06:07:46,692 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-03 06:07:47,302 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-03 06:07:47,319 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/username]/ensure: created[0m
2018-12-03 06:07:47,336 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/password]/ensure: created[0m
2018-12-03 06:07:47,354 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-03 06:07:47,372 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-03 06:07:47,390 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-03 06:07:48,693 INFO: [mNotice: /Stage[main]/Heat/Oslo::Messaging::Notifications[heat_config]/Heat_config[oslo_messaging_notifications/driver]/ensure: created[0m
2018-12-03 06:07:48,730 INFO: [mNotice: /Stage[main]/Heat/Oslo::Messaging::Default[heat_config]/Heat_config[DEFAULT/rpc_response_timeout]/ensure: created[0m
2018-12-03 06:07:48,749 INFO: [mNotice: /Stage[main]/Heat/Oslo::Messaging::Default[heat_config]/Heat_config[DEFAULT/transport_url]/ensure: created[0m
2018-12-03 06:07:48,785 INFO: [mNotice: /Stage[main]/Heat/Oslo::Middleware[heat_config]/Heat_config[oslo_middleware/enable_proxy_headers_parsing]/ensure: created[0m
2018-12-03 06:07:48,804 INFO: [mNotice: /Stage[main]/Heat::Policy/Oslo::Policy[heat_config]/Heat_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-03 06:07:48,842 INFO: [mNotice: /Stage[main]/Heat::Cors/Oslo::Cors[heat_config]/Heat_config[cors/allowed_origin]/ensure: created[0m
2018-12-03 06:07:48,868 INFO: [mNotice: /Stage[main]/Heat::Cors/Oslo::Cors[heat_config]/Heat_config[cors/expose_headers]/ensure: created[0m
2018-12-03 06:07:48,886 INFO: [mNotice: /Stage[main]/Heat::Cors/Oslo::Cors[heat_config]/Heat_config[cors/max_age]/ensure: created[0m
2018-12-03 06:07:48,913 INFO: [mNotice: /Stage[main]/Heat::Cors/Oslo::Cors[heat_config]/Heat_config[cors/allow_headers]/ensure: created[0m
2018-12-03 06:07:48,914 INFO: [mNotice: /Stage[main]/Heat::Deps/Anchor[heat::config::end]: Triggered 'refresh' from 49 events[0m
2018-12-03 06:07:48,947 INFO: [mNotice: /Stage[main]/Heat::Db::Mysql/Openstacklib::Db::Mysql[heat]/Mysql_database[heat]/ensure: created[0m
2018-12-03 06:07:48,983 INFO: [mNotice: /Stage[main]/Nova::Cors/Oslo::Cors[nova_config]/Nova_config[cors/allowed_origin]/ensure: created[0m
2018-12-03 06:07:49,048 INFO: [mNotice: /Stage[main]/Nova::Cors/Oslo::Cors[nova_config]/Nova_config[cors/expose_headers]/ensure: created[0m
2018-12-03 06:07:49,086 INFO: [mNotice: /Stage[main]/Nova::Cors/Oslo::Cors[nova_config]/Nova_config[cors/max_age]/ensure: created[0m
2018-12-03 06:07:49,546 INFO: [mNotice: /Stage[main]/Nova::Cors/Oslo::Cors[nova_config]/Nova_config[cors/allow_methods]/ensure: created[0m
2018-12-03 06:07:49,588 INFO: [mNotice: /Stage[main]/Nova::Cors/Oslo::Cors[nova_config]/Nova_config[cors/allow_headers]/ensure: created[0m
2018-12-03 06:07:49,589 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::config::end]: Triggered 'refresh' from 104 events[0m
2018-12-03 06:07:49,626 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql/Openstacklib::Db::Mysql[nova]/Mysql_database[nova]/ensure: created[0m
2018-12-03 06:07:49,657 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql/Openstacklib::Db::Mysql[nova_cell0]/Mysql_database[nova_cell0]/ensure: created[0m
2018-12-03 06:07:49,688 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql_api/Openstacklib::Db::Mysql[nova_api]/Mysql_database[nova_api]/ensure: created[0m
2018-12-03 06:07:49,717 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql_placement/Openstacklib::Db::Mysql[nova_placement]/Mysql_database[nova_placement]/ensure: created[0m
2018-12-03 06:07:49,748 INFO: [mNotice: /Stage[main]/Ironic::Logging/Oslo::Log[ironic_config]/Ironic_config[DEFAULT/debug]/ensure: created[0m
2018-12-03 06:07:49,823 INFO: [mNotice: /Stage[main]/Ironic::Logging/Oslo::Log[ironic_config]/Ironic_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-03 06:07:50,542 INFO: [mNotice: /Stage[main]/Ironic::Db/Oslo::Db[ironic_config]/Ironic_config[database/connection]/ensure: created[0m
2018-12-03 06:07:50,821 INFO: [mNotice: /Stage[main]/Ironic/Oslo::Messaging::Default[ironic_config]/Ironic_config[DEFAULT/rpc_response_timeout]/ensure: created[0m
2018-12-03 06:07:50,870 INFO: [mNotice: /Stage[main]/Ironic/Oslo::Messaging::Default[ironic_config]/Ironic_config[DEFAULT/transport_url]/ensure: created[0m
2018-12-03 06:07:52,525 INFO: [mNotice: /Stage[main]/Ironic::Policy/Oslo::Policy[ironic_config]/Ironic_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-03 06:07:52,603 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-03 06:07:53,053 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-03 06:07:53,418 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-03 06:07:53,447 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/username]/ensure: created[0m
2018-12-03 06:07:53,483 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/password]/ensure: created[0m
2018-12-03 06:07:53,515 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-03 06:07:53,977 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-03 06:07:54,006 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-03 06:07:54,081 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[boot]/Ironic_config[DEFAULT/enabled_boot_interfaces]/ensure: created[0m
2018-12-03 06:07:54,133 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[console]/Ironic_config[DEFAULT/enabled_console_interfaces]/ensure: created[0m
2018-12-03 06:07:54,179 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[deploy]/Ironic_config[DEFAULT/enabled_deploy_interfaces]/ensure: created[0m
2018-12-03 06:07:54,224 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[inspect]/Ironic_config[DEFAULT/enabled_inspect_interfaces]/ensure: created[0m
2018-12-03 06:07:54,253 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[inspect]/Ironic_config[DEFAULT/default_inspect_interface]/ensure: created[0m
2018-12-03 06:07:54,284 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[management]/Ironic_config[DEFAULT/enabled_management_interfaces]/ensure: created[0m
2018-12-03 06:07:54,359 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[power]/Ironic_config[DEFAULT/enabled_power_interfaces]/ensure: created[0m
2018-12-03 06:07:54,404 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[raid]/Ironic_config[DEFAULT/enabled_raid_interfaces]/ensure: created[0m
2018-12-03 06:07:54,948 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[vendor]/Ironic_config[DEFAULT/enabled_vendor_interfaces]/ensure: created[0m
2018-12-03 06:07:54,974 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Logging/Oslo::Log[ironic_inspector_config]/Ironic_inspector_config[DEFAULT/debug]/ensure: created[0m
2018-12-03 06:07:54,996 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Logging/Oslo::Log[ironic_inspector_config]/Ironic_inspector_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-03 06:07:55,093 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Db/Oslo::Db[ironic_inspector_config]/Ironic_inspector_config[database/connection]/ensure: created[0m
2018-12-03 06:07:55,182 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-03 06:07:55,194 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-03 06:07:55,309 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-03 06:07:55,318 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/username]/ensure: created[0m
2018-12-03 06:07:55,328 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/password]/ensure: created[0m
2018-12-03 06:07:55,337 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-03 06:07:55,347 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-03 06:07:55,794 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-03 06:07:55,808 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Cors/Oslo::Cors[ironic_inspector_config]/Ironic_inspector_config[cors/allowed_origin]/ensure: created[0m
2018-12-03 06:07:55,822 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Cors/Oslo::Cors[ironic_inspector_config]/Ironic_inspector_config[cors/expose_headers]/ensure: created[0m
2018-12-03 06:07:55,831 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Cors/Oslo::Cors[ironic_inspector_config]/Ironic_inspector_config[cors/max_age]/ensure: created[0m
2018-12-03 06:07:55,842 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Cors/Oslo::Cors[ironic_inspector_config]/Ironic_inspector_config[cors/allow_methods]/ensure: created[0m
2018-12-03 06:07:55,851 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Cors/Oslo::Cors[ironic_inspector_config]/Ironic_inspector_config[cors/allow_headers]/ensure: created[0m
2018-12-03 06:07:55,852 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic-inspector::config::end]: Triggered 'refresh' from 45 events[0m
2018-12-03 06:07:55,863 INFO: [mNotice: /Stage[main]/Ironic::Pxe/Xinetd::Service[tftp]/File[/etc/xinetd.d/tftp]/content: content changed '{md5}678efd3887a91cd4e0955aa6c8b12257' to '{md5}50a3a2ab0bde157472ab030b96fb23e4'[0m
2018-12-03 06:07:56,171 INFO: [mNotice: /Stage[main]/Xinetd/Service[xinetd]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:07:56,183 INFO: [mNotice: /Stage[main]/Ironic::Pxe/Ironic::Pxe::Tftpboot_file[pxelinux.0]/File[/tftpboot/pxelinux.0]/ensure: defined content as '{md5}e28413b36fee074c30554f8c9e54ca2c'[0m
2018-12-03 06:07:56,190 INFO: [mNotice: /Stage[main]/Ironic::Pxe/Ironic::Pxe::Tftpboot_file[chain.c32]/File[/tftpboot/chain.c32]/ensure: defined content as '{md5}b1b9c82af57fffaf21612dbac538b5ac'[0m
2018-12-03 06:07:56,221 INFO: [mNotice: /Stage[main]/Ironic::Cors/Oslo::Cors[ironic_config]/Ironic_config[cors/allowed_origin]/ensure: created[0m
2018-12-03 06:07:56,265 INFO: [mNotice: /Stage[main]/Ironic::Cors/Oslo::Cors[ironic_config]/Ironic_config[cors/expose_headers]/ensure: created[0m
2018-12-03 06:07:56,294 INFO: [mNotice: /Stage[main]/Ironic::Cors/Oslo::Cors[ironic_config]/Ironic_config[cors/max_age]/ensure: created[0m
2018-12-03 06:07:56,323 INFO: [mNotice: /Stage[main]/Ironic::Cors/Oslo::Cors[ironic_config]/Ironic_config[cors/allow_methods]/ensure: created[0m
2018-12-03 06:07:56,355 INFO: [mNotice: /Stage[main]/Ironic::Cors/Oslo::Cors[ironic_config]/Ironic_config[cors/allow_headers]/ensure: created[0m
2018-12-03 06:07:56,356 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::config::end]: Triggered 'refresh' from 95 events[0m
2018-12-03 06:07:56,393 INFO: [mNotice: /Stage[main]/Ironic::Db::Mysql/Openstacklib::Db::Mysql[ironic]/Mysql_database[ironic]/ensure: created[0m
2018-12-03 06:07:56,402 INFO: [mNotice: /Stage[main]/Mistral::Db/Oslo::Db[mistral_config]/Mistral_config[database/connection]/ensure: created[0m
2018-12-03 06:07:56,416 INFO: [mNotice: /Stage[main]/Mistral::Logging/Oslo::Log[mistral_config]/Mistral_config[DEFAULT/debug]/ensure: created[0m
2018-12-03 06:07:56,421 INFO: [mNotice: /Stage[main]/Mistral::Logging/Oslo::Log[mistral_config]/Mistral_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-03 06:07:56,439 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-03 06:07:56,443 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-03 06:07:56,466 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-03 06:07:56,468 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/username]/ensure: created[0m
2018-12-03 06:07:56,471 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/password]/ensure: created[0m
2018-12-03 06:07:56,472 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-03 06:07:56,475 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-03 06:07:56,477 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-03 06:07:56,481 INFO: [mNotice: /Stage[main]/Mistral/Oslo::Messaging::Default[mistral_config]/Mistral_config[DEFAULT/rpc_response_timeout]/ensure: created[0m
2018-12-03 06:07:56,502 INFO: [mNotice: /Stage[main]/Mistral/Oslo::Messaging::Rabbit[mistral_config]/Mistral_config[oslo_messaging_rabbit/rabbit_password]/ensure: created[0m
2018-12-03 06:07:56,508 INFO: [mNotice: /Stage[main]/Mistral/Oslo::Messaging::Rabbit[mistral_config]/Mistral_config[oslo_messaging_rabbit/rabbit_userid]/ensure: created[0m
2018-12-03 06:07:56,987 INFO: [mNotice: /Stage[main]/Mistral/Oslo::Messaging::Rabbit[mistral_config]/Mistral_config[oslo_messaging_rabbit/rabbit_host]/ensure: created[0m
2018-12-03 06:07:56,998 INFO: [mNotice: /Stage[main]/Mistral::Policy/Oslo::Policy[mistral_config]/Mistral_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-03 06:07:57,006 INFO: [mNotice: /Stage[main]/Mistral::Cors/Oslo::Cors[mistral_config]/Mistral_config[cors/allowed_origin]/ensure: created[0m
2018-12-03 06:07:57,009 INFO: [mNotice: /Stage[main]/Mistral::Cors/Oslo::Cors[mistral_config]/Mistral_config[cors/expose_headers]/ensure: created[0m
2018-12-03 06:07:57,015 INFO: [mNotice: /Stage[main]/Mistral::Cors/Oslo::Cors[mistral_config]/Mistral_config[cors/allow_headers]/ensure: created[0m
2018-12-03 06:07:57,015 INFO: [mNotice: /Stage[main]/Mistral::Deps/Anchor[mistral::config::end]: Triggered 'refresh' from 25 events[0m
2018-12-03 06:07:57,048 INFO: [mNotice: /Stage[main]/Mistral::Db::Mysql/Openstacklib::Db::Mysql[mistral]/Mysql_database[mistral]/ensure: created[0m
2018-12-03 06:07:57,098 INFO: [mNotice: /Stage[main]/Zaqar::Logging/Oslo::Log[zaqar_config]/Zaqar_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-03 06:07:57,245 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-03 06:07:57,261 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-03 06:07:57,927 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-03 06:07:57,947 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/username]/ensure: created[0m
2018-12-03 06:07:57,962 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/password]/ensure: created[0m
2018-12-03 06:07:57,977 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-03 06:07:57,993 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-03 06:07:58,009 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-03 06:07:58,033 INFO: [mNotice: /Stage[main]/Zaqar::Policy/Oslo::Policy[zaqar_config]/Zaqar_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-03 06:07:58,058 INFO: [mNotice: /Stage[main]/Zaqar::Deps/Anchor[zaqar::config::end]: Triggered 'refresh' from 25 events[0m
2018-12-03 06:07:58,107 INFO: [mNotice: /Stage[main]/Zaqar::Db::Mysql/Openstacklib::Db::Mysql[zaqar]/Mysql_database[zaqar]/ensure: created[0m
2018-12-03 06:07:58,116 INFO: [mNotice: /Stage[main]/Main/Zaqar::Server_instance[1]/File[/etc/zaqar/1.conf]/ensure: defined content as '{md5}7750571c15d53265e703cc60ea7938af'[0m
2018-12-03 06:07:58,140 INFO: [mNotice: /Stage[main]/Ssh::Server::Config/Concat[/etc/ssh/sshd_config]/File[/etc/ssh/sshd_config]/content: content changed '{md5}40d961cd3154f0439fcac1a50bd77b96' to '{md5}56dac259c7b06586e1726ee6685171ba'[0m
2018-12-03 06:07:58,277 INFO: [mNotice: /Stage[main]/Ssh::Server::Service/Service[sshd]: Triggered 'refresh' from 2 events[0m
2018-12-03 06:07:58,305 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Certmonger_user/Tripleo::Certmonger::Haproxy[undercloud-haproxy-public]/Concat[/etc/pki/tls/certs/undercloud-172.16.0.10.pem]/File[/etc/pki/tls/certs/undercloud-172.16.0.10.pem]/ensure: defined content as '{md5}29e6e69ee19153a247134528492c86a9'[0m
2018-12-03 06:07:58,391 INFO: [mNotice: /Stage[main]/Haproxy/Haproxy::Instance[haproxy]/Haproxy::Install[haproxy]/Package[haproxy]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:07:59,822 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[keystone_admin]/Tripleo::Firewall::Rule[100 keystone_admin_haproxy]/Firewall[100 keystone_admin_haproxy ipv4]/ensure: created[0m
2018-12-03 06:08:00,748 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[keystone_admin]/Tripleo::Firewall::Rule[100 keystone_admin_haproxy]/Firewall[100 keystone_admin_haproxy ipv6]/ensure: created[0m
2018-12-03 06:08:02,236 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[keystone_public]/Tripleo::Firewall::Rule[100 keystone_public_haproxy]/Firewall[100 keystone_public_haproxy ipv4]/ensure: created[0m
2018-12-03 06:08:03,160 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[keystone_public]/Tripleo::Firewall::Rule[100 keystone_public_haproxy]/Firewall[100 keystone_public_haproxy ipv6]/ensure: created[0m
2018-12-03 06:08:04,660 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[keystone_public]/Tripleo::Firewall::Rule[100 keystone_public_haproxy_ssl]/Firewall[100 keystone_public_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:08:06,075 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[keystone_public]/Tripleo::Firewall::Rule[100 keystone_public_haproxy_ssl]/Firewall[100 keystone_public_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:08:07,595 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[neutron]/Tripleo::Firewall::Rule[100 neutron_haproxy]/Firewall[100 neutron_haproxy ipv4]/ensure: created[0m
2018-12-03 06:08:08,542 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[neutron]/Tripleo::Firewall::Rule[100 neutron_haproxy]/Firewall[100 neutron_haproxy ipv6]/ensure: created[0m
2018-12-03 06:08:10,052 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[neutron]/Tripleo::Firewall::Rule[100 neutron_haproxy_ssl]/Firewall[100 neutron_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:08:10,997 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[neutron]/Tripleo::Firewall::Rule[100 neutron_haproxy_ssl]/Firewall[100 neutron_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:08:12,514 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[glance_api]/Tripleo::Firewall::Rule[100 glance_api_haproxy]/Firewall[100 glance_api_haproxy ipv4]/ensure: created[0m
2018-12-03 06:08:13,951 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[glance_api]/Tripleo::Firewall::Rule[100 glance_api_haproxy]/Firewall[100 glance_api_haproxy ipv6]/ensure: created[0m
2018-12-03 06:08:15,485 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[glance_api]/Tripleo::Firewall::Rule[100 glance_api_haproxy_ssl]/Firewall[100 glance_api_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:08:16,448 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[glance_api]/Tripleo::Firewall::Rule[100 glance_api_haproxy_ssl]/Firewall[100 glance_api_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:08:17,990 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_osapi]/Tripleo::Firewall::Rule[100 nova_osapi_haproxy]/Firewall[100 nova_osapi_haproxy ipv4]/ensure: created[0m
2018-12-03 06:08:19,446 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_osapi]/Tripleo::Firewall::Rule[100 nova_osapi_haproxy]/Firewall[100 nova_osapi_haproxy ipv6]/ensure: created[0m
2018-12-03 06:08:20,990 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_osapi]/Tripleo::Firewall::Rule[100 nova_osapi_haproxy_ssl]/Firewall[100 nova_osapi_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:08:21,969 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_osapi]/Tripleo::Firewall::Rule[100 nova_osapi_haproxy_ssl]/Firewall[100 nova_osapi_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:08:23,981 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_placement]/Tripleo::Firewall::Rule[100 nova_placement_haproxy]/Firewall[100 nova_placement_haproxy ipv4]/ensure: created[0m
2018-12-03 06:08:24,973 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_placement]/Tripleo::Firewall::Rule[100 nova_placement_haproxy]/Firewall[100 nova_placement_haproxy ipv6]/ensure: created[0m
2018-12-03 06:08:26,527 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_placement]/Tripleo::Firewall::Rule[100 nova_placement_haproxy_ssl]/Firewall[100 nova_placement_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:08:28,004 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_placement]/Tripleo::Firewall::Rule[100 nova_placement_haproxy_ssl]/Firewall[100 nova_placement_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:08:29,572 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_metadata]/Tripleo::Firewall::Rule[100 nova_metadata_haproxy]/Firewall[100 nova_metadata_haproxy ipv4]/ensure: created[0m
2018-12-03 06:08:31,051 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_metadata]/Tripleo::Firewall::Rule[100 nova_metadata_haproxy]/Firewall[100 nova_metadata_haproxy ipv6]/ensure: created[0m
2018-12-03 06:08:32,620 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[aodh]/Tripleo::Firewall::Rule[100 aodh_haproxy]/Firewall[100 aodh_haproxy ipv4]/ensure: created[0m
2018-12-03 06:08:34,110 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[aodh]/Tripleo::Firewall::Rule[100 aodh_haproxy]/Firewall[100 aodh_haproxy ipv6]/ensure: created[0m
2018-12-03 06:08:35,682 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[aodh]/Tripleo::Firewall::Rule[100 aodh_haproxy_ssl]/Firewall[100 aodh_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:08:36,696 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[aodh]/Tripleo::Firewall::Rule[100 aodh_haproxy_ssl]/Firewall[100 aodh_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:08:38,735 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[panko]/Tripleo::Firewall::Rule[100 panko_haproxy]/Firewall[100 panko_haproxy ipv4]/ensure: created[0m
2018-12-03 06:08:39,750 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[panko]/Tripleo::Firewall::Rule[100 panko_haproxy]/Firewall[100 panko_haproxy ipv6]/ensure: created[0m
2018-12-03 06:08:41,795 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[panko]/Tripleo::Firewall::Rule[100 panko_haproxy_ssl]/Firewall[100 panko_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:08:43,302 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[panko]/Tripleo::Firewall::Rule[100 panko_haproxy_ssl]/Firewall[100 panko_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:08:44,909 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[gnocchi]/Tripleo::Firewall::Rule[100 gnocchi_haproxy]/Firewall[100 gnocchi_haproxy ipv4]/ensure: created[0m
2018-12-03 06:08:46,432 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[gnocchi]/Tripleo::Firewall::Rule[100 gnocchi_haproxy]/Firewall[100 gnocchi_haproxy ipv6]/ensure: created[0m
2018-12-03 06:08:48,053 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[gnocchi]/Tripleo::Firewall::Rule[100 gnocchi_haproxy_ssl]/Firewall[100 gnocchi_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:08:49,578 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[gnocchi]/Tripleo::Firewall::Rule[100 gnocchi_haproxy_ssl]/Firewall[100 gnocchi_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:08:51,208 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[mistral]/Tripleo::Firewall::Rule[100 mistral_haproxy]/Firewall[100 mistral_haproxy ipv4]/ensure: created[0m
2018-12-03 06:08:52,738 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[mistral]/Tripleo::Firewall::Rule[100 mistral_haproxy]/Firewall[100 mistral_haproxy ipv6]/ensure: created[0m
2018-12-03 06:08:54,363 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[mistral]/Tripleo::Firewall::Rule[100 mistral_haproxy_ssl]/Firewall[100 mistral_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:08:55,911 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[mistral]/Tripleo::Firewall::Rule[100 mistral_haproxy_ssl]/Firewall[100 mistral_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:08:58,003 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[swift_proxy_server]/Tripleo::Firewall::Rule[100 swift_proxy_server_haproxy]/Firewall[100 swift_proxy_server_haproxy ipv4]/ensure: created[0m
2018-12-03 06:08:59,552 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[swift_proxy_server]/Tripleo::Firewall::Rule[100 swift_proxy_server_haproxy]/Firewall[100 swift_proxy_server_haproxy ipv6]/ensure: created[0m
2018-12-03 06:09:01,194 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[swift_proxy_server]/Tripleo::Firewall::Rule[100 swift_proxy_server_haproxy_ssl]/Firewall[100 swift_proxy_server_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:09:02,757 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[swift_proxy_server]/Tripleo::Firewall::Rule[100 swift_proxy_server_haproxy_ssl]/Firewall[100 swift_proxy_server_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:09:04,867 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[heat_api]/Tripleo::Firewall::Rule[100 heat_api_haproxy]/Firewall[100 heat_api_haproxy ipv4]/ensure: created[0m
2018-12-03 06:09:06,428 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[heat_api]/Tripleo::Firewall::Rule[100 heat_api_haproxy]/Firewall[100 heat_api_haproxy ipv6]/ensure: created[0m
2018-12-03 06:09:08,089 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[heat_api]/Tripleo::Firewall::Rule[100 heat_api_haproxy_ssl]/Firewall[100 heat_api_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:09:09,664 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[heat_api]/Tripleo::Firewall::Rule[100 heat_api_haproxy_ssl]/Firewall[100 heat_api_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:09:11,784 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic]/Tripleo::Firewall::Rule[100 ironic_haproxy]/Firewall[100 ironic_haproxy ipv4]/ensure: created[0m
2018-12-03 06:09:13,360 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic]/Tripleo::Firewall::Rule[100 ironic_haproxy]/Firewall[100 ironic_haproxy ipv6]/ensure: created[0m
2018-12-03 06:09:15,029 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic]/Tripleo::Firewall::Rule[100 ironic_haproxy_ssl]/Firewall[100 ironic_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:09:16,612 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic]/Tripleo::Firewall::Rule[100 ironic_haproxy_ssl]/Firewall[100 ironic_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:09:18,760 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic-inspector]/Tripleo::Firewall::Rule[100 ironic-inspector_haproxy]/Firewall[100 ironic-inspector_haproxy ipv4]/ensure: created[0m
2018-12-03 06:09:20,353 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic-inspector]/Tripleo::Firewall::Rule[100 ironic-inspector_haproxy]/Firewall[100 ironic-inspector_haproxy ipv6]/ensure: created[0m
2018-12-03 06:09:22,506 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic-inspector]/Tripleo::Firewall::Rule[100 ironic-inspector_haproxy_ssl]/Firewall[100 ironic-inspector_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:09:24,103 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic-inspector]/Tripleo::Firewall::Rule[100 ironic-inspector_haproxy_ssl]/Firewall[100 ironic-inspector_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:09:26,256 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[docker-registry]/Tripleo::Firewall::Rule[100 docker-registry_haproxy]/Firewall[100 docker-registry_haproxy ipv4]/ensure: created[0m
2018-12-03 06:09:27,862 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[docker-registry]/Tripleo::Firewall::Rule[100 docker-registry_haproxy]/Firewall[100 docker-registry_haproxy ipv6]/ensure: created[0m
2018-12-03 06:09:29,559 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[docker-registry]/Tripleo::Firewall::Rule[100 docker-registry_haproxy_ssl]/Firewall[100 docker-registry_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:09:31,631 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[docker-registry]/Tripleo::Firewall::Rule[100 docker-registry_haproxy_ssl]/Firewall[100 docker-registry_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:09:33,343 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_api]/Tripleo::Firewall::Rule[100 zaqar_api_haproxy]/Firewall[100 zaqar_api_haproxy ipv4]/ensure: created[0m
2018-12-03 06:09:35,409 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_api]/Tripleo::Firewall::Rule[100 zaqar_api_haproxy]/Firewall[100 zaqar_api_haproxy ipv6]/ensure: created[0m
2018-12-03 06:09:37,115 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_api]/Tripleo::Firewall::Rule[100 zaqar_api_haproxy_ssl]/Firewall[100 zaqar_api_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:09:39,207 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_api]/Tripleo::Firewall::Rule[100 zaqar_api_haproxy_ssl]/Firewall[100 zaqar_api_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:09:40,939 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_ws]/Tripleo::Firewall::Rule[100 zaqar_ws_haproxy]/Firewall[100 zaqar_ws_haproxy ipv4]/ensure: created[0m
2018-12-03 06:09:43,086 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_ws]/Tripleo::Firewall::Rule[100 zaqar_ws_haproxy]/Firewall[100 zaqar_ws_haproxy ipv6]/ensure: created[0m
2018-12-03 06:09:45,282 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_ws]/Tripleo::Firewall::Rule[100 zaqar_ws_haproxy_ssl]/Firewall[100 zaqar_ws_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:09:46,921 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_ws]/Tripleo::Firewall::Rule[100 zaqar_ws_haproxy_ssl]/Firewall[100 zaqar_ws_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:09:49,118 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ui]/Tripleo::Firewall::Rule[100 ui_haproxy]/Firewall[100 ui_haproxy ipv4]/ensure: created[0m
2018-12-03 06:09:50,765 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ui]/Tripleo::Firewall::Rule[100 ui_haproxy]/Firewall[100 ui_haproxy ipv6]/ensure: created[0m
2018-12-03 06:09:52,974 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ui]/Tripleo::Firewall::Rule[100 ui_haproxy_ssl]/Firewall[100 ui_haproxy_ssl ipv4]/ensure: created[0m
2018-12-03 06:09:54,635 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ui]/Tripleo::Firewall::Rule[100 ui_haproxy_ssl]/Firewall[100 ui_haproxy_ssl ipv6]/ensure: created[0m
2018-12-03 06:09:55,274 INFO: [mNotice: /Stage[main]/Keepalived::Service/Service[keepalived]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:10:04,954 INFO: [mNotice: /Stage[main]/Rabbitmq::Install/Package[rabbitmq-server]/ensure: created[0m
2018-12-03 06:10:04,959 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[/etc/rabbitmq]/owner: owner changed 'rabbitmq' to 'root'[0m
2018-12-03 06:10:04,960 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[/etc/rabbitmq]/group: group changed 'rabbitmq' to 'root'[0m
2018-12-03 06:10:04,967 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[/etc/rabbitmq/ssl]/ensure: created[0m
2018-12-03 06:10:04,977 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[rabbitmq.config]/content: content changed '{md5}b346ec0a8320f85f795bf612f6b02da7' to '{md5}350e5a8df33b06d7ac6f1e8bb24adaaf'[0m
2018-12-03 06:10:04,977 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[rabbitmq.config]/owner: owner changed 'rabbitmq' to 'root'[0m
2018-12-03 06:10:04,978 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[rabbitmq.config]/mode: mode changed '0644' to '0640'[0m
2018-12-03 06:10:04,986 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[rabbitmq-env.config]/ensure: defined content as '{md5}390e4051ad2dc97e52b6efec7b1df680'[0m
2018-12-03 06:10:04,992 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[rabbitmq-inetrc]/ensure: defined content as '{md5}12f8d1a1f9f57f23c1be6c7bf2286e73'[0m
2018-12-03 06:10:04,998 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[rabbitmqadmin.conf]/ensure: defined content as '{md5}44d4ef5cb86ab30e6127e83939ef09c4'[0m
2018-12-03 06:10:05,000 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[/etc/systemd/system/rabbitmq-server.service.d]/ensure: created[0m
2018-12-03 06:10:05,008 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[/etc/systemd/system/rabbitmq-server.service.d/limits.conf]/ensure: defined content as '{md5}8eb9ff6c576b9869944215af3a568c2e'[0m
2018-12-03 06:10:05,130 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/Exec[rabbitmq-systemd-reload]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:10:05,138 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[/etc/security/limits.d/rabbitmq-server.conf]/ensure: defined content as '{md5}5ddc6ba5fcaeddd5b1565e5adfda5236'[0m
2018-12-03 06:10:08,248 INFO: [mNotice: /Stage[main]/Rabbitmq/Rabbitmq_plugin[rabbitmq_management]/ensure: created[0m
2018-12-03 06:10:13,270 INFO: [mNotice: /Stage[main]/Rabbitmq::Service/Service[rabbitmq-server]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:10:13,312 INFO: [mNotice: /Stage[main]/Rabbitmq::Install::Rabbitmqadmin/Archive[rabbitmqadmin]/ensure: download archive from http://172.16.0.1:15672/cli/rabbitmqadmin to /var/lib/rabbitmq/rabbitmqadmin  without cleanup[0m
2018-12-03 06:10:13,321 INFO: [mNotice: /Stage[main]/Rabbitmq::Install::Rabbitmqadmin/File[/usr/local/bin/rabbitmqadmin]/ensure: defined content as '{md5}76394723569012aa8a197a08f1b53926'[0m
2018-12-03 06:10:13,428 INFO: [mNotice: /Stage[main]/Glance::Db::Mysql/Openstacklib::Db::Mysql[glance]/Openstacklib::Db::Mysql::Host_access[glance_%]/Mysql_user[glance@%]/ensure: created[0m
2018-12-03 06:10:13,636 INFO: [mNotice: /Stage[main]/Glance::Db::Mysql/Openstacklib::Db::Mysql[glance]/Openstacklib::Db::Mysql::Host_access[glance_%]/Mysql_grant[glance@%/glance.*]/ensure: created[0m
2018-12-03 06:10:13,776 INFO: [mNotice: /Stage[main]/Glance::Db::Mysql/Openstacklib::Db::Mysql[glance]/Openstacklib::Db::Mysql::Host_access[glance_172.16.0.1]/Mysql_user[glance@172.16.0.1]/ensure: created[0m
2018-12-03 06:10:13,813 INFO: [mNotice: /Stage[main]/Glance::Db::Mysql/Openstacklib::Db::Mysql[glance]/Openstacklib::Db::Mysql::Host_access[glance_172.16.0.1]/Mysql_grant[glance@172.16.0.1/glance.*]/ensure: created[0m
2018-12-03 06:10:13,850 INFO: [mNotice: /Stage[main]/Glance::Deps/Anchor[glance::db::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:10:13,954 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql/Openstacklib::Db::Mysql[nova]/Openstacklib::Db::Mysql::Host_access[nova_%]/Mysql_user[nova@%]/ensure: created[0m
2018-12-03 06:10:13,991 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql/Openstacklib::Db::Mysql[nova]/Openstacklib::Db::Mysql::Host_access[nova_%]/Mysql_grant[nova@%/nova.*]/ensure: created[0m
2018-12-03 06:10:14,131 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql/Openstacklib::Db::Mysql[nova]/Openstacklib::Db::Mysql::Host_access[nova_172.16.0.1]/Mysql_user[nova@172.16.0.1]/ensure: created[0m
2018-12-03 06:10:14,168 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql/Openstacklib::Db::Mysql[nova]/Openstacklib::Db::Mysql::Host_access[nova_172.16.0.1]/Mysql_grant[nova@172.16.0.1/nova.*]/ensure: created[0m
2018-12-03 06:10:14,241 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql/Openstacklib::Db::Mysql[nova_cell0]/Openstacklib::Db::Mysql::Host_access[nova_cell0_%]/Mysql_grant[nova@%/nova_cell0.*]/ensure: created[0m
2018-12-03 06:10:14,312 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql/Openstacklib::Db::Mysql[nova_cell0]/Openstacklib::Db::Mysql::Host_access[nova_cell0_172.16.0.1]/Mysql_grant[nova@172.16.0.1/nova_cell0.*]/ensure: created[0m
2018-12-03 06:10:14,453 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql_api/Openstacklib::Db::Mysql[nova_api]/Openstacklib::Db::Mysql::Host_access[nova_api_%]/Mysql_user[nova_api@%]/ensure: created[0m
2018-12-03 06:10:14,489 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql_api/Openstacklib::Db::Mysql[nova_api]/Openstacklib::Db::Mysql::Host_access[nova_api_%]/Mysql_grant[nova_api@%/nova_api.*]/ensure: created[0m
2018-12-03 06:10:14,628 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql_api/Openstacklib::Db::Mysql[nova_api]/Openstacklib::Db::Mysql::Host_access[nova_api_172.16.0.1]/Mysql_user[nova_api@172.16.0.1]/ensure: created[0m
2018-12-03 06:10:14,665 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql_api/Openstacklib::Db::Mysql[nova_api]/Openstacklib::Db::Mysql::Host_access[nova_api_172.16.0.1]/Mysql_grant[nova_api@172.16.0.1/nova_api.*]/ensure: created[0m
2018-12-03 06:10:14,808 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql_placement/Openstacklib::Db::Mysql[nova_placement]/Openstacklib::Db::Mysql::Host_access[nova_placement_%]/Mysql_user[nova_placement@%]/ensure: created[0m
2018-12-03 06:10:14,845 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql_placement/Openstacklib::Db::Mysql[nova_placement]/Openstacklib::Db::Mysql::Host_access[nova_placement_%]/Mysql_grant[nova_placement@%/nova_placement.*]/ensure: created[0m
2018-12-03 06:10:14,984 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql_placement/Openstacklib::Db::Mysql[nova_placement]/Openstacklib::Db::Mysql::Host_access[nova_placement_172.16.0.1]/Mysql_user[nova_placement@172.16.0.1]/ensure: created[0m
2018-12-03 06:10:15,021 INFO: [mNotice: /Stage[main]/Nova::Db::Mysql_placement/Openstacklib::Db::Mysql[nova_placement]/Openstacklib::Db::Mysql::Host_access[nova_placement_172.16.0.1]/Mysql_grant[nova_placement@172.16.0.1/nova_placement.*]/ensure: created[0m
2018-12-03 06:10:15,058 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::db::end]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:10:15,059 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::dbsync_api::begin]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:10:22,294 INFO: [mNotice: /Stage[main]/Nova::Db::Sync_api/Exec[nova-db-sync-api]/returns: executed successfully[0m
2018-12-03 06:10:27,483 INFO: [mNotice: /Stage[main]/Nova::Db::Sync_api/Exec[nova-db-sync-api]: Triggered 'refresh' from 4 events[0m
2018-12-03 06:10:27,484 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::dbsync_api::end]: Triggered 'refresh' from 2 events[0m
2018-12-03 06:10:27,486 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::cell_v2::begin]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:10:27,487 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::db_online_data_migrations::begin]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:10:32,890 INFO: [mNotice: /Stage[main]/Nova::Cell_v2::Map_cell0/Exec[nova-cell_v2-map_cell0]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:10:32,892 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::cell_v2::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:10:44,130 INFO: [mNotice: /Stage[main]/Nova::Cell_v2::Simple_setup/Nova_cell_v2[default]/ensure: created[0m
2018-12-03 06:10:44,237 INFO: [mNotice: /Stage[main]/Neutron::Db::Mysql/Openstacklib::Db::Mysql[neutron]/Openstacklib::Db::Mysql::Host_access[neutron_%]/Mysql_user[neutron@%]/ensure: created[0m
2018-12-03 06:10:44,273 INFO: [mNotice: /Stage[main]/Neutron::Db::Mysql/Openstacklib::Db::Mysql[neutron]/Openstacklib::Db::Mysql::Host_access[neutron_%]/Mysql_grant[neutron@%/neutron.*]/ensure: created[0m
2018-12-03 06:10:44,413 INFO: [mNotice: /Stage[main]/Neutron::Db::Mysql/Openstacklib::Db::Mysql[neutron]/Openstacklib::Db::Mysql::Host_access[neutron_172.16.0.1]/Mysql_user[neutron@172.16.0.1]/ensure: created[0m
2018-12-03 06:10:44,449 INFO: [mNotice: /Stage[main]/Neutron::Db::Mysql/Openstacklib::Db::Mysql[neutron]/Openstacklib::Db::Mysql::Host_access[neutron_172.16.0.1]/Mysql_grant[neutron@172.16.0.1/neutron.*]/ensure: created[0m
2018-12-03 06:10:44,487 INFO: [mNotice: /Stage[main]/Neutron::Deps/Anchor[neutron::db::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:10:44,591 INFO: [mNotice: /Stage[main]/Heat::Db::Mysql/Openstacklib::Db::Mysql[heat]/Openstacklib::Db::Mysql::Host_access[heat_%]/Mysql_user[heat@%]/ensure: created[0m
2018-12-03 06:10:44,628 INFO: [mNotice: /Stage[main]/Heat::Db::Mysql/Openstacklib::Db::Mysql[heat]/Openstacklib::Db::Mysql::Host_access[heat_%]/Mysql_grant[heat@%/heat.*]/ensure: created[0m
2018-12-03 06:10:44,768 INFO: [mNotice: /Stage[main]/Heat::Db::Mysql/Openstacklib::Db::Mysql[heat]/Openstacklib::Db::Mysql::Host_access[heat_172.16.0.1]/Mysql_user[heat@172.16.0.1]/ensure: created[0m
2018-12-03 06:10:44,805 INFO: [mNotice: /Stage[main]/Heat::Db::Mysql/Openstacklib::Db::Mysql[heat]/Openstacklib::Db::Mysql::Host_access[heat_172.16.0.1]/Mysql_grant[heat@172.16.0.1/heat.*]/ensure: created[0m
2018-12-03 06:10:44,842 INFO: [mNotice: /Stage[main]/Heat::Deps/Anchor[heat::db::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:10:44,946 INFO: [mNotice: /Stage[main]/Ironic::Db::Mysql/Openstacklib::Db::Mysql[ironic]/Openstacklib::Db::Mysql::Host_access[ironic_%]/Mysql_user[ironic@%]/ensure: created[0m
2018-12-03 06:10:44,984 INFO: [mNotice: /Stage[main]/Ironic::Db::Mysql/Openstacklib::Db::Mysql[ironic]/Openstacklib::Db::Mysql::Host_access[ironic_%]/Mysql_grant[ironic@%/ironic.*]/ensure: created[0m
2018-12-03 06:10:45,123 INFO: [mNotice: /Stage[main]/Ironic::Db::Mysql/Openstacklib::Db::Mysql[ironic]/Openstacklib::Db::Mysql::Host_access[ironic_172.16.0.1]/Mysql_user[ironic@172.16.0.1]/ensure: created[0m
2018-12-03 06:10:45,159 INFO: [mNotice: /Stage[main]/Ironic::Db::Mysql/Openstacklib::Db::Mysql[ironic]/Openstacklib::Db::Mysql::Host_access[ironic_172.16.0.1]/Mysql_grant[ironic@172.16.0.1/ironic.*]/ensure: created[0m
2018-12-03 06:10:45,197 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::db::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:10:45,302 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Db::Mysql/Openstacklib::Db::Mysql[ironic-inspector]/Openstacklib::Db::Mysql::Host_access[ironic-inspector_%]/Mysql_user[ironic-inspector@%]/ensure: created[0m
2018-12-03 06:10:45,338 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Db::Mysql/Openstacklib::Db::Mysql[ironic-inspector]/Openstacklib::Db::Mysql::Host_access[ironic-inspector_%]/Mysql_grant[ironic-inspector@%/ironic-inspector.*]/ensure: created[0m
2018-12-03 06:10:45,482 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Db::Mysql/Openstacklib::Db::Mysql[ironic-inspector]/Openstacklib::Db::Mysql::Host_access[ironic-inspector_172.16.0.1]/Mysql_user[ironic-inspector@172.16.0.1]/ensure: created[0m
2018-12-03 06:10:45,518 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Db::Mysql/Openstacklib::Db::Mysql[ironic-inspector]/Openstacklib::Db::Mysql::Host_access[ironic-inspector_172.16.0.1]/Mysql_grant[ironic-inspector@172.16.0.1/ironic-inspector.*]/ensure: created[0m
2018-12-03 06:10:46,911 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Db::Sync/Exec[ironic-inspector-dbsync]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:10:46,913 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic-inspector::dbsync::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:10:46,914 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic-inspector::service::begin]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:10:49,356 INFO: [mNotice: /Stage[main]/Swift::Storage::Account/Swift::Storage::Generic[account]/Package[swift-account]/ensure: created[0m
2018-12-03 06:10:51,873 INFO: [mNotice: /Stage[main]/Swift::Storage::Container/Swift::Storage::Generic[container]/Package[swift-container]/ensure: created[0m
2018-12-03 06:10:54,390 INFO: [mNotice: /Stage[main]/Swift::Storage::Object/Swift::Storage::Generic[object]/Package[swift-object]/ensure: created[0m
2018-12-03 06:10:54,392 INFO: [mNotice: /Stage[main]/Swift::Deps/Anchor[swift::install::end]: Triggered 'refresh' from 5 events[0m
2018-12-03 06:10:54,404 INFO: [mNotice: /Stage[main]/Swift/File[/var/lib/swift]/group: group changed 'root' to 'swift'[0m
2018-12-03 06:10:54,408 INFO: [mNotice: /Stage[main]/Swift/File[/etc/swift/swift.conf]/owner: owner changed 'root' to 'swift'[0m
2018-12-03 06:10:54,412 INFO: [mNotice: /Stage[main]/Swift/Swift_config[swift-hash/swift_hash_path_suffix]/value: value changed '%SWIFT_HASH_PATH_SUFFIX%' to '3b40319cbc5c5439e83c1ba3cceb5a69eb9a44ba'[0m
2018-12-03 06:10:54,416 INFO: [mNotice: /Stage[main]/Swift/Swift_config[swift-constraints/max_header_size]/ensure: created[0m
2018-12-03 06:10:54,422 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/bind_ip]/ensure: created[0m
2018-12-03 06:10:54,424 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/workers]/value: value changed '8' to '2'[0m
2018-12-03 06:10:54,428 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/log_name]/ensure: created[0m
2018-12-03 06:10:54,431 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/log_facility]/ensure: created[0m
2018-12-03 06:10:54,437 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/log_level]/ensure: created[0m
2018-12-03 06:10:54,440 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/log_headers]/ensure: created[0m
2018-12-03 06:10:54,444 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/log_address]/ensure: created[0m
2018-12-03 06:10:54,449 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[pipeline:main/pipeline]/value: value changed 'catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk tempurl ratelimit copy container-quotas account-quotas slo dlo versioned_writes proxy-logging proxy-server' to 'catch_errors healthcheck proxy-logging cache ratelimit bulk tempurl formpost authtoken keystone staticweb copy slo dlo versioned_writes proxy-logging proxy-server'[0m
2018-12-03 06:10:54,453 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/set log_name]/ensure: created[0m
2018-12-03 06:10:54,456 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/set log_facility]/ensure: created[0m
2018-12-03 06:10:54,460 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/set log_level]/ensure: created[0m
2018-12-03 06:10:54,463 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/set log_address]/ensure: created[0m
2018-12-03 06:10:54,467 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/log_handoffs]/ensure: created[0m
2018-12-03 06:10:54,470 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/allow_account_management]/value: value changed 'true' to 'True'[0m
2018-12-03 06:10:54,472 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/account_autocreate]/value: value changed 'true' to 'True'[0m
2018-12-03 06:10:54,478 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/node_timeout]/ensure: created[0m
2018-12-03 06:10:54,481 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/cors_allow_origin]/ensure: created[0m
2018-12-03 06:10:54,484 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/strict_cors_mode]/ensure: created[0m
2018-12-03 06:10:54,497 INFO: [mNotice: /Stage[main]/Swift::Proxy::Bulk/Swift_proxy_config[filter:bulk/max_containers_per_extraction]/ensure: created[0m
2018-12-03 06:10:54,502 INFO: [mNotice: /Stage[main]/Swift::Proxy::Bulk/Swift_proxy_config[filter:bulk/max_failed_extractions]/ensure: created[0m
2018-12-03 06:10:54,505 INFO: [mNotice: /Stage[main]/Swift::Proxy::Bulk/Swift_proxy_config[filter:bulk/max_deletes_per_request]/ensure: created[0m
2018-12-03 06:10:54,509 INFO: [mNotice: /Stage[main]/Swift::Proxy::Bulk/Swift_proxy_config[filter:bulk/yield_frequency]/ensure: created[0m
2018-12-03 06:10:54,518 INFO: [mNotice: /Stage[main]/Swift::Proxy::Keystone/Swift_proxy_config[filter:keystone/reseller_prefix]/ensure: created[0m
2018-12-03 06:10:54,521 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/File[/var/cache/swift]/mode: mode changed '0755' to '0700'[0m
2018-12-03 06:10:54,526 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/log_name]/ensure: created[0m
2018-12-03 06:10:54,527 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/signing_dir]/value: value changed '/tmp/keystone-signing-swift' to '/var/cache/swift'[0m
2018-12-03 06:10:54,532 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/www_authenticate_uri]/ensure: created[0m
2018-12-03 06:10:54,535 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/auth_url]/ensure: created[0m
2018-12-03 06:10:54,538 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/auth_plugin]/ensure: created[0m
2018-12-03 06:10:54,541 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/project_domain_id]/ensure: created[0m
2018-12-03 06:10:54,544 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/user_domain_id]/ensure: created[0m
2018-12-03 06:10:54,549 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/project_name]/ensure: created[0m
2018-12-03 06:10:55,052 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/username]/ensure: created[0m
2018-12-03 06:10:55,057 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/password]/ensure: created[0m
2018-12-03 06:10:55,061 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/delay_auth_decision]/ensure: created[0m
2018-12-03 06:10:55,063 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/cache]/ensure: created[0m
2018-12-03 06:10:55,066 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/include_service_catalog]/ensure: created[0m
2018-12-03 06:10:55,070 INFO: [mNotice: /Stage[main]/Swift::Proxy::Staticweb/Swift_proxy_config[filter:staticweb/use]/ensure: created[0m
2018-12-03 06:10:55,077 INFO: [mNotice: /Stage[main]/Swift::Proxy::Copy/Swift_proxy_config[filter:copy/object_post_as_copy]/value: value changed 'false' to 'True'[0m
2018-12-03 06:10:55,083 INFO: [mNotice: /Stage[main]/Swift::Proxy::Slo/Swift_proxy_config[filter:slo/max_manifest_segments]/ensure: created[0m
2018-12-03 06:10:55,085 INFO: [mNotice: /Stage[main]/Swift::Proxy::Slo/Swift_proxy_config[filter:slo/max_manifest_size]/ensure: created[0m
2018-12-03 06:10:55,090 INFO: [mNotice: /Stage[main]/Swift::Proxy::Slo/Swift_proxy_config[filter:slo/min_segment_size]/ensure: created[0m
2018-12-03 06:10:55,092 INFO: [mNotice: /Stage[main]/Swift::Proxy::Slo/Swift_proxy_config[filter:slo/rate_limit_after_segment]/ensure: created[0m
2018-12-03 06:10:55,097 INFO: [mNotice: /Stage[main]/Swift::Proxy::Slo/Swift_proxy_config[filter:slo/rate_limit_segments_per_sec]/ensure: created[0m
2018-12-03 06:10:55,099 INFO: [mNotice: /Stage[main]/Swift::Proxy::Slo/Swift_proxy_config[filter:slo/max_get_time]/ensure: created[0m
2018-12-03 06:10:55,105 INFO: [mNotice: /Stage[main]/Swift::Proxy::Dlo/Swift_proxy_config[filter:dlo/rate_limit_after_segment]/ensure: created[0m
2018-12-03 06:10:55,108 INFO: [mNotice: /Stage[main]/Swift::Proxy::Dlo/Swift_proxy_config[filter:dlo/rate_limit_segments_per_sec]/ensure: created[0m
2018-12-03 06:10:55,111 INFO: [mNotice: /Stage[main]/Swift::Proxy::Dlo/Swift_proxy_config[filter:dlo/max_get_time]/ensure: created[0m
2018-12-03 06:10:55,117 INFO: [mNotice: /Stage[main]/Swift::Proxy::Versioned_writes/Swift_proxy_config[filter:versioned_writes/allow_versioned_writes]/ensure: created[0m
2018-12-03 06:10:55,123 INFO: [mNotice: /Stage[main]/Swift::Proxy::Ratelimit/Swift_proxy_config[filter:ratelimit/clock_accuracy]/ensure: created[0m
2018-12-03 06:10:55,125 INFO: [mNotice: /Stage[main]/Swift::Proxy::Ratelimit/Swift_proxy_config[filter:ratelimit/max_sleep_time_seconds]/ensure: created[0m
2018-12-03 06:10:55,128 INFO: [mNotice: /Stage[main]/Swift::Proxy::Ratelimit/Swift_proxy_config[filter:ratelimit/log_sleep_time_seconds]/ensure: created[0m
2018-12-03 06:10:55,133 INFO: [mNotice: /Stage[main]/Swift::Proxy::Ratelimit/Swift_proxy_config[filter:ratelimit/rate_buffer_seconds]/ensure: created[0m
2018-12-03 06:10:55,136 INFO: [mNotice: /Stage[main]/Swift::Proxy::Ratelimit/Swift_proxy_config[filter:ratelimit/account_ratelimit]/ensure: created[0m
2018-12-03 06:10:55,153 INFO: [mNotice: /Stage[main]/Swift::Proxy::Formpost/Swift_proxy_config[filter:formpost/use]/ensure: created[0m
2018-12-03 06:10:55,159 INFO: [mNotice: /Stage[main]/Main/File[/srv/node]/ensure: created[0m
2018-12-03 06:10:55,163 INFO: [mNotice: /Stage[main]/Main/File[/srv/node/1]/ensure: created[0m
2018-12-03 06:10:55,641 INFO: [mNotice: /Stage[main]/Swift::Ringbuilder/Swift::Ringbuilder::Create[object]/Exec[create_object]/returns: executed successfully[0m
2018-12-03 06:10:56,108 INFO: [1;33mWarning: Unexpected line: Ring file /etc/swift/object.ring.gz not found, probably it hasn't been written yet[0m
2018-12-03 06:10:56,108 INFO: [1;33mWarning: Unexpected line: Devices:   id region zone ip address:port replication ip:port  name weight partitions balance flags meta[0m
2018-12-03 06:10:56,109 INFO: [1;33mWarning: Unexpected line: There are no devices in this ring, or all devices have been deleted[0m
2018-12-03 06:10:56,567 INFO: [mNotice: /Stage[main]/Main/Ring_object_device[172.16.0.1:6000/1]/ensure: created[0m
2018-12-03 06:10:57,033 INFO: [mNotice: /Stage[main]/Swift::Ringbuilder/Swift::Ringbuilder::Create[account]/Exec[create_account]/returns: executed successfully[0m
2018-12-03 06:10:57,493 INFO: [1;33mWarning: Unexpected line: Ring file /etc/swift/account.ring.gz not found, probably it hasn't been written yet[0m
2018-12-03 06:10:57,493 INFO: [1;33mWarning: Unexpected line: Devices:   id region zone ip address:port replication ip:port  name weight partitions balance flags meta[0m
2018-12-03 06:10:57,494 INFO: [1;33mWarning: Unexpected line: There are no devices in this ring, or all devices have been deleted[0m
2018-12-03 06:10:57,966 INFO: [mNotice: /Stage[main]/Main/Ring_account_device[172.16.0.1:6002/1]/ensure: created[0m
2018-12-03 06:10:58,443 INFO: [mNotice: /Stage[main]/Swift::Ringbuilder/Swift::Ringbuilder::Create[container]/Exec[create_container]/returns: executed successfully[0m
2018-12-03 06:10:58,909 INFO: [1;33mWarning: Unexpected line: Ring file /etc/swift/container.ring.gz not found, probably it hasn't been written yet[0m
2018-12-03 06:10:58,909 INFO: [1;33mWarning: Unexpected line: Devices:   id region zone ip address:port replication ip:port  name weight partitions balance flags meta[0m
2018-12-03 06:10:58,910 INFO: [1;33mWarning: Unexpected line: There are no devices in this ring, or all devices have been deleted[0m
2018-12-03 06:10:59,378 INFO: [mNotice: /Stage[main]/Main/Ring_container_device[172.16.0.1:6001/1]/ensure: created[0m
2018-12-03 06:10:59,914 INFO: [mNotice: /Stage[main]/Swift::Ringbuilder/Swift::Ringbuilder::Rebalance[object]/Exec[rebalance_object]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:11:00,446 INFO: [mNotice: /Stage[main]/Swift::Ringbuilder/Swift::Ringbuilder::Rebalance[account]/Exec[rebalance_account]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:11:00,987 INFO: [mNotice: /Stage[main]/Swift::Ringbuilder/Swift::Ringbuilder::Rebalance[container]/Exec[rebalance_container]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:11:00,991 INFO: [mNotice: /Stage[main]/Swift::Storage::Account/Swift::Storage::Generic[account]/File[/etc/swift/account-server/]/owner: owner changed 'root' to 'swift'[0m
2018-12-03 06:11:00,992 INFO: [mNotice: /Stage[main]/Swift::Storage::Account/Swift::Storage::Generic[account]/File[/etc/swift/account-server/]/group: group changed 'root' to 'swift'[0m
2018-12-03 06:11:00,998 INFO: [mNotice: /Stage[main]/Swift::Storage::Container/Swift::Storage::Generic[container]/File[/etc/swift/container-server/]/owner: owner changed 'root' to 'swift'[0m
2018-12-03 06:11:01,003 INFO: [mNotice: /Stage[main]/Swift::Storage::Container/Swift::Storage::Generic[container]/File[/etc/swift/container-server/]/group: group changed 'root' to 'swift'[0m
2018-12-03 06:11:01,007 INFO: [mNotice: /Stage[main]/Swift::Storage::Object/Swift::Storage::Generic[object]/File[/etc/swift/object-server/]/owner: owner changed 'root' to 'swift'[0m
2018-12-03 06:11:01,007 INFO: [mNotice: /Stage[main]/Swift::Storage::Object/Swift::Storage::Generic[object]/File[/etc/swift/object-server/]/group: group changed 'root' to 'swift'[0m
2018-12-03 06:11:01,038 INFO: [mNotice: /Stage[main]/Swift::Storage::All/Swift::Storage::Server[6002]/Concat[/etc/swift/account-server.conf]/File[/etc/swift/account-server.conf]/content: content changed '{md5}07e5a1a1e5a0ab83d745e20680eb32c1' to '{md5}144199aa8b50ff6cf7fbac3fa24b667d'[0m
2018-12-03 06:11:01,039 INFO: [mNotice: /Stage[main]/Swift::Storage::All/Swift::Storage::Server[6002]/Concat[/etc/swift/account-server.conf]/File[/etc/swift/account-server.conf]/mode: mode changed '0640' to '0644'[0m
2018-12-03 06:11:01,064 INFO: [mNotice: /Stage[main]/Swift::Storage::All/Swift::Storage::Server[6001]/Concat[/etc/swift/container-server.conf]/File[/etc/swift/container-server.conf]/content: content changed '{md5}4998257eb89ff63e838b37686ebb1ee7' to '{md5}ff8453ec614df7da5c22e3fad444cb5c'[0m
2018-12-03 06:11:01,065 INFO: [mNotice: /Stage[main]/Swift::Storage::All/Swift::Storage::Server[6001]/Concat[/etc/swift/container-server.conf]/File[/etc/swift/container-server.conf]/mode: mode changed '0640' to '0644'[0m
2018-12-03 06:11:01,089 INFO: [mNotice: /Stage[main]/Swift::Storage::All/Swift::Storage::Server[6000]/Concat[/etc/swift/object-server.conf]/File[/etc/swift/object-server.conf]/content: content changed '{md5}8c3bfdea900f37c8b2cbd5d9fe5d664c' to '{md5}17629e1211783d4e936aa7b2779aeea3'[0m
2018-12-03 06:11:01,090 INFO: [mNotice: /Stage[main]/Swift::Storage::All/Swift::Storage::Server[6000]/Concat[/etc/swift/object-server.conf]/File[/etc/swift/object-server.conf]/mode: mode changed '0640' to '0644'[0m
2018-12-03 06:11:01,094 INFO: [mNotice: /Stage[main]/Swift::Deps/Anchor[swift::config::end]: Triggered 'refresh' from 62 events[0m
2018-12-03 06:11:01,095 INFO: [mNotice: /Stage[main]/Swift::Deps/Anchor[swift::service::begin]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:11:01,212 INFO: [mNotice: /Stage[main]/Mistral::Db::Mysql/Openstacklib::Db::Mysql[mistral]/Openstacklib::Db::Mysql::Host_access[mistral_%]/Mysql_user[mistral@%]/ensure: created[0m
2018-12-03 06:11:01,248 INFO: [mNotice: /Stage[main]/Mistral::Db::Mysql/Openstacklib::Db::Mysql[mistral]/Openstacklib::Db::Mysql::Host_access[mistral_%]/Mysql_grant[mistral@%/mistral.*]/ensure: created[0m
2018-12-03 06:11:01,389 INFO: [mNotice: /Stage[main]/Mistral::Db::Mysql/Openstacklib::Db::Mysql[mistral]/Openstacklib::Db::Mysql::Host_access[mistral_172.16.0.1]/Mysql_user[mistral@172.16.0.1]/ensure: created[0m
2018-12-03 06:11:01,425 INFO: [mNotice: /Stage[main]/Mistral::Db::Mysql/Openstacklib::Db::Mysql[mistral]/Openstacklib::Db::Mysql::Host_access[mistral_172.16.0.1]/Mysql_grant[mistral@172.16.0.1/mistral.*]/ensure: created[0m
2018-12-03 06:11:01,463 INFO: [mNotice: /Stage[main]/Mistral::Deps/Anchor[mistral::db::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:11:01,566 INFO: [mNotice: /Stage[main]/Zaqar::Db::Mysql/Openstacklib::Db::Mysql[zaqar]/Openstacklib::Db::Mysql::Host_access[zaqar_%]/Mysql_user[zaqar@%]/ensure: created[0m
2018-12-03 06:11:01,602 INFO: [mNotice: /Stage[main]/Zaqar::Db::Mysql/Openstacklib::Db::Mysql[zaqar]/Openstacklib::Db::Mysql::Host_access[zaqar_%]/Mysql_grant[zaqar@%/zaqar.*]/ensure: created[0m
2018-12-03 06:11:01,741 INFO: [mNotice: /Stage[main]/Zaqar::Db::Mysql/Openstacklib::Db::Mysql[zaqar]/Openstacklib::Db::Mysql::Host_access[zaqar_172.16.0.1]/Mysql_user[zaqar@172.16.0.1]/ensure: created[0m
2018-12-03 06:11:01,778 INFO: [mNotice: /Stage[main]/Zaqar::Db::Mysql/Openstacklib::Db::Mysql[zaqar]/Openstacklib::Db::Mysql::Host_access[zaqar_172.16.0.1]/Mysql_grant[zaqar@172.16.0.1/zaqar.*]/ensure: created[0m
2018-12-03 06:11:01,816 INFO: [mNotice: /Stage[main]/Zaqar::Deps/Anchor[zaqar::db::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:11:01,887 INFO: [mNotice: /Stage[main]/Haproxy/Haproxy::Instance[haproxy]/Haproxy::Config[haproxy]/Concat[/etc/haproxy/haproxy.cfg]/File[/etc/haproxy/haproxy.cfg]/content: content changed '{md5}1f337186b0e1ba5ee82760cb437fb810' to '{md5}1b8d79a33bff27510b08218227794399'[0m
2018-12-03 06:11:01,888 INFO: [mNotice: /Stage[main]/Haproxy/Haproxy::Instance[haproxy]/Haproxy::Config[haproxy]/Concat[/etc/haproxy/haproxy.cfg]/File[/etc/haproxy/haproxy.cfg]/mode: mode changed '0644' to '0640'[0m
2018-12-03 06:11:02,407 INFO: [mNotice: /Stage[main]/Haproxy/Haproxy::Instance[haproxy]/Haproxy::Service[haproxy]/File[/etc/sysconfig/haproxy]/content: content changed '{md5}2d8c2707741c581482214fefeed95a47' to '{md5}e2410b4501510c91882f106e6db20a3b'[0m
2018-12-03 06:11:02,965 INFO: [mNotice: /Stage[main]/Haproxy/Haproxy::Instance[haproxy]/Haproxy::Service[haproxy]/Service[haproxy]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:11:02,967 INFO: [mNotice: /Stage[main]/Haproxy/Haproxy::Instance[haproxy]/Anchor[haproxy::haproxy::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:11:07,890 INFO: [mNotice: /Stage[main]/Keystone/Package[keystone]/ensure: created[0m
2018-12-03 06:11:07,969 INFO: [mNotice: /Stage[main]/Keystone::Deps/Anchor[keystone::install::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:11:07,997 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[DEFAULT/admin_token]/ensure: created[0m
2018-12-03 06:11:08,023 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[DEFAULT/public_bind_host]/ensure: created[0m
2018-12-03 06:11:08,047 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[DEFAULT/admin_bind_host]/ensure: created[0m
2018-12-03 06:11:08,070 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[DEFAULT/public_port]/ensure: created[0m
2018-12-03 06:11:08,099 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[DEFAULT/admin_port]/ensure: created[0m
2018-12-03 06:11:08,156 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[DEFAULT/public_endpoint]/ensure: created[0m
2018-12-03 06:11:08,190 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[token/driver]/ensure: created[0m
2018-12-03 06:11:08,212 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[token/expiration]/ensure: created[0m
2018-12-03 06:11:08,258 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[ssl/enable]/ensure: created[0m
2018-12-03 06:11:08,350 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[catalog/driver]/ensure: created[0m
2018-12-03 06:11:08,373 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[catalog/template_file]/ensure: created[0m
2018-12-03 06:11:08,964 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[token/provider]/ensure: created[0m
2018-12-03 06:11:09,020 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[eventlet_server/admin_workers]/ensure: created[0m
2018-12-03 06:11:09,042 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[eventlet_server/public_workers]/ensure: created[0m
2018-12-03 06:11:09,046 INFO: [mNotice: /Stage[main]/Keystone/File[/etc/keystone/fernet-keys]/ensure: created[0m
2018-12-03 06:11:09,050 INFO: [mNotice: /Stage[main]/Keystone/File[/etc/keystone/credential-keys]/ensure: created[0m
2018-12-03 06:11:09,073 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[fernet_tokens/key_repository]/ensure: created[0m
2018-12-03 06:11:09,095 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[token/revoke_by_id]/ensure: created[0m
2018-12-03 06:11:09,118 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[fernet_tokens/max_active_keys]/ensure: created[0m
2018-12-03 06:11:09,140 INFO: [mNotice: /Stage[main]/Keystone/Keystone_config[credential/key_repository]/ensure: created[0m
2018-12-03 06:11:09,154 INFO: [mNotice: /Stage[main]/Apache::Mod::Mime/File[mime.conf]/ensure: defined content as '{md5}9da85e58f3bd6c780ce76db603b7f028'[0m
2018-12-03 06:11:09,159 INFO: [mNotice: /Stage[main]/Apache::Mod::Mime_magic/File[mime_magic.conf]/ensure: defined content as '{md5}b258529b332429e2ff8344f726a95457'[0m
2018-12-03 06:11:09,164 INFO: [mNotice: /Stage[main]/Apache::Mod::Alias/File[alias.conf]/ensure: defined content as '{md5}983e865be85f5e0daaed7433db82995e'[0m
2018-12-03 06:11:09,170 INFO: [mNotice: /Stage[main]/Apache::Mod::Autoindex/File[autoindex.conf]/ensure: defined content as '{md5}2421a3c6df32c7e38c2a7a22afdf5728'[0m
2018-12-03 06:11:09,176 INFO: [mNotice: /Stage[main]/Apache::Mod::Deflate/File[deflate.conf]/ensure: defined content as '{md5}a045d750d819b1e9dae3fbfb3f20edd5'[0m
2018-12-03 06:11:09,182 INFO: [mNotice: /Stage[main]/Apache::Mod::Dir/File[dir.conf]/ensure: defined content as '{md5}c741d8ea840e6eb999d739eed47c69d7'[0m
2018-12-03 06:11:09,187 INFO: [mNotice: /Stage[main]/Apache::Mod::Negotiation/File[negotiation.conf]/ensure: defined content as '{md5}47284b5580b986a6ba32580b6ffb9fd7'[0m
2018-12-03 06:11:09,192 INFO: [mNotice: /Stage[main]/Apache::Mod::Setenvif/File[setenvif.conf]/ensure: defined content as '{md5}c7ede4173da1915b7ec088201f030c28'[0m
2018-12-03 06:11:09,199 INFO: [mNotice: /Stage[main]/Apache::Mod::Prefork/File[/etc/httpd/conf.modules.d/prefork.conf]/ensure: defined content as '{md5}f58b0483b70b4e73b5f67ff37b8f24a0'[0m
2018-12-03 06:11:09,204 INFO: [mNotice: /Stage[main]/Keystone::Wsgi::Apache/File[/var/www/cgi-bin/keystone]/ensure: created[0m
2018-12-03 06:11:09,212 INFO: [mNotice: /Stage[main]/Keystone::Wsgi::Apache/File[keystone_wsgi_admin]/ensure: defined content as '{md5}d6dda52b0e14d80a652ecf42686d3962'[0m
2018-12-03 06:11:09,220 INFO: [mNotice: /Stage[main]/Keystone::Wsgi::Apache/File[keystone_wsgi_main]/ensure: defined content as '{md5}072422f0d75777ed1783e6910b3ddc58'[0m
2018-12-03 06:11:09,224 INFO: [mNotice: /Stage[main]/Keystone::Cron::Token_flush/Cron[keystone-manage token_flush]/ensure: created[0m
2018-12-03 06:11:09,276 INFO: [mNotice: /Stage[main]/Main/Keystone_config[ec2/driver]/ensure: created[0m
2018-12-03 06:11:09,277 INFO: [mNotice: /Stage[main]/Cinder::Deps/Anchor[cinder::service::end]: Triggered 'refresh' from 36 events[0m
2018-12-03 06:11:09,287 INFO: [mNotice: /Stage[main]/Apache::Mod::Proxy/File[proxy.conf]/ensure: defined content as '{md5}9eab682d8c4c89abd0ff20c1a60b908d'[0m
2018-12-03 06:11:09,314 INFO: [mNotice: /Stage[main]/Keystone::Logging/Oslo::Log[keystone_config]/Keystone_config[DEFAULT/debug]/ensure: created[0m
2018-12-03 06:11:09,373 INFO: [mNotice: /Stage[main]/Keystone::Logging/Oslo::Log[keystone_config]/Keystone_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-03 06:11:10,086 INFO: [mNotice: /Stage[main]/Keystone::Policy/Oslo::Policy[keystone_config]/Keystone_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-03 06:11:10,153 INFO: [mNotice: /Stage[main]/Keystone::Db/Oslo::Db[keystone_config]/Keystone_config[database/connection]/ensure: created[0m
2018-12-03 06:11:10,812 INFO: [mNotice: /Stage[main]/Glance::Deps/Anchor[glance::dbsync::begin]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:11:10,816 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::dbsync::begin]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:11:10,816 INFO: [mNotice: /Stage[main]/Neutron::Deps/Anchor[neutron::dbsync::begin]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:11:10,816 INFO: [mNotice: /Stage[main]/Heat::Deps/Anchor[heat::dbsync::begin]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:11:10,817 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::dbsync::begin]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:11:13,640 INFO: [mNotice: /Stage[main]/Glance::Db::Sync/Exec[glance-manage db_sync]/returns: executed successfully[0m
2018-12-03 06:11:15,406 INFO: [mNotice: /Stage[main]/Glance::Db::Sync/Exec[glance-manage db_sync]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:11:15,411 INFO: [mNotice: /Stage[main]/Glance::Deps/Anchor[glance::dbsync::end]: Triggered 'refresh' from 2 events[0m
2018-12-03 06:11:18,851 INFO: [mNotice: /Stage[main]/Glance::Db::Metadefs/Exec[glance-manage db_load_metadefs]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:11:43,148 INFO: [mNotice: /Stage[main]/Nova::Db::Sync/Exec[nova-db-sync]/returns: executed successfully[0m
2018-12-03 06:11:48,801 INFO: [mNotice: /Stage[main]/Nova::Db::Sync/Exec[nova-db-sync]: Triggered 'refresh' from 4 events[0m
2018-12-03 06:11:48,810 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::dbsync::end]: Triggered 'refresh' from 2 events[0m
2018-12-03 06:11:48,810 INFO: [mNotice: /Stage[main]/Nova::Cron::Archive_deleted_rows/Cron[nova-manage db archive_deleted_rows]/ensure: created[0m
2018-12-03 06:11:54,745 INFO: [mNotice: /Stage[main]/Neutron::Db::Sync/Exec[neutron-db-sync]/returns: executed successfully[0m
2018-12-03 06:11:56,988 INFO: [mNotice: /Stage[main]/Neutron::Db::Sync/Exec[neutron-db-sync]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:11:56,988 INFO: [mNotice: /Stage[main]/Neutron::Deps/Anchor[neutron::dbsync::end]: Triggered 'refresh' from 2 events[0m
2018-12-03 06:11:56,992 INFO: [mNotice: /Stage[main]/Neutron::Deps/Anchor[neutron::service::begin]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:11:57,581 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Service[neutron-dhcp-service]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:11:58,132 INFO: [mNotice: /Stage[main]/Neutron::Agents::L3/Service[neutron-l3]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:11:58,776 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Service[neutron-ovs-agent-service]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:12:01,616 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Service[neutron-destroy-patch-ports-service]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:12:04,159 INFO: [mNotice: /Stage[main]/Heat::Db::Sync/Exec[heat-dbsync]/returns: executed successfully[0m
2018-12-03 06:12:05,873 INFO: [mNotice: /Stage[main]/Heat::Db::Sync/Exec[heat-dbsync]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:12:05,875 INFO: [mNotice: /Stage[main]/Heat::Deps/Anchor[heat::dbsync::end]: Triggered 'refresh' from 2 events[0m
2018-12-03 06:12:08,711 INFO: [mNotice: /Stage[main]/Ironic::Db::Sync/Exec[ironic-dbsync]/returns: executed successfully[0m
2018-12-03 06:12:10,525 INFO: [mNotice: /Stage[main]/Ironic::Db::Sync/Exec[ironic-dbsync]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:12:10,527 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::dbsync::end]: Triggered 'refresh' from 2 events[0m
2018-12-03 06:12:10,528 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::db_online_data_migrations::begin]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:12:14,759 INFO: [mNotice: /Stage[main]/Ironic::Db::Online_data_migrations/Exec[ironic-db-online-data-migrations]/returns: executed successfully[0m
2018-12-03 06:12:18,967 INFO: [mNotice: /Stage[main]/Ironic::Db::Online_data_migrations/Exec[ironic-db-online-data-migrations]: Triggered 'refresh' from 4 events[0m
2018-12-03 06:12:18,968 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::db_online_data_migrations::end]: Triggered 'refresh' from 2 events[0m
2018-12-03 06:12:18,968 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::service::begin]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:12:19,072 INFO: [mNotice: /Stage[main]/Ironic::Api/Service[ironic-api]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:12:19,722 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Service[ironic-conductor]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:12:19,723 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::service::end]: Triggered 'refresh' from 2 events[0m
2018-12-03 06:12:19,723 INFO: [mNotice: /Stage[main]/Mistral::Deps/Anchor[mistral::dbsync::begin]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:12:23,866 INFO: [mNotice: /Stage[main]/Mistral::Db::Sync/Exec[mistral-db-sync]/returns: executed successfully[0m
2018-12-03 06:12:26,389 INFO: [mNotice: /Stage[main]/Mistral::Db::Sync/Exec[mistral-db-sync]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:12:26,389 INFO: [mNotice: /Stage[main]/Zaqar::Deps/Anchor[zaqar::dbsync::begin]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:12:27,671 INFO: [mNotice: /Stage[main]/Zaqar::Db::Sync/Exec[zaqar-db-sync]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:12:27,675 INFO: [mNotice: /Stage[main]/Zaqar::Deps/Anchor[zaqar::dbsync::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:12:27,675 INFO: [mNotice: /Stage[main]/Zaqar::Deps/Anchor[zaqar::service::begin]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:12:27,769 INFO: [mNotice: /Stage[main]/Zaqar::Server/Service[openstack-zaqar]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:12:27,925 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::service::begin]: Triggered 'refresh' from 5 events[0m
2018-12-03 06:12:27,927 INFO: [mNotice: /Stage[main]/Heat::Deps/Anchor[heat::service::begin]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:12:28,021 INFO: [mNotice: /Stage[main]/Heat::Api/Service[heat-api]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:12:28,118 INFO: [mNotice: /Stage[main]/Heat::Api_cfn/Service[heat-api-cfn]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:12:28,788 INFO: [mNotice: /Stage[main]/Heat::Engine/Service[heat-engine]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:12:29,330 INFO: [mNotice: /Stage[main]/Keystone/Oslo::Middleware[keystone_config]/Keystone_config[oslo_middleware/enable_proxy_headers_parsing]/ensure: created[0m
2018-12-03 06:12:29,390 INFO: [mNotice: /Stage[main]/Keystone/Oslo::Messaging::Notifications[keystone_config]/Keystone_config[oslo_messaging_notifications/driver]/ensure: created[0m
2018-12-03 06:12:29,428 INFO: [mNotice: /Stage[main]/Keystone/Oslo::Messaging::Notifications[keystone_config]/Keystone_config[oslo_messaging_notifications/topics]/ensure: created[0m
2018-12-03 06:12:29,556 INFO: [mNotice: /Stage[main]/Keystone/Oslo::Messaging::Rabbit[keystone_config]/Keystone_config[oslo_messaging_rabbit/rabbit_password]/ensure: created[0m
2018-12-03 06:12:29,626 INFO: [mNotice: /Stage[main]/Keystone/Oslo::Messaging::Rabbit[keystone_config]/Keystone_config[oslo_messaging_rabbit/rabbit_userid]/ensure: created[0m
2018-12-03 06:12:29,697 INFO: [mNotice: /Stage[main]/Keystone/Oslo::Messaging::Rabbit[keystone_config]/Keystone_config[oslo_messaging_rabbit/rabbit_host]/ensure: created[0m
2018-12-03 06:12:29,785 INFO: [mNotice: /Stage[main]/Apache/Concat[/etc/httpd/conf/ports.conf]/File[/etc/httpd/conf/ports.conf]/ensure: defined content as '{md5}a7c131f3c51a0718ba2b06eca8a7eae6'[0m
2018-12-03 06:12:29,795 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf/httpd.conf]/content: content changed '{md5}f5e7449c0f17bc856e86011cb5d152ba' to '{md5}63b9c6df02bde45fcc9d0b7519de25e1'[0m
2018-12-03 06:12:30,280 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[log_config]/File[log_config.load]/ensure: defined content as '{md5}785d35cb285e190d589163b45263ca89'[0m
2018-12-03 06:12:30,287 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[systemd]/File[systemd.load]/ensure: defined content as '{md5}26e5d44aae258b3e9d821cbbbd3e2826'[0m
2018-12-03 06:12:30,296 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[unixd]/File[unixd.load]/ensure: defined content as '{md5}0e8468ecc1265f8947b8725f4d1be9c0'[0m
2018-12-03 06:12:30,302 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[authz_host]/File[authz_host.load]/ensure: defined content as '{md5}d1045f54d2798499ca0f030ca0eef920'[0m
2018-12-03 06:12:30,308 INFO: [mNotice: /Stage[main]/Apache::Mod::Actions/Apache::Mod[actions]/File[actions.load]/ensure: defined content as '{md5}599866dfaf734f60f7e2d41ee8235515'[0m
2018-12-03 06:12:30,316 INFO: [mNotice: /Stage[main]/Apache::Mod::Authn_core/Apache::Mod[authn_core]/File[authn_core.load]/ensure: defined content as '{md5}704d6e8b02b0eca0eba4083960d16c52'[0m
2018-12-03 06:12:30,322 INFO: [mNotice: /Stage[main]/Apache::Mod::Cache/Apache::Mod[cache]/File[cache.load]/ensure: defined content as '{md5}01e4d392225b518a65b0f7d6c4e21d29'[0m
2018-12-03 06:12:30,329 INFO: [mNotice: /Stage[main]/Apache::Mod::Ext_filter/Apache::Mod[ext_filter]/File[ext_filter.load]/ensure: defined content as '{md5}76d5e0ac3411a4be57ac33ebe2e52ac8'[0m
2018-12-03 06:12:30,335 INFO: [mNotice: /Stage[main]/Apache::Mod::Mime/Apache::Mod[mime]/File[mime.load]/ensure: defined content as '{md5}e36257b9efab01459141d423cae57c7c'[0m
2018-12-03 06:12:30,342 INFO: [mNotice: /Stage[main]/Apache::Mod::Mime_magic/Apache::Mod[mime_magic]/File[mime_magic.load]/ensure: defined content as '{md5}cb8670bb2fb352aac7ebf3a85d52094c'[0m
2018-12-03 06:12:30,349 INFO: [mNotice: /Stage[main]/Apache::Mod::Rewrite/Apache::Mod[rewrite]/File[rewrite.load]/ensure: defined content as '{md5}26e2683352fc1599f29573ff0d934e79'[0m
2018-12-03 06:12:30,355 INFO: [mNotice: /Stage[main]/Apache::Mod::Speling/Apache::Mod[speling]/File[speling.load]/ensure: defined content as '{md5}f82e9e6b871a276c324c9eeffcec8a61'[0m
2018-12-03 06:12:30,362 INFO: [mNotice: /Stage[main]/Apache::Mod::Suexec/Apache::Mod[suexec]/File[suexec.load]/ensure: defined content as '{md5}c7d5c61c534ba423a79b0ae78ff9be35'[0m
2018-12-03 06:12:30,368 INFO: [mNotice: /Stage[main]/Apache::Mod::Version/Apache::Mod[version]/File[version.load]/ensure: defined content as '{md5}1c9243de22ace4dc8266442c48ae0c92'[0m
2018-12-03 06:12:30,375 INFO: [mNotice: /Stage[main]/Apache::Mod::Vhost_alias/Apache::Mod[vhost_alias]/File[vhost_alias.load]/ensure: defined content as '{md5}eca907865997d50d5130497665c3f82e'[0m
2018-12-03 06:12:30,381 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[auth_digest]/File[auth_digest.load]/ensure: defined content as '{md5}df9e85f8da0b239fe8e698ae7ead4f60'[0m
2018-12-03 06:12:30,387 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[authn_anon]/File[authn_anon.load]/ensure: defined content as '{md5}bf57b94b5aec35476fc2a2dc3861f132'[0m
2018-12-03 06:12:30,393 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[authn_dbm]/File[authn_dbm.load]/ensure: defined content as '{md5}90ee8f8ef1a017cacadfda4225e10651'[0m
2018-12-03 06:12:30,401 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[authz_dbm]/File[authz_dbm.load]/ensure: defined content as '{md5}c1363277984d22f99b70f7dce8753b60'[0m
2018-12-03 06:12:30,407 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[authz_owner]/File[authz_owner.load]/ensure: defined content as '{md5}f30a9be1016df87f195449d9e02d1857'[0m
2018-12-03 06:12:30,413 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[expires]/File[expires.load]/ensure: defined content as '{md5}f0825bad1e470de86ffabeb86dcc5d95'[0m
2018-12-03 06:12:30,419 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[include]/File[include.load]/ensure: defined content as '{md5}88095a914eedc3c2c184dd5d74c3954c'[0m
2018-12-03 06:12:30,425 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[logio]/File[logio.load]/ensure: defined content as '{md5}084533c7a44e9129d0e6df952e2472b6'[0m
2018-12-03 06:12:30,432 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[substitute]/File[substitute.load]/ensure: defined content as '{md5}8077c34a71afcf41c8fc644830935915'[0m
2018-12-03 06:12:30,438 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[usertrack]/File[usertrack.load]/ensure: defined content as '{md5}e95fbbf030fabec98b948f8dc217775c'[0m
2018-12-03 06:12:30,444 INFO: [mNotice: /Stage[main]/Apache::Mod::Alias/Apache::Mod[alias]/File[alias.load]/ensure: defined content as '{md5}3cf2fa309ccae4c29a4b875d0894cd79'[0m
2018-12-03 06:12:30,451 INFO: [mNotice: /Stage[main]/Apache::Mod::Authn_file/Apache::Mod[authn_file]/File[authn_file.load]/ensure: defined content as '{md5}d41656680003d7b890267bb73621c60b'[0m
2018-12-03 06:12:30,459 INFO: [mNotice: /Stage[main]/Apache::Mod::Autoindex/Apache::Mod[autoindex]/File[autoindex.load]/ensure: defined content as '{md5}515cdf5b573e961a60d2931d39248648'[0m
2018-12-03 06:12:30,465 INFO: [mNotice: /Stage[main]/Apache::Mod::Dav/Apache::Mod[dav]/File[dav.load]/ensure: defined content as '{md5}588e496251838c4840c14b28b5aa7881'[0m
2018-12-03 06:12:30,472 INFO: [mNotice: /Stage[main]/Apache::Mod::Dav_fs/File[dav_fs.conf]/ensure: defined content as '{md5}899a57534f3d84efa81887ec93c90c9b'[0m
2018-12-03 06:12:30,478 INFO: [mNotice: /Stage[main]/Apache::Mod::Dav_fs/Apache::Mod[dav_fs]/File[dav_fs.load]/ensure: defined content as '{md5}2996277c73b1cd684a9a3111c355e0d3'[0m
2018-12-03 06:12:30,485 INFO: [mNotice: /Stage[main]/Apache::Mod::Deflate/Apache::Mod[deflate]/File[deflate.load]/ensure: defined content as '{md5}2d1a1afcae0c70557251829a8586eeaf'[0m
2018-12-03 06:12:30,491 INFO: [mNotice: /Stage[main]/Apache::Mod::Dir/Apache::Mod[dir]/File[dir.load]/ensure: defined content as '{md5}1bfb1c2a46d7351fc9eb47c659dee068'[0m
2018-12-03 06:12:30,499 INFO: [mNotice: /Stage[main]/Apache::Mod::Negotiation/Apache::Mod[negotiation]/File[negotiation.load]/ensure: defined content as '{md5}d262ee6a5f20d9dd7f87770638dc2ccd'[0m
2018-12-03 06:12:30,506 INFO: [mNotice: /Stage[main]/Apache::Mod::Setenvif/Apache::Mod[setenvif]/File[setenvif.load]/ensure: defined content as '{md5}ec6c99f7cc8e35bdbcf8028f652c9f6d'[0m
2018-12-03 06:12:30,995 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[auth_basic]/File[auth_basic.load]/ensure: defined content as '{md5}494bcf4b843f7908675d663d8dc1bdc8'[0m
2018-12-03 06:12:31,002 INFO: [mNotice: /Stage[main]/Apache::Mod::Filter/Apache::Mod[filter]/File[filter.load]/ensure: defined content as '{md5}66a1e2064a140c3e7dca7ac33877700e'[0m
2018-12-03 06:12:31,009 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[authz_core]/File[authz_core.load]/ensure: defined content as '{md5}39942569bff2abdb259f9a347c7246bc'[0m
2018-12-03 06:12:31,015 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[access_compat]/File[access_compat.load]/ensure: defined content as '{md5}d5feb88bec4570e2dbc41cce7e0de003'[0m
2018-12-03 06:12:31,050 INFO: [mNotice: /Stage[main]/Apache::Mod::Authz_user/Apache::Mod[authz_user]/File[authz_user.load]/ensure: defined content as '{md5}63594303ee808423679b1ea13dd5a784'[0m
2018-12-03 06:12:31,057 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[authz_groupfile]/File[authz_groupfile.load]/ensure: defined content as '{md5}ae005a36b3ac8c20af36c434561c8a75'[0m
2018-12-03 06:12:31,065 INFO: [mNotice: /Stage[main]/Apache::Mod::Env/Apache::Mod[env]/File[env.load]/ensure: defined content as '{md5}d74184d40d0ee24ba02626a188ee7e1a'[0m
2018-12-03 06:12:31,072 INFO: [mNotice: /Stage[main]/Apache::Mod::Prefork/Apache::Mpm[prefork]/File[/etc/httpd/conf.modules.d/prefork.load]/ensure: defined content as '{md5}157529aafcf03fa491bc924103e4608e'[0m
2018-12-03 06:12:31,080 INFO: [mNotice: /Stage[main]/Apache::Mod::Cgi/Apache::Mod[cgi]/File[cgi.load]/ensure: defined content as '{md5}ac20c5c5779b37ab06b480d6485a0881'[0m
2018-12-03 06:12:31,215 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.d/README]/ensure: removed[0m
2018-12-03 06:12:31,216 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.d/autoindex.conf]/ensure: removed[0m
2018-12-03 06:12:31,219 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.d/userdir.conf]/ensure: removed[0m
2018-12-03 06:12:31,221 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.d/welcome.conf]/ensure: removed[0m
2018-12-03 06:12:31,237 INFO: [mNotice: /Stage[main]/Apache::Mod::Wsgi/File[wsgi.conf]/ensure: defined content as '{md5}8b3feb3fc2563de439920bb2c52cbd11'[0m
2018-12-03 06:12:31,245 INFO: [mNotice: /Stage[main]/Nova::Wsgi::Apache_placement/File[/etc/httpd/conf.d/00-nova-placement-api.conf]/content: content changed '{md5}611e31d39e1635bfabc0aafc51b43d0b' to '{md5}612d455490cfecc4b51db6656ea39240'[0m
2018-12-03 06:12:31,260 INFO: [mNotice: /Stage[main]/Tripleo::Ui/File[/etc/httpd/conf.d/openstack-tripleo-ui.conf]/content: content changed '{md5}0bb5ccf9a90544699ec07adf8028d99a' to '{md5}ec9dfa67b5507ef6f7a8bba6345bc07d'[0m
2018-12-03 06:12:31,279 INFO: [mNotice: /Stage[main]/Apache::Mod::Wsgi/Apache::Mod[wsgi]/File[wsgi.load]/ensure: defined content as '{md5}e1795e051e7aae1f865fde0d3b86a507'[0m
2018-12-03 06:12:31,306 INFO: [mNotice: /Stage[main]/Keystone::Cors/Oslo::Cors[keystone_config]/Keystone_config[cors/allowed_origin]/ensure: created[0m
2018-12-03 06:12:31,378 INFO: [mNotice: /Stage[main]/Nova::Wsgi::Apache_api/Openstacklib::Wsgi::Apache[nova_api_wsgi]/File[/var/www/cgi-bin/nova]/ensure: created[0m
2018-12-03 06:12:31,387 INFO: [mNotice: /Stage[main]/Nova::Wsgi::Apache_api/Openstacklib::Wsgi::Apache[nova_api_wsgi]/File[nova_api_wsgi]/ensure: defined content as '{md5}8bcfb466d72544dd31a4f339243ed669'[0m
2018-12-03 06:12:31,402 INFO: [mNotice: /Stage[main]/Nova::Wsgi::Apache_placement/Openstacklib::Wsgi::Apache[placement_wsgi]/File[placement_wsgi]/ensure: defined content as '{md5}2c992c50344eb1765282cb9fb70126db'[0m
2018-12-03 06:12:37,043 INFO: [mNotice: /Stage[main]/Nova::Conductor/Nova::Generic_service[conductor]/Service[nova-conductor]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:12:42,999 INFO: [mNotice: /Stage[main]/Nova::Scheduler/Nova::Generic_service[scheduler]/Service[nova-scheduler]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:12:43,540 INFO: [mNotice: /Stage[main]/Apache::Mod::Headers/Apache::Mod[headers]/File[headers.load]/ensure: defined content as '{md5}96094c96352002c43ada5bdf8650ff38'[0m
2018-12-03 06:12:43,547 INFO: [mNotice: /Stage[main]/Ironic::Wsgi::Apache/Openstacklib::Wsgi::Apache[ironic_wsgi]/File[/var/www/cgi-bin/ironic]/ensure: created[0m
2018-12-03 06:12:43,555 INFO: [mNotice: /Stage[main]/Ironic::Wsgi::Apache/Openstacklib::Wsgi::Apache[ironic_wsgi]/File[ironic_wsgi]/ensure: defined content as '{md5}1d56c8d9da9a51b60ed54ef55cb43c99'[0m
2018-12-03 06:12:43,565 INFO: [mNotice: /Stage[main]/Apache::Mod::Proxy/Apache::Mod[proxy]/File[proxy.load]/ensure: defined content as '{md5}fe26a0a70f572eb256a3c6c183a62223'[0m
2018-12-03 06:12:43,573 INFO: [mNotice: /Stage[main]/Apache::Mod::Proxy_http/Apache::Mod[proxy_http]/File[proxy_http.load]/ensure: defined content as '{md5}0329b852b123a914fca8b072de61f913'[0m
2018-12-03 06:12:43,580 INFO: [mNotice: /Stage[main]/Apache::Mod::Proxy_wstunnel/Apache::Mod[proxy_wstunnel]/File[proxy_wstunnel.load]/ensure: defined content as '{md5}8036815f495618f4dde9d68796622e1c'[0m
2018-12-03 06:12:44,295 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/00-base.conf]/ensure: removed[0m
2018-12-03 06:12:44,298 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/00-dav.conf]/ensure: removed[0m
2018-12-03 06:12:44,300 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/00-lua.conf]/ensure: removed[0m
2018-12-03 06:12:44,302 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/00-mpm.conf]/ensure: removed[0m
2018-12-03 06:12:44,305 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/00-proxy.conf]/ensure: removed[0m
2018-12-03 06:12:44,307 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/00-systemd.conf]/ensure: removed[0m
2018-12-03 06:12:44,309 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/01-cgi.conf]/ensure: removed[0m
2018-12-03 06:12:44,312 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/10-wsgi.conf]/ensure: removed[0m
2018-12-03 06:12:44,325 INFO: [mNotice: /Stage[main]/Tripleo::Ui/File[/var/www/openstack-tripleo-ui/dist/tripleo_ui_config.js]/ensure: defined content as '{md5}417a6a8b2b286a7388e1c2d13eadd38d'[0m
2018-12-03 06:12:44,333 INFO: [mNotice: /Stage[main]/Zaqar::Wsgi::Apache/Openstacklib::Wsgi::Apache[zaqar_wsgi]/File[/var/www/cgi-bin/zaqar]/ensure: created[0m
2018-12-03 06:12:44,340 INFO: [mNotice: /Stage[main]/Zaqar::Wsgi::Apache/Openstacklib::Wsgi::Apache[zaqar_wsgi]/File[zaqar_wsgi]/ensure: defined content as '{md5}3a0f81ec944ad0c68f7db4b58b1f72d6'[0m
2018-12-03 06:12:44,924 INFO: [mNotice: /Stage[main]/Main/Zaqar::Server_instance[1]/Service[openstack-zaqar@1]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:12:44,926 INFO: [mNotice: /Stage[main]/Zaqar::Deps/Anchor[zaqar::service::end]: Triggered 'refresh' from 2 events[0m
2018-12-03 06:12:44,953 INFO: [mNotice: /Stage[main]/Apache/Apache::Vhost[default]/Concat[15-default.conf]/File[/etc/httpd/conf.d/15-default.conf]/ensure: defined content as '{md5}a430bf4e003be964b419e7aea251c6c4'[0m
2018-12-03 06:12:45,012 INFO: [mNotice: /Stage[main]/Keystone::Wsgi::Apache/Apache::Vhost[keystone_wsgi_main]/Concat[10-keystone_wsgi_main.conf]/File[/etc/httpd/conf.d/10-keystone_wsgi_main.conf]/ensure: defined content as '{md5}80665a698f2df102177de039c7fbbe45'[0m
2018-12-03 06:12:45,044 INFO: [mNotice: /Stage[main]/Keystone::Wsgi::Apache/Apache::Vhost[keystone_wsgi_admin]/Concat[10-keystone_wsgi_admin.conf]/File[/etc/httpd/conf.d/10-keystone_wsgi_admin.conf]/ensure: defined content as '{md5}cd2cc2a8388f5f999a07989031c975fb'[0m
2018-12-03 06:12:45,068 INFO: [mNotice: /Stage[main]/Heat::Wsgi::Apache_api/Heat::Wsgi::Apache[api]/Openstacklib::Wsgi::Apache[heat_api_wsgi]/File[/var/www/cgi-bin/heat]/ensure: created[0m
2018-12-03 06:12:45,076 INFO: [mNotice: /Stage[main]/Heat::Wsgi::Apache_api/Heat::Wsgi::Apache[api]/Openstacklib::Wsgi::Apache[heat_api_wsgi]/File[heat_api_wsgi]/ensure: defined content as '{md5}640891728ce5d46ae40234228561597c'[0m
2018-12-03 06:12:45,085 INFO: [mNotice: /Stage[main]/Heat::Wsgi::Apache_api_cfn/Heat::Wsgi::Apache[api_cfn]/Openstacklib::Wsgi::Apache[heat_api_cfn_wsgi]/File[heat_api_cfn_wsgi]/ensure: defined content as '{md5}c3ae61ab87649c8cdfab8977da2b194b'[0m
2018-12-03 06:12:45,115 INFO: [mNotice: /Stage[main]/Ironic::Pxe/Apache::Vhost[ipxe_vhost]/Concat[10-ipxe_vhost.conf]/File[/etc/httpd/conf.d/10-ipxe_vhost.conf]/ensure: defined content as '{md5}0ffa81700d1dc962149c4ec89737928f'[0m
2018-12-03 06:12:45,671 INFO: [mNotice: /Stage[main]/Tripleo::Ui/Apache::Vhost[tripleo-ui]/Concat[25-tripleo-ui.conf]/File[/etc/httpd/conf.d/25-tripleo-ui.conf]/ensure: defined content as '{md5}3ef602d7a1686e0980dcff438450010e'[0m
2018-12-03 06:12:45,714 INFO: [mNotice: /Stage[main]/Nova::Wsgi::Apache_api/Openstacklib::Wsgi::Apache[nova_api_wsgi]/Apache::Vhost[nova_api_wsgi]/Concat[10-nova_api_wsgi.conf]/File[/etc/httpd/conf.d/10-nova_api_wsgi.conf]/ensure: defined content as '{md5}9fe0302800651bc7e03c93e4c50f82aa'[0m
2018-12-03 06:12:45,746 INFO: [mNotice: /Stage[main]/Nova::Wsgi::Apache_placement/Openstacklib::Wsgi::Apache[placement_wsgi]/Apache::Vhost[placement_wsgi]/Concat[10-placement_wsgi.conf]/File[/etc/httpd/conf.d/10-placement_wsgi.conf]/ensure: defined content as '{md5}10280d194ead53aec1571fbaf448ac12'[0m
2018-12-03 06:12:45,796 INFO: [mNotice: /Stage[main]/Ironic::Wsgi::Apache/Openstacklib::Wsgi::Apache[ironic_wsgi]/Apache::Vhost[ironic_wsgi]/Concat[10-ironic_wsgi.conf]/File[/etc/httpd/conf.d/10-ironic_wsgi.conf]/ensure: defined content as '{md5}e3d3e50b5906289ab7e30f5537396718'[0m
2018-12-03 06:12:45,830 INFO: [mNotice: /Stage[main]/Zaqar::Wsgi::Apache/Openstacklib::Wsgi::Apache[zaqar_wsgi]/Apache::Vhost[zaqar_wsgi]/Concat[10-zaqar_wsgi.conf]/File[/etc/httpd/conf.d/10-zaqar_wsgi.conf]/ensure: defined content as '{md5}6ec4780db46772a4852aa38e7f954cdf'[0m
2018-12-03 06:12:45,869 INFO: [mNotice: /Stage[main]/Heat::Wsgi::Apache_api/Heat::Wsgi::Apache[api]/Openstacklib::Wsgi::Apache[heat_api_wsgi]/Apache::Vhost[heat_api_wsgi]/Concat[10-heat_api_wsgi.conf]/File[/etc/httpd/conf.d/10-heat_api_wsgi.conf]/ensure: defined content as '{md5}208e8a1d54d67b2d5f7742e94474ac64'[0m
2018-12-03 06:12:45,903 INFO: [mNotice: /Stage[main]/Heat::Wsgi::Apache_api_cfn/Heat::Wsgi::Apache[api_cfn]/Openstacklib::Wsgi::Apache[heat_api_cfn_wsgi]/Apache::Vhost[heat_api_cfn_wsgi]/Concat[10-heat_api_cfn_wsgi.conf]/File[/etc/httpd/conf.d/10-heat_api_cfn_wsgi.conf]/ensure: defined content as '{md5}00c136d27a0cbfd291563c330bf70607'[0m
2018-12-03 06:12:45,927 INFO: [mNotice: /Stage[main]/Keystone::Deps/Anchor[keystone::config::end]: Triggered 'refresh' from 41 events[0m
2018-12-03 06:12:47,670 INFO: [mNotice: /Stage[main]/Keystone/Exec[keystone-manage fernet_setup]: Triggered 'refresh' from 2 events[0m
2018-12-03 06:12:49,382 INFO: [mNotice: /Stage[main]/Keystone/Exec[keystone-manage credential_setup]: Triggered 'refresh' from 2 events[0m
2018-12-03 06:12:49,423 INFO: [mNotice: /Stage[main]/Keystone::Db::Mysql/Openstacklib::Db::Mysql[keystone]/Mysql_database[keystone]/ensure: created[0m
2018-12-03 06:12:49,535 INFO: [mNotice: /Stage[main]/Keystone::Db::Mysql/Openstacklib::Db::Mysql[keystone]/Openstacklib::Db::Mysql::Host_access[keystone_%]/Mysql_user[keystone@%]/ensure: created[0m
2018-12-03 06:12:49,574 INFO: [mNotice: /Stage[main]/Keystone::Db::Mysql/Openstacklib::Db::Mysql[keystone]/Openstacklib::Db::Mysql::Host_access[keystone_%]/Mysql_grant[keystone@%/keystone.*]/ensure: created[0m
2018-12-03 06:12:49,719 INFO: [mNotice: /Stage[main]/Keystone::Db::Mysql/Openstacklib::Db::Mysql[keystone]/Openstacklib::Db::Mysql::Host_access[keystone_172.16.0.1]/Mysql_user[keystone@172.16.0.1]/ensure: created[0m
2018-12-03 06:12:49,758 INFO: [mNotice: /Stage[main]/Keystone::Db::Mysql/Openstacklib::Db::Mysql[keystone]/Openstacklib::Db::Mysql::Host_access[keystone_172.16.0.1]/Mysql_grant[keystone@172.16.0.1/keystone.*]/ensure: created[0m
2018-12-03 06:12:49,796 INFO: [mNotice: /Stage[main]/Keystone::Deps/Anchor[keystone::db::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:12:49,797 INFO: [mNotice: /Stage[main]/Keystone::Deps/Anchor[keystone::dbsync::begin]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:12:55,959 INFO: [mNotice: /Stage[main]/Keystone::Db::Sync/Exec[keystone-manage db_sync]/returns: executed successfully[0m
2018-12-03 06:12:58,478 INFO: [mNotice: /Stage[main]/Keystone::Db::Sync/Exec[keystone-manage db_sync]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:12:58,479 INFO: [mNotice: /Stage[main]/Keystone::Deps/Anchor[keystone::dbsync::end]: Triggered 'refresh' from 2 events[0m
2018-12-03 06:13:01,172 INFO: [mNotice: /Stage[main]/Keystone/Exec[keystone-manage bootstrap]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:13:01,174 INFO: [mNotice: /Stage[main]/Keystone::Deps/Anchor[keystone::service::begin]: Triggered 'refresh' from 6 events[0m
2018-12-03 06:13:01,935 INFO: [mNotice: /Stage[main]/Apache::Service/Service[httpd]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:13:01,936 INFO: [mNotice: /Stage[main]/Keystone::Deps/Anchor[keystone::service::end]: Triggered 'refresh' from 36 events[0m
2018-12-03 06:13:08,318 INFO: [mNotice: /Stage[main]/Heat::Keystone::Auth/Keystone_role[heat_stack_user]/ensure: created[0m
2018-12-03 06:13:10,211 INFO: [mNotice: /Stage[main]/Swift::Keystone::Auth/Keystone_role[swiftoperator]/ensure: created[0m
2018-12-03 06:13:12,922 INFO: [mNotice: /Stage[main]/Heat::Keystone::Domain/Keystone_domain[heat_stack]/ensure: created[0m
2018-12-03 06:13:17,533 INFO: [mNotice: /Stage[main]/Heat::Keystone::Domain/Keystone_user[heat_admin::heat_stack]/ensure: created[0m
2018-12-03 06:13:21,894 INFO: [mNotice: /Stage[main]/Heat::Keystone::Domain/Keystone_user_role[heat_admin::heat_stack@::heat_stack]/ensure: created[0m
2018-12-03 06:13:24,563 INFO: [mNotice: /Stage[main]/Keystone::Endpoint/Keystone::Resource::Service_identity[keystone]/Keystone_service[keystone::identity]/ensure: created[0m
2018-12-03 06:13:31,340 INFO: [mNotice: /Stage[main]/Keystone::Endpoint/Keystone::Resource::Service_identity[keystone]/Keystone_endpoint[regionOne/keystone::identity]/ensure: created[0m
2018-12-03 06:13:31,361 INFO: [1;33mWarning: Puppet::Type::Keystone_tenant::ProviderOpenstack: Support for a resource without the domain set is deprecated in Liberty cycle. It will be dropped in the M-cycle. Currently using 'Default' as default domain name while the default domain id is '07fb3b8a48e9422db7a69e995f9ae55c'.[0m
2018-12-03 06:13:35,394 INFO: [mNotice: /Stage[main]/Keystone::Roles::Admin/Keystone_tenant[service]/ensure: created[0m
2018-12-03 06:13:35,395 INFO: [mNotice: /Stage[main]/Keystone::Roles::Admin/Keystone_tenant[admin]/description: description changed 'Bootstrap project for initializing the cloud.' to 'admin tenant'[0m
2018-12-03 06:13:41,582 INFO: [mNotice: /Stage[main]/Keystone::Roles::Admin/Keystone_user[admin]/email: defined 'email' as 'root@localhost'[0m
2018-12-03 06:13:51,310 INFO: [mNotice: /Stage[main]/Heat::Keystone::Auth/Keystone::Resource::Service_identity[heat]/Keystone_user[heat]/ensure: created[0m
2018-12-03 06:13:56,922 INFO: [mNotice: /Stage[main]/Heat::Keystone::Auth/Keystone::Resource::Service_identity[heat]/Keystone_user_role[heat@service]/ensure: created[0m
2018-12-03 06:13:58,257 INFO: [mNotice: /Stage[main]/Heat::Keystone::Auth/Keystone::Resource::Service_identity[heat]/Keystone_service[heat::orchestration]/ensure: created[0m
2018-12-03 06:14:03,603 INFO: [mNotice: /Stage[main]/Heat::Keystone::Auth/Keystone::Resource::Service_identity[heat]/Keystone_endpoint[regionOne/heat::orchestration]/ensure: created[0m
2018-12-03 06:14:06,792 INFO: [mNotice: /Stage[main]/Heat::Keystone::Auth_cfn/Keystone::Resource::Service_identity[heat-cfn]/Keystone_user[heat-cfn]/ensure: created[0m
2018-12-03 06:14:11,017 INFO: [mNotice: /Stage[main]/Heat::Keystone::Auth_cfn/Keystone::Resource::Service_identity[heat-cfn]/Keystone_user_role[heat-cfn@service]/ensure: created[0m
2018-12-03 06:14:11,019 INFO: [mNotice: /Stage[main]/Heat::Deps/Anchor[heat::service::end]: Triggered 'refresh' from 5 events[0m
2018-12-03 06:14:12,345 INFO: [mNotice: /Stage[main]/Heat::Keystone::Auth_cfn/Keystone::Resource::Service_identity[heat-cfn]/Keystone_service[heat-cfn::cloudformation]/ensure: created[0m
2018-12-03 06:14:17,718 INFO: [mNotice: /Stage[main]/Heat::Keystone::Auth_cfn/Keystone::Resource::Service_identity[heat-cfn]/Keystone_endpoint[regionOne/heat-cfn::cloudformation]/ensure: created[0m
2018-12-03 06:14:20,879 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Auth/Keystone::Resource::Service_identity[neutron]/Keystone_user[neutron]/ensure: created[0m
2018-12-03 06:14:25,106 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Auth/Keystone::Resource::Service_identity[neutron]/Keystone_user_role[neutron@service]/ensure: created[0m
2018-12-03 06:14:26,448 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Auth/Keystone::Resource::Service_identity[neutron]/Keystone_service[neutron::network]/ensure: created[0m
2018-12-03 06:14:31,785 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Auth/Keystone::Resource::Service_identity[neutron]/Keystone_endpoint[regionOne/neutron::network]/ensure: created[0m
2018-12-03 06:14:45,581 INFO: [mNotice: /Stage[main]/Neutron::Server/Service[neutron-server]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:14:48,740 INFO: [mNotice: /Stage[main]/Glance::Keystone::Auth/Keystone::Resource::Service_identity[glance]/Keystone_user[glance]/ensure: created[0m
2018-12-03 06:14:52,946 INFO: [mNotice: /Stage[main]/Glance::Keystone::Auth/Keystone::Resource::Service_identity[glance]/Keystone_user_role[glance@service]/ensure: created[0m
2018-12-03 06:14:54,270 INFO: [mNotice: /Stage[main]/Glance::Keystone::Auth/Keystone::Resource::Service_identity[glance]/Keystone_service[glance::image]/ensure: created[0m
2018-12-03 06:14:59,609 INFO: [mNotice: /Stage[main]/Glance::Keystone::Auth/Keystone::Resource::Service_identity[glance]/Keystone_endpoint[regionOne/glance::image]/ensure: created[0m
2018-12-03 06:14:59,611 INFO: [mNotice: /Stage[main]/Glance::Deps/Anchor[glance::service::begin]: Triggered 'refresh' from 5 events[0m
2018-12-03 06:15:02,783 INFO: [mNotice: /Stage[main]/Nova::Keystone::Auth/Keystone::Resource::Service_identity[nova]/Keystone_user[nova]/ensure: created[0m
2018-12-03 06:15:06,995 INFO: [mNotice: /Stage[main]/Nova::Keystone::Auth/Keystone::Resource::Service_identity[nova]/Keystone_user_role[nova@service]/ensure: created[0m
2018-12-03 06:15:08,315 INFO: [mNotice: /Stage[main]/Nova::Keystone::Auth/Keystone::Resource::Service_identity[nova]/Keystone_service[nova::compute]/ensure: created[0m
2018-12-03 06:15:13,656 INFO: [mNotice: /Stage[main]/Nova::Keystone::Auth/Keystone::Resource::Service_identity[nova]/Keystone_endpoint[regionOne/nova::compute]/ensure: created[0m
2018-12-03 06:15:16,829 INFO: [mNotice: /Stage[main]/Nova::Keystone::Auth_placement/Keystone::Resource::Service_identity[placement]/Keystone_user[placement]/ensure: created[0m
2018-12-03 06:15:21,068 INFO: [mNotice: /Stage[main]/Nova::Keystone::Auth_placement/Keystone::Resource::Service_identity[placement]/Keystone_user_role[placement@service]/ensure: created[0m
2018-12-03 06:15:22,394 INFO: [mNotice: /Stage[main]/Nova::Keystone::Auth_placement/Keystone::Resource::Service_identity[placement]/Keystone_service[placement::placement]/ensure: created[0m
2018-12-03 06:15:27,762 INFO: [mNotice: /Stage[main]/Nova::Keystone::Auth_placement/Keystone::Resource::Service_identity[placement]/Keystone_endpoint[regionOne/placement::placement]/ensure: created[0m
2018-12-03 06:15:30,928 INFO: [mNotice: /Stage[main]/Swift::Keystone::Auth/Keystone::Resource::Service_identity[swift]/Keystone_user[swift]/ensure: created[0m
2018-12-03 06:15:35,171 INFO: [mNotice: /Stage[main]/Swift::Keystone::Auth/Keystone::Resource::Service_identity[swift]/Keystone_user_role[swift@service]/ensure: created[0m
2018-12-03 06:15:36,493 INFO: [mNotice: /Stage[main]/Swift::Keystone::Auth/Keystone::Resource::Service_identity[swift]/Keystone_service[swift::object-store]/ensure: created[0m
2018-12-03 06:15:41,863 INFO: [mNotice: /Stage[main]/Swift::Keystone::Auth/Keystone::Resource::Service_identity[swift]/Keystone_endpoint[regionOne/swift::object-store]/ensure: created[0m
2018-12-03 06:15:45,037 INFO: [mNotice: /Stage[main]/Ironic::Keystone::Auth/Keystone::Resource::Service_identity[ironic]/Keystone_user[ironic]/ensure: created[0m
2018-12-03 06:15:49,266 INFO: [mNotice: /Stage[main]/Ironic::Keystone::Auth/Keystone::Resource::Service_identity[ironic]/Keystone_user_role[ironic@service]/ensure: created[0m
2018-12-03 06:15:50,585 INFO: [mNotice: /Stage[main]/Ironic::Keystone::Auth/Keystone::Resource::Service_identity[ironic]/Keystone_service[ironic::baremetal]/ensure: created[0m
2018-12-03 06:15:55,952 INFO: [mNotice: /Stage[main]/Ironic::Keystone::Auth/Keystone::Resource::Service_identity[ironic]/Keystone_endpoint[regionOne/ironic::baremetal]/ensure: created[0m
2018-12-03 06:15:59,163 INFO: [mNotice: /Stage[main]/Ironic::Keystone::Auth_inspector/Keystone::Resource::Service_identity[ironic-inspector]/Keystone_user[ironic-inspector]/ensure: created[0m
2018-12-03 06:16:03,380 INFO: [mNotice: /Stage[main]/Ironic::Keystone::Auth_inspector/Keystone::Resource::Service_identity[ironic-inspector]/Keystone_user_role[ironic-inspector@service]/ensure: created[0m
2018-12-03 06:16:04,704 INFO: [mNotice: /Stage[main]/Ironic::Keystone::Auth_inspector/Keystone::Resource::Service_identity[ironic-inspector]/Keystone_service[ironic-inspector::baremetal-introspection]/ensure: created[0m
2018-12-03 06:16:10,027 INFO: [mNotice: /Stage[main]/Ironic::Keystone::Auth_inspector/Keystone::Resource::Service_identity[ironic-inspector]/Keystone_endpoint[regionOne/ironic-inspector::baremetal-introspection]/ensure: created[0m
2018-12-03 06:16:15,664 INFO: [mNotice: /Stage[main]/Nova::Api/Nova::Generic_service[api]/Service[nova-api]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:16:16,827 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift::Service[swift-proxy-server]/Service[swift-proxy-server]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:16:17,473 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift::Service[swift-object-expirer]/Service[swift-object-expirer]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:16:20,645 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Auth/Keystone::Resource::Service_identity[mistral]/Keystone_user[mistral]/ensure: created[0m
2018-12-03 06:16:24,829 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Auth/Keystone::Resource::Service_identity[mistral]/Keystone_user_role[mistral@service]/ensure: created[0m
2018-12-03 06:16:26,146 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Auth/Keystone::Resource::Service_identity[mistral]/Keystone_service[mistral::workflowv2]/ensure: created[0m
2018-12-03 06:16:31,484 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Auth/Keystone::Resource::Service_identity[mistral]/Keystone_endpoint[regionOne/mistral::workflowv2]/ensure: created[0m
2018-12-03 06:16:34,647 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Auth/Keystone::Resource::Service_identity[zaqar]/Keystone_user[zaqar]/ensure: created[0m
2018-12-03 06:16:35,968 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Auth/Keystone::Resource::Service_identity[zaqar]/Keystone_role[ResellerAdmin]/ensure: created[0m
2018-12-03 06:16:41,597 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Auth/Keystone::Resource::Service_identity[zaqar]/Keystone_user_role[zaqar@service]/ensure: created[0m
2018-12-03 06:16:42,922 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Auth/Keystone::Resource::Service_identity[zaqar]/Keystone_service[zaqar::messaging]/ensure: created[0m
2018-12-03 06:16:48,282 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Auth/Keystone::Resource::Service_identity[zaqar]/Keystone_endpoint[regionOne/zaqar::messaging]/ensure: created[0m
2018-12-03 06:16:51,485 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Auth_websocket/Keystone::Resource::Service_identity[zaqar-websocket]/Keystone_user[zaqar-websocket]/ensure: created[0m
2018-12-03 06:16:55,695 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Auth_websocket/Keystone::Resource::Service_identity[zaqar-websocket]/Keystone_user_role[zaqar-websocket@service]/ensure: created[0m
2018-12-03 06:16:57,020 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Auth_websocket/Keystone::Resource::Service_identity[zaqar-websocket]/Keystone_service[zaqar-websocket::messaging-websocket]/ensure: created[0m
2018-12-03 06:17:02,364 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Auth_websocket/Keystone::Resource::Service_identity[zaqar-websocket]/Keystone_endpoint[regionOne/zaqar-websocket::messaging-websocket]/ensure: created[0m
2018-12-03 06:17:03,119 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Service[ironic-neutron-agent-service]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:03,125 INFO: [mNotice: /Stage[main]/Neutron::Deps/Anchor[neutron::service::end]: Triggered 'refresh' from 6 events[0m
2018-12-03 06:17:03,811 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Service[ironic-inspector]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:04,541 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Service[ironic-inspector-dnsmasq]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:04,543 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic-inspector::service::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:17:19,141 INFO: [mNotice: /Stage[main]/Mistral::Db::Sync/Exec[mistral-db-populate]/returns: executed successfully[0m
2018-12-03 06:17:33,628 INFO: [mNotice: /Stage[main]/Mistral::Db::Sync/Exec[mistral-db-populate]: Triggered 'refresh' from 5 events[0m
2018-12-03 06:17:33,629 INFO: [mNotice: /Stage[main]/Mistral::Deps/Anchor[mistral::dbsync::end]: Triggered 'refresh' from 4 events[0m
2018-12-03 06:17:33,630 INFO: [mNotice: /Stage[main]/Mistral::Deps/Anchor[mistral::service::begin]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:17:34,330 INFO: [mNotice: /Stage[main]/Mistral::Api/Service[mistral-api]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:35,045 INFO: [mNotice: /Stage[main]/Mistral::Engine/Service[mistral-engine]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:36,124 INFO: [mNotice: /Stage[main]/Mistral::Executor/Service[mistral-executor]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:36,125 INFO: [mNotice: /Stage[main]/Mistral::Deps/Anchor[mistral::service::end]: Triggered 'refresh' from 3 events[0m
2018-12-03 06:17:42,284 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova::Generic_service[compute]/Service[nova-compute]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:42,286 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::service::end]: Triggered 'refresh' from 4 events[0m
2018-12-03 06:17:42,288 INFO: [mNotice: /Stage[main]/Nova::Logging/File[/var/log/nova/nova-manage.log]/seluser: seluser changed 'unconfined_u' to 'system_u'[0m
2018-12-03 06:17:47,713 INFO: [mNotice: /Stage[main]/Nova::Cell_v2::Discover_hosts/Exec[nova-cell_v2-discover_hosts]: Triggered 'refresh' from 2 events[0m
2018-12-03 06:17:48,426 INFO: [mNotice: /Stage[main]/Swift::Storage::Account/Swift::Service[swift-account-reaper]/Service[swift-account-reaper]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:49,069 INFO: [mNotice: /Stage[main]/Swift::Storage::Container/Swift::Service[swift-container-updater]/Service[swift-container-updater]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:49,733 INFO: [mNotice: /Stage[main]/Swift::Storage::Container/Swift::Service[swift-container-sync]/Service[swift-container-sync]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:50,389 INFO: [mNotice: /Stage[main]/Swift::Storage::Object/Swift::Service[swift-object-updater]/Service[swift-object-updater]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:51,031 INFO: [mNotice: /Stage[main]/Swift::Storage::Object/Swift::Service[swift-object-reconstructor]/Service[swift-object-reconstructor]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:51,705 INFO: [mNotice: /Stage[main]/Swift::Storage::Account/Swift::Storage::Generic[account]/Swift::Service[swift-account-server]/Service[swift-account-server]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:52,331 INFO: [mNotice: /Stage[main]/Swift::Storage::Account/Swift::Storage::Generic[account]/Swift::Service[swift-account-replicator]/Service[swift-account-replicator]: Triggered 'refresh' from 4 events[0m
2018-12-03 06:17:52,435 INFO: [mNotice: /Stage[main]/Swift::Storage::Account/Swift::Storage::Generic[account]/Swift::Service[swift-account-auditor]/Service[swift-account-auditor]: Triggered 'refresh' from 4 events[0m
2018-12-03 06:17:53,104 INFO: [mNotice: /Stage[main]/Swift::Storage::Container/Swift::Storage::Generic[container]/Swift::Service[swift-container-server]/Service[swift-container-server]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:53,210 INFO: [mNotice: /Stage[main]/Swift::Storage::Container/Swift::Storage::Generic[container]/Swift::Service[swift-container-replicator]/Service[swift-container-replicator]: Triggered 'refresh' from 4 events[0m
2018-12-03 06:17:53,317 INFO: [mNotice: /Stage[main]/Swift::Storage::Container/Swift::Storage::Generic[container]/Swift::Service[swift-container-auditor]/Service[swift-container-auditor]: Triggered 'refresh' from 4 events[0m
2018-12-03 06:17:54,013 INFO: [mNotice: /Stage[main]/Swift::Storage::Object/Swift::Storage::Generic[object]/Swift::Service[swift-object-server]/Service[swift-object-server]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:54,112 INFO: [mNotice: /Stage[main]/Swift::Storage::Object/Swift::Storage::Generic[object]/Swift::Service[swift-object-replicator]/Service[swift-object-replicator]: Triggered 'refresh' from 4 events[0m
2018-12-03 06:17:54,213 INFO: [mNotice: /Stage[main]/Swift::Storage::Object/Swift::Storage::Generic[object]/Swift::Service[swift-object-auditor]/Service[swift-object-auditor]: Triggered 'refresh' from 4 events[0m
2018-12-03 06:17:54,214 INFO: [mNotice: /Stage[main]/Swift::Deps/Anchor[swift::service::end]: Triggered 'refresh' from 16 events[0m
2018-12-03 06:17:54,897 INFO: [mNotice: /Stage[main]/Glance::Api/Service[glance-api]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-03 06:17:54,899 INFO: [mNotice: /Stage[main]/Glance::Deps/Anchor[glance::service::end]: Triggered 'refresh' from 1 events[0m
2018-12-03 06:17:57,187 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Post/Tripleo::Firewall::Rule[998 log all]/Firewall[998 log all ipv4]/ensure: created[0m
2018-12-03 06:17:58,971 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Post/Tripleo::Firewall::Rule[998 log all]/Firewall[998 log all ipv6]/ensure: created[0m
2018-12-03 06:18:01,324 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Post/Tripleo::Firewall::Rule[999 drop all]/Firewall[999 drop all ipv4]/ensure: created[0m
2018-12-03 06:18:03,082 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Post/Tripleo::Firewall::Rule[999 drop all]/Firewall[999 drop all ipv6]/ensure: created[0m
2018-12-03 06:18:03,169 INFO: [mNotice: /Stage[main]/Firewall::Linux::Redhat/File[/etc/sysconfig/iptables]/seluser: seluser changed 'unconfined_u' to 'system_u'[0m
2018-12-03 06:18:03,174 INFO: [mNotice: /Stage[main]/Firewall::Linux::Redhat/File[/etc/sysconfig/ip6tables]/seluser: seluser changed 'unconfined_u' to 'system_u'[0m
2018-12-03 06:18:06,444 INFO: [mNotice: Applied catalog in 923.87 seconds[0m
2018-12-03 06:18:06,568 INFO: Changes:
2018-12-03 06:18:06,568 INFO:             Total: 1160
2018-12-03 06:18:06,568 INFO: Events:
2018-12-03 06:18:06,568 INFO:           Success: 1160
2018-12-03 06:18:06,569 INFO:             Total: 1160
2018-12-03 06:18:06,569 INFO: Resources:
2018-12-03 06:18:06,569 INFO:           Changed: 1148
2018-12-03 06:18:06,569 INFO:       Out of sync: 1148
2018-12-03 06:18:06,569 INFO:         Restarted: 116
2018-12-03 06:18:06,570 INFO:             Total: 2934
2018-12-03 06:18:06,570 INFO: Time:
2018-12-03 06:18:06,570 INFO:        Filebucket: 0.00
2018-12-03 06:18:06,570 INFO:        Policy rcd: 0.00
2018-12-03 06:18:06,570 INFO:          Schedule: 0.00
2018-12-03 06:18:06,570 INFO:    Neutron api config: 0.00
2018-12-03 06:18:06,571 INFO:         Resources: 0.00
2018-12-03 06:18:06,571 INFO:      Swift config: 0.00
2018-12-03 06:18:06,571 INFO:       Concat file: 0.00
2018-12-03 06:18:06,571 INFO:    Glance swift config: 0.01
2018-12-03 06:18:06,571 INFO:    Nova paste api ini: 0.01
2018-12-03 06:18:06,572 INFO:            Sysctl: 0.01
2018-12-03 06:18:06,572 INFO:    Ironic neutron agent config: 0.02
2018-12-03 06:18:06,572 INFO:    Swift object expirer config: 0.02
2018-12-03 06:18:06,572 INFO:            Anchor: 0.02
2018-12-03 06:18:06,572 INFO:    Concat fragment: 0.02
2018-12-03 06:18:06,573 INFO:             Group: 0.02
2018-12-03 06:18:06,573 INFO:    Neutron agent ovs: 0.04
2018-12-03 06:18:06,573 INFO:           Archive: 0.04
2018-12-03 06:18:06,573 INFO:    Sysctl runtime: 0.04
2018-12-03 06:18:06,573 INFO:    Neutron dhcp agent config: 0.04
2018-12-03 06:18:06,574 INFO:    Neutron plugin ml2: 0.05
2018-12-03 06:18:06,574 INFO:         Vs bridge: 0.05
2018-12-03 06:18:06,574 INFO:              User: 0.07
2018-12-03 06:18:06,574 INFO:              Cron: 0.07
2018-12-03 06:18:06,574 INFO:    Neutron l3 agent config: 0.23
2018-12-03 06:18:06,575 INFO:    Glance cache config: 0.26
2018-12-03 06:18:06,575 INFO:    Glance registry config: 0.32
2018-12-03 06:18:06,575 INFO:    Mysql database: 0.38
2018-12-03 06:18:06,575 INFO:    Certmonger certificate: 0.55
2018-12-03 06:18:06,576 INFO:    Mistral config: 0.59
2018-12-03 06:18:06,576 INFO:    Swift proxy config: 0.69
2018-12-03 06:18:06,576 INFO:    Ring object device: 0.92
2018-12-03 06:18:06,576 INFO:    Ring account device: 0.93
2018-12-03 06:18:06,577 INFO:    Ring container device: 0.93
2018-12-03 06:18:06,577 INFO:            Augeas: 1.31
2018-12-03 06:18:06,577 INFO:    Keystone domain: 1.33
2018-12-03 06:18:06,577 INFO:    Ironic inspector config: 1.37
2018-12-03 06:18:06,577 INFO:      Zaqar config: 1.56
2018-12-03 06:18:06,577 INFO:       Mysql grant: 1.68
2018-12-03 06:18:06,578 INFO:     Mysql datadir: 10.00
2018-12-03 06:18:06,578 INFO:          Last run: 1543835886
2018-12-03 06:18:06,578 INFO:          Firewall: 168.40
2018-12-03 06:18:06,578 INFO:    Keystone service: 17.22
2018-12-03 06:18:06,578 INFO:        Mysql user: 2.36
2018-12-03 06:18:06,579 INFO:    Keystone tenant: 2.74
2018-12-03 06:18:06,579 INFO:    Neutron config: 2.97
2018-12-03 06:18:06,579 INFO:           Package: 209.09
2018-12-03 06:18:06,579 INFO:       Nova config: 22.14
2018-12-03 06:18:06,579 INFO:    Rabbitmq plugin: 3.11
2018-12-03 06:18:06,580 INFO:              File: 3.66
2018-12-03 06:18:06,580 INFO:       Heat config: 3.80
2018-12-03 06:18:06,580 INFO:    Keystone config: 3.86
2018-12-03 06:18:06,580 INFO:    Config retrieval: 33.87
2018-12-03 06:18:06,580 INFO:    Glance api config: 4.71
2018-12-03 06:18:06,581 INFO:     Keystone role: 4.74
2018-12-03 06:18:06,581 INFO:     Keystone user: 49.36
2018-12-03 06:18:06,581 INFO:      Nova cell v2: 5.38
2018-12-03 06:18:06,581 INFO:    Keystone user role: 62.50
2018-12-03 06:18:06,581 INFO:    Keystone endpoint: 69.63
2018-12-03 06:18:06,582 INFO:           Service: 71.41
2018-12-03 06:18:06,582 INFO:              Exec: 84.75
2018-12-03 06:18:06,582 INFO:             Total: 858.43
2018-12-03 06:18:06,582 INFO:     Ironic config: 9.11
2018-12-03 06:18:06,582 INFO: Version:
2018-12-03 06:18:06,582 INFO:            Config: 1543834928
2018-12-03 06:18:06,583 INFO:            Puppet: 4.8.2
2018-12-03 06:18:25,692 INFO: + rc=2
2018-12-03 06:18:25,692 INFO: + set -e
2018-12-03 06:18:25,693 INFO: + echo 'puppet apply exited with exit code 2'
2018-12-03 06:18:25,693 INFO: puppet apply exited with exit code 2
2018-12-03 06:18:25,693 INFO: + '[' 2 '!=' 2 -a 2 '!=' 0 ']'
2018-12-03 06:18:25,696 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 50-puppet-stack-config completed
2018-12-03 06:18:25,698 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 ----------------------- PROFILING -----------------------
2018-12-03 06:18:25,700 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018
2018-12-03 06:18:25,703 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 Target: configure.d
2018-12-03 06:18:25,705 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018
2018-12-03 06:18:25,707 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 Script                                     Seconds
2018-12-03 06:18:25,709 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 ---------------------------------------  ----------
2018-12-03 06:18:25,710 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018
2018-12-03 06:18:25,722 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 10-hiera-disable                              0.004
2018-12-03 06:18:25,730 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 20-os-apply-config                            0.184
2018-12-03 06:18:25,738 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 30-reload-keepalived                          0.016
2018-12-03 06:18:25,745 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 40-hiera-datafiles                            0.183
2018-12-03 06:18:25,753 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 50-puppet-stack-config                      983.522
2018-12-03 06:18:25,757 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018
2018-12-03 06:18:25,759 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 --------------------- END PROFILING ---------------------
2018-12-03 06:18:25,759 INFO: [2018-12-03 06:18:25,759] (os-refresh-config) [INFO] Completed phase configure
2018-12-03 06:18:25,760 INFO: [2018-12-03 06:18:25,759] (os-refresh-config) [INFO] Starting phase post-configure
2018-12-03 06:18:25,774 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 Running /usr/libexec/os-refresh-config/post-configure.d/10-iptables
2018-12-03 06:18:25,777 INFO: + set -o pipefail
2018-12-03 06:18:25,777 INFO: + EXTERNAL_BRIDGE=br-ctlplane
2018-12-03 06:18:25,778 INFO: + iptables -w -t nat -C PREROUTING -d 169.254.169.254/32 -i br-ctlplane -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 8775
2018-12-03 06:18:25,780 INFO: iptables: No chain/target/match by that name.
2018-12-03 06:18:25,780 INFO: + iptables -w -t nat -I PREROUTING -d 169.254.169.254/32 -i br-ctlplane -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 8775
2018-12-03 06:18:25,795 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 10-iptables completed
2018-12-03 06:18:25,798 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 Running /usr/libexec/os-refresh-config/post-configure.d/80-seedstack-masquerade
2018-12-03 06:18:25,802 INFO: + RULES_SCRIPT=/var/opt/undercloud-stack/masquerade
2018-12-03 06:18:25,803 INFO: + . /var/opt/undercloud-stack/masquerade
2018-12-03 06:18:25,803 INFO: ++ IPTCOMMAND=iptables
2018-12-03 06:18:25,803 INFO: ++ [[ 172.16.0.1 =~ : ]]
2018-12-03 06:18:25,803 INFO: ++ iptables -w -t nat -F BOOTSTACK_MASQ_NEW
2018-12-03 06:18:25,804 INFO: iptables: No chain/target/match by that name.
2018-12-03 06:18:25,804 INFO: ++ true
2018-12-03 06:18:25,804 INFO: ++ iptables -w -t nat -D POSTROUTING -j BOOTSTACK_MASQ_NEW
2018-12-03 06:18:25,806 INFO: iptables v1.4.21: Couldn't load target `BOOTSTACK_MASQ_NEW':No such file or directory
2018-12-03 06:18:25,806 INFO: 
2018-12-03 06:18:25,806 INFO: Try `iptables -h' or 'iptables --help' for more information.
2018-12-03 06:18:25,806 INFO: ++ true
2018-12-03 06:18:25,807 INFO: ++ iptables -w -t nat -X BOOTSTACK_MASQ_NEW
2018-12-03 06:18:25,807 INFO: iptables: No chain/target/match by that name.
2018-12-03 06:18:25,807 INFO: ++ true
2018-12-03 06:18:25,808 INFO: ++ iptables -w -t nat -N BOOTSTACK_MASQ_NEW
2018-12-03 06:18:25,809 INFO: ++ NETWORK=172.16.0.0/24
2018-12-03 06:18:25,810 INFO: ++ NETWORKS=172.16.0.0/24,192.168.24.0/24,
2018-12-03 06:18:25,810 INFO: ++ NETWORKS=172.16.0.0/24,192.168.24.0/24
2018-12-03 06:18:25,810 INFO: ++ iptables -w -t nat -A BOOTSTACK_MASQ_NEW -s 172.16.0.0/24 -d 172.16.0.0/24,192.168.24.0/24 -j RETURN
2018-12-03 06:18:25,811 INFO: ++ iptables -w -t nat -A BOOTSTACK_MASQ_NEW -s 172.16.0.0/24 -j MASQUERADE
2018-12-03 06:18:25,813 INFO: ++ NETWORK=192.168.24.0/24
2018-12-03 06:18:25,813 INFO: ++ NETWORKS=172.16.0.0/24,192.168.24.0/24,
2018-12-03 06:18:25,814 INFO: ++ NETWORKS=172.16.0.0/24,192.168.24.0/24
2018-12-03 06:18:25,814 INFO: ++ iptables -w -t nat -A BOOTSTACK_MASQ_NEW -s 192.168.24.0/24 -d 172.16.0.0/24,192.168.24.0/24 -j RETURN
2018-12-03 06:18:25,815 INFO: ++ iptables -w -t nat -A BOOTSTACK_MASQ_NEW -s 192.168.24.0/24 -j MASQUERADE
2018-12-03 06:18:25,817 INFO: ++ iptables -w -t nat -I POSTROUTING -j BOOTSTACK_MASQ_NEW
2018-12-03 06:18:25,819 INFO: ++ iptables -w -t nat -F BOOTSTACK_MASQ
2018-12-03 06:18:25,820 INFO: iptables: No chain/target/match by that name.
2018-12-03 06:18:25,820 INFO: ++ true
2018-12-03 06:18:25,821 INFO: ++ iptables -w -t nat -D POSTROUTING -j BOOTSTACK_MASQ
2018-12-03 06:18:25,822 INFO: iptables v1.4.21: Couldn't load target `BOOTSTACK_MASQ':No such file or directory
2018-12-03 06:18:25,822 INFO: 
2018-12-03 06:18:25,822 INFO: Try `iptables -h' or 'iptables --help' for more information.
2018-12-03 06:18:25,823 INFO: ++ true
2018-12-03 06:18:25,823 INFO: ++ iptables -w -t nat -X BOOTSTACK_MASQ
2018-12-03 06:18:25,824 INFO: iptables: No chain/target/match by that name.
2018-12-03 06:18:25,824 INFO: ++ true
2018-12-03 06:18:25,824 INFO: ++ iptables -w -t nat -E BOOTSTACK_MASQ_NEW BOOTSTACK_MASQ
2018-12-03 06:18:25,826 INFO: ++ iptables -w -D FORWARD -j REJECT --reject-with icmp-host-prohibited
2018-12-03 06:18:25,828 INFO: + iptables-save
2018-12-03 06:18:25,835 INFO: + /bin/test -f /etc/sysconfig/iptables
2018-12-03 06:18:25,836 INFO: + /bin/grep -q neutron- /etc/sysconfig/iptables
2018-12-03 06:18:25,838 INFO: + /bin/test -f /etc/sysconfig/ip6tables
2018-12-03 06:18:25,839 INFO: + /bin/grep -q neutron- /etc/sysconfig/ip6tables
2018-12-03 06:18:25,841 INFO: + /bin/test -f /etc/sysconfig/iptables
2018-12-03 06:18:25,842 INFO: + /bin/grep -v '\-m comment \--comment' /etc/sysconfig/iptables
2018-12-03 06:18:25,843 INFO: + /bin/grep -q ironic-inspector
2018-12-03 06:18:25,844 INFO: + /bin/test -f /etc/sysconfig/ip6tables
2018-12-03 06:18:25,845 INFO: + /bin/grep -v '\-m comment \--comment' /etc/sysconfig/ip6tables
2018-12-03 06:18:25,846 INFO: + /bin/grep -q ironic-inspector
2018-12-03 06:18:25,850 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 80-seedstack-masquerade completed
2018-12-03 06:18:25,852 INFO: dib-run-parts Mon Dec  3 06:18:25 EST 2018 Running /usr/libexec/os-refresh-config/post-configure.d/98-undercloud-setup
2018-12-03 06:18:25,856 INFO: + source /root/tripleo-undercloud-passwords
2018-12-03 06:18:25,856 INFO: +++ sudo hiera admin_password
2018-12-03 06:18:26,001 INFO: ++ UNDERCLOUD_ADMIN_PASSWORD=527462cc3bc8c7744b741446431c6cfcd6f0d8ee
2018-12-03 06:18:26,002 INFO: +++ sudo hiera keystone::admin_token
2018-12-03 06:18:26,136 INFO: ++ UNDERCLOUD_ADMIN_TOKEN=92a1390dfc94341d01fe9da7ef4e13a8c3e69d4e
2018-12-03 06:18:26,137 INFO: +++ sudo hiera ceilometer::metering_secret
2018-12-03 06:18:26,268 INFO: ++ UNDERCLOUD_CEILOMETER_METERING_SECRET=1ae67fad7ea2873fe97db7ba808f6d672604c937
2018-12-03 06:18:26,269 INFO: +++ sudo hiera ceilometer::keystone::authtoken::password
2018-12-03 06:18:26,401 INFO: ++ UNDERCLOUD_CEILOMETER_PASSWORD=dea401a21e09ba8d8cd4f80683de939aac57ec8f
2018-12-03 06:18:26,401 INFO: +++ sudo hiera snmpd_readonly_user_password
2018-12-03 06:18:26,533 INFO: ++ UNDERCLOUD_CEILOMETER_SNMPD_PASSWORD=nil
2018-12-03 06:18:26,533 INFO: +++ sudo hiera snmpd_readonly_user_name
2018-12-03 06:18:26,665 INFO: ++ UNDERCLOUD_CEILOMETER_SNMPD_USER=nil
2018-12-03 06:18:26,665 INFO: +++ sudo hiera admin_password
2018-12-03 06:18:26,795 INFO: ++ UNDERCLOUD_DB_PASSWORD=527462cc3bc8c7744b741446431c6cfcd6f0d8ee
2018-12-03 06:18:26,796 INFO: +++ sudo hiera glance::api::keystone_password
2018-12-03 06:18:26,927 INFO: ++ UNDERCLOUD_GLANCE_PASSWORD=nil
2018-12-03 06:18:26,927 INFO: +++ sudo hiera tripleo::haproxy::haproxy_stats_password
2018-12-03 06:18:27,058 INFO: ++ UNDERCLOUD_HAPROXY_STATS_PASSWORD=64f9d8d7663243db239b772f79b57b85b0646a44
2018-12-03 06:18:27,058 INFO: +++ sudo hiera heat::engine::auth_encryption_key
2018-12-03 06:18:27,193 INFO: ++ UNDERCLOUD_HEAT_ENCRYPTION_KEY=6045d94c318c54abe94ba2a5594b0ab7
2018-12-03 06:18:27,194 INFO: +++ sudo hiera heat::keystone_password
2018-12-03 06:18:27,326 INFO: ++ UNDERCLOUD_HEAT_PASSWORD=nil
2018-12-03 06:18:27,327 INFO: +++ sudo hiera heat_stack_domain_admin_password
2018-12-03 06:18:27,461 INFO: ++ UNDERCLOUD_HEAT_STACK_DOMAIN_ADMIN_PASSWORD=dfd9981cb4df1327df752ca065e2430a49284b28
2018-12-03 06:18:27,462 INFO: +++ sudo hiera horizon_secret_key
2018-12-03 06:18:27,606 INFO: ++ UNDERCLOUD_HORIZON_SECRET_KEY=9b7ba66180a76f8a66673a8d03fc986dc7b9fbfe
2018-12-03 06:18:27,606 INFO: +++ sudo hiera ironic::api::authtoken::password
2018-12-03 06:18:27,741 INFO: ++ UNDERCLOUD_IRONIC_PASSWORD=1b0ab87e4a1b3414321b1de70e6ba1028626e93a
2018-12-03 06:18:27,741 INFO: +++ sudo hiera neutron::server::auth_password
2018-12-03 06:18:27,873 INFO: ++ UNDERCLOUD_NEUTRON_PASSWORD=nil
2018-12-03 06:18:27,873 INFO: +++ sudo hiera nova::keystone::authtoken::password
2018-12-03 06:18:28,003 INFO: ++ UNDERCLOUD_NOVA_PASSWORD=edf03cfa07e73fb909396b35750947cff6106a03
2018-12-03 06:18:28,004 INFO: +++ sudo hiera rabbit_cookie
2018-12-03 06:18:28,139 INFO: ++ UNDERCLOUD_RABBIT_COOKIE=54456ead6169862b0e117436443859fdd6e3e9dc
2018-12-03 06:18:28,139 INFO: +++ sudo hiera rabbit_password
2018-12-03 06:18:28,273 INFO: ++ UNDERCLOUD_RABBIT_PASSWORD=nil
2018-12-03 06:18:28,274 INFO: +++ sudo hiera rabbit_username
2018-12-03 06:18:28,406 INFO: ++ UNDERCLOUD_RABBIT_USERNAME=nil
2018-12-03 06:18:28,406 INFO: +++ sudo hiera swift::swift_hash_suffix
2018-12-03 06:18:28,541 INFO: ++ UNDERCLOUD_SWIFT_HASH_SUFFIX=nil
2018-12-03 06:18:28,541 INFO: +++ sudo hiera swift::proxy::authtoken::admin_password
2018-12-03 06:18:28,682 INFO: ++ UNDERCLOUD_SWIFT_PASSWORD=nil
2018-12-03 06:18:28,683 INFO: +++ sudo hiera mistral::admin_password
2018-12-03 06:18:28,818 INFO: ++ UNDERCLOUD_MISTRAL_PASSWORD=nil
2018-12-03 06:18:28,819 INFO: +++ sudo hiera zaqar::keystone::authtoken::password
2018-12-03 06:18:28,951 INFO: ++ UNDERCLOUD_ZAQAR_PASSWORD=6115144bed3a148eae14d7e08d98beaa4838ac8b
2018-12-03 06:18:28,951 INFO: +++ sudo hiera cinder::keystone::authtoken::password
2018-12-03 06:18:29,083 INFO: ++ UNDERCLOUD_CINDER_PASSWORD=9b18e8acf1f75069d9ccb7374948b8986486d5c8
2018-12-03 06:18:29,084 INFO: + source /root/stackrc
2018-12-03 06:18:29,084 INFO: +++ set
2018-12-03 06:18:29,084 INFO: +++ awk '{FS="="}  /^OS_/ {print $1}'
2018-12-03 06:18:29,087 INFO: ++ NOVA_VERSION=1.1
2018-12-03 06:18:29,087 INFO: ++ export NOVA_VERSION
2018-12-03 06:18:29,087 INFO: ++ OS_PASSWORD=527462cc3bc8c7744b741446431c6cfcd6f0d8ee
2018-12-03 06:18:29,087 INFO: ++ export OS_PASSWORD
2018-12-03 06:18:29,088 INFO: ++ OS_AUTH_TYPE=password
2018-12-03 06:18:29,088 INFO: ++ export OS_AUTH_TYPE
2018-12-03 06:18:29,088 INFO: ++ OS_AUTH_URL=https://172.16.0.10:13000/
2018-12-03 06:18:29,088 INFO: ++ PYTHONWARNINGS='ignore:Certificate has no, ignore:A true SSLContext object is not available'
2018-12-03 06:18:29,088 INFO: ++ export OS_AUTH_URL
2018-12-03 06:18:29,089 INFO: ++ export PYTHONWARNINGS
2018-12-03 06:18:29,089 INFO: ++ OS_USERNAME=admin
2018-12-03 06:18:29,089 INFO: ++ OS_PROJECT_NAME=admin
2018-12-03 06:18:29,089 INFO: ++ COMPUTE_API_VERSION=1.1
2018-12-03 06:18:29,089 INFO: ++ IRONIC_API_VERSION=1.34
2018-12-03 06:18:29,090 INFO: ++ OS_BAREMETAL_API_VERSION=1.34
2018-12-03 06:18:29,090 INFO: ++ OS_NO_CACHE=True
2018-12-03 06:18:29,090 INFO: ++ OS_CLOUDNAME=undercloud
2018-12-03 06:18:29,090 INFO: ++ export OS_USERNAME
2018-12-03 06:18:29,090 INFO: ++ export OS_PROJECT_NAME
2018-12-03 06:18:29,091 INFO: ++ export COMPUTE_API_VERSION
2018-12-03 06:18:29,091 INFO: ++ export IRONIC_API_VERSION
2018-12-03 06:18:29,091 INFO: ++ export OS_BAREMETAL_API_VERSION
2018-12-03 06:18:29,091 INFO: ++ export OS_NO_CACHE
2018-12-03 06:18:29,091 INFO: ++ export OS_CLOUDNAME
2018-12-03 06:18:29,092 INFO: ++ OS_IDENTITY_API_VERSION=3
2018-12-03 06:18:29,092 INFO: ++ export OS_IDENTITY_API_VERSION
2018-12-03 06:18:29,092 INFO: ++ OS_PROJECT_DOMAIN_NAME=Default
2018-12-03 06:18:29,092 INFO: ++ export OS_PROJECT_DOMAIN_NAME
2018-12-03 06:18:29,092 INFO: ++ OS_USER_DOMAIN_NAME=Default
2018-12-03 06:18:29,093 INFO: ++ export OS_USER_DOMAIN_NAME
2018-12-03 06:18:29,093 INFO: ++ '[' -z '' ']'
2018-12-03 06:18:29,093 INFO: ++ export PS1=
2018-12-03 06:18:29,093 INFO: ++ PS1=
2018-12-03 06:18:29,093 INFO: ++ export 'PS1=${OS_CLOUDNAME:+($OS_CLOUDNAME)} '
2018-12-03 06:18:29,093 INFO: ++ PS1='${OS_CLOUDNAME:+($OS_CLOUDNAME)} '
2018-12-03 06:18:29,094 INFO: ++ export CLOUDPROMPT_ENABLED=1
2018-12-03 06:18:29,094 INFO: ++ CLOUDPROMPT_ENABLED=1
2018-12-03 06:18:29,094 INFO: + INSTACK_ROOT=
2018-12-03 06:18:29,094 INFO: + export INSTACK_ROOT
2018-12-03 06:18:29,094 INFO: + '[' -n '' ']'
2018-12-03 06:18:29,095 INFO: + '[' '!' -f /root/.ssh/authorized_keys ']'
2018-12-03 06:18:29,095 INFO: + '[' '!' -f /root/.ssh/id_rsa ']'
2018-12-03 06:18:29,095 INFO: + ssh-keygen -b 1024 -N '' -f /root/.ssh/id_rsa
2018-12-03 06:18:29,130 INFO: Generating public/private rsa key pair.
2018-12-03 06:18:29,130 INFO: Your identification has been saved in /root/.ssh/id_rsa.
2018-12-03 06:18:29,131 INFO: Your public key has been saved in /root/.ssh/id_rsa.pub.
2018-12-03 06:18:29,131 INFO: The key fingerprint is:
2018-12-03 06:18:29,131 INFO: SHA256:ny61i+8Yzb4h9XoiBvgKB1B/A+eRv65d+52Vvyrl5zs root@undercloud.example.com
2018-12-03 06:18:29,131 INFO: The key's randomart image is:
2018-12-03 06:18:29,132 INFO: +---[RSA 1024]----+
2018-12-03 06:18:29,132 INFO: |  . . o.         |
2018-12-03 06:18:29,132 INFO: | . . +..         |
2018-12-03 06:18:29,132 INFO: |.   . +.         |
2018-12-03 06:18:29,132 INFO: | .   . ..        |
2018-12-03 06:18:29,133 INFO: |  .  .  S..      |
2018-12-03 06:18:29,133 INFO: |   .. . .=.o .  .|
2018-12-03 06:18:29,133 INFO: |  . .. oo.B.+  ..|
2018-12-03 06:18:29,133 INFO: |   o  ..=Oo=.o E.|
2018-12-03 06:18:29,133 INFO: |    ...o+=X*o.*+=|
2018-12-03 06:18:29,133 INFO: +----[SHA256]-----+
2018-12-03 06:18:29,134 INFO: + cat /root/.ssh/id_rsa.pub
2018-12-03 06:18:29,134 INFO: + '[' -e /usr/sbin/getenforce ']'
2018-12-03 06:18:29,134 INFO: ++ getenforce
2018-12-03 06:18:29,135 INFO: + '[' Enforcing == Enforcing ']'
2018-12-03 06:18:29,135 INFO: + set +e
2018-12-03 06:18:29,135 INFO: ++ find /root/.ssh/ -exec ls -lZ '{}' ';'
2018-12-03 06:18:29,136 INFO: ++ grep -v ssh_home_t
2018-12-03 06:18:29,148 INFO: + selinux_wrong_permission=
2018-12-03 06:18:29,148 INFO: + set -e
2018-12-03 06:18:29,148 INFO: + '[' -n '' ']'
2018-12-03 06:18:29,148 INFO: ++ openstack project show admin
2018-12-03 06:18:29,149 INFO: ++ awk '$2=="id" {print $4}'
2018-12-03 06:18:32,568 INFO: + openstack quota set --cores -1 --instances -1 --ram -1 f41e7348daff4254892e5cd759aa8806
2018-12-03 06:18:37,438 INFO: + rm -rf /root/.novaclient
2018-12-03 06:18:37,444 INFO: dib-run-parts Mon Dec  3 06:18:37 EST 2018 98-undercloud-setup completed
2018-12-03 06:18:37,445 INFO: dib-run-parts Mon Dec  3 06:18:37 EST 2018 Running /usr/libexec/os-refresh-config/post-configure.d/99-refresh-completed
2018-12-03 06:18:37,450 INFO: ++ os-apply-config --key completion-handle --type raw --key-default ''
2018-12-03 06:18:37,730 INFO: [2018/12/03 06:18:37 AM] [WARNING] DEPRECATED: falling back to /var/run/os-collect-config/os_config_files.json
2018-12-03 06:18:37,739 INFO: + HANDLE=
2018-12-03 06:18:37,740 INFO: ++ os-apply-config --key completion-signal --type raw --key-default ''
2018-12-03 06:18:38,015 INFO: [2018/12/03 06:18:38 AM] [WARNING] DEPRECATED: falling back to /var/run/os-collect-config/os_config_files.json
2018-12-03 06:18:38,024 INFO: + SIGNAL=
2018-12-03 06:18:38,025 INFO: ++ os-apply-config --key instance-id --type raw --key-default ''
2018-12-03 06:18:38,302 INFO: [2018/12/03 06:18:38 AM] [WARNING] DEPRECATED: falling back to /var/run/os-collect-config/os_config_files.json
2018-12-03 06:18:38,311 INFO: + ID=
2018-12-03 06:18:38,311 INFO: + '[' -n '' ']'
2018-12-03 06:18:38,312 INFO: + exit 0
2018-12-03 06:18:38,316 INFO: dib-run-parts Mon Dec  3 06:18:38 EST 2018 99-refresh-completed completed
2018-12-03 06:18:38,318 INFO: dib-run-parts Mon Dec  3 06:18:38 EST 2018 ----------------------- PROFILING -----------------------
2018-12-03 06:18:38,319 INFO: dib-run-parts Mon Dec  3 06:18:38 EST 2018
2018-12-03 06:18:38,322 INFO: dib-run-parts Mon Dec  3 06:18:38 EST 2018 Target: post-configure.d
2018-12-03 06:18:38,324 INFO: dib-run-parts Mon Dec  3 06:18:38 EST 2018
2018-12-03 06:18:38,326 INFO: dib-run-parts Mon Dec  3 06:18:38 EST 2018 Script                                     Seconds
2018-12-03 06:18:38,328 INFO: dib-run-parts Mon Dec  3 06:18:38 EST 2018 ---------------------------------------  ----------
2018-12-03 06:18:38,330 INFO: dib-run-parts Mon Dec  3 06:18:38 EST 2018
2018-12-03 06:18:38,341 INFO: dib-run-parts Mon Dec  3 06:18:38 EST 2018 10-iptables                                   0.018
2018-12-03 06:18:38,348 INFO: dib-run-parts Mon Dec  3 06:18:38 EST 2018 80-seedstack-masquerade                       0.050
2018-12-03 06:18:38,356 INFO: dib-run-parts Mon Dec  3 06:18:38 EST 2018 98-undercloud-setup                          11.588
2018-12-03 06:18:38,364 INFO: dib-run-parts Mon Dec  3 06:18:38 EST 2018 99-refresh-completed                          0.867
2018-12-03 06:18:38,368 INFO: dib-run-parts Mon Dec  3 06:18:38 EST 2018
2018-12-03 06:18:38,369 INFO: dib-run-parts Mon Dec  3 06:18:38 EST 2018 --------------------- END PROFILING ---------------------
2018-12-03 06:18:38,370 INFO: [2018-12-03 06:18:38,370] (os-refresh-config) [INFO] Completed phase post-configure
2018-12-03 06:18:38,381 INFO: os-refresh-config completed successfully
2018-12-03 06:18:38,751 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13000/ -H "Accept: application/json" -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5"
2018-12-03 06:18:38,754 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-03 06:18:38,774 DEBUG: https://172.16.0.10:13000 "GET / HTTP/1.1" 300 601
2018-12-03 06:18:38,786 DEBUG: RESP: [300] Date: Mon, 03 Dec 2018 11:18:38 GMT Server: Apache Vary: X-Auth-Token Content-Length: 601 Content-Type: application/json 
RESP BODY: {"versions": {"values": [{"status": "stable", "updated": "2018-02-28T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v3+json"}], "id": "v3.10", "links": [{"href": "https://172.16.0.10:13000/v3/", "rel": "self"}]}, {"status": "deprecated", "updated": "2016-08-04T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v2.0+json"}], "id": "v2.0", "links": [{"href": "https://172.16.0.10:13000/v2.0/", "rel": "self"}, {"href": "https://docs.openstack.org/", "type": "text/html", "rel": "describedby"}]}]}}

2018-12-03 06:18:38,787 DEBUG: Making authentication request to https://172.16.0.10:13000/v3/auth/tokens
2018-12-03 06:18:39,446 DEBUG: https://172.16.0.10:13000 "POST /v3/auth/tokens HTTP/1.1" 201 8096
2018-12-03 06:18:39,448 DEBUG: {"token": {"is_domain": false, "methods": ["password"], "roles": [{"id": "faa7ae54793f4277a142cde9530e003f", "name": "admin"}], "expires_at": "2018-12-03T15:18:39.000000Z", "project": {"domain": {"id": "default", "name": "Default"}, "id": "f41e7348daff4254892e5cd759aa8806", "name": "admin"}, "catalog": [{"endpoints": [{"url": "http://172.16.0.11:6385", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "0790be2fdc3b49d08f54f8afdd4a13f5"}, {"url": "https://172.16.0.10:13385", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "4d1975c16140453f8778e1fb9ac95da8"}, {"url": "http://172.16.0.11:6385", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "6d2c8e36ee28476ba57a9055e744d319"}], "type": "baremetal", "id": "2631f10c56d84d278c6347ffd617c17e", "name": "ironic"}, {"endpoints": [{"url": "http://172.16.0.11:8778/placement", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "17ffbfdf60bb4b1fa47d54afdfd65841"}, {"url": "http://172.16.0.11:8778/placement", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "4f70d07307cc47f7bef0a5e3b8fe3a67"}, {"url": "https://172.16.0.10:13778/placement", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "89249f96d951451f922f786665cd8ac0"}], "type": "placement", "id": "33d36ef2b525461882431a456c280ca4", "name": "placement"}, {"endpoints": [{"url": "http://172.16.0.11:35357", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "339fd082d7a348e0883ed89ccaf17e64"}, {"url": "http://172.16.0.11:5000", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "38e31d810fa946de8ccfbf1f7c3046b8"}, {"url": "https://172.16.0.10:13000", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "760586442ad141a8bb0f1ebc452fbf23"}], "type": "identity", "id": "3ecdc56d07224eac9452c019cb2faf2c", "name": "keystone"}, {"endpoints": [{"url": "http://172.16.0.11:8004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "19b04fec944847e3ae21b20f16b49cd1"}, {"url": "https://172.16.0.10:13004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "1bd9e950b8f64c758f56e16c1e416eb9"}, {"url": "http://172.16.0.11:8004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "f207a1d68f524c00be7cd409f67eb602"}], "type": "orchestration", "id": "6b802215e15547b4933f6ad11ff28780", "name": "heat"}, {"endpoints": [{"url": "http://172.16.0.11:8774/v2.1", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "04bf5d0432f24b62b7db4d32eb6e9185"}, {"url": "https://172.16.0.10:13774/v2.1", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "8e337cbbb3504e5e9f93d010f2a920f4"}, {"url": "http://172.16.0.11:8774/v2.1", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "97a0c76942474f85b0fc3128fe3d349c"}], "type": "compute", "id": "6c68f42f71d5448eb11cc7cd8a8b2b30", "name": "nova"}, {"endpoints": [{"url": "http://172.16.0.11:9696", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "8e04bd214b8f46b993b9f8b13f8ec47f"}, {"url": "http://172.16.0.11:9696", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "a133c18dcc214604a3bb74d5dc2e5c5f"}, {"url": "https://172.16.0.10:13696", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "df06914815264173860e975b90e1569c"}], "type": "network", "id": "acf2ad1a66ec4ae09be00688d95b112c", "name": "neutron"}, {"endpoints": [{"url": "http://172.16.0.11:5050", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "9d61189438cb45e3ac270c69205221ca"}, {"url": "http://172.16.0.11:5050", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "c2d37cd497924ce59d5bac0690d9e64b"}, {"url": "https://172.16.0.10:13050", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "d0b3f48230e349edafeac286fdeb8252"}], "type": "baremetal-introspection", "id": "b7b8e51747704c76888dcd514b7addf9", "name": "ironic-inspector"}, {"endpoints": [{"url": "http://172.16.0.11:8000/v1/f41e7348daff4254892e5cd759aa8806", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "848bf35e6ba8481fb799cc941e4c33de"}, {"url": "http://172.16.0.11:8000/v1/f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "a160fbb2b9a046f6902860dbf9e7900c"}, {"url": "https://172.16.0.10:13800/v1/f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "a314c5a74b3b4ca0ad7294b66e726139"}], "type": "cloudformation", "id": "cb16845f1f7c483e9775a0080437bfe9", "name": "heat-cfn"}, {"endpoints": [{"url": "http://172.16.0.11:9292", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "3561bb34989645fea114a7e5b4e443c7"}, {"url": "http://172.16.0.11:9292", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "8d54c0c6f22d43658500ee4b14d0d925"}, {"url": "https://172.16.0.10:13292", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "ee991dbd122a46aa90bb402e88d2cc12"}], "type": "image", "id": "cbe7192c36d147ceb3a550dce9ff3586", "name": "glance"}, {"endpoints": [{"url": "http://172.16.0.11:8888", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "068c25c0200a4a8aba4b6f6f6947a0f5"}, {"url": "http://172.16.0.11:8888", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "98d3307f2ce44059ac228d580f163ca1"}, {"url": "https://172.16.0.10:13888", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "a3978de44cb84351874ab38de3b38c15"}], "type": "messaging", "id": "d90c70e049d14593b7b666788ee43b6e", "name": "zaqar"}, {"endpoints": [{"url": "wss://172.16.0.10:9000", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "2eabe4530b0f41a9a78684eaffc82974"}, {"url": "ws://172.16.0.11:9000", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "300aa4234cf04b44b3de4d8604e6a75d"}, {"url": "ws://172.16.0.11:9000", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "bdaf3488dbab44a1ba43364b5d974e83"}], "type": "messaging-websocket", "id": "dc2d191d5b0e4491a88b1e372b18dbfe", "name": "zaqar-websocket"}, {"endpoints": [{"url": "http://172.16.0.11:8080", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "1e4eab20558248289100f101cfcf4e1e"}, {"url": "http://172.16.0.11:8080/v1/AUTH_f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "6ee57e5f8aec4f3fa81e3ffef1b6f261"}, {"url": "https://172.16.0.10:13808/v1/AUTH_f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "e4c21afceb3f4a8da0f2d75ba050dcdd"}], "type": "object-store", "id": "ea9a151f0e6342dbb1aeba5c885e208f", "name": "swift"}, {"endpoints": [{"url": "https://172.16.0.10:13989/v2", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "15a1660af0f2476cbda3fe163c56f368"}, {"url": "http://172.16.0.11:8989/v2", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "63fadaa6590b47a3bb91a7c789dce370"}, {"url": "http://172.16.0.11:8989/v2", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "b3b5817a37ef4565b92c2fc34b95f250"}], "type": "workflowv2", "id": "f7c8197dd44f4085baddd516aa6e7d7e", "name": "mistral"}], "user": {"domain": {"id": "default", "name": "Default"}, "password_expires_at": null, "name": "admin", "id": "b0c11cd6e04b4e08ac31e050977f22cf"}, "audit_ids": ["lTthNlEZRXCBX7VzLlIfdQ"], "issued_at": "2018-12-03T11:18:39.000000Z"}}
2018-12-03 06:18:39,487 DEBUG: found extension EntryPoint.parse('v2token = keystoneauth1.loading._plugins.identity.v2:Token')
2018-12-03 06:18:39,487 DEBUG: found extension EntryPoint.parse('none = keystoneauth1.loading._plugins.noauth:NoAuth')
2018-12-03 06:18:39,487 DEBUG: found extension EntryPoint.parse('v3oauth1 = keystoneauth1.extras.oauth1._loading:V3OAuth1')
2018-12-03 06:18:39,487 DEBUG: found extension EntryPoint.parse('admin_token = keystoneauth1.loading._plugins.admin_token:AdminToken')
2018-12-03 06:18:39,487 DEBUG: found extension EntryPoint.parse('v3oidcauthcode = keystoneauth1.loading._plugins.identity.v3:OpenIDConnectAuthorizationCode')
2018-12-03 06:18:39,488 DEBUG: found extension EntryPoint.parse('v2password = keystoneauth1.loading._plugins.identity.v2:Password')
2018-12-03 06:18:39,488 DEBUG: found extension EntryPoint.parse('v3samlpassword = keystoneauth1.extras._saml2._loading:Saml2Password')
2018-12-03 06:18:39,488 DEBUG: found extension EntryPoint.parse('v3password = keystoneauth1.loading._plugins.identity.v3:Password')
2018-12-03 06:18:39,488 DEBUG: found extension EntryPoint.parse('v3adfspassword = keystoneauth1.extras._saml2._loading:ADFSPassword')
2018-12-03 06:18:39,488 DEBUG: found extension EntryPoint.parse('v3oidcaccesstoken = keystoneauth1.loading._plugins.identity.v3:OpenIDConnectAccessToken')
2018-12-03 06:18:39,488 DEBUG: found extension EntryPoint.parse('v3oidcpassword = keystoneauth1.loading._plugins.identity.v3:OpenIDConnectPassword')
2018-12-03 06:18:39,488 DEBUG: found extension EntryPoint.parse('v3kerberos = keystoneauth1.extras.kerberos._loading:Kerberos')
2018-12-03 06:18:39,488 DEBUG: found extension EntryPoint.parse('token = keystoneauth1.loading._plugins.identity.generic:Token')
2018-12-03 06:18:39,488 DEBUG: found extension EntryPoint.parse('v3oidcclientcredentials = keystoneauth1.loading._plugins.identity.v3:OpenIDConnectClientCredentials')
2018-12-03 06:18:39,489 DEBUG: found extension EntryPoint.parse('v3tokenlessauth = keystoneauth1.loading._plugins.identity.v3:TokenlessAuth')
2018-12-03 06:18:39,489 DEBUG: found extension EntryPoint.parse('v3token = keystoneauth1.loading._plugins.identity.v3:Token')
2018-12-03 06:18:39,489 DEBUG: found extension EntryPoint.parse('v3totp = keystoneauth1.loading._plugins.identity.v3:TOTP')
2018-12-03 06:18:39,489 DEBUG: found extension EntryPoint.parse('v3applicationcredential = keystoneauth1.loading._plugins.identity.v3:ApplicationCredential')
2018-12-03 06:18:39,489 DEBUG: found extension EntryPoint.parse('password = keystoneauth1.loading._plugins.identity.generic:Password')
2018-12-03 06:18:39,490 DEBUG: found extension EntryPoint.parse('v3fedkerb = keystoneauth1.extras.kerberos._loading:MappedKerberos')
2018-12-03 06:18:39,490 DEBUG: found extension EntryPoint.parse('v1password = swiftclient.authv1:PasswordLoader')
2018-12-03 06:18:39,490 DEBUG: found extension EntryPoint.parse('noauth = cinderclient.contrib.noauth:CinderNoAuthLoader')
2018-12-03 06:18:39,490 DEBUG: found extension EntryPoint.parse('token_endpoint = openstackclient.api.auth_plugin:TokenEndpoint')
2018-12-03 06:18:39,501 DEBUG: Manager defaults:unknown running task network.GET.networks
2018-12-03 06:18:39,502 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13000/ -H "Accept: application/json" -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5"
2018-12-03 06:18:39,503 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-03 06:18:39,522 DEBUG: https://172.16.0.10:13000 "GET / HTTP/1.1" 300 601
2018-12-03 06:18:39,523 DEBUG: RESP: [300] Date: Mon, 03 Dec 2018 11:18:39 GMT Server: Apache Vary: X-Auth-Token Content-Length: 601 Content-Type: application/json 
RESP BODY: {"versions": {"values": [{"status": "stable", "updated": "2018-02-28T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v3+json"}], "id": "v3.10", "links": [{"href": "https://172.16.0.10:13000/v3/", "rel": "self"}]}, {"status": "deprecated", "updated": "2016-08-04T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v2.0+json"}], "id": "v2.0", "links": [{"href": "https://172.16.0.10:13000/v2.0/", "rel": "self"}, {"href": "https://docs.openstack.org/", "type": "text/html", "rel": "describedby"}]}]}}

2018-12-03 06:18:39,523 DEBUG: Making authentication request to https://172.16.0.10:13000/v3/auth/tokens
2018-12-03 06:18:40,185 DEBUG: https://172.16.0.10:13000 "POST /v3/auth/tokens HTTP/1.1" 201 8096
2018-12-03 06:18:40,186 DEBUG: {"token": {"is_domain": false, "methods": ["password"], "roles": [{"id": "faa7ae54793f4277a142cde9530e003f", "name": "admin"}], "expires_at": "2018-12-03T15:18:40.000000Z", "project": {"domain": {"id": "default", "name": "Default"}, "id": "f41e7348daff4254892e5cd759aa8806", "name": "admin"}, "catalog": [{"endpoints": [{"url": "http://172.16.0.11:6385", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "0790be2fdc3b49d08f54f8afdd4a13f5"}, {"url": "https://172.16.0.10:13385", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "4d1975c16140453f8778e1fb9ac95da8"}, {"url": "http://172.16.0.11:6385", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "6d2c8e36ee28476ba57a9055e744d319"}], "type": "baremetal", "id": "2631f10c56d84d278c6347ffd617c17e", "name": "ironic"}, {"endpoints": [{"url": "http://172.16.0.11:8778/placement", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "17ffbfdf60bb4b1fa47d54afdfd65841"}, {"url": "http://172.16.0.11:8778/placement", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "4f70d07307cc47f7bef0a5e3b8fe3a67"}, {"url": "https://172.16.0.10:13778/placement", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "89249f96d951451f922f786665cd8ac0"}], "type": "placement", "id": "33d36ef2b525461882431a456c280ca4", "name": "placement"}, {"endpoints": [{"url": "http://172.16.0.11:35357", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "339fd082d7a348e0883ed89ccaf17e64"}, {"url": "http://172.16.0.11:5000", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "38e31d810fa946de8ccfbf1f7c3046b8"}, {"url": "https://172.16.0.10:13000", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "760586442ad141a8bb0f1ebc452fbf23"}], "type": "identity", "id": "3ecdc56d07224eac9452c019cb2faf2c", "name": "keystone"}, {"endpoints": [{"url": "http://172.16.0.11:8004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "19b04fec944847e3ae21b20f16b49cd1"}, {"url": "https://172.16.0.10:13004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "1bd9e950b8f64c758f56e16c1e416eb9"}, {"url": "http://172.16.0.11:8004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "f207a1d68f524c00be7cd409f67eb602"}], "type": "orchestration", "id": "6b802215e15547b4933f6ad11ff28780", "name": "heat"}, {"endpoints": [{"url": "http://172.16.0.11:8774/v2.1", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "04bf5d0432f24b62b7db4d32eb6e9185"}, {"url": "https://172.16.0.10:13774/v2.1", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "8e337cbbb3504e5e9f93d010f2a920f4"}, {"url": "http://172.16.0.11:8774/v2.1", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "97a0c76942474f85b0fc3128fe3d349c"}], "type": "compute", "id": "6c68f42f71d5448eb11cc7cd8a8b2b30", "name": "nova"}, {"endpoints": [{"url": "http://172.16.0.11:9696", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "8e04bd214b8f46b993b9f8b13f8ec47f"}, {"url": "http://172.16.0.11:9696", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "a133c18dcc214604a3bb74d5dc2e5c5f"}, {"url": "https://172.16.0.10:13696", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "df06914815264173860e975b90e1569c"}], "type": "network", "id": "acf2ad1a66ec4ae09be00688d95b112c", "name": "neutron"}, {"endpoints": [{"url": "http://172.16.0.11:5050", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "9d61189438cb45e3ac270c69205221ca"}, {"url": "http://172.16.0.11:5050", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "c2d37cd497924ce59d5bac0690d9e64b"}, {"url": "https://172.16.0.10:13050", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "d0b3f48230e349edafeac286fdeb8252"}], "type": "baremetal-introspection", "id": "b7b8e51747704c76888dcd514b7addf9", "name": "ironic-inspector"}, {"endpoints": [{"url": "http://172.16.0.11:8000/v1/f41e7348daff4254892e5cd759aa8806", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "848bf35e6ba8481fb799cc941e4c33de"}, {"url": "http://172.16.0.11:8000/v1/f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "a160fbb2b9a046f6902860dbf9e7900c"}, {"url": "https://172.16.0.10:13800/v1/f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "a314c5a74b3b4ca0ad7294b66e726139"}], "type": "cloudformation", "id": "cb16845f1f7c483e9775a0080437bfe9", "name": "heat-cfn"}, {"endpoints": [{"url": "http://172.16.0.11:9292", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "3561bb34989645fea114a7e5b4e443c7"}, {"url": "http://172.16.0.11:9292", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "8d54c0c6f22d43658500ee4b14d0d925"}, {"url": "https://172.16.0.10:13292", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "ee991dbd122a46aa90bb402e88d2cc12"}], "type": "image", "id": "cbe7192c36d147ceb3a550dce9ff3586", "name": "glance"}, {"endpoints": [{"url": "http://172.16.0.11:8888", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "068c25c0200a4a8aba4b6f6f6947a0f5"}, {"url": "http://172.16.0.11:8888", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "98d3307f2ce44059ac228d580f163ca1"}, {"url": "https://172.16.0.10:13888", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "a3978de44cb84351874ab38de3b38c15"}], "type": "messaging", "id": "d90c70e049d14593b7b666788ee43b6e", "name": "zaqar"}, {"endpoints": [{"url": "wss://172.16.0.10:9000", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "2eabe4530b0f41a9a78684eaffc82974"}, {"url": "ws://172.16.0.11:9000", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "300aa4234cf04b44b3de4d8604e6a75d"}, {"url": "ws://172.16.0.11:9000", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "bdaf3488dbab44a1ba43364b5d974e83"}], "type": "messaging-websocket", "id": "dc2d191d5b0e4491a88b1e372b18dbfe", "name": "zaqar-websocket"}, {"endpoints": [{"url": "http://172.16.0.11:8080", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "1e4eab20558248289100f101cfcf4e1e"}, {"url": "http://172.16.0.11:8080/v1/AUTH_f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "6ee57e5f8aec4f3fa81e3ffef1b6f261"}, {"url": "https://172.16.0.10:13808/v1/AUTH_f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "e4c21afceb3f4a8da0f2d75ba050dcdd"}], "type": "object-store", "id": "ea9a151f0e6342dbb1aeba5c885e208f", "name": "swift"}, {"endpoints": [{"url": "https://172.16.0.10:13989/v2", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "15a1660af0f2476cbda3fe163c56f368"}, {"url": "http://172.16.0.11:8989/v2", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "63fadaa6590b47a3bb91a7c789dce370"}, {"url": "http://172.16.0.11:8989/v2", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "b3b5817a37ef4565b92c2fc34b95f250"}], "type": "workflowv2", "id": "f7c8197dd44f4085baddd516aa6e7d7e", "name": "mistral"}], "user": {"domain": {"id": "default", "name": "Default"}, "password_expires_at": null, "name": "admin", "id": "b0c11cd6e04b4e08ac31e050977f22cf"}, "audit_ids": ["1Mbkf1iERpmr-VCxxarIFw"], "issued_at": "2018-12-03T11:18:40.000000Z"}}
2018-12-03 06:18:40,189 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13696 -H "Accept: application/json" -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5"
2018-12-03 06:18:40,190 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-03 06:18:40,209 DEBUG: https://172.16.0.10:13696 "GET / HTTP/1.1" 200 121
2018-12-03 06:18:40,209 DEBUG: RESP: [200] Content-Length: 121 Content-Type: application/json Date: Mon, 03 Dec 2018 11:18:40 GMT 
RESP BODY: {"versions": [{"status": "CURRENT", "id": "v2.0", "links": [{"href": "http://172.16.0.10:13696/v2.0/", "rel": "self"}]}]}

2018-12-03 06:18:40,210 DEBUG: REQ: curl -g -i -X GET "https://172.16.0.10:13696/v2.0/networks?name=ctlplane" -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}3c0b7ae0bc532f0b9e8a6cf2a017b4275d8ab554"
2018-12-03 06:18:41,671 DEBUG: https://172.16.0.10:13696 "GET /v2.0/networks?name=ctlplane HTTP/1.1" 200 15
2018-12-03 06:18:41,672 DEBUG: RESP: [200] Content-Type: application/json Content-Length: 15 X-Openstack-Request-Id: req-a6b2e696-9590-491a-9651-109ec7088403 Date: Mon, 03 Dec 2018 11:18:41 GMT 
RESP BODY: {"networks":[]}

2018-12-03 06:18:41,673 DEBUG: GET call to network for https://172.16.0.10:13696/v2.0/networks?name=ctlplane used request id req-a6b2e696-9590-491a-9651-109ec7088403
2018-12-03 06:18:41,673 DEBUG: Manager defaults:unknown ran task network.GET.networks in 2.17114400864s
2018-12-03 06:18:41,674 DEBUG: Manager defaults:unknown running task network.POST.networks
2018-12-03 06:18:41,676 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13696/v2.0/networks -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "Content-Type: application/json" -H "X-Auth-Token: {SHA1}3c0b7ae0bc532f0b9e8a6cf2a017b4275d8ab554" -d '{"network": {"mtu": 1500, "name": "ctlplane", "provider:physical_network": "ctlplane", "provider:network_type": "flat"}}'
2018-12-03 06:18:42,437 DEBUG: https://172.16.0.10:13696 "POST /v2.0/networks HTTP/1.1" 201 648
2018-12-03 06:18:42,438 DEBUG: RESP: [201] Content-Type: application/json Content-Length: 648 X-Openstack-Request-Id: req-7ed0ebcc-9fe5-4f8f-9d2c-47f6c7dac0b1 Date: Mon, 03 Dec 2018 11:18:42 GMT 
RESP BODY: {"network":{"provider:physical_network":"ctlplane","ipv6_address_scope":null,"revision_number":2,"port_security_enabled":true,"provider:network_type":"flat","id":"b05efc83-67c8-42f2-a461-2961b6b2d56a","router:external":false,"availability_zone_hints":[],"availability_zones":[],"ipv4_address_scope":null,"shared":false,"project_id":"f41e7348daff4254892e5cd759aa8806","l2_adjacency":true,"status":"ACTIVE","subnets":[],"description":"","tags":[],"updated_at":"2018-12-03T11:18:42Z","provider:segmentation_id":null,"name":"ctlplane","admin_state_up":true,"tenant_id":"f41e7348daff4254892e5cd759aa8806","created_at":"2018-12-03T11:18:42Z","mtu":1500}}

2018-12-03 06:18:42,438 DEBUG: POST call to network for https://172.16.0.10:13696/v2.0/networks used request id req-7ed0ebcc-9fe5-4f8f-9d2c-47f6c7dac0b1
2018-12-03 06:18:42,438 DEBUG: Manager defaults:unknown ran task network.POST.networks in 0.764605045319s
2018-12-03 06:18:42,440 INFO: Network created openstack.network.v2.network.Network(provider:physical_network=ctlplane, ipv6_address_scope=None, revision_number=2, port_security_enabled=True, provider:network_type=flat, id=b05efc83-67c8-42f2-a461-2961b6b2d56a, router:external=False, availability_zone_hints=[], availability_zones=[], ipv4_address_scope=None, shared=False, project_id=f41e7348daff4254892e5cd759aa8806, status=ACTIVE, subnets=[], description=, tags=[], updated_at=2018-12-03T11:18:42Z, provider:segmentation_id=None, name=ctlplane, admin_state_up=True, created_at=2018-12-03T11:18:42Z, mtu=1500)
2018-12-03 06:18:42,441 DEBUG: Manager defaults:unknown running task network.GET.segments
2018-12-03 06:18:42,443 DEBUG: REQ: curl -g -i -X GET "https://172.16.0.10:13696/v2.0/segments?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a" -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}3c0b7ae0bc532f0b9e8a6cf2a017b4275d8ab554"
2018-12-03 06:18:42,471 DEBUG: https://172.16.0.10:13696 "GET /v2.0/segments?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a HTTP/1.1" 200 232
2018-12-03 06:18:42,472 DEBUG: RESP: [200] Content-Type: application/json Content-Length: 232 X-Openstack-Request-Id: req-3abd43de-5040-4689-a71a-ec86784f1e31 Date: Mon, 03 Dec 2018 11:18:42 GMT 
RESP BODY: {"segments": [{"name": null, "network_id": "b05efc83-67c8-42f2-a461-2961b6b2d56a", "segmentation_id": null, "network_type": "flat", "physical_network": "ctlplane", "id": "78acab06-eea4-4d62-9365-2cd4edbebdcf", "description": null}]}

2018-12-03 06:18:42,472 DEBUG: GET call to network for https://172.16.0.10:13696/v2.0/segments?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a used request id req-3abd43de-5040-4689-a71a-ec86784f1e31
2018-12-03 06:18:42,472 DEBUG: Manager defaults:unknown ran task network.GET.segments in 0.0312230587006s
2018-12-03 06:18:42,473 DEBUG: Manager defaults:unknown running task network.DELETE.segments
2018-12-03 06:18:42,476 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13696/v2.0/segments/78acab06-eea4-4d62-9365-2cd4edbebdcf -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "Accept: " -H "X-Auth-Token: {SHA1}3c0b7ae0bc532f0b9e8a6cf2a017b4275d8ab554"
2018-12-03 06:18:43,062 DEBUG: https://172.16.0.10:13696 "DELETE /v2.0/segments/78acab06-eea4-4d62-9365-2cd4edbebdcf HTTP/1.1" 204 0
2018-12-03 06:18:43,063 DEBUG: RESP: [204] X-Openstack-Request-Id: req-16aa31eb-1915-4530-8b86-fb32e87d324e Content-Length: 0 Date: Mon, 03 Dec 2018 11:18:43 GMT 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-03 06:18:43,063 DEBUG: DELETE call to network for https://172.16.0.10:13696/v2.0/segments/78acab06-eea4-4d62-9365-2cd4edbebdcf used request id req-16aa31eb-1915-4530-8b86-fb32e87d324e
2018-12-03 06:18:43,063 DEBUG: Manager defaults:unknown ran task network.DELETE.segments in 0.589530944824s
2018-12-03 06:18:43,063 INFO: Default segment on network ctlplane deleted.
2018-12-03 06:18:43,064 DEBUG: Manager defaults:unknown running task network.GET.subnets
2018-12-03 06:18:43,067 DEBUG: REQ: curl -g -i -X GET "https://172.16.0.10:13696/v2.0/subnets?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a&cidr=172.16.0.0%2F24" -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}3c0b7ae0bc532f0b9e8a6cf2a017b4275d8ab554"
2018-12-03 06:18:43,112 DEBUG: https://172.16.0.10:13696 "GET /v2.0/subnets?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a&cidr=172.16.0.0%2F24 HTTP/1.1" 200 14
2018-12-03 06:18:43,113 DEBUG: RESP: [200] Content-Type: application/json Content-Length: 14 X-Openstack-Request-Id: req-04ad18c1-9e32-4704-96d0-5d3f58b5f138 Date: Mon, 03 Dec 2018 11:18:43 GMT 
RESP BODY: {"subnets":[]}

2018-12-03 06:18:43,113 DEBUG: GET call to network for https://172.16.0.10:13696/v2.0/subnets?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a&cidr=172.16.0.0%2F24 used request id req-04ad18c1-9e32-4704-96d0-5d3f58b5f138
2018-12-03 06:18:43,113 DEBUG: Manager defaults:unknown ran task network.GET.subnets in 0.0489039421082s
2018-12-03 06:18:43,114 DEBUG: Manager defaults:unknown running task network.GET.subnets
2018-12-03 06:18:43,117 DEBUG: REQ: curl -g -i -X GET "https://172.16.0.10:13696/v2.0/subnets?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a&cidr=172.16.0.0%2F24" -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}3c0b7ae0bc532f0b9e8a6cf2a017b4275d8ab554"
2018-12-03 06:18:43,164 DEBUG: https://172.16.0.10:13696 "GET /v2.0/subnets?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a&cidr=172.16.0.0%2F24 HTTP/1.1" 200 14
2018-12-03 06:18:43,165 DEBUG: RESP: [200] Content-Type: application/json Content-Length: 14 X-Openstack-Request-Id: req-46f2b3c9-052a-442f-a109-152206ded61a Date: Mon, 03 Dec 2018 11:18:43 GMT 
RESP BODY: {"subnets":[]}

2018-12-03 06:18:43,166 DEBUG: GET call to network for https://172.16.0.10:13696/v2.0/subnets?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a&cidr=172.16.0.0%2F24 used request id req-46f2b3c9-052a-442f-a109-152206ded61a
2018-12-03 06:18:43,166 DEBUG: Manager defaults:unknown ran task network.GET.subnets in 0.0513198375702s
2018-12-03 06:18:43,167 DEBUG: Manager defaults:unknown running task network.GET.segments
2018-12-03 06:18:43,169 DEBUG: REQ: curl -g -i -X GET "https://172.16.0.10:13696/v2.0/segments?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a&physical_network=ctlplane" -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}3c0b7ae0bc532f0b9e8a6cf2a017b4275d8ab554"
2018-12-03 06:18:43,191 DEBUG: https://172.16.0.10:13696 "GET /v2.0/segments?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a&physical_network=ctlplane HTTP/1.1" 200 16
2018-12-03 06:18:43,192 DEBUG: RESP: [200] Content-Type: application/json Content-Length: 16 X-Openstack-Request-Id: req-7a31649c-eec4-4d63-b1e4-a8a213fe031d Date: Mon, 03 Dec 2018 11:18:43 GMT 
RESP BODY: {"segments": []}

2018-12-03 06:18:43,192 DEBUG: GET call to network for https://172.16.0.10:13696/v2.0/segments?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a&physical_network=ctlplane used request id req-7a31649c-eec4-4d63-b1e4-a8a213fe031d
2018-12-03 06:18:43,192 DEBUG: Manager defaults:unknown ran task network.GET.segments in 0.0253689289093s
2018-12-03 06:18:43,193 DEBUG: Manager defaults:unknown running task network.POST.segments
2018-12-03 06:18:43,195 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13696/v2.0/segments -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "Content-Type: application/json" -H "X-Auth-Token: {SHA1}3c0b7ae0bc532f0b9e8a6cf2a017b4275d8ab554" -d '{"segment": {"network_id": "b05efc83-67c8-42f2-a461-2961b6b2d56a", "physical_network": "ctlplane", "name": "ctlplane-subnet", "network_type": "flat"}}'
2018-12-03 06:18:43,907 DEBUG: https://172.16.0.10:13696 "POST /v2.0/segments HTTP/1.1" 201 242
2018-12-03 06:18:43,908 DEBUG: RESP: [201] Content-Type: application/json Content-Length: 242 X-Openstack-Request-Id: req-a8b01c33-7fc3-4ee7-9950-a5306123e3e7 Date: Mon, 03 Dec 2018 11:18:43 GMT 
RESP BODY: {"segment": {"name": "ctlplane-subnet", "network_id": "b05efc83-67c8-42f2-a461-2961b6b2d56a", "segmentation_id": null, "network_type": "flat", "physical_network": "ctlplane", "id": "2294dc63-421f-4dd6-9390-7ff844825546", "description": null}}

2018-12-03 06:18:43,909 DEBUG: POST call to network for https://172.16.0.10:13696/v2.0/segments used request id req-a8b01c33-7fc3-4ee7-9950-a5306123e3e7
2018-12-03 06:18:43,909 DEBUG: Manager defaults:unknown ran task network.POST.segments in 0.715935945511s
2018-12-03 06:18:43,909 INFO: Neutron Segment created openstack.network.v2.segment.Segment(name=ctlplane-subnet, network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a, segmentation_id=None, id=2294dc63-421f-4dd6-9390-7ff844825546, physical_network=ctlplane, network_type=flat, description=None)
2018-12-03 06:18:43,910 DEBUG: Manager defaults:unknown running task network.POST.subnets
2018-12-03 06:18:43,913 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13696/v2.0/subnets -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "Content-Type: application/json" -H "X-Auth-Token: {SHA1}3c0b7ae0bc532f0b9e8a6cf2a017b4275d8ab554" -d '{"subnet": {"name": "ctlplane-subnet", "enable_dhcp": true, "segment_id": null, "network_id": "b05efc83-67c8-42f2-a461-2961b6b2d56a", "allocation_pools": [{"start": "172.16.0.20", "end": "172.16.0.120"}], "host_routes": [{"nexthop": "172.16.0.1", "destination": "169.254.169.254/32"}], "ip_version": "4", "gateway_ip": "172.16.0.1", "cidr": "172.16.0.0/24"}}'
2018-12-03 06:18:44,568 DEBUG: https://172.16.0.10:13696 "POST /v2.0/subnets HTTP/1.1" 201 696
2018-12-03 06:18:44,569 DEBUG: RESP: [201] Content-Type: application/json Content-Length: 696 X-Openstack-Request-Id: req-aa875c90-6f38-46c1-8040-33d24903fd50 Date: Mon, 03 Dec 2018 11:18:44 GMT 
RESP BODY: {"subnet":{"updated_at":"2018-12-03T11:18:44Z","ipv6_ra_mode":null,"allocation_pools":[{"start":"172.16.0.20","end":"172.16.0.120"}],"host_routes":[{"destination":"169.254.169.254/32","nexthop":"172.16.0.1"}],"revision_number":0,"ipv6_address_mode":null,"id":"d1642875-0c08-4152-8d6a-dc5d7c4797dd","dns_nameservers":[],"gateway_ip":"172.16.0.1","project_id":"f41e7348daff4254892e5cd759aa8806","description":"","tags":[],"cidr":"172.16.0.0/24","subnetpool_id":null,"service_types":[],"name":"ctlplane-subnet","enable_dhcp":true,"segment_id":null,"network_id":"b05efc83-67c8-42f2-a461-2961b6b2d56a","tenant_id":"f41e7348daff4254892e5cd759aa8806","created_at":"2018-12-03T11:18:44Z","ip_version":4}}

2018-12-03 06:18:44,569 DEBUG: POST call to network for https://172.16.0.10:13696/v2.0/subnets used request id req-aa875c90-6f38-46c1-8040-33d24903fd50
2018-12-03 06:18:44,569 DEBUG: Manager defaults:unknown ran task network.POST.subnets in 0.658739089966s
2018-12-03 06:18:44,571 INFO: Subnet created openstack.network.v2.subnet.Subnet(service_types=[], description=, enable_dhcp=True, tags=[], network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a, tenant_id=f41e7348daff4254892e5cd759aa8806, created_at=2018-12-03T11:18:44Z, segment_id=None, dns_nameservers=[], updated_at=2018-12-03T11:18:44Z, gateway_ip=172.16.0.1, ipv6_ra_mode=None, allocation_pools=[{u'start': u'172.16.0.20', u'end': u'172.16.0.120'}], host_routes=[{u'nexthop': u'172.16.0.1', u'destination': u'169.254.169.254/32'}], revision_number=0, ip_version=4, ipv6_address_mode=None, cidr=172.16.0.0/24, id=d1642875-0c08-4152-8d6a-dc5d7c4797dd, subnetpool_id=None, name=ctlplane-subnet)
2018-12-03 06:18:44,853 INFO: Generated new ssh key in ~/.ssh/id_rsa
2018-12-03 06:18:44,857 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13774/v2.1/os-keypairs/default -H "User-Agent: python-novaclient" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:18:44,859 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-03 06:18:46,126 DEBUG: https://172.16.0.10:13774 "GET /v2.1/os-keypairs/default HTTP/1.1" 404 113
2018-12-03 06:18:46,127 DEBUG: RESP: [404] Date: Mon, 03 Dec 2018 11:18:44 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version x-openstack-request-id: req-0ddfd842-adf3-463b-8bff-93d0b36d9150 x-compute-request-id: req-0ddfd842-adf3-463b-8bff-93d0b36d9150 Content-Length: 113 Content-Type: application/json; charset=UTF-8 
RESP BODY: {"itemNotFound": {"message": "Keypair default not found for user b0c11cd6e04b4e08ac31e050977f22cf", "code": 404}}

2018-12-03 06:18:46,127 DEBUG: GET call to compute for https://172.16.0.10:13774/v2.1/os-keypairs/default used request id req-0ddfd842-adf3-463b-8bff-93d0b36d9150
2018-12-03 06:18:46,130 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/os-keypairs -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"keypair": {"public_key": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsGB6ru/SfZvWgoivXrfUKZekW4lL7MCBsO221ycpqDy2GrqoJmzEkxn/Tor/6djTsjaQGWgmlXNGG9VHGGRFxAsDByK1x529bFoRwCCwwxMG/wmm5AFgvCUZISIblL2xVkN2+g5wUbnzMakaUkx+vYLY31aVmMkgh1Q+NtX48ZJ/92PcQZLMnEZOiPj7wYqPJhC/OisewdWZ8vDcD5JuEai++ET3BDxulEqitU9crtVNdURXusQ91RounhWaHTXIro0T2tKKprBbUb7R2CQQbb8TBpyH1O7+iK3j2LFDQzlS7YAunXAAQ2kr1RuiH5lSALw/5i5Gai+kD3e3kxz8h stack@undercloud.example.com", "name": "default"}}'
2018-12-03 06:18:46,449 DEBUG: https://172.16.0.10:13774 "POST /v2.1/os-keypairs HTTP/1.1" 200 485
2018-12-03 06:18:46,450 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:46 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-3d1f4731-b1d5-4a34-a08a-a4aebb625b70 x-compute-request-id: req-3d1f4731-b1d5-4a34-a08a-a4aebb625b70 Content-Encoding: gzip Content-Length: 485 Content-Type: application/json 
RESP BODY: {"keypair": {"public_key": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsGB6ru/SfZvWgoivXrfUKZekW4lL7MCBsO221ycpqDy2GrqoJmzEkxn/Tor/6djTsjaQGWgmlXNGG9VHGGRFxAsDByK1x529bFoRwCCwwxMG/wmm5AFgvCUZISIblL2xVkN2+g5wUbnzMakaUkx+vYLY31aVmMkgh1Q+NtX48ZJ/92PcQZLMnEZOiPj7wYqPJhC/OisewdWZ8vDcD5JuEai++ET3BDxulEqitU9crtVNdURXusQ91RounhWaHTXIro0T2tKKprBbUb7R2CQQbb8TBpyH1O7+iK3j2LFDQzlS7YAunXAAQ2kr1RuiH5lSALw/5i5Gai+kD3e3kxz8h stack@undercloud.example.com", "user_id": "b0c11cd6e04b4e08ac31e050977f22cf", "name": "default", "fingerprint": "d5:30:1f:ed:f6:c3:ab:88:14:dd:8b:a7:00:86:96:fd"}}

2018-12-03 06:18:46,451 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/os-keypairs used request id req-3d1f4731-b1d5-4a34-a08a-a4aebb625b70
2018-12-03 06:18:46,475 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13774/v2.1/flavors/detail -H "User-Agent: python-novaclient" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:18:46,514 DEBUG: https://172.16.0.10:13774 "GET /v2.1/flavors/detail HTTP/1.1" 200 15
2018-12-03 06:18:46,515 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:46 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version x-openstack-request-id: req-919efaa9-bb8b-4ff2-871f-45689178178d x-compute-request-id: req-919efaa9-bb8b-4ff2-871f-45689178178d Content-Length: 15 Content-Type: application/json 
RESP BODY: {"flavors": []}

2018-12-03 06:18:46,515 DEBUG: GET call to compute for https://172.16.0.10:13774/v2.1/flavors/detail used request id req-919efaa9-bb8b-4ff2-871f-45689178178d
2018-12-03 06:18:46,517 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13385/v1/nodes/?fields=uuid,resource_class -H "X-OpenStack-Ironic-API-Version: 1.21" -H "User-Agent: python-ironicclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:18:46,518 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-03 06:18:46,871 DEBUG: https://172.16.0.10:13385 "GET /v1/nodes/?fields=uuid,resource_class HTTP/1.1" 200 13
2018-12-03 06:18:46,872 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:46 GMT Server: Apache X-OpenStack-Ironic-API-Minimum-Version: 1.1 X-OpenStack-Ironic-API-Maximum-Version: 1.38 X-OpenStack-Ironic-API-Version: 1.21 Openstack-Request-Id: req-90bab427-860d-4ac3-9fbf-cb78d2442614 Content-Length: 13 Keep-Alive: timeout=15, max=100 Connection: Keep-Alive Content-Type: application/json 
RESP BODY: {"nodes": []}

2018-12-03 06:18:46,874 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13774/v2.1/flavors/detail -H "User-Agent: python-novaclient" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:18:46,914 DEBUG: https://172.16.0.10:13774 "GET /v2.1/flavors/detail HTTP/1.1" 200 15
2018-12-03 06:18:46,915 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:46 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version x-openstack-request-id: req-dd3b1c65-5d51-4125-8f22-970a7372734e x-compute-request-id: req-dd3b1c65-5d51-4125-8f22-970a7372734e Content-Length: 15 Content-Type: application/json 
RESP BODY: {"flavors": []}

2018-12-03 06:18:46,915 DEBUG: GET call to compute for https://172.16.0.10:13774/v2.1/flavors/detail used request id req-dd3b1c65-5d51-4125-8f22-970a7372734e
2018-12-03 06:18:46,918 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"flavor": {"vcpus": 1, "disk": 40, "name": "baremetal", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "ram": 4096, "id": null, "swap": 0}}'
2018-12-03 06:18:46,995 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors HTTP/1.1" 200 282
2018-12-03 06:18:46,996 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:46 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-7d7a9e68-f621-40f6-b98c-7f4a5a9a0f0d x-compute-request-id: req-7d7a9e68-f621-40f6-b98c-7f4a5a9a0f0d Content-Encoding: gzip Content-Length: 282 Content-Type: application/json 
RESP BODY: {"flavor": {"name": "baremetal", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "43e6e126-f6ef-4446-8f90-790fd4848d87"}}

2018-12-03 06:18:46,996 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors used request id req-7d7a9e68-f621-40f6-b98c-7f4a5a9a0f0d
2018-12-03 06:18:46,999 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87/os-extra_specs -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"extra_specs": {"resources:CUSTOM_BAREMETAL": "1", "resources:DISK_GB": "0", "capabilities:boot_option": "local", "resources:MEMORY_MB": "0", "resources:VCPU": "0"}}'
2018-12-03 06:18:47,064 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87/os-extra_specs HTTP/1.1" 200 136
2018-12-03 06:18:47,065 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-6f15d9b3-1b45-4f60-b9e0-b78587ab6571 x-compute-request-id: req-6f15d9b3-1b45-4f60-b9e0-b78587ab6571 Content-Encoding: gzip Content-Length: 136 Content-Type: application/json 
RESP BODY: {"extra_specs": {"resources:CUSTOM_BAREMETAL": "1", "resources:DISK_GB": "0", "capabilities:boot_option": "local", "resources:MEMORY_MB": "0", "resources:VCPU": "0"}}

2018-12-03 06:18:47,065 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87/os-extra_specs used request id req-6f15d9b3-1b45-4f60-b9e0-b78587ab6571
2018-12-03 06:18:47,065 INFO: Created flavor "baremetal" with profile "None"
2018-12-03 06:18:47,068 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"flavor": {"vcpus": 1, "disk": 40, "name": "control", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "ram": 4096, "id": null, "swap": 0}}'
2018-12-03 06:18:47,102 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors HTTP/1.1" 200 281
2018-12-03 06:18:47,103 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-f84006da-0f73-4872-88c6-8829130c04d4 x-compute-request-id: req-f84006da-0f73-4872-88c6-8829130c04d4 Content-Encoding: gzip Content-Length: 281 Content-Type: application/json 
RESP BODY: {"flavor": {"name": "control", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de"}}

2018-12-03 06:18:47,104 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors used request id req-f84006da-0f73-4872-88c6-8829130c04d4
2018-12-03 06:18:47,106 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de/os-extra_specs -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "control", "resources:MEMORY_MB": "0", "resources:VCPU": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:DISK_GB": "0"}}'
2018-12-03 06:18:47,169 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de/os-extra_specs HTTP/1.1" 200 149
2018-12-03 06:18:47,170 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-6d5911c0-4bf9-42c7-8503-a26098303b04 x-compute-request-id: req-6d5911c0-4bf9-42c7-8503-a26098303b04 Content-Encoding: gzip Content-Length: 149 Content-Type: application/json 
RESP BODY: {"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "control", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:VCPU": "0"}}

2018-12-03 06:18:47,170 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de/os-extra_specs used request id req-6d5911c0-4bf9-42c7-8503-a26098303b04
2018-12-03 06:18:47,171 INFO: Created flavor "control" with profile "control"
2018-12-03 06:18:47,173 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"flavor": {"vcpus": 1, "disk": 40, "name": "compute", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "ram": 4096, "id": null, "swap": 0}}'
2018-12-03 06:18:47,206 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors HTTP/1.1" 200 282
2018-12-03 06:18:47,207 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-e0b905e3-8594-4d4c-811f-35d369d1b0ea x-compute-request-id: req-e0b905e3-8594-4d4c-811f-35d369d1b0ea Content-Encoding: gzip Content-Length: 282 Content-Type: application/json 
RESP BODY: {"flavor": {"name": "compute", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "78342552-7d94-4dd7-9b5b-ee087fdfeead"}}

2018-12-03 06:18:47,207 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors used request id req-e0b905e3-8594-4d4c-811f-35d369d1b0ea
2018-12-03 06:18:47,210 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead/os-extra_specs -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "compute", "resources:MEMORY_MB": "0", "resources:VCPU": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:DISK_GB": "0"}}'
2018-12-03 06:18:47,274 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead/os-extra_specs HTTP/1.1" 200 150
2018-12-03 06:18:47,275 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-51bfb21a-6f69-404d-b70f-fb899617e656 x-compute-request-id: req-51bfb21a-6f69-404d-b70f-fb899617e656 Content-Encoding: gzip Content-Length: 150 Content-Type: application/json 
RESP BODY: {"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "compute", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:VCPU": "0"}}

2018-12-03 06:18:47,275 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead/os-extra_specs used request id req-51bfb21a-6f69-404d-b70f-fb899617e656
2018-12-03 06:18:47,275 INFO: Created flavor "compute" with profile "compute"
2018-12-03 06:18:47,277 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"flavor": {"vcpus": 1, "disk": 40, "name": "ceph-storage", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "ram": 4096, "id": null, "swap": 0}}'
2018-12-03 06:18:47,313 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors HTTP/1.1" 200 286
2018-12-03 06:18:47,314 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-c788976e-cf9e-4263-b120-e5aeeeb20a07 x-compute-request-id: req-c788976e-cf9e-4263-b120-e5aeeeb20a07 Content-Encoding: gzip Content-Length: 286 Content-Type: application/json 
RESP BODY: {"flavor": {"name": "ceph-storage", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/59e72551-d212-4658-bab1-9281d25f3f50", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/59e72551-d212-4658-bab1-9281d25f3f50", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "59e72551-d212-4658-bab1-9281d25f3f50"}}

2018-12-03 06:18:47,314 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors used request id req-c788976e-cf9e-4263-b120-e5aeeeb20a07
2018-12-03 06:18:47,317 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors/59e72551-d212-4658-bab1-9281d25f3f50/os-extra_specs -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "ceph-storage", "resources:MEMORY_MB": "0", "resources:VCPU": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:DISK_GB": "0"}}'
2018-12-03 06:18:47,381 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors/59e72551-d212-4658-bab1-9281d25f3f50/os-extra_specs HTTP/1.1" 200 153
2018-12-03 06:18:47,382 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-38e52ec3-a940-4f06-8964-71173b3a4fc5 x-compute-request-id: req-38e52ec3-a940-4f06-8964-71173b3a4fc5 Content-Encoding: gzip Content-Length: 153 Content-Type: application/json 
RESP BODY: {"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "ceph-storage", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:VCPU": "0"}}

2018-12-03 06:18:47,382 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors/59e72551-d212-4658-bab1-9281d25f3f50/os-extra_specs used request id req-38e52ec3-a940-4f06-8964-71173b3a4fc5
2018-12-03 06:18:47,382 INFO: Created flavor "ceph-storage" with profile "ceph-storage"
2018-12-03 06:18:47,384 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"flavor": {"vcpus": 1, "disk": 40, "name": "block-storage", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "ram": 4096, "id": null, "swap": 0}}'
2018-12-03 06:18:47,418 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors HTTP/1.1" 200 286
2018-12-03 06:18:47,419 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-ef6283d9-48b5-489e-b143-24b710369b20 x-compute-request-id: req-ef6283d9-48b5-489e-b143-24b710369b20 Content-Encoding: gzip Content-Length: 286 Content-Type: application/json 
RESP BODY: {"flavor": {"name": "block-storage", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/fd9db8be-824f-424a-9795-be74dc2cede4", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/fd9db8be-824f-424a-9795-be74dc2cede4", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "fd9db8be-824f-424a-9795-be74dc2cede4"}}

2018-12-03 06:18:47,420 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors used request id req-ef6283d9-48b5-489e-b143-24b710369b20
2018-12-03 06:18:47,422 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors/fd9db8be-824f-424a-9795-be74dc2cede4/os-extra_specs -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "block-storage", "resources:MEMORY_MB": "0", "resources:VCPU": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:DISK_GB": "0"}}'
2018-12-03 06:18:47,486 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors/fd9db8be-824f-424a-9795-be74dc2cede4/os-extra_specs HTTP/1.1" 200 154
2018-12-03 06:18:47,488 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-4be0c6a0-9a4b-44ba-904b-744b4b166476 x-compute-request-id: req-4be0c6a0-9a4b-44ba-904b-744b4b166476 Content-Encoding: gzip Content-Length: 154 Content-Type: application/json 
RESP BODY: {"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "block-storage", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:VCPU": "0"}}

2018-12-03 06:18:47,488 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors/fd9db8be-824f-424a-9795-be74dc2cede4/os-extra_specs used request id req-4be0c6a0-9a4b-44ba-904b-744b4b166476
2018-12-03 06:18:47,488 INFO: Created flavor "block-storage" with profile "block-storage"
2018-12-03 06:18:47,490 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"flavor": {"vcpus": 1, "disk": 40, "name": "swift-storage", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "ram": 4096, "id": null, "swap": 0}}'
2018-12-03 06:18:47,529 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors HTTP/1.1" 200 285
2018-12-03 06:18:47,530 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-b94a9215-1108-46a1-b47e-f2f8ee0fe24f x-compute-request-id: req-b94a9215-1108-46a1-b47e-f2f8ee0fe24f Content-Encoding: gzip Content-Length: 285 Content-Type: application/json 
RESP BODY: {"flavor": {"name": "swift-storage", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "4f67e9c1-a6ca-4bee-a904-94fb5a41dd73"}}

2018-12-03 06:18:47,530 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors used request id req-b94a9215-1108-46a1-b47e-f2f8ee0fe24f
2018-12-03 06:18:47,533 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73/os-extra_specs -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "swift-storage", "resources:MEMORY_MB": "0", "resources:VCPU": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:DISK_GB": "0"}}'
2018-12-03 06:18:47,594 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73/os-extra_specs HTTP/1.1" 200 154
2018-12-03 06:18:47,596 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:18:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-066cef2c-3f7e-4b85-a27d-ba93bd4b05d4 x-compute-request-id: req-066cef2c-3f7e-4b85-a27d-ba93bd4b05d4 Content-Encoding: gzip Content-Length: 154 Content-Type: application/json 
RESP BODY: {"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "swift-storage", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:VCPU": "0"}}

2018-12-03 06:18:47,596 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73/os-extra_specs used request id req-066cef2c-3f7e-4b85-a27d-ba93bd4b05d4
2018-12-03 06:18:47,596 INFO: Created flavor "swift-storage" with profile "swift-storage"
2018-12-03 06:18:47,597 INFO: Configuring Mistral workbooks
2018-12-03 06:18:47,597 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:18:47,599 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-03 06:18:48,727 DEBUG: https://172.16.0.10:13989 "GET /v2/workbooks HTTP/1.1" 200 17
2018-12-03 06:18:48,729 DEBUG: RESP: [200] Content-Length: 17 Content-Type: application/json Date: Mon, 03 Dec 2018 11:18:48 GMT Connection: keep-alive 
RESP BODY: {"workbooks": []}

2018-12-03 06:18:48,729 DEBUG: HTTP GET https://172.16.0.10:13989/v2/workbooks 200
2018-12-03 06:18:48,730 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/workflows -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:18:48,758 DEBUG: https://172.16.0.10:13989 "GET /v2/workflows HTTP/1.1" 200 3608
2018-12-03 06:18:48,760 DEBUG: RESP: [200] Content-Length: 3608 Content-Type: application/json Date: Mon, 03 Dec 2018 11:18:48 GMT Connection: keep-alive 
RESP BODY: {"workflows": [{"definition": "---\nversion: \"2.0\"\n\nstd.delete_instance:\n  type: direct\n\n  input:\n    - instance_id\n\n  description: Deletes VM.\n\n  tasks:\n    delete_vm:\n      description: Destroy VM.\n      action: nova.servers_delete server=<% $.instance_id %>\n      wait-after: 10\n      on-success:\n        - find_given_vm\n\n    find_given_vm:\n      description: Checks that VM is already deleted.\n      action: nova.servers_find id=<% $.instance_id %>\n      on-error:\n        - succeed\n\n", "name": "std.delete_instance", "tags": [], "created_at": "2018-12-03 11:17:33", "namespace": "", "updated_at": null, "scope": "public", "input": "instance_id", "project_id": "<default-project>", "id": "409eca82-bab3-490c-a90d-2b4cac2e389d"}, {"definition": "---\nversion: '2.0'\n\nstd.create_instance:\n  type: direct\n\n  description: |\n    Creates VM and waits till VM OS is up and running.\n\n  input:\n    - name\n    - image_id\n    - flavor_id\n    - ssh_username: null\n    - ssh_password: null\n\n    # Name of previously created keypair to inject into the instance.\n    # Either ssh credentials or keypair must be provided.\n    - key_name: null\n\n    # Security_groups: A list of security group names\n    - security_groups: null\n\n    # An ordered list of nics to be added to this server, with information about connected networks, fixed IPs, port etc.\n    # Example: nics: [{\"net-id\": \"27aa8c1c-d6b8-4474-b7f7-6cdcf63ac856\"}]\n    - nics: null\n\n  task-defaults:\n    on-error:\n      - delete_vm\n\n  output:\n    ip: <% $.vm_ip %>\n    id: <% $.vm_id %>\n    name: <% $.name %>\n    status: <% $.status %>\n\n  tasks:\n    create_vm:\n      description: Initial request to create a VM.\n      action: nova.servers_create name=<% $.name %> image=<% $.image_id %> flavor=<% $.flavor_id %>\n      input:\n        key_name: <% $.key_name %>\n        security_groups: <% $.security_groups %>\n        nics: <% $.nics %>\n      publish:\n        vm_id: <% task(create_vm).result.id %>\n      on-success:\n        - search_for_ip\n\n    search_for_ip:\n      description: Gets first free ip from Nova floating IPs.\n      action: nova.floating_ips_findall instance_id=null\n      publish:\n        vm_ip: <% task(search_for_ip).result[0].ip %>\n      on-success:\n        - wait_vm_active\n\n    wait_vm_active:\n      description: Waits till VM is ACTIVE.\n      action: nova.servers_find id=<% $.vm_id %> status=\"ACTIVE\"\n      retry:\n        count: 10\n        delay: 10\n      publish:\n        status: <% task(wait_vm_active).result.status %>\n      on-success:\n        - associate_ip\n\n    associate_ip:\n      description: Associate server with one of floating IPs.\n      action: nova.servers_add_floating_ip server=<% $.vm_id %> address=<% $.vm_ip %>\n      wait-after: 5\n      on-success:\n        - wait_ssh\n\n    wait_ssh:\n      description: Wait till operating system on the VM is up (SSH command).\n      action: std.wait_ssh username=<% $.ssh_username %> password=<% $.ssh_password %> host=<% $.vm_ip %>\n      retry:\n        count: 10\n        delay: 10\n\n    delete_vm:\n      description: Destroy VM.\n      workflow: std.delete_instance instance_id=<% $.vm_id %>\n      on-complete:\n        - fail\n", "name": "std.create_instance", "tags": [], "created_at": "2018-12-03 11:17:33", "namespace": "", "updated_at": null, "scope": "public", "input": "name, image_id, flavor_id, ssh_username=None, ssh_password=None, key_name=None, security_groups=None, nics=None", "project_id": "<default-project>", "id": "b1da74f0-9db7-4243-8d4e-826019cdc804"}]}

2018-12-03 06:18:48,760 DEBUG: HTTP GET https://172.16.0.10:13989/v2/workflows 200
2018-12-03 06:18:48,761 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/cron_triggers -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:18:48,890 DEBUG: https://172.16.0.10:13989 "GET /v2/cron_triggers HTTP/1.1" 200 21
2018-12-03 06:18:48,891 DEBUG: RESP: [200] Content-Length: 21 Content-Type: application/json Date: Mon, 03 Dec 2018 11:18:48 GMT Connection: keep-alive 
RESP BODY: {"cron_triggers": []}

2018-12-03 06:18:48,891 DEBUG: HTTP GET https://172.16.0.10:13989/v2/cron_triggers 200
2018-12-03 06:18:48,902 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.access.v1
description: TripleO administration access workflows

workflows:

  enable_ssh_admin:
    description: >-
      This workflow creates an admin user on the overcloud nodes,
      which can then be used for connecting for automated
      administrative or deployment tasks, e.g. via Ansible. The
      workflow can be used both for Nova-managed and split-stack
      deployments, assuming the correct input values are passed
      in. The workflow defaults to Nova-managed approach, for which no
      additional parameters need to be supplied. In case of
      split-stack, temporary ssh connection details (user, key, list
      of servers) need to be provided -- these are only used
      temporarily to create the actual ssh admin user for use by
      Mistral.
    tags:
      - tripleo-common-managed
    input:
      - ssh_private_key: null
      - ssh_user: null
      - ssh_servers: []
      - overcloud_admin: tripleo-admin
      - queue_name: tripleo
    tasks:
      get_pubkey:
        action: tripleo.validations.get_pubkey
        on-success: generate_playbook
        publish:
          pubkey: <% task().result %>

      generate_playbook:
        on-success:
          - create_admin_via_nova: <% $.ssh_private_key = null %>
          - create_admin_via_ssh: <% $.ssh_private_key != null %>
        publish:
          create_admin_tasks:
            - name: create user <% $.overcloud_admin %>
              user:
                name: '<% $.overcloud_admin %>'
            - name: grant admin rights to user <% $.overcloud_admin %>
              copy:
                dest: /etc/sudoers.d/<% $.overcloud_admin %>
                content: |
                  <% $.overcloud_admin %> ALL=(ALL) NOPASSWD:ALL
                mode: 0440
            - name: ensure .ssh dir exists for user <% $.overcloud_admin %>
              file:
                path: /home/<% $.overcloud_admin %>/.ssh
                state: directory
                owner: <% $.overcloud_admin %>
                group: <% $.overcloud_admin %>
                mode: 0700
            - name: ensure authorized_keys file exists for user <% $.overcloud_admin %>
              file:
                path: /home/<% $.overcloud_admin %>/.ssh/authorized_keys
                state: touch
                owner: <% $.overcloud_admin %>
                group: <% $.overcloud_admin %>
                mode: 0700
            - name: authorize TripleO Mistral key for user <% $.overcloud_admin %>
              lineinfile:
                path: /home/<% $.overcloud_admin %>/.ssh/authorized_keys
                line: <% $.pubkey %>
                regexp: "Generated by TripleO"

      # Nova variant
      create_admin_via_nova:
        workflow: tripleo.access.v1.create_admin_via_nova
        input:
          queue_name: <% $.queue_name %>
          ssh_servers: <% $.ssh_servers %>
          tasks: <% $.create_admin_tasks %>
          overcloud_admin: <% $.overcloud_admin %>

      # SSH variant
      create_admin_via_ssh:
        workflow: tripleo.access.v1.create_admin_via_ssh
        input:
          ssh_private_key: <% $.ssh_private_key %>
          ssh_user: <% $.ssh_user %>
          ssh_servers: <% $.ssh_servers %>
          tasks: <% $.create_admin_tasks %>

  create_admin_via_nova:
    input:
      - tasks
      - queue_name: tripleo
      - ssh_servers: []
      - overcloud_admin: tripleo-admin
      - ansible_extra_env_variables:
            ANSIBLE_HOST_KEY_CHECKING: 'False'
    tags:
      - tripleo-common-managed
    tasks:
      get_servers:
        action: nova.servers_list
        on-success: create_admin
        publish:
          servers: <% let(root => $) -> task().result._info.where($.addresses.ctlplane.addr.any($ in $root.ssh_servers)) %>

      create_admin:
        workflow: tripleo.deployment.v1.deploy_on_server
        on-success: get_privkey
        with-items: server in <% $.servers %>
        input:
          server_name: <% $.server.name %>
          server_uuid: <% $.server.id %>
          queue_name: <% $.queue_name %>
          config_name: create_admin
          group: ansible
          config: |
            - hosts: localhost
              connection: local
              tasks: <% json_pp($.tasks) %>

      get_privkey:
        action: tripleo.validations.get_privkey
        on-success: wait_for_occ
        publish:
          privkey: <% task().result %>

      wait_for_occ:
        action: tripleo.ansible-playbook
        input:
          inventory:
            overcloud:
              hosts: <% $.ssh_servers.toDict($, {}) %>
          remote_user: <% $.overcloud_admin %>
          ssh_private_key: <% $.privkey %>
          extra_env_variables: <% $.ansible_extra_env_variables %>
          playbook:
            - hosts: overcloud
              gather_facts: no
              tasks:
                - name: wait for connection
                  wait_for_connection:
                    sleep: 5
                    timeout: 300

  create_admin_via_ssh:
    input:
      - tasks
      - ssh_private_key
      - ssh_user
      - ssh_servers
      - ansible_extra_env_variables:
            ANSIBLE_HOST_KEY_CHECKING: 'False'

    tags:
      - tripleo-common-managed
    tasks:
      write_tmp_playbook:
        action: tripleo.ansible-playbook
        input:
          inventory:
            overcloud:
              hosts: <% $.ssh_servers.toDict($, {}) %>
          remote_user: <% $.ssh_user %>
          ssh_private_key: <% $.ssh_private_key %>
          extra_env_variables: <% $.ansible_extra_env_variables %>
          become: true
          become_user: root
          playbook:
            - hosts: overcloud
              tasks: <% $.tasks %>
'
2018-12-03 06:18:49,309 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 6130
2018-12-03 06:18:49,310 DEBUG: RESP: [201] Content-Length: 6130 Content-Type: application/json Date: Mon, 03 Dec 2018 11:18:49 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.access.v1\ndescription: TripleO administration access workflows\n\nworkflows:\n\n  enable_ssh_admin:\n    description: >-\n      This workflow creates an admin user on the overcloud nodes,\n      which can then be used for connecting for automated\n      administrative or deployment tasks, e.g. via Ansible. The\n      workflow can be used both for Nova-managed and split-stack\n      deployments, assuming the correct input values are passed\n      in. The workflow defaults to Nova-managed approach, for which no\n      additional parameters need to be supplied. In case of\n      split-stack, temporary ssh connection details (user, key, list\n      of servers) need to be provided -- these are only used\n      temporarily to create the actual ssh admin user for use by\n      Mistral.\n    tags:\n      - tripleo-common-managed\n    input:\n      - ssh_private_key: null\n      - ssh_user: null\n      - ssh_servers: []\n      - overcloud_admin: tripleo-admin\n      - queue_name: tripleo\n    tasks:\n      get_pubkey:\n        action: tripleo.validations.get_pubkey\n        on-success: generate_playbook\n        publish:\n          pubkey: <% task().result %>\n\n      generate_playbook:\n        on-success:\n          - create_admin_via_nova: <% $.ssh_private_key = null %>\n          - create_admin_via_ssh: <% $.ssh_private_key != null %>\n        publish:\n          create_admin_tasks:\n            - name: create user <% $.overcloud_admin %>\n              user:\n                name: '<% $.overcloud_admin %>'\n            - name: grant admin rights to user <% $.overcloud_admin %>\n              copy:\n                dest: /etc/sudoers.d/<% $.overcloud_admin %>\n                content: |\n                  <% $.overcloud_admin %> ALL=(ALL) NOPASSWD:ALL\n                mode: 0440\n            - name: ensure .ssh dir exists for user <% $.overcloud_admin %>\n              file:\n                path: /home/<% $.overcloud_admin %>/.ssh\n                state: directory\n                owner: <% $.overcloud_admin %>\n                group: <% $.overcloud_admin %>\n                mode: 0700\n            - name: ensure authorized_keys file exists for user <% $.overcloud_admin %>\n              file:\n                path: /home/<% $.overcloud_admin %>/.ssh/authorized_keys\n                state: touch\n                owner: <% $.overcloud_admin %>\n                group: <% $.overcloud_admin %>\n                mode: 0700\n            - name: authorize TripleO Mistral key for user <% $.overcloud_admin %>\n              lineinfile:\n                path: /home/<% $.overcloud_admin %>/.ssh/authorized_keys\n                line: <% $.pubkey %>\n                regexp: \"Generated by TripleO\"\n\n      # Nova variant\n      create_admin_via_nova:\n        workflow: tripleo.access.v1.create_admin_via_nova\n        input:\n          queue_name: <% $.queue_name %>\n          ssh_servers: <% $.ssh_servers %>\n          tasks: <% $.create_admin_tasks %>\n          overcloud_admin: <% $.overcloud_admin %>\n\n      # SSH variant\n      create_admin_via_ssh:\n        workflow: tripleo.access.v1.create_admin_via_ssh\n        input:\n          ssh_private_key: <% $.ssh_private_key %>\n          ssh_user: <% $.ssh_user %>\n          ssh_servers: <% $.ssh_servers %>\n          tasks: <% $.create_admin_tasks %>\n\n  create_admin_via_nova:\n    input:\n      - tasks\n      - queue_name: tripleo\n      - ssh_servers: []\n      - overcloud_admin: tripleo-admin\n      - ansible_extra_env_variables:\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n    tags:\n      - tripleo-common-managed\n    tasks:\n      get_servers:\n        action: nova.servers_list\n        on-success: create_admin\n        publish:\n          servers: <% let(root => $) -> task().result._info.where($.addresses.ctlplane.addr.any($ in $root.ssh_servers)) %>\n\n      create_admin:\n        workflow: tripleo.deployment.v1.deploy_on_server\n        on-success: get_privkey\n        with-items: server in <% $.servers %>\n        input:\n          server_name: <% $.server.name %>\n          server_uuid: <% $.server.id %>\n          queue_name: <% $.queue_name %>\n          config_name: create_admin\n          group: ansible\n          config: |\n            - hosts: localhost\n              connection: local\n              tasks: <% json_pp($.tasks) %>\n\n      get_privkey:\n        action: tripleo.validations.get_privkey\n        on-success: wait_for_occ\n        publish:\n          privkey: <% task().result %>\n\n      wait_for_occ:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            overcloud:\n              hosts: <% $.ssh_servers.toDict($, {}) %>\n          remote_user: <% $.overcloud_admin %>\n          ssh_private_key: <% $.privkey %>\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          playbook:\n            - hosts: overcloud\n              gather_facts: no\n              tasks:\n                - name: wait for connection\n                  wait_for_connection:\n                    sleep: 5\n                    timeout: 300\n\n  create_admin_via_ssh:\n    input:\n      - tasks\n      - ssh_private_key\n      - ssh_user\n      - ssh_servers\n      - ansible_extra_env_variables:\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n\n    tags:\n      - tripleo-common-managed\n    tasks:\n      write_tmp_playbook:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            overcloud:\n              hosts: <% $.ssh_servers.toDict($, {}) %>\n          remote_user: <% $.ssh_user %>\n          ssh_private_key: <% $.ssh_private_key %>\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          become: true\n          become_user: root\n          playbook:\n            - hosts: overcloud\n              tasks: <% $.tasks %>\n", "name": "tripleo.access.v1", "tags": [], "created_at": "2018-12-03 11:18:49", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "a834aa44-b2b5-4f7b-82c7-8a4b8ed1819c"}

2018-12-03 06:18:49,310 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:18:49,312 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.baremetal.v1
description: TripleO Baremetal Workflows

workflows:

  set_node_state:
    input:
      - node_uuid
      - state_action
      - target_state
      - error_states:
          # The default includes all failure states, even unused by TripleO.
          - 'error'
          - 'adopt failed'
          - 'clean failed'
          - 'deploy failed'
          - 'inspect failed'
          - 'rescue failed'

    tags:
      - tripleo-common-managed

    tasks:

      set_provision_state:
        on-success: wait_for_provision_state
        on-error: set_provision_state_failed
        action: ironic.node_set_provision_state node_uuid=<% $.node_uuid %> state=<% $.state_action %>

      set_provision_state_failed:
        publish:
          message: <% task(set_provision_state).result %>
        on-complete: fail

      wait_for_provision_state:
        action: ironic.node_get
        input:
          node_id: <% $.node_uuid %>
          fields: ['provision_state', 'last_error']
        timeout: 1200 #20 minutes
        retry:
          delay: 3
          count: 400
          continue-on: <% not task().result.provision_state in [$.target_state] + $.error_states %>
        on-complete:
          - state_not_reached: <% task().result.provision_state != $.target_state %>

      state_not_reached:
        publish:
          message: >-
            Node <% $.node_uuid %> did not reach state "<% $.target_state %>",
            the state is "<% task(wait_for_provision_state).result.provision_state %>",
            error: <% task(wait_for_provision_state).result.last_error %>
        on-complete: fail

    output-on-error:
      result: <% $.message %>

  set_power_state:
    input:
      - node_uuid
      - state_action
      - target_state
      - error_state: 'error'

    tags:
      - tripleo-common-managed

    tasks:

      set_power_state:
        on-success: wait_for_power_state
        on-error: set_power_state_failed
        action: ironic.node_set_power_state node_id=<% $.node_uuid %> state=<% $.state_action %>

      set_power_state_failed:
        publish:
          message: <% task(set_power_state).result %>
        on-complete: fail

      wait_for_power_state:
        action: ironic.node_get
        input:
          node_id: <% $.node_uuid %>
          fields: ['power_state', 'last_error']
        timeout: 120 #2 minutes
        retry:
          delay: 6
          count: 20
          continue-on: <% not task().result.power_state in [$.target_state, $.error_state] %>
        on-complete:
          - state_not_reached: <% task().result.power_state != $.target_state %>

      state_not_reached:
        publish:
          message: >-
            Node <% $.node_uuid %> did not reach power state "<% $.target_state %>",
            the state is "<% task(wait_for_power_state).result.power_state %>",
            error: <% task(wait_for_power_state).result.last_error %>
        on-complete: fail

    output-on-error:
      result: <% $.message %>

  manual_cleaning:
    input:
      - node_uuid
      - clean_steps
      - timeout: 7200  # 2 hours (cleaning can take really long)
      - retry_delay: 10
      - retry_count: 720
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      set_provision_state:
        on-success: wait_for_provision_state
        on-error: set_provision_state_failed
        action: ironic.node_set_provision_state node_uuid=<% $.node_uuid %> state='clean' cleansteps=<% $.clean_steps %>

      set_provision_state_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(set_provision_state).result %>

      wait_for_provision_state:
        on-success: send_message
        action: ironic.node_get node_id=<% $.node_uuid %>
        timeout: <% $.timeout %>
        retry:
          delay: <% $.retry_delay %>
          count: <% $.retry_count %>
          continue-on: <% task().result.provision_state != 'manageable' %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.manual_cleaning
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  validate_nodes:
    description: Validate nodes JSON

    input:
     - nodes_json
     - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      validate_nodes:
        action: tripleo.baremetal.validate_nodes
        on-success: send_message
        on-error: validation_failed
        input:
           nodes_json: <% $.nodes_json %>

      validation_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(validate_nodes).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.validate_nodes
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  register_or_update:
    description: Take nodes JSON and create nodes in a "manageable" state

    input:
     - nodes_json
     - remove: False
     - queue_name: tripleo
     - kernel_name: null
     - ramdisk_name: null
     - instance_boot_option: local
     - initial_state: manageable

    tags:
      - tripleo-common-managed

    tasks:

      validate_input:
        workflow: tripleo.baremetal.v1.validate_nodes
        on-success: register_or_update_nodes
        on-error: validation_failed
        input:
          nodes_json: <% $.nodes_json %>
          queue_name: <% $.queue_name %>

      validation_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(validate_input).result %>
          registered_nodes: []

      register_or_update_nodes:
        action: tripleo.baremetal.register_or_update_nodes
        on-success:
          - set_nodes_managed: <% $.initial_state != "enroll" %>
          - send_message: <% $.initial_state = "enroll" %>
        on-error: set_status_failed_register_or_update_nodes
        input:
           nodes_json: <% $.nodes_json %>
           remove: <% $.remove %>
           kernel_name: <% $.kernel_name %>
           ramdisk_name: <% $.ramdisk_name %>
           instance_boot_option: <% $.instance_boot_option %>
        publish:
          registered_nodes: <% task().result %>
          new_nodes: <% task().result.where($.provision_state = 'enroll') %>

      set_status_failed_register_or_update_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(register_or_update_nodes).result %>
          registered_nodes: []

      set_nodes_managed:
        on-success:
          - set_nodes_available: <% $.initial_state = "available" %>
          - send_message: <% $.initial_state != "available" %>
        on-error: set_status_failed_nodes_managed
        workflow: tripleo.baremetal.v1.manage
        input:
          node_uuids: <% $.new_nodes.uuid %>
          queue_name: <% $.queue_name %>
        publish:
          status: SUCCESS
          message: <% $.new_nodes.len() %> node(s) successfully moved to the "manageable" state.

      set_status_failed_nodes_managed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(set_nodes_managed).result %>

      set_nodes_available:
        on-success: send_message
        on-error: set_status_failed_nodes_available
        workflow: tripleo.baremetal.v1.provide node_uuids=<% $.new_nodes.uuid %> queue_name=<% $.queue_name %>
        publish:
          status: SUCCESS
          message: <% $.new_nodes.len() %> node(s) successfully moved to the "available" state.

      set_status_failed_nodes_available:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(set_nodes_available).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.register_or_update
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                registered_nodes: <% $.registered_nodes or [] %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  provide:
    description: Take a list of nodes and move them to "available"

    input:
      - node_uuids
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      set_nodes_available:
        on-success: cell_v2_discover_hosts
        on-error: set_status_failed_nodes_available
        with-items: uuid in <% $.node_uuids %>
        workflow: tripleo.baremetal.v1.set_node_state
        input:
          node_uuid: <% $.uuid %>
          queue_name: <% $.queue_name %>
          state_action: 'provide'
          target_state: 'available'

      set_status_failed_nodes_available:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(set_nodes_available).result %>

      cell_v2_discover_hosts:
        on-success: try_power_off
        on-error: cell_v2_discover_hosts_failed
        workflow: tripleo.baremetal.v1.cellv2_discovery
        input:
          node_uuids: <% $.node_uuids %>
          queue_name: <% $.queue_name %>
        timeout: 900 #15 minutes
        retry:
          delay: 30
          count: 30

      cell_v2_discover_hosts_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(cell_v2_discover_hosts).result %>

      try_power_off:
        on-success: send_message
        on-error: power_off_failed
        with-items: uuid in <% $.node_uuids %>
        workflow: tripleo.baremetal.v1.set_power_state
        input:
          node_uuid: <% $.uuid %>
          queue_name: <% $.queue_name %>
          state_action: 'off'
          target_state: 'power off'
        publish:
          status: SUCCESS
          message: <% $.node_uuids.len() %> node(s) successfully moved to the "available" state.

      power_off_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(try_power_off).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.provide
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  provide_manageable_nodes:
    description: Provide all nodes in a 'manageable' state.

    input:
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      get_manageable_nodes:
        action: ironic.node_list maintenance=False associated=False
        on-success: provide_manageable
        on-error: set_status_failed_get_manageable_nodes
        publish:
          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>

      set_status_failed_get_manageable_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_manageable_nodes).result %>

      provide_manageable:
        on-success: send_message
        workflow: tripleo.baremetal.v1.provide
        input:
          node_uuids: <% $.managed_nodes %>
          queue_name: <% $.queue_name %>
        publish:
          status: SUCCESS

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.provide_manageable_nodes
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  manage:
    description: Set a list of nodes to 'manageable' state

    input:
      - node_uuids
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      set_nodes_manageable:
        on-success: send_message
        on-error: set_status_failed_nodes_manageable
        with-items: uuid in <% $.node_uuids %>
        workflow: tripleo.baremetal.v1.set_node_state
        input:
          node_uuid: <% $.uuid %>
          state_action: 'manage'
          target_state: 'manageable'
          error_states:
            # node going back to enroll designates power credentials failure
            - 'enroll'
            - 'error'

      set_status_failed_nodes_manageable:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(set_nodes_manageable).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.manage
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  _introspect:
    description: >
        An internal workflow. The tripleo.baremetal.v1.introspect workflow
        should be used for introspection.

    input:
      - node_uuid
      - timeout
      - queue_name

    output:
      result: <% task(start_introspection).result %>

    tags:
      - tripleo-common-managed

    tasks:
      start_introspection:
        action: baremetal_introspection.introspect uuid=<% $.node_uuid %>
        on-success: wait_for_introspection_to_finish
        on-error: set_status_failed_start_introspection

      set_status_failed_start_introspection:
        publish:
          status: FAILED
          message: <% task(start_introspection).result %>
          introspected_nodes: []
        on-success: send_message

      wait_for_introspection_to_finish:
        action: baremetal_introspection.wait_for_finish
        input:
          uuids: <% [$.node_uuid] %>
          # The interval is 10 seconds, so divide to make the overall timeout
          # in seconds correct.
          max_retries: <% $.timeout / 10 %>
          retry_interval: 10
        publish:
          introspected_node: <% task().result.values().first() %>
          status: <% bool(task().result.values().first().error) and "FAILED" or "SUCCESS" %>
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-success: wait_for_introspection_to_finish_success
        on-error: wait_for_introspection_to_finish_error

      wait_for_introspection_to_finish_success:
        publish:
          message: <% "Introspection of node {0} completed. Status:{1}. Errors:{2}".format($.introspected_node.uuid, $.status, $.introspected_node.error) %>
        on-success: send_message

      wait_for_introspection_to_finish_error:
        publish:
          message: <% "Introspection of node {0} timed out.".format($.node_uuid) %>
        on-success: send_message

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1._introspect
              payload:
                status: <% $.status %>
                message: <% $.message %>
                introspected_node: <% $.get('introspected_node') %>
                node_uuid: <% $.node_uuid %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  introspect:
    description: >
        Take a list of nodes and move them through introspection.

        By default each node will attempt introspection up to 3 times (two
        retries plus the initial attemp) if it fails. This behaviour can be
        modified by changing the max_retry_attempts input.

        The workflow will assume the node has timed out after 20 minutes (1200
        seconds). This can be changed by passing the node_timeout input in
        seconds.

    input:
      - node_uuids
      - run_validations: False
      - queue_name: tripleo
      - concurrency: 20
      - max_retry_attempts: 2
      - node_timeout: 1200

    tags:
      - tripleo-common-managed

    task-defaults:
      on-error: unhandled_error

    tasks:
      initialize:
        publish:
          introspection_attempt: 1
        on-complete:
          - run_validations: <% $.run_validations %>
          - introspect_nodes: <% not $.run_validations %>

      run_validations:
        workflow: tripleo.validations.v1.run_groups
        input:
          group_names:
            - 'pre-introspection'
          queue_name: <% $.queue_name %>
        on-success: introspect_nodes
        on-error: set_validations_failed

      set_validations_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(run_validations).result %>

      introspect_nodes:
        with-items: uuid in <% $.node_uuids %>
        concurrency: <% $.concurrency %>
        workflow: _introspect
        input:
          node_uuid: <% $.uuid %>
          queue_name: <% $.queue_name %>
          timeout: <% $.node_timeout %>
        # on-error is triggered if one or more nodes failed introspection. We
        # still go to get_introspection_status as it will collect the result
        # for each node. Unless we hit the retry limit.
        on-error:
         - get_introspection_status: <% $.introspection_attempt <= $.max_retry_attempts %>
         - max_retry_attempts_reached: <% $.introspection_attempt > $.max_retry_attempts %>
        on-success: get_introspection_status

      get_introspection_status:
        with-items: uuid in <% $.node_uuids %>
        action: baremetal_introspection.get_status
        input:
          uuid: <% $.uuid %>
        publish:
          introspected_nodes: <% task().result.toDict($.uuid, $) %>
          # Currently there is no way for us to ignore user introspection
          # aborts. This means we will retry aborted nodes until the Ironic API
          # gives us more details (error code or a boolean to show aborts etc.)
          # If a node hasn't finished, we consider it to be failed.
          # TODO(d0ugal): When possible, don't retry introspection of nodes
          #               that a user manually aborted.
          failed_introspection: <% task().result.where($.finished = true and $.error != null).select($.uuid) + task().result.where($.finished = false).select($.uuid) %>
        publish-on-error:
          # If a node fails to start introspection, getting the status can fail.
          # When that happens, the result is a string and the nodes need to be
          # filtered out.
          introspected_nodes: <% task().result.where(isDict($)).toDict($.uuid, $) %>
          # If there was an error, the exception string we get doesn't give us
          # the UUID. So we use a set difference to find the UUIDs missing in
          # the results. These are then added to the failed nodes.
          failed_introspection: <% ($.node_uuids.toSet() - task().result.where(isDict($)).select($.uuid).toSet()) + task().result.where(isDict($)).where($.finished = true and $.error != null).toSet() + task().result.where(isDict($)).where($.finished = false).toSet() %>
        on-error: increase_attempt_counter
        on-success:
          - successful_introspection: <% $.failed_introspection.len() = 0 %>
          - increase_attempt_counter: <% $.failed_introspection.len() > 0 %>

      increase_attempt_counter:
        publish:
          introspection_attempt: <% $.introspection_attempt + 1 %>
        on-complete:
          retry_failed_nodes

      retry_failed_nodes:
        publish:
          status: RUNNING
          message: <% 'Retrying {0} nodes that failed introspection. Attempt {1} of {2} '.format($.failed_introspection.len(), $.introspection_attempt, $.max_retry_attempts + 1) %>
          # We are about to retry, update the tracking stats.
          node_uuids: <% $.failed_introspection %>
        on-success:
          - send_message
          - introspect_nodes

      max_retry_attempts_reached:
        publish:
          status: FAILED
          message: <% 'Retry limit reached with {0} nodes still failing introspection'.format($.failed_introspection.len()) %>
        on-complete: send_message

      successful_introspection:
        publish:
          status: SUCCESS
          message: Successfully introspected <% $.introspected_nodes.len() %> node(s).
        on-complete: send_message

      unhandled_error:
        publish:
          status: FAILED
          message: "Unhandled workflow error"
        on-complete: send_message

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.introspect
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                introspected_nodes: <% $.get('introspected_nodes', []) %>
                failed_introspection: <% $.get('failed_introspection', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  introspect_manageable_nodes:
    description: Introspect all nodes in a 'manageable' state.

    input:
      - run_validations: False
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      get_manageable_nodes:
        action: ironic.node_list maintenance=False associated=False
        on-success: validate_nodes
        on-error: set_status_failed_get_manageable_nodes
        publish:
          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>

      set_status_failed_get_manageable_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_manageable_nodes).result %>

      validate_nodes:
        on-success:
          - introspect_manageable: <% $.managed_nodes.len() > 0 %>
          - set_status_failed_no_nodes: <% $.managed_nodes.len() = 0 %>

      set_status_failed_no_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: No manageable nodes to introspect. Check node states and maintenance.

      introspect_manageable:
        on-success: send_message
        on-error: set_status_introspect_manageable
        workflow: tripleo.baremetal.v1.introspect
        input:
          node_uuids: <% $.managed_nodes %>
          run_validations: <% $.run_validations %>
          queue_name: <% $.queue_name %>
        publish:
          introspected_nodes: <% task().result.introspected_nodes %>

      set_status_introspect_manageable:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(introspect_manageable).result %>
          introspected_nodes: []

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.introspect_manageable_nodes
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                introspected_nodes: <% $.get('introspected_nodes', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  configure:
    description: Take a list of manageable nodes and update their boot configuration.

    input:
      - node_uuids
      - queue_name: tripleo
      - kernel_name: bm-deploy-kernel
      - ramdisk_name: bm-deploy-ramdisk
      - instance_boot_option: null
      - root_device: null
      - root_device_minimum_size: 4
      - overwrite_root_device_hints: False

    tags:
      - tripleo-common-managed

    tasks:

      configure_boot:
        on-success: configure_root_device
        on-error: set_status_failed_configure_boot
        with-items: node_uuid in <% $.node_uuids %>
        action: tripleo.baremetal.configure_boot node_uuid=<% $.node_uuid %> kernel_name=<% $.kernel_name %> ramdisk_name=<% $.ramdisk_name %> instance_boot_option=<% $.instance_boot_option %>

      configure_root_device:
        on-success: send_message
        on-error: set_status_failed_configure_root_device
        with-items: node_uuid in <% $.node_uuids %>
        action: tripleo.baremetal.configure_root_device node_uuid=<% $.node_uuid %> root_device=<% $.root_device %> minimum_size=<% $.root_device_minimum_size %> overwrite=<% $.overwrite_root_device_hints %>
        publish:
          status: SUCCESS
          message: 'Successfully configured the nodes.'

      set_status_failed_configure_boot:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(configure_boot).result %>

      set_status_failed_configure_root_device:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(configure_root_device).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.configure
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  configure_manageable_nodes:
    description: Update the boot configuration of all nodes in 'manageable' state.

    input:
      - queue_name: tripleo
      - kernel_name: 'bm-deploy-kernel'
      - ramdisk_name: 'bm-deploy-ramdisk'
      - instance_boot_option: null
      - root_device: null
      - root_device_minimum_size: 4
      - overwrite_root_device_hints: False

    tags:
      - tripleo-common-managed

    tasks:

      get_manageable_nodes:
        action: ironic.node_list maintenance=False associated=False
        on-success: configure_manageable
        on-error: set_status_failed_get_manageable_nodes
        publish:
          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>

      configure_manageable:
        on-success: send_message
        on-error: set_status_failed_configure_manageable
        workflow: tripleo.baremetal.v1.configure
        input:
          node_uuids: <% $.managed_nodes %>
          queue_name: <% $.queue_name %>
          kernel_name: <% $.kernel_name %>
          ramdisk_name: <% $.ramdisk_name %>
          instance_boot_option: <% $.instance_boot_option %>
          root_device: <% $.root_device %>
          root_device_minimum_size: <% $.root_device_minimum_size %>
          overwrite_root_device_hints: <% $.overwrite_root_device_hints %>
        publish:
          message: 'Manageable nodes configured successfully.'

      set_status_failed_configure_manageable:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(configure_manageable).result %>

      set_status_failed_get_manageable_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_manageable_nodes).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.configure_manageable_nodes
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  tag_node:
    description: Tag a node with a role
    input:
      - node_uuid
      - role: null
      - queue_name: tripleo

    task-defaults:
      on-error: send_message

    tags:
      - tripleo-common-managed

    tasks:

      update_node:
        on-success: send_message
        action: tripleo.baremetal.update_node_capability node_uuid=<% $.node_uuid %> capability='profile' value=<% $.role %>
        publish:
          message: <% task().result %>
          status: SUCCESS

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.tag_node
              payload:
                status: <% $.get('status', 'FAILED') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  tag_nodes:
    description: Runs the tag_node workflow in a loop
    input:
      - tag_node_uuids
      - untag_node_uuids
      - role
      - plan: overcloud
      - queue_name: tripleo

    task-defaults:
      on-error: send_message

    tags:
      - tripleo-common-managed

    tasks:

      tag_nodes:
        with-items: node_uuid in <% $.tag_node_uuids %>
        workflow: tripleo.baremetal.v1.tag_node
        input:
          node_uuid: <% $.node_uuid %>
          queue_name: <% $.queue_name %>
          role: <% $.role %>
        concurrency: 1
        on-success: untag_nodes

      untag_nodes:
        with-items: node_uuid in <% $.untag_node_uuids %>
        workflow: tripleo.baremetal.v1.tag_node
        input:
          node_uuid: <% $.node_uuid %>
          queue_name: <% $.queue_name %>
        concurrency: 1
        on-success: update_role_parameters

      update_role_parameters:
        on-success: send_message
        action: tripleo.parameters.update_role role=<% $.role %> container=<% $.plan %>
        publish:
          message: <% task().result %>
          status: SUCCESS

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.tag_nodes
              payload:
                status: <% $.get('status', 'FAILED') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  nodes_with_hint:
    description: Find nodes matching a hint regex
    input:
      - hint_regex
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      get_nodes:
        with-items: provision_state in <% ['available', 'active'] %>
        action: ironic.node_list maintenance=false provision_state=<% $.provision_state %> detail=true
        on-success: get_matching_nodes
        on-error: set_status_failed_get_nodes

      get_matching_nodes:
        with-items: node in <% task(get_nodes).result.flatten() %>
        action: tripleo.baremetal.get_node_hint node=<% $.node %>
        on-success: send_message
        on-error: set_status_failed_get_matching_nodes
        publish:
          matching_nodes: <% let(hint_regex => $.hint_regex) -> task().result.where($.hint and $.hint.matches($hint_regex)).uuid %>

      set_status_failed_get_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_nodes).result %>

      set_status_failed_get_matching_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_matching_nodes).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.nodes_with_hint
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                matching_nodes: <% $.matching_nodes or [] %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  nodes_with_profile:
    description: Find nodes with a specific profile
    input:
      - profile
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      get_active_nodes:
        action: ironic.node_list maintenance=false provision_state='active' detail=true
        on-success: get_available_nodes
        on-error: set_status_failed_get_active_nodes

      get_available_nodes:
        action: ironic.node_list maintenance=false provision_state='available' detail=true
        on-success: get_matching_nodes
        on-error: set_status_failed_get_available_nodes

      get_matching_nodes:
        with-items: node in <% task(get_available_nodes).result + task(get_active_nodes).result %>
        action: tripleo.baremetal.get_profile node=<% $.node %>
        on-success: send_message
        on-error: set_status_failed_get_matching_nodes
        publish:
          matching_nodes: <% let(input_profile_name => $.profile) -> task().result.where($.profile = $input_profile_name).uuid %>

      set_status_failed_get_active_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_active_nodes).result %>

      set_status_failed_get_available_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_available_nodes).result %>

      set_status_failed_get_matching_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_matching_nodes).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.nodes_with_profile
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                matching_nodes: <% $.matching_nodes or [] %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  create_raid_configuration:
    description: Create and apply RAID configuration for given nodes
    input:
      - node_uuids
      - configuration
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      set_configuration:
        with-items: node_uuid in <% $.node_uuids %>
        action: ironic.node_set_target_raid_config node_ident=<% $.node_uuid %> target_raid_config=<% $.configuration %>
        on-success: apply_configuration
        on-error: set_configuration_failed

      set_configuration_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(set_configuration).result %>

      apply_configuration:
        with-items: node_uuid in <% $.node_uuids %>
        workflow: tripleo.baremetal.v1.manual_cleaning
        input:
          node_uuid: <% $.node_uuid %>
          clean_steps:
            - interface: raid
              step: delete_configuration
            - interface: raid
              step: create_configuration
          timeout: 1800  # building RAID should be fast than general cleaning
          retry_count: 180
          retry_delay: 10
        on-success: send_message
        on-error: apply_configuration_failed
        publish:
          message: <% task().result %>
          status: SUCCESS

      apply_configuration_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(apply_configuration).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.create_raid_configuration
              payload:
                status: <% $.get('status', 'FAILED') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>


  cellv2_discovery:
    description: Run cell_v2 host discovery

    input:
      - node_uuids
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      cell_v2_discover_hosts:
        on-success: wait_for_nova_resources
        on-error: cell_v2_discover_hosts_failed
        action: tripleo.baremetal.cell_v2_discover_hosts

      cell_v2_discover_hosts_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(cell_v2_discover_hosts).result %>

      wait_for_nova_resources:
        on-success: send_message
        on-error: wait_for_nova_resources_failed
        with-items: node_uuid in <% $.node_uuids %>
        action: nova.hypervisors_find hypervisor_hostname=<% $.node_uuid %>

      wait_for_nova_resources_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(wait_for_nova_resources).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.cellv2_discovery
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>


  discover_nodes:
    description: Run nodes discovery over the given IP range

    input:
      - ip_addresses
      - credentials
      - ports: [623]
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      get_all_nodes:
        action: ironic.node_list
        input:
          fields: ["uuid", "driver", "driver_info"]
          limit: 0
        on-success: get_candidate_nodes
        on-error: get_all_nodes_failed
        publish:
          existing_nodes: <% task().result %>

      get_all_nodes_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_all_nodes).result %>

      get_candidate_nodes:
        action: tripleo.baremetal.get_candidate_nodes
        input:
          ip_addresses: <% $.ip_addresses %>
          credentials: <% $.credentials %>
          ports: <% $.ports %>
          existing_nodes: <% $.existing_nodes %>
        on-success: probe_nodes
        on-error: get_candidate_nodes_failed
        publish:
          candidates: <% task().result %>

      get_candidate_nodes_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_candidate_nodes).result %>

      probe_nodes:
        action: tripleo.baremetal.probe_node
        on-success: send_message
        on-error: probe_nodes_failed
        input:
          ip: <% $.node.ip %>
          port: <% $.node.port %>
          username: <% $.node.username %>
          password: <% $.node.password %>
        with-items:
          - node in <% $.candidates %>
        publish:
          nodes_json: <% task().result.where($ != null) %>

      probe_nodes_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(probe_nodes).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.discover_nodes
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                nodes_json: <% $.get('nodes_json', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  discover_and_enroll_nodes:
    description: Run nodes discovery over the given IP range and enroll nodes

    input:
      - ip_addresses
      - credentials
      - ports: [623]
      - kernel_name: null
      - ramdisk_name: null
      - instance_boot_option: local
      - initial_state: manageable
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      discover_nodes:
        workflow: tripleo.baremetal.v1.discover_nodes
        input:
          ip_addresses: <% $.ip_addresses %>
          ports: <% $.ports %>
          credentials: <% $.credentials %>
          queue_name: <% $.queue_name %>
        on-success: enroll_nodes
        on-error: discover_nodes_failed
        publish:
          nodes_json: <% task().result.nodes_json %>

      discover_nodes_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(discover_nodes).result %>

      enroll_nodes:
        workflow: tripleo.baremetal.v1.register_or_update
        input:
          nodes_json: <% $.nodes_json %>
          kernel_name: <% $.kernel_name %>
          ramdisk_name: <% $.ramdisk_name %>
          instance_boot_option: <% $.instance_boot_option %>
          initial_state: <% $.initial_state %>
        on-success: send_message
        on-error: enroll_nodes_failed
        publish:
          registered_nodes: <% task().result.registered_nodes %>

      enroll_nodes_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(enroll_nodes).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.discover_and_enroll_nodes
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                registered_nodes: <% $.get('registered_nodes', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-03 06:18:54,429 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 45010
2018-12-03 06:18:54,471 DEBUG: RESP: [201] Content-Length: 45010 Content-Type: application/json Date: Mon, 03 Dec 2018 11:18:54 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.baremetal.v1\ndescription: TripleO Baremetal Workflows\n\nworkflows:\n\n  set_node_state:\n    input:\n      - node_uuid\n      - state_action\n      - target_state\n      - error_states:\n          # The default includes all failure states, even unused by TripleO.\n          - 'error'\n          - 'adopt failed'\n          - 'clean failed'\n          - 'deploy failed'\n          - 'inspect failed'\n          - 'rescue failed'\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_provision_state:\n        on-success: wait_for_provision_state\n        on-error: set_provision_state_failed\n        action: ironic.node_set_provision_state node_uuid=<% $.node_uuid %> state=<% $.state_action %>\n\n      set_provision_state_failed:\n        publish:\n          message: <% task(set_provision_state).result %>\n        on-complete: fail\n\n      wait_for_provision_state:\n        action: ironic.node_get\n        input:\n          node_id: <% $.node_uuid %>\n          fields: ['provision_state', 'last_error']\n        timeout: 1200 #20 minutes\n        retry:\n          delay: 3\n          count: 400\n          continue-on: <% not task().result.provision_state in [$.target_state] + $.error_states %>\n        on-complete:\n          - state_not_reached: <% task().result.provision_state != $.target_state %>\n\n      state_not_reached:\n        publish:\n          message: >-\n            Node <% $.node_uuid %> did not reach state \"<% $.target_state %>\",\n            the state is \"<% task(wait_for_provision_state).result.provision_state %>\",\n            error: <% task(wait_for_provision_state).result.last_error %>\n        on-complete: fail\n\n    output-on-error:\n      result: <% $.message %>\n\n  set_power_state:\n    input:\n      - node_uuid\n      - state_action\n      - target_state\n      - error_state: 'error'\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_power_state:\n        on-success: wait_for_power_state\n        on-error: set_power_state_failed\n        action: ironic.node_set_power_state node_id=<% $.node_uuid %> state=<% $.state_action %>\n\n      set_power_state_failed:\n        publish:\n          message: <% task(set_power_state).result %>\n        on-complete: fail\n\n      wait_for_power_state:\n        action: ironic.node_get\n        input:\n          node_id: <% $.node_uuid %>\n          fields: ['power_state', 'last_error']\n        timeout: 120 #2 minutes\n        retry:\n          delay: 6\n          count: 20\n          continue-on: <% not task().result.power_state in [$.target_state, $.error_state] %>\n        on-complete:\n          - state_not_reached: <% task().result.power_state != $.target_state %>\n\n      state_not_reached:\n        publish:\n          message: >-\n            Node <% $.node_uuid %> did not reach power state \"<% $.target_state %>\",\n            the state is \"<% task(wait_for_power_state).result.power_state %>\",\n            error: <% task(wait_for_power_state).result.last_error %>\n        on-complete: fail\n\n    output-on-error:\n      result: <% $.message %>\n\n  manual_cleaning:\n    input:\n      - node_uuid\n      - clean_steps\n      - timeout: 7200  # 2 hours (cleaning can take really long)\n      - retry_delay: 10\n      - retry_count: 720\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_provision_state:\n        on-success: wait_for_provision_state\n        on-error: set_provision_state_failed\n        action: ironic.node_set_provision_state node_uuid=<% $.node_uuid %> state='clean' cleansteps=<% $.clean_steps %>\n\n      set_provision_state_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_provision_state).result %>\n\n      wait_for_provision_state:\n        on-success: send_message\n        action: ironic.node_get node_id=<% $.node_uuid %>\n        timeout: <% $.timeout %>\n        retry:\n          delay: <% $.retry_delay %>\n          count: <% $.retry_count %>\n          continue-on: <% task().result.provision_state != 'manageable' %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.manual_cleaning\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_nodes:\n    description: Validate nodes JSON\n\n    input:\n     - nodes_json\n     - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      validate_nodes:\n        action: tripleo.baremetal.validate_nodes\n        on-success: send_message\n        on-error: validation_failed\n        input:\n           nodes_json: <% $.nodes_json %>\n\n      validation_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(validate_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.validate_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  register_or_update:\n    description: Take nodes JSON and create nodes in a \"manageable\" state\n\n    input:\n     - nodes_json\n     - remove: False\n     - queue_name: tripleo\n     - kernel_name: null\n     - ramdisk_name: null\n     - instance_boot_option: local\n     - initial_state: manageable\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      validate_input:\n        workflow: tripleo.baremetal.v1.validate_nodes\n        on-success: register_or_update_nodes\n        on-error: validation_failed\n        input:\n          nodes_json: <% $.nodes_json %>\n          queue_name: <% $.queue_name %>\n\n      validation_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(validate_input).result %>\n          registered_nodes: []\n\n      register_or_update_nodes:\n        action: tripleo.baremetal.register_or_update_nodes\n        on-success:\n          - set_nodes_managed: <% $.initial_state != \"enroll\" %>\n          - send_message: <% $.initial_state = \"enroll\" %>\n        on-error: set_status_failed_register_or_update_nodes\n        input:\n           nodes_json: <% $.nodes_json %>\n           remove: <% $.remove %>\n           kernel_name: <% $.kernel_name %>\n           ramdisk_name: <% $.ramdisk_name %>\n           instance_boot_option: <% $.instance_boot_option %>\n        publish:\n          registered_nodes: <% task().result %>\n          new_nodes: <% task().result.where($.provision_state = 'enroll') %>\n\n      set_status_failed_register_or_update_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(register_or_update_nodes).result %>\n          registered_nodes: []\n\n      set_nodes_managed:\n        on-success:\n          - set_nodes_available: <% $.initial_state = \"available\" %>\n          - send_message: <% $.initial_state != \"available\" %>\n        on-error: set_status_failed_nodes_managed\n        workflow: tripleo.baremetal.v1.manage\n        input:\n          node_uuids: <% $.new_nodes.uuid %>\n          queue_name: <% $.queue_name %>\n        publish:\n          status: SUCCESS\n          message: <% $.new_nodes.len() %> node(s) successfully moved to the \"manageable\" state.\n\n      set_status_failed_nodes_managed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_nodes_managed).result %>\n\n      set_nodes_available:\n        on-success: send_message\n        on-error: set_status_failed_nodes_available\n        workflow: tripleo.baremetal.v1.provide node_uuids=<% $.new_nodes.uuid %> queue_name=<% $.queue_name %>\n        publish:\n          status: SUCCESS\n          message: <% $.new_nodes.len() %> node(s) successfully moved to the \"available\" state.\n\n      set_status_failed_nodes_available:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_nodes_available).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.register_or_update\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                registered_nodes: <% $.registered_nodes or [] %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  provide:\n    description: Take a list of nodes and move them to \"available\"\n\n    input:\n      - node_uuids\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_nodes_available:\n        on-success: cell_v2_discover_hosts\n        on-error: set_status_failed_nodes_available\n        with-items: uuid in <% $.node_uuids %>\n        workflow: tripleo.baremetal.v1.set_node_state\n        input:\n          node_uuid: <% $.uuid %>\n          queue_name: <% $.queue_name %>\n          state_action: 'provide'\n          target_state: 'available'\n\n      set_status_failed_nodes_available:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_nodes_available).result %>\n\n      cell_v2_discover_hosts:\n        on-success: try_power_off\n        on-error: cell_v2_discover_hosts_failed\n        workflow: tripleo.baremetal.v1.cellv2_discovery\n        input:\n          node_uuids: <% $.node_uuids %>\n          queue_name: <% $.queue_name %>\n        timeout: 900 #15 minutes\n        retry:\n          delay: 30\n          count: 30\n\n      cell_v2_discover_hosts_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(cell_v2_discover_hosts).result %>\n\n      try_power_off:\n        on-success: send_message\n        on-error: power_off_failed\n        with-items: uuid in <% $.node_uuids %>\n        workflow: tripleo.baremetal.v1.set_power_state\n        input:\n          node_uuid: <% $.uuid %>\n          queue_name: <% $.queue_name %>\n          state_action: 'off'\n          target_state: 'power off'\n        publish:\n          status: SUCCESS\n          message: <% $.node_uuids.len() %> node(s) successfully moved to the \"available\" state.\n\n      power_off_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(try_power_off).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.provide\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  provide_manageable_nodes:\n    description: Provide all nodes in a 'manageable' state.\n\n    input:\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_manageable_nodes:\n        action: ironic.node_list maintenance=False associated=False\n        on-success: provide_manageable\n        on-error: set_status_failed_get_manageable_nodes\n        publish:\n          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>\n\n      set_status_failed_get_manageable_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_manageable_nodes).result %>\n\n      provide_manageable:\n        on-success: send_message\n        workflow: tripleo.baremetal.v1.provide\n        input:\n          node_uuids: <% $.managed_nodes %>\n          queue_name: <% $.queue_name %>\n        publish:\n          status: SUCCESS\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.provide_manageable_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  manage:\n    description: Set a list of nodes to 'manageable' state\n\n    input:\n      - node_uuids\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_nodes_manageable:\n        on-success: send_message\n        on-error: set_status_failed_nodes_manageable\n        with-items: uuid in <% $.node_uuids %>\n        workflow: tripleo.baremetal.v1.set_node_state\n        input:\n          node_uuid: <% $.uuid %>\n          state_action: 'manage'\n          target_state: 'manageable'\n          error_states:\n            # node going back to enroll designates power credentials failure\n            - 'enroll'\n            - 'error'\n\n      set_status_failed_nodes_manageable:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_nodes_manageable).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.manage\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  _introspect:\n    description: >\n        An internal workflow. The tripleo.baremetal.v1.introspect workflow\n        should be used for introspection.\n\n    input:\n      - node_uuid\n      - timeout\n      - queue_name\n\n    output:\n      result: <% task(start_introspection).result %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      start_introspection:\n        action: baremetal_introspection.introspect uuid=<% $.node_uuid %>\n        on-success: wait_for_introspection_to_finish\n        on-error: set_status_failed_start_introspection\n\n      set_status_failed_start_introspection:\n        publish:\n          status: FAILED\n          message: <% task(start_introspection).result %>\n          introspected_nodes: []\n        on-success: send_message\n\n      wait_for_introspection_to_finish:\n        action: baremetal_introspection.wait_for_finish\n        input:\n          uuids: <% [$.node_uuid] %>\n          # The interval is 10 seconds, so divide to make the overall timeout\n          # in seconds correct.\n          max_retries: <% $.timeout / 10 %>\n          retry_interval: 10\n        publish:\n          introspected_node: <% task().result.values().first() %>\n          status: <% bool(task().result.values().first().error) and \"FAILED\" or \"SUCCESS\" %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-success: wait_for_introspection_to_finish_success\n        on-error: wait_for_introspection_to_finish_error\n\n      wait_for_introspection_to_finish_success:\n        publish:\n          message: <% \"Introspection of node {0} completed. Status:{1}. Errors:{2}\".format($.introspected_node.uuid, $.status, $.introspected_node.error) %>\n        on-success: send_message\n\n      wait_for_introspection_to_finish_error:\n        publish:\n          message: <% \"Introspection of node {0} timed out.\".format($.node_uuid) %>\n        on-success: send_message\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1._introspect\n              payload:\n                status: <% $.status %>\n                message: <% $.message %>\n                introspected_node: <% $.get('introspected_node') %>\n                node_uuid: <% $.node_uuid %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  introspect:\n    description: >\n        Take a list of nodes and move them through introspection.\n\n        By default each node will attempt introspection up to 3 times (two\n        retries plus the initial attemp) if it fails. This behaviour can be\n        modified by changing the max_retry_attempts input.\n\n        The workflow will assume the node has timed out after 20 minutes (1200\n        seconds). This can be changed by passing the node_timeout input in\n        seconds.\n\n    input:\n      - node_uuids\n      - run_validations: False\n      - queue_name: tripleo\n      - concurrency: 20\n      - max_retry_attempts: 2\n      - node_timeout: 1200\n\n    tags:\n      - tripleo-common-managed\n\n    task-defaults:\n      on-error: unhandled_error\n\n    tasks:\n      initialize:\n        publish:\n          introspection_attempt: 1\n        on-complete:\n          - run_validations: <% $.run_validations %>\n          - introspect_nodes: <% not $.run_validations %>\n\n      run_validations:\n        workflow: tripleo.validations.v1.run_groups\n        input:\n          group_names:\n            - 'pre-introspection'\n          queue_name: <% $.queue_name %>\n        on-success: introspect_nodes\n        on-error: set_validations_failed\n\n      set_validations_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(run_validations).result %>\n\n      introspect_nodes:\n        with-items: uuid in <% $.node_uuids %>\n        concurrency: <% $.concurrency %>\n        workflow: _introspect\n        input:\n          node_uuid: <% $.uuid %>\n          queue_name: <% $.queue_name %>\n          timeout: <% $.node_timeout %>\n        # on-error is triggered if one or more nodes failed introspection. We\n        # still go to get_introspection_status as it will collect the result\n        # for each node. Unless we hit the retry limit.\n        on-error:\n         - get_introspection_status: <% $.introspection_attempt <= $.max_retry_attempts %>\n         - max_retry_attempts_reached: <% $.introspection_attempt > $.max_retry_attempts %>\n        on-success: get_introspection_status\n\n      get_introspection_status:\n        with-items: uuid in <% $.node_uuids %>\n        action: baremetal_introspection.get_status\n        input:\n          uuid: <% $.uuid %>\n        publish:\n          introspected_nodes: <% task().result.toDict($.uuid, $) %>\n          # Currently there is no way for us to ignore user introspection\n          # aborts. This means we will retry aborted nodes until the Ironic API\n          # gives us more details (error code or a boolean to show aborts etc.)\n          # If a node hasn't finished, we consider it to be failed.\n          # TODO(d0ugal): When possible, don't retry introspection of nodes\n          #               that a user manually aborted.\n          failed_introspection: <% task().result.where($.finished = true and $.error != null).select($.uuid) + task().result.where($.finished = false).select($.uuid) %>\n        publish-on-error:\n          # If a node fails to start introspection, getting the status can fail.\n          # When that happens, the result is a string and the nodes need to be\n          # filtered out.\n          introspected_nodes: <% task().result.where(isDict($)).toDict($.uuid, $) %>\n          # If there was an error, the exception string we get doesn't give us\n          # the UUID. So we use a set difference to find the UUIDs missing in\n          # the results. These are then added to the failed nodes.\n          failed_introspection: <% ($.node_uuids.toSet() - task().result.where(isDict($)).select($.uuid).toSet()) + task().result.where(isDict($)).where($.finished = true and $.error != null).toSet() + task().result.where(isDict($)).where($.finished = false).toSet() %>\n        on-error: increase_attempt_counter\n        on-success:\n          - successful_introspection: <% $.failed_introspection.len() = 0 %>\n          - increase_attempt_counter: <% $.failed_introspection.len() > 0 %>\n\n      increase_attempt_counter:\n        publish:\n          introspection_attempt: <% $.introspection_attempt + 1 %>\n        on-complete:\n          retry_failed_nodes\n\n      retry_failed_nodes:\n        publish:\n          status: RUNNING\n          message: <% 'Retrying {0} nodes that failed introspection. Attempt {1} of {2} '.format($.failed_introspection.len(), $.introspection_attempt, $.max_retry_attempts + 1) %>\n          # We are about to retry, update the tracking stats.\n          node_uuids: <% $.failed_introspection %>\n        on-success:\n          - send_message\n          - introspect_nodes\n\n      max_retry_attempts_reached:\n        publish:\n          status: FAILED\n          message: <% 'Retry limit reached with {0} nodes still failing introspection'.format($.failed_introspection.len()) %>\n        on-complete: send_message\n\n      successful_introspection:\n        publish:\n          status: SUCCESS\n          message: Successfully introspected <% $.introspected_nodes.len() %> node(s).\n        on-complete: send_message\n\n      unhandled_error:\n        publish:\n          status: FAILED\n          message: \"Unhandled workflow error\"\n        on-complete: send_message\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.introspect\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                introspected_nodes: <% $.get('introspected_nodes', []) %>\n                failed_introspection: <% $.get('failed_introspection', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  introspect_manageable_nodes:\n    description: Introspect all nodes in a 'manageable' state.\n\n    input:\n      - run_validations: False\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_manageable_nodes:\n        action: ironic.node_list maintenance=False associated=False\n        on-success: validate_nodes\n        on-error: set_status_failed_get_manageable_nodes\n        publish:\n          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>\n\n      set_status_failed_get_manageable_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_manageable_nodes).result %>\n\n      validate_nodes:\n        on-success:\n          - introspect_manageable: <% $.managed_nodes.len() > 0 %>\n          - set_status_failed_no_nodes: <% $.managed_nodes.len() = 0 %>\n\n      set_status_failed_no_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: No manageable nodes to introspect. Check node states and maintenance.\n\n      introspect_manageable:\n        on-success: send_message\n        on-error: set_status_introspect_manageable\n        workflow: tripleo.baremetal.v1.introspect\n        input:\n          node_uuids: <% $.managed_nodes %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          introspected_nodes: <% task().result.introspected_nodes %>\n\n      set_status_introspect_manageable:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(introspect_manageable).result %>\n          introspected_nodes: []\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.introspect_manageable_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                introspected_nodes: <% $.get('introspected_nodes', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  configure:\n    description: Take a list of manageable nodes and update their boot configuration.\n\n    input:\n      - node_uuids\n      - queue_name: tripleo\n      - kernel_name: bm-deploy-kernel\n      - ramdisk_name: bm-deploy-ramdisk\n      - instance_boot_option: null\n      - root_device: null\n      - root_device_minimum_size: 4\n      - overwrite_root_device_hints: False\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      configure_boot:\n        on-success: configure_root_device\n        on-error: set_status_failed_configure_boot\n        with-items: node_uuid in <% $.node_uuids %>\n        action: tripleo.baremetal.configure_boot node_uuid=<% $.node_uuid %> kernel_name=<% $.kernel_name %> ramdisk_name=<% $.ramdisk_name %> instance_boot_option=<% $.instance_boot_option %>\n\n      configure_root_device:\n        on-success: send_message\n        on-error: set_status_failed_configure_root_device\n        with-items: node_uuid in <% $.node_uuids %>\n        action: tripleo.baremetal.configure_root_device node_uuid=<% $.node_uuid %> root_device=<% $.root_device %> minimum_size=<% $.root_device_minimum_size %> overwrite=<% $.overwrite_root_device_hints %>\n        publish:\n          status: SUCCESS\n          message: 'Successfully configured the nodes.'\n\n      set_status_failed_configure_boot:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(configure_boot).result %>\n\n      set_status_failed_configure_root_device:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(configure_root_device).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.configure\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  configure_manageable_nodes:\n    description: Update the boot configuration of all nodes in 'manageable' state.\n\n    input:\n      - queue_name: tripleo\n      - kernel_name: 'bm-deploy-kernel'\n      - ramdisk_name: 'bm-deploy-ramdisk'\n      - instance_boot_option: null\n      - root_device: null\n      - root_device_minimum_size: 4\n      - overwrite_root_device_hints: False\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_manageable_nodes:\n        action: ironic.node_list maintenance=False associated=False\n        on-success: configure_manageable\n        on-error: set_status_failed_get_manageable_nodes\n        publish:\n          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>\n\n      configure_manageable:\n        on-success: send_message\n        on-error: set_status_failed_configure_manageable\n        workflow: tripleo.baremetal.v1.configure\n        input:\n          node_uuids: <% $.managed_nodes %>\n          queue_name: <% $.queue_name %>\n          kernel_name: <% $.kernel_name %>\n          ramdisk_name: <% $.ramdisk_name %>\n          instance_boot_option: <% $.instance_boot_option %>\n          root_device: <% $.root_device %>\n          root_device_minimum_size: <% $.root_device_minimum_size %>\n          overwrite_root_device_hints: <% $.overwrite_root_device_hints %>\n        publish:\n          message: 'Manageable nodes configured successfully.'\n\n      set_status_failed_configure_manageable:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(configure_manageable).result %>\n\n      set_status_failed_get_manageable_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_manageable_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.configure_manageable_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  tag_node:\n    description: Tag a node with a role\n    input:\n      - node_uuid\n      - role: null\n      - queue_name: tripleo\n\n    task-defaults:\n      on-error: send_message\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      update_node:\n        on-success: send_message\n        action: tripleo.baremetal.update_node_capability node_uuid=<% $.node_uuid %> capability='profile' value=<% $.role %>\n        publish:\n          message: <% task().result %>\n          status: SUCCESS\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.tag_node\n              payload:\n                status: <% $.get('status', 'FAILED') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  tag_nodes:\n    description: Runs the tag_node workflow in a loop\n    input:\n      - tag_node_uuids\n      - untag_node_uuids\n      - role\n      - plan: overcloud\n      - queue_name: tripleo\n\n    task-defaults:\n      on-error: send_message\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      tag_nodes:\n        with-items: node_uuid in <% $.tag_node_uuids %>\n        workflow: tripleo.baremetal.v1.tag_node\n        input:\n          node_uuid: <% $.node_uuid %>\n          queue_name: <% $.queue_name %>\n          role: <% $.role %>\n        concurrency: 1\n        on-success: untag_nodes\n\n      untag_nodes:\n        with-items: node_uuid in <% $.untag_node_uuids %>\n        workflow: tripleo.baremetal.v1.tag_node\n        input:\n          node_uuid: <% $.node_uuid %>\n          queue_name: <% $.queue_name %>\n        concurrency: 1\n        on-success: update_role_parameters\n\n      update_role_parameters:\n        on-success: send_message\n        action: tripleo.parameters.update_role role=<% $.role %> container=<% $.plan %>\n        publish:\n          message: <% task().result %>\n          status: SUCCESS\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.tag_nodes\n              payload:\n                status: <% $.get('status', 'FAILED') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  nodes_with_hint:\n    description: Find nodes matching a hint regex\n    input:\n      - hint_regex\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_nodes:\n        with-items: provision_state in <% ['available', 'active'] %>\n        action: ironic.node_list maintenance=false provision_state=<% $.provision_state %> detail=true\n        on-success: get_matching_nodes\n        on-error: set_status_failed_get_nodes\n\n      get_matching_nodes:\n        with-items: node in <% task(get_nodes).result.flatten() %>\n        action: tripleo.baremetal.get_node_hint node=<% $.node %>\n        on-success: send_message\n        on-error: set_status_failed_get_matching_nodes\n        publish:\n          matching_nodes: <% let(hint_regex => $.hint_regex) -> task().result.where($.hint and $.hint.matches($hint_regex)).uuid %>\n\n      set_status_failed_get_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_nodes).result %>\n\n      set_status_failed_get_matching_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_matching_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.nodes_with_hint\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                matching_nodes: <% $.matching_nodes or [] %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  nodes_with_profile:\n    description: Find nodes with a specific profile\n    input:\n      - profile\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_active_nodes:\n        action: ironic.node_list maintenance=false provision_state='active' detail=true\n        on-success: get_available_nodes\n        on-error: set_status_failed_get_active_nodes\n\n      get_available_nodes:\n        action: ironic.node_list maintenance=false provision_state='available' detail=true\n        on-success: get_matching_nodes\n        on-error: set_status_failed_get_available_nodes\n\n      get_matching_nodes:\n        with-items: node in <% task(get_available_nodes).result + task(get_active_nodes).result %>\n        action: tripleo.baremetal.get_profile node=<% $.node %>\n        on-success: send_message\n        on-error: set_status_failed_get_matching_nodes\n        publish:\n          matching_nodes: <% let(input_profile_name => $.profile) -> task().result.where($.profile = $input_profile_name).uuid %>\n\n      set_status_failed_get_active_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_active_nodes).result %>\n\n      set_status_failed_get_available_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_available_nodes).result %>\n\n      set_status_failed_get_matching_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_matching_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.nodes_with_profile\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                matching_nodes: <% $.matching_nodes or [] %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  create_raid_configuration:\n    description: Create and apply RAID configuration for given nodes\n    input:\n      - node_uuids\n      - configuration\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_configuration:\n        with-items: node_uuid in <% $.node_uuids %>\n        action: ironic.node_set_target_raid_config node_ident=<% $.node_uuid %> target_raid_config=<% $.configuration %>\n        on-success: apply_configuration\n        on-error: set_configuration_failed\n\n      set_configuration_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_configuration).result %>\n\n      apply_configuration:\n        with-items: node_uuid in <% $.node_uuids %>\n        workflow: tripleo.baremetal.v1.manual_cleaning\n        input:\n          node_uuid: <% $.node_uuid %>\n          clean_steps:\n            - interface: raid\n              step: delete_configuration\n            - interface: raid\n              step: create_configuration\n          timeout: 1800  # building RAID should be fast than general cleaning\n          retry_count: 180\n          retry_delay: 10\n        on-success: send_message\n        on-error: apply_configuration_failed\n        publish:\n          message: <% task().result %>\n          status: SUCCESS\n\n      apply_configuration_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(apply_configuration).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.create_raid_configuration\n              payload:\n                status: <% $.get('status', 'FAILED') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n\n  cellv2_discovery:\n    description: Run cell_v2 host discovery\n\n    input:\n      - node_uuids\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      cell_v2_discover_hosts:\n        on-success: wait_for_nova_resources\n        on-error: cell_v2_discover_hosts_failed\n        action: tripleo.baremetal.cell_v2_discover_hosts\n\n      cell_v2_discover_hosts_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(cell_v2_discover_hosts).result %>\n\n      wait_for_nova_resources:\n        on-success: send_message\n        on-error: wait_for_nova_resources_failed\n        with-items: node_uuid in <% $.node_uuids %>\n        action: nova.hypervisors_find hypervisor_hostname=<% $.node_uuid %>\n\n      wait_for_nova_resources_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(wait_for_nova_resources).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.cellv2_discovery\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n\n  discover_nodes:\n    description: Run nodes discovery over the given IP range\n\n    input:\n      - ip_addresses\n      - credentials\n      - ports: [623]\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_all_nodes:\n        action: ironic.node_list\n        input:\n          fields: [\"uuid\", \"driver\", \"driver_info\"]\n          limit: 0\n        on-success: get_candidate_nodes\n        on-error: get_all_nodes_failed\n        publish:\n          existing_nodes: <% task().result %>\n\n      get_all_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_all_nodes).result %>\n\n      get_candidate_nodes:\n        action: tripleo.baremetal.get_candidate_nodes\n        input:\n          ip_addresses: <% $.ip_addresses %>\n          credentials: <% $.credentials %>\n          ports: <% $.ports %>\n          existing_nodes: <% $.existing_nodes %>\n        on-success: probe_nodes\n        on-error: get_candidate_nodes_failed\n        publish:\n          candidates: <% task().result %>\n\n      get_candidate_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_candidate_nodes).result %>\n\n      probe_nodes:\n        action: tripleo.baremetal.probe_node\n        on-success: send_message\n        on-error: probe_nodes_failed\n        input:\n          ip: <% $.node.ip %>\n          port: <% $.node.port %>\n          username: <% $.node.username %>\n          password: <% $.node.password %>\n        with-items:\n          - node in <% $.candidates %>\n        publish:\n          nodes_json: <% task().result.where($ != null) %>\n\n      probe_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(probe_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.discover_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                nodes_json: <% $.get('nodes_json', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  discover_and_enroll_nodes:\n    description: Run nodes discovery over the given IP range and enroll nodes\n\n    input:\n      - ip_addresses\n      - credentials\n      - ports: [623]\n      - kernel_name: null\n      - ramdisk_name: null\n      - instance_boot_option: local\n      - initial_state: manageable\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      discover_nodes:\n        workflow: tripleo.baremetal.v1.discover_nodes\n        input:\n          ip_addresses: <% $.ip_addresses %>\n          ports: <% $.ports %>\n          credentials: <% $.credentials %>\n          queue_name: <% $.queue_name %>\n        on-success: enroll_nodes\n        on-error: discover_nodes_failed\n        publish:\n          nodes_json: <% task().result.nodes_json %>\n\n      discover_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(discover_nodes).result %>\n\n      enroll_nodes:\n        workflow: tripleo.baremetal.v1.register_or_update\n        input:\n          nodes_json: <% $.nodes_json %>\n          kernel_name: <% $.kernel_name %>\n          ramdisk_name: <% $.ramdisk_name %>\n          instance_boot_option: <% $.instance_boot_option %>\n          initial_state: <% $.initial_state %>\n        on-success: send_message\n        on-error: enroll_nodes_failed\n        publish:\n          registered_nodes: <% task().result.registered_nodes %>\n\n      enroll_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(enroll_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.discover_and_enroll_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                registered_nodes: <% $.get('registered_nodes', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1", "tags": [], "created_at": "2018-12-03 11:18:54", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "3f64b5b3-ae06-4dcd-a3cd-20e97786c148"}

2018-12-03 06:18:54,472 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:18:54,474 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.storage.v1
description: TripleO manages Ceph with ceph-ansible

workflows:
  ceph-install:
    # allows for additional extra_vars via workflow input
    input:
      - ansible_playbook_verbosity: 0
      - ansible_skip_tags: 'package-install,with_pkg'
      - ansible_env_variables: {}
      - ansible_extra_env_variables:
          ANSIBLE_CONFIG: /usr/share/ceph-ansible/ansible.cfg
          ANSIBLE_ACTION_PLUGINS: /usr/share/ceph-ansible/plugins/actions/
          ANSIBLE_ROLES_PATH: /usr/share/ceph-ansible/roles/
          ANSIBLE_RETRY_FILES_ENABLED: 'False'
          ANSIBLE_LOG_PATH: /var/log/mistral/ceph-install-workflow.log
          ANSIBLE_LIBRARY: /usr/share/ceph-ansible/library/
          ANSIBLE_SSH_RETRIES: '3'
          ANSIBLE_HOST_KEY_CHECKING: 'False'
          DEFAULT_FORKS: '25'
      - ceph_ansible_extra_vars: {}
      - ceph_ansible_playbook: /usr/share/ceph-ansible/site-docker.yml.sample
      - node_data_lookup: '{}'
      - swift_container: 'ceph_ansible_fetch_dir'
    tags:
      - tripleo-common-managed
    tasks:
      set_swift_container:
        publish:
          swift_container: <% concat(env().get('heat_stack_name', ''), '_', $.swift_container) %>
        on-complete: collect_puppet_hieradata
      collect_puppet_hieradata:
        on-success: check_hieradata
        publish:
          hieradata: <% env().get('role_merged_configs', {}).values().select($.keys()).flatten().select(regex('^ceph::profile::params::osds$').search($)).where($ != null).toSet() %>
      check_hieradata:
        on-success:
        - set_blacklisted_ips: <% not bool($.hieradata) %>
        - fail(msg=<% 'Ceph deployment stopped, puppet-ceph hieradata found. Convert it into ceph-ansible variables. {0}'.format($.hieradata) %>): <% bool($.hieradata) %>
      set_blacklisted_ips:
        publish:
          blacklisted_ips: <% env().get('blacklisted_ip_addresses', []) %>
        on-success: set_ip_lists
      set_ip_lists:
        publish:
          mgr_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mgr_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          mon_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mon_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          osd_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_osd_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          mds_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mds_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          rgw_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_rgw_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          nfs_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_nfs_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          rbdmirror_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_rbdmirror_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          client_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_client_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
        on-success: merge_ip_lists
      merge_ip_lists:
        publish:
          ips_list: <% ($.mgr_ips + $.mon_ips + $.osd_ips + $.mds_ips + $.rgw_ips + $.nfs_ips + $.rbdmirror_ips + $.client_ips).toSet() %>
        on-success: enable_ssh_admin
      enable_ssh_admin:
        workflow: tripleo.access.v1.enable_ssh_admin
        input:
          ssh_servers: <% $.ips_list %>
        on-success: get_private_key
      get_private_key:
        action: tripleo.validations.get_privkey
        publish:
          private_key: <% task().result %>
        on-success: make_fetch_directory
      make_fetch_directory:
        action: tripleo.files.make_temp_dir
        publish:
          fetch_directory: <% task().result.path %>
        on-success: verify_container_exists
      verify_container_exists:
        action: swift.head_container container=<% $.swift_container %>
        on-success: restore_fetch_directory
        on-error: create_container
      restore_fetch_directory:
        action: tripleo.files.restore_temp_dir_from_swift
        input:
          container: <% $.swift_container %>
          path: <% $.fetch_directory %>
        on-success: collect_nodes_uuid
      create_container:
        action: swift.put_container container=<% $.swift_container %>
        on-success: collect_nodes_uuid
      collect_nodes_uuid:
        action: tripleo.ansible-playbook
        input:
          inventory:
            overcloud:
              hosts: <% $.ips_list.toDict($, {}) %>
          remote_user: tripleo-admin
          become: true
          become_user: root
          verbosity: 0
          ssh_private_key: <% $.private_key %>
          #NOTE(gfidente): set ANSIBLE_CALLBACK_WHITELIST to empty string to avoid spurious output
          #in the json output. The publish: directive will in fact parse the output.
          extra_env_variables:
            ANSIBLE_CALLBACK_WHITELIST: ''
            ANSIBLE_HOST_KEY_CHECKING: 'False'
            ANSIBLE_STDOUT_CALLBACK: 'json'
          playbook:
            - hosts: overcloud
              gather_facts: no
              tasks:
                - name: collect machine id
                  command: dmidecode -s system-uuid
                  register: dmidecode
                  # NOTE(tonyb): 0 == no error, 1 == -EPERM or bad data and 2 == Command not found
                  # 1 and 2 aren't great but shouldn't cause the deploy to fail.  If we're using
                  # the node specific data we'll fail then.  If we aren't then lets keep moving
                  failed_when: dmidecode.rc not in [0, 1, 2]
        publish:
          ansible_output: <% json_parse(task().result.stderr) %>
        on-success: set_ip_uuids
      set_ip_uuids:
        publish:
          ip_uuids: <% let(root => $.ansible_output.get('plays')[0].get('tasks')[0].get('hosts')) -> $.ips_list.toDict($, $root.get($).get('stdout')) %>
        on-success: parse_node_data_lookup
      parse_node_data_lookup:
        publish:
          json_node_data_lookup: <% json_parse($.node_data_lookup) %>
        on-error:
          - fail(msg=<% 'Ceph deployment stopped, NodeDataLookup (node_data_lookup) is not valid JSON. {0}'.format($.node_data_lookup) %>)
        on-success: map_node_data_lookup
      map_node_data_lookup:
        publish:
          ips_data: <% let(uuids => $.ip_uuids, root => $) -> $.ips_list.toDict($, $root.json_node_data_lookup.get($uuids.get($, "NO-UUID-FOUND"), {})) %>
        on-success: set_role_vars
      set_role_vars:
        publish:
          # NOTE(gfidente): collect role settings from all tht roles
          mgr_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mgr_ansible_vars', {})).aggregate($1 + $2) %>
          mon_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mon_ansible_vars', {})).aggregate($1 + $2) %>
          osd_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_osd_ansible_vars', {})).aggregate($1 + $2) %>
          mds_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mds_ansible_vars', {})).aggregate($1 + $2) %>
          rgw_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_rgw_ansible_vars', {})).aggregate($1 + $2) %>
          nfs_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_nfs_ansible_vars', {})).aggregate($1 + $2) %>
          rbdmirror_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_rbdmirror_ansible_vars', {})).aggregate($1 + $2) %>
          client_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_client_ansible_vars', {})).aggregate($1 + $2) %>
        on-success: build_extra_vars
      build_extra_vars:
        publish:
          # NOTE(gfidente): merge vars from all ansible roles
          extra_vars: <% {'fetch_directory'=> $.fetch_directory} + $.mgr_vars + $.mon_vars + $.osd_vars + $.mds_vars + $.rgw_vars + $.nfs_vars + $.client_vars + $.rbdmirror_vars + $.ceph_ansible_extra_vars %>
        on-success: ceph_install
      ceph_install:
        with-items: playbook in <% list($.ceph_ansible_playbook).flatten() %>
        concurrency: 1
        action: tripleo.ansible-playbook
        input:
          trash_output: true
          inventory:
            mgrs:
              hosts: <% let(root => $) -> $.mgr_ips.toDict($, $root.ips_data.get($, {})) %>
            mons:
              hosts: <% let(root => $) -> $.mon_ips.toDict($, $root.ips_data.get($, {})) %>
            osds:
              hosts: <% let(root => $) -> $.osd_ips.toDict($, $root.ips_data.get($, {})) %>
            mdss:
              hosts: <% let(root => $) -> $.mds_ips.toDict($, $root.ips_data.get($, {})) %>
            rgws:
              hosts: <% let(root => $) -> $.rgw_ips.toDict($, $root.ips_data.get($, {})) %>
            nfss:
              hosts: <% let(root => $) -> $.nfs_ips.toDict($, $root.ips_data.get($, {})) %>
            rbdmirrors:
              hosts: <% let(root => $) -> $.rbdmirror_ips.toDict($, $root.ips_data.get($, {})) %>
            clients:
              hosts: <% let(root => $) -> $.client_ips.toDict($, $root.ips_data.get($, {})) %>
            all:
              vars: <% $.extra_vars %>
          playbook: <% $.playbook %>
          remote_user: tripleo-admin
          become: true
          become_user: root
          verbosity: <% $.ansible_playbook_verbosity %>
          ssh_private_key: <% $.private_key %>
          skip_tags: <% $.ansible_skip_tags %>
          extra_env_variables: <% $.ansible_extra_env_variables.mergeWith($.ansible_env_variables) %>
          extra_vars:
            ireallymeanit: 'yes'
        publish:
          output: <% task().result %>
        on-complete: save_fetch_directory
      save_fetch_directory:
        action: tripleo.files.save_temp_dir_to_swift
        input:
          container: <% $.swift_container %>
          path: <% $.fetch_directory %>
        on-success: purge_fetch_directory
      purge_fetch_directory:
        action: tripleo.files.remove_temp_dir path=<% $.fetch_directory %>
'
2018-12-03 06:18:55,277 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 10739
2018-12-03 06:18:55,279 DEBUG: RESP: [201] Content-Length: 10739 Content-Type: application/json Date: Mon, 03 Dec 2018 11:18:55 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.storage.v1\ndescription: TripleO manages Ceph with ceph-ansible\n\nworkflows:\n  ceph-install:\n    # allows for additional extra_vars via workflow input\n    input:\n      - ansible_playbook_verbosity: 0\n      - ansible_skip_tags: 'package-install,with_pkg'\n      - ansible_env_variables: {}\n      - ansible_extra_env_variables:\n          ANSIBLE_CONFIG: /usr/share/ceph-ansible/ansible.cfg\n          ANSIBLE_ACTION_PLUGINS: /usr/share/ceph-ansible/plugins/actions/\n          ANSIBLE_ROLES_PATH: /usr/share/ceph-ansible/roles/\n          ANSIBLE_RETRY_FILES_ENABLED: 'False'\n          ANSIBLE_LOG_PATH: /var/log/mistral/ceph-install-workflow.log\n          ANSIBLE_LIBRARY: /usr/share/ceph-ansible/library/\n          ANSIBLE_SSH_RETRIES: '3'\n          ANSIBLE_HOST_KEY_CHECKING: 'False'\n          DEFAULT_FORKS: '25'\n      - ceph_ansible_extra_vars: {}\n      - ceph_ansible_playbook: /usr/share/ceph-ansible/site-docker.yml.sample\n      - node_data_lookup: '{}'\n      - swift_container: 'ceph_ansible_fetch_dir'\n    tags:\n      - tripleo-common-managed\n    tasks:\n      set_swift_container:\n        publish:\n          swift_container: <% concat(env().get('heat_stack_name', ''), '_', $.swift_container) %>\n        on-complete: collect_puppet_hieradata\n      collect_puppet_hieradata:\n        on-success: check_hieradata\n        publish:\n          hieradata: <% env().get('role_merged_configs', {}).values().select($.keys()).flatten().select(regex('^ceph::profile::params::osds$').search($)).where($ != null).toSet() %>\n      check_hieradata:\n        on-success:\n        - set_blacklisted_ips: <% not bool($.hieradata) %>\n        - fail(msg=<% 'Ceph deployment stopped, puppet-ceph hieradata found. Convert it into ceph-ansible variables. {0}'.format($.hieradata) %>): <% bool($.hieradata) %>\n      set_blacklisted_ips:\n        publish:\n          blacklisted_ips: <% env().get('blacklisted_ip_addresses', []) %>\n        on-success: set_ip_lists\n      set_ip_lists:\n        publish:\n          mgr_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mgr_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          mon_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mon_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          osd_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_osd_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          mds_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mds_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          rgw_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_rgw_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          nfs_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_nfs_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          rbdmirror_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_rbdmirror_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          client_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_client_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n        on-success: merge_ip_lists\n      merge_ip_lists:\n        publish:\n          ips_list: <% ($.mgr_ips + $.mon_ips + $.osd_ips + $.mds_ips + $.rgw_ips + $.nfs_ips + $.rbdmirror_ips + $.client_ips).toSet() %>\n        on-success: enable_ssh_admin\n      enable_ssh_admin:\n        workflow: tripleo.access.v1.enable_ssh_admin\n        input:\n          ssh_servers: <% $.ips_list %>\n        on-success: get_private_key\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: make_fetch_directory\n      make_fetch_directory:\n        action: tripleo.files.make_temp_dir\n        publish:\n          fetch_directory: <% task().result.path %>\n        on-success: verify_container_exists\n      verify_container_exists:\n        action: swift.head_container container=<% $.swift_container %>\n        on-success: restore_fetch_directory\n        on-error: create_container\n      restore_fetch_directory:\n        action: tripleo.files.restore_temp_dir_from_swift\n        input:\n          container: <% $.swift_container %>\n          path: <% $.fetch_directory %>\n        on-success: collect_nodes_uuid\n      create_container:\n        action: swift.put_container container=<% $.swift_container %>\n        on-success: collect_nodes_uuid\n      collect_nodes_uuid:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            overcloud:\n              hosts: <% $.ips_list.toDict($, {}) %>\n          remote_user: tripleo-admin\n          become: true\n          become_user: root\n          verbosity: 0\n          ssh_private_key: <% $.private_key %>\n          #NOTE(gfidente): set ANSIBLE_CALLBACK_WHITELIST to empty string to avoid spurious output\n          #in the json output. The publish: directive will in fact parse the output.\n          extra_env_variables:\n            ANSIBLE_CALLBACK_WHITELIST: ''\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n            ANSIBLE_STDOUT_CALLBACK: 'json'\n          playbook:\n            - hosts: overcloud\n              gather_facts: no\n              tasks:\n                - name: collect machine id\n                  command: dmidecode -s system-uuid\n                  register: dmidecode\n                  # NOTE(tonyb): 0 == no error, 1 == -EPERM or bad data and 2 == Command not found\n                  # 1 and 2 aren't great but shouldn't cause the deploy to fail.  If we're using\n                  # the node specific data we'll fail then.  If we aren't then lets keep moving\n                  failed_when: dmidecode.rc not in [0, 1, 2]\n        publish:\n          ansible_output: <% json_parse(task().result.stderr) %>\n        on-success: set_ip_uuids\n      set_ip_uuids:\n        publish:\n          ip_uuids: <% let(root => $.ansible_output.get('plays')[0].get('tasks')[0].get('hosts')) -> $.ips_list.toDict($, $root.get($).get('stdout')) %>\n        on-success: parse_node_data_lookup\n      parse_node_data_lookup:\n        publish:\n          json_node_data_lookup: <% json_parse($.node_data_lookup) %>\n        on-error:\n          - fail(msg=<% 'Ceph deployment stopped, NodeDataLookup (node_data_lookup) is not valid JSON. {0}'.format($.node_data_lookup) %>)\n        on-success: map_node_data_lookup\n      map_node_data_lookup:\n        publish:\n          ips_data: <% let(uuids => $.ip_uuids, root => $) -> $.ips_list.toDict($, $root.json_node_data_lookup.get($uuids.get($, \"NO-UUID-FOUND\"), {})) %>\n        on-success: set_role_vars\n      set_role_vars:\n        publish:\n          # NOTE(gfidente): collect role settings from all tht roles\n          mgr_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mgr_ansible_vars', {})).aggregate($1 + $2) %>\n          mon_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mon_ansible_vars', {})).aggregate($1 + $2) %>\n          osd_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_osd_ansible_vars', {})).aggregate($1 + $2) %>\n          mds_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mds_ansible_vars', {})).aggregate($1 + $2) %>\n          rgw_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_rgw_ansible_vars', {})).aggregate($1 + $2) %>\n          nfs_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_nfs_ansible_vars', {})).aggregate($1 + $2) %>\n          rbdmirror_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_rbdmirror_ansible_vars', {})).aggregate($1 + $2) %>\n          client_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_client_ansible_vars', {})).aggregate($1 + $2) %>\n        on-success: build_extra_vars\n      build_extra_vars:\n        publish:\n          # NOTE(gfidente): merge vars from all ansible roles\n          extra_vars: <% {'fetch_directory'=> $.fetch_directory} + $.mgr_vars + $.mon_vars + $.osd_vars + $.mds_vars + $.rgw_vars + $.nfs_vars + $.client_vars + $.rbdmirror_vars + $.ceph_ansible_extra_vars %>\n        on-success: ceph_install\n      ceph_install:\n        with-items: playbook in <% list($.ceph_ansible_playbook).flatten() %>\n        concurrency: 1\n        action: tripleo.ansible-playbook\n        input:\n          trash_output: true\n          inventory:\n            mgrs:\n              hosts: <% let(root => $) -> $.mgr_ips.toDict($, $root.ips_data.get($, {})) %>\n            mons:\n              hosts: <% let(root => $) -> $.mon_ips.toDict($, $root.ips_data.get($, {})) %>\n            osds:\n              hosts: <% let(root => $) -> $.osd_ips.toDict($, $root.ips_data.get($, {})) %>\n            mdss:\n              hosts: <% let(root => $) -> $.mds_ips.toDict($, $root.ips_data.get($, {})) %>\n            rgws:\n              hosts: <% let(root => $) -> $.rgw_ips.toDict($, $root.ips_data.get($, {})) %>\n            nfss:\n              hosts: <% let(root => $) -> $.nfs_ips.toDict($, $root.ips_data.get($, {})) %>\n            rbdmirrors:\n              hosts: <% let(root => $) -> $.rbdmirror_ips.toDict($, $root.ips_data.get($, {})) %>\n            clients:\n              hosts: <% let(root => $) -> $.client_ips.toDict($, $root.ips_data.get($, {})) %>\n            all:\n              vars: <% $.extra_vars %>\n          playbook: <% $.playbook %>\n          remote_user: tripleo-admin\n          become: true\n          become_user: root\n          verbosity: <% $.ansible_playbook_verbosity %>\n          ssh_private_key: <% $.private_key %>\n          skip_tags: <% $.ansible_skip_tags %>\n          extra_env_variables: <% $.ansible_extra_env_variables.mergeWith($.ansible_env_variables) %>\n          extra_vars:\n            ireallymeanit: 'yes'\n        publish:\n          output: <% task().result %>\n        on-complete: save_fetch_directory\n      save_fetch_directory:\n        action: tripleo.files.save_temp_dir_to_swift\n        input:\n          container: <% $.swift_container %>\n          path: <% $.fetch_directory %>\n        on-success: purge_fetch_directory\n      purge_fetch_directory:\n        action: tripleo.files.remove_temp_dir path=<% $.fetch_directory %>\n", "name": "tripleo.storage.v1", "tags": [], "created_at": "2018-12-03 11:18:55", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "ccee3c73-2ffd-4da6-8d56-7ece3d0b6cc1"}

2018-12-03 06:18:55,279 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:18:55,280 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.deployment.v1
description: TripleO deployment workflows

workflows:

  deploy_on_server:

    input:
      - server_uuid
      - server_name
      - config
      - config_name
      - group
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      deploy_config:
        action: tripleo.deployment.config
        on-complete: send_message
        input:
          server_id: <% $.server_uuid %>
          name: <% $.config_name %>
          config: <% $.config %>
          group: <% $.group %>
        publish:
          stdout: <% task().result.deploy_stdout %>
          stderr: <% task().result.deploy_stderr %>
          status_code: <% task().result.deploy_status_code %>
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.deploy_on_server
              payload:
                status: <% $.get("status", "SUCCESS") %>
                message: <% $.get("message", "") %>
                server_uuid: <% $.server_uuid %>
                server_name: <% $.server_name %>
                config_name: <% $.config_name %>
                status_code: <% $.get("status_code", "") %>
                stdout: <% $.get("stdout", "") %>
                stderr: <% $.get("stderr", "") %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  deploy_on_servers:

    input:
      - server_name
      - config_name
      - config
      - group: script
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      check_if_all_servers:
        on-success:
          - get_servers_matching: <% $.server_name != "all" %>
          - get_all_servers: <% $.server_name = "all" %>

      get_servers_matching:
        action: nova.servers_list
        on-success: deploy_on_servers
        publish:
          servers_with_name: <% task().result._info.where($.name.indexOf(execution().input.server_name) > -1) %>

      get_all_servers:
        action: nova.servers_list
        on-success: deploy_on_servers
        publish:
          servers_with_name: <% task().result._info %>

      deploy_on_servers:
        on-success: send_success_message
        on-error: send_failed_message
        with-items: server in <% $.servers_with_name %>
        workflow: tripleo.deployment.v1.deploy_on_server
        input:
          server_name: <% $.server.name %>
          server_uuid: <% $.server.id %>
          config: <% $.config %>
          config_name: <% $.config_name %>
          group: <% $.group %>
          queue_name: <% $.queue_name %>

      send_success_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.deploy_on_servers
              payload:
                status: SUCCESS
                execution: <% execution() %>

      send_failed_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.deploy_on_servers
              payload:
                status: FAILED
                message: <% task(deploy_on_servers).result %>
                execution: <% execution() %>
        on-success: fail

  deploy_plan:

    description: >
      Deploy the overcloud for a plan.

    input:
      - container
      - run_validations: False
      - timeout: 240
      - skip_deploy_identifier: False
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      add_validation_ssh_key:
        workflow: tripleo.validations.v1.add_validation_ssh_key_parameter
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>
        on-complete:
          - run_validations: <% $.run_validations %>
          - create_swift_rings_backup_plan: <% not $.run_validations %>

      run_validations:
        workflow: tripleo.validations.v1.run_groups
        input:
          group_names:
            - 'pre-deployment'
          plan: <% $.container %>
          queue_name: <% $.queue_name %>
        on-success: create_swift_rings_backup_plan
        on-error: set_validations_failed

      set_validations_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(run_validations).result %>

      create_swift_rings_backup_plan:
        workflow: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan
        on-success: cell_v2_discover_hosts
        on-error: create_swift_rings_backup_plan_set_status_failed
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>
          use_default_templates: true

      cell_v2_discover_hosts:
        on-success: deploy
        on-error: cell_v2_discover_hosts_failed
        action: tripleo.baremetal.cell_v2_discover_hosts

      cell_v2_discover_hosts_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(cell_v2_discover_hosts).result %>

      deploy:
        action: tripleo.deployment.deploy
        input:
          timeout: <% $.timeout %>
          container: <% $.container %>
          skip_deploy_identifier: <% $.skip_deploy_identifier %>
        on-success: send_message
        on-error: set_deployment_failed

      create_swift_rings_backup_plan_set_status_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(create_swift_rings_backup_plan).result %>

      set_deployment_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(deploy).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.deploy_plan
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  get_horizon_url:

    description: >
      Retrieve the Horizon URL from the Overcloud stack.

    input:
      - stack: overcloud
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    output:
      horizon_url: <% $.horizon_url %>

    tasks:
      get_horizon_url:
        action: heat.stacks_get
        input:
          stack_id: <% $.stack %>
        publish:
          horizon_url: <% task().result.outputs.where($.output_key = "EndpointMap").output_value.HorizonPublic.uri.single() %>
        on-success: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      notify_zaqar:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.get_horizon_url
              payload:
                horizon_url: <% $.get('horizon_url', '') %>
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  config_download_deploy:

    description: >
      Configure the overcloud with config-download.

    input:
      - timeout: 240
      - queue_name: tripleo
      - plan_name: overcloud
      - work_dir: /var/lib/mistral
      - verbosity: 1
      - blacklist: []

    tags:
      - tripleo-common-managed

    tasks:

      get_blacklisted_hostnames:
        action: heat.stacks_output_show
        input:
          stack_id: <% $.plan_name %>
          output_key: BlacklistedHostnames
        publish:
          blacklisted_hostnames: <% task().result.output.output_value.where($ != "") %>
        on-success: get_config
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      get_config:
        action: tripleo.config.get_overcloud_config
        input:
          container: <% $.get('plan_name') %>
        on-success: download_config
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      download_config:
        action: tripleo.config.download_config
        input:
          work_dir: <% $.get('work_dir') %>/<% execution().id %>
        on-success: send_msg_config_download
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      send_msg_config_download:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.config_download
              payload:
                status: <% $.get('status', 'RUNNING') %>
                message: Config downloaded at <% $.get('work_dir') %>/<% execution().id %>
                execution: <% execution() %>
        on-success: get_private_key

      get_private_key:
        action: tripleo.validations.get_privkey
        publish:
          private_key: <% task().result %>
        on-success: generate_inventory
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      generate_inventory:
        action: tripleo.ansible-generate-inventory
        input:
          ansible_ssh_user: tripleo-admin
          work_dir: <% $.get('work_dir') %>/<% execution().id %>
          plan_name: <% $.get('plan_name') %>
        publish:
          inventory: <% task().result %>
        on-success: send_msg_generate_inventory
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      send_msg_generate_inventory:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.config_download
              payload:
                status: <% $.get('status', 'RUNNING') %>
                message: Inventory generated at <% $.get('inventory') %>
                execution: <% execution() %>
        on-success: send_msg_run_ansible

      send_msg_run_ansible:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.config_download
              payload:
                status: <% $.get('status', 'RUNNING') %>
                message: >
                  Running ansible playbook at <% $.get('work_dir') %>/<% execution().id %>/deploy_steps_playbook.yaml.
                  See log file at <% $.get('work_dir') %>/<% execution().id %>/ansible.log for progress.
                  ...
                execution: <% execution() %>
        on-success: run_ansible

      run_ansible:
        action: tripleo.ansible-playbook
        input:
          inventory: <% $.inventory %>
          playbook: <% $.get('work_dir') %>/<% execution().id %>/deploy_steps_playbook.yaml
          remote_user: tripleo-admin
          ssh_extra_args: '-o StrictHostKeyChecking=no'
          ssh_private_key: <% $.private_key %>
          use_openstack_credentials: true
          verbosity: <% $.get('verbosity') %>
          become: true
          timeout: <% $.timeout %>
          work_dir: <% $.get('work_dir') %>/<% execution().id %>
          queue_name: <% $.queue_name %>
          reproduce_command: true
          trash_output: true
          blacklisted_hostnames: <% $.blacklisted_hostnames %>
        publish:
          log_path: <% task(run_ansible).result.get('log_path') %>
        on-success:
          - ansible_passed: <% task().result.returncode = 0 %>
          - ansible_failed: <% task().result.returncode != 0 %>
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/ansible.log.

      ansible_passed:
        on-success: send_message
        publish:
          status: SUCCESS
          message: Ansible passed.

      ansible_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/ansible.log.

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.config_download
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-03 06:18:56,695 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 14085
2018-12-03 06:18:56,697 DEBUG: RESP: [201] Content-Length: 14085 Content-Type: application/json Date: Mon, 03 Dec 2018 11:18:56 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.deployment.v1\ndescription: TripleO deployment workflows\n\nworkflows:\n\n  deploy_on_server:\n\n    input:\n      - server_uuid\n      - server_name\n      - config\n      - config_name\n      - group\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      deploy_config:\n        action: tripleo.deployment.config\n        on-complete: send_message\n        input:\n          server_id: <% $.server_uuid %>\n          name: <% $.config_name %>\n          config: <% $.config %>\n          group: <% $.group %>\n        publish:\n          stdout: <% task().result.deploy_stdout %>\n          stderr: <% task().result.deploy_stderr %>\n          status_code: <% task().result.deploy_status_code %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.deploy_on_server\n              payload:\n                status: <% $.get(\"status\", \"SUCCESS\") %>\n                message: <% $.get(\"message\", \"\") %>\n                server_uuid: <% $.server_uuid %>\n                server_name: <% $.server_name %>\n                config_name: <% $.config_name %>\n                status_code: <% $.get(\"status_code\", \"\") %>\n                stdout: <% $.get(\"stdout\", \"\") %>\n                stderr: <% $.get(\"stderr\", \"\") %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  deploy_on_servers:\n\n    input:\n      - server_name\n      - config_name\n      - config\n      - group: script\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      check_if_all_servers:\n        on-success:\n          - get_servers_matching: <% $.server_name != \"all\" %>\n          - get_all_servers: <% $.server_name = \"all\" %>\n\n      get_servers_matching:\n        action: nova.servers_list\n        on-success: deploy_on_servers\n        publish:\n          servers_with_name: <% task().result._info.where($.name.indexOf(execution().input.server_name) > -1) %>\n\n      get_all_servers:\n        action: nova.servers_list\n        on-success: deploy_on_servers\n        publish:\n          servers_with_name: <% task().result._info %>\n\n      deploy_on_servers:\n        on-success: send_success_message\n        on-error: send_failed_message\n        with-items: server in <% $.servers_with_name %>\n        workflow: tripleo.deployment.v1.deploy_on_server\n        input:\n          server_name: <% $.server.name %>\n          server_uuid: <% $.server.id %>\n          config: <% $.config %>\n          config_name: <% $.config_name %>\n          group: <% $.group %>\n          queue_name: <% $.queue_name %>\n\n      send_success_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.deploy_on_servers\n              payload:\n                status: SUCCESS\n                execution: <% execution() %>\n\n      send_failed_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.deploy_on_servers\n              payload:\n                status: FAILED\n                message: <% task(deploy_on_servers).result %>\n                execution: <% execution() %>\n        on-success: fail\n\n  deploy_plan:\n\n    description: >\n      Deploy the overcloud for a plan.\n\n    input:\n      - container\n      - run_validations: False\n      - timeout: 240\n      - skip_deploy_identifier: False\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      add_validation_ssh_key:\n        workflow: tripleo.validations.v1.add_validation_ssh_key_parameter\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n        on-complete:\n          - run_validations: <% $.run_validations %>\n          - create_swift_rings_backup_plan: <% not $.run_validations %>\n\n      run_validations:\n        workflow: tripleo.validations.v1.run_groups\n        input:\n          group_names:\n            - 'pre-deployment'\n          plan: <% $.container %>\n          queue_name: <% $.queue_name %>\n        on-success: create_swift_rings_backup_plan\n        on-error: set_validations_failed\n\n      set_validations_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(run_validations).result %>\n\n      create_swift_rings_backup_plan:\n        workflow: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan\n        on-success: cell_v2_discover_hosts\n        on-error: create_swift_rings_backup_plan_set_status_failed\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n          use_default_templates: true\n\n      cell_v2_discover_hosts:\n        on-success: deploy\n        on-error: cell_v2_discover_hosts_failed\n        action: tripleo.baremetal.cell_v2_discover_hosts\n\n      cell_v2_discover_hosts_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(cell_v2_discover_hosts).result %>\n\n      deploy:\n        action: tripleo.deployment.deploy\n        input:\n          timeout: <% $.timeout %>\n          container: <% $.container %>\n          skip_deploy_identifier: <% $.skip_deploy_identifier %>\n        on-success: send_message\n        on-error: set_deployment_failed\n\n      create_swift_rings_backup_plan_set_status_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(create_swift_rings_backup_plan).result %>\n\n      set_deployment_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(deploy).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.deploy_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  get_horizon_url:\n\n    description: >\n      Retrieve the Horizon URL from the Overcloud stack.\n\n    input:\n      - stack: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    output:\n      horizon_url: <% $.horizon_url %>\n\n    tasks:\n      get_horizon_url:\n        action: heat.stacks_get\n        input:\n          stack_id: <% $.stack %>\n        publish:\n          horizon_url: <% task().result.outputs.where($.output_key = \"EndpointMap\").output_value.HorizonPublic.uri.single() %>\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.get_horizon_url\n              payload:\n                horizon_url: <% $.get('horizon_url', '') %>\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  config_download_deploy:\n\n    description: >\n      Configure the overcloud with config-download.\n\n    input:\n      - timeout: 240\n      - queue_name: tripleo\n      - plan_name: overcloud\n      - work_dir: /var/lib/mistral\n      - verbosity: 1\n      - blacklist: []\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_blacklisted_hostnames:\n        action: heat.stacks_output_show\n        input:\n          stack_id: <% $.plan_name %>\n          output_key: BlacklistedHostnames\n        publish:\n          blacklisted_hostnames: <% task().result.output.output_value.where($ != \"\") %>\n        on-success: get_config\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_config:\n        action: tripleo.config.get_overcloud_config\n        input:\n          container: <% $.get('plan_name') %>\n        on-success: download_config\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      download_config:\n        action: tripleo.config.download_config\n        input:\n          work_dir: <% $.get('work_dir') %>/<% execution().id %>\n        on-success: send_msg_config_download\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      send_msg_config_download:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.config_download\n              payload:\n                status: <% $.get('status', 'RUNNING') %>\n                message: Config downloaded at <% $.get('work_dir') %>/<% execution().id %>\n                execution: <% execution() %>\n        on-success: get_private_key\n\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: generate_inventory\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      generate_inventory:\n        action: tripleo.ansible-generate-inventory\n        input:\n          ansible_ssh_user: tripleo-admin\n          work_dir: <% $.get('work_dir') %>/<% execution().id %>\n          plan_name: <% $.get('plan_name') %>\n        publish:\n          inventory: <% task().result %>\n        on-success: send_msg_generate_inventory\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      send_msg_generate_inventory:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.config_download\n              payload:\n                status: <% $.get('status', 'RUNNING') %>\n                message: Inventory generated at <% $.get('inventory') %>\n                execution: <% execution() %>\n        on-success: send_msg_run_ansible\n\n      send_msg_run_ansible:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.config_download\n              payload:\n                status: <% $.get('status', 'RUNNING') %>\n                message: >\n                  Running ansible playbook at <% $.get('work_dir') %>/<% execution().id %>/deploy_steps_playbook.yaml.\n                  See log file at <% $.get('work_dir') %>/<% execution().id %>/ansible.log for progress.\n                  ...\n                execution: <% execution() %>\n        on-success: run_ansible\n\n      run_ansible:\n        action: tripleo.ansible-playbook\n        input:\n          inventory: <% $.inventory %>\n          playbook: <% $.get('work_dir') %>/<% execution().id %>/deploy_steps_playbook.yaml\n          remote_user: tripleo-admin\n          ssh_extra_args: '-o StrictHostKeyChecking=no'\n          ssh_private_key: <% $.private_key %>\n          use_openstack_credentials: true\n          verbosity: <% $.get('verbosity') %>\n          become: true\n          timeout: <% $.timeout %>\n          work_dir: <% $.get('work_dir') %>/<% execution().id %>\n          queue_name: <% $.queue_name %>\n          reproduce_command: true\n          trash_output: true\n          blacklisted_hostnames: <% $.blacklisted_hostnames %>\n        publish:\n          log_path: <% task(run_ansible).result.get('log_path') %>\n        on-success:\n          - ansible_passed: <% task().result.returncode = 0 %>\n          - ansible_failed: <% task().result.returncode != 0 %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/ansible.log.\n\n      ansible_passed:\n        on-success: send_message\n        publish:\n          status: SUCCESS\n          message: Ansible passed.\n\n      ansible_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/ansible.log.\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.config_download\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.deployment.v1", "tags": [], "created_at": "2018-12-03 11:18:56", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "4eb617a8-75b7-4f5e-93f2-16ec8184be11"}

2018-12-03 06:18:56,697 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:18:56,698 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.derive_params.v1
description: TripleO Workflows to derive deployment parameters from the introspected data

workflows:

  derive_parameters:
    description: The main workflow for deriving parameters from the introspected data

    input:
      - plan: overcloud
      - queue_name: tripleo
      - user_inputs: {}

    tags:
      - tripleo-common-managed

    tasks:
      get_flattened_parameters:
        action: tripleo.parameters.get_flatten container=<% $.plan %>
        publish:
          environment_parameters: <% task().result.environment_parameters %>
          heat_resource_tree: <% task().result.heat_resource_tree %>
        on-success:
          - get_roles: <% $.environment_parameters and $.heat_resource_tree %>
          - set_status_failed_get_flattened_parameters: <% (not $.environment_parameters) or (not $.heat_resource_tree) %>
        on-error: set_status_failed_get_flattened_parameters

      get_roles:
        action: tripleo.role.list container=<% $.plan %>
        publish:
          role_name_list: <% task().result %>
        on-success:
          - get_valid_roles: <% $.role_name_list %>
          - set_status_failed_get_roles: <% not $.role_name_list %>
        on-error: set_status_failed_on_error_get_roles

      # Obtain only the roles which has count > 0, by checking <RoleName>Count parameter, like ComputeCount
      get_valid_roles:
        publish:
          valid_role_name_list: <% let(hr => $.heat_resource_tree.parameters) -> $.role_name_list.where(int($hr.get(concat($, 'Count'), {}).get('default', 0)) > 0) %>
        on-success:
          - for_each_role: <% $.valid_role_name_list %>
          - set_status_failed_get_valid_roles: <% not $.valid_role_name_list %>

      # Execute the basic preparation workflow for each role to get introspection data
      for_each_role:
        with-items: role_name in <% $.valid_role_name_list %>
        concurrency: 1
        workflow: _derive_parameters_per_role
        input:
          plan: <% $.plan %>
          role_name: <% $.role_name %>
          environment_parameters: <% $.environment_parameters %>
          heat_resource_tree: <% $.heat_resource_tree %>
          user_inputs: <% $.user_inputs %>
        publish:
          # Gets all the roles derived parameters as dictionary
          result: <% task().result.select($.get('derived_parameters', {})).sum() %>
        on-success: reset_derive_parameters_in_plan
        on-error: set_status_failed_for_each_role

      reset_derive_parameters_in_plan:
        action: tripleo.parameters.reset
        input:
          container: <% $.plan %>
          key: 'derived_parameters'
        on-success:
          # Add the derived parameters to the deployment plan only when $.result
          # (the derived parameters) is non-empty. Otherwise, we're done.
          - update_derive_parameters_in_plan: <% $.result %>
          - send_message: <% not $.result %>
        on-error: set_status_failed_reset_derive_parameters_in_plan

      update_derive_parameters_in_plan:
        action: tripleo.parameters.update
        input:
          container: <% $.plan %>
          key: 'derived_parameters'
          parameters: <% $.get('result', {}) %>
        on-success: send_message
        on-error: set_status_failed_update_derive_parameters_in_plan

      set_status_failed_get_flattened_parameters:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_flattened_parameters).result %>

      set_status_failed_get_roles:
        on-success: send_message
        publish:
          status: FAILED
          message: "Unable to determine the list of roles in the deployment plan"

      set_status_failed_on_error_get_roles:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_roles).result %>

      set_status_failed_get_valid_roles:
        on-success: send_message
        publish:
          status: FAILED
          message: 'Unable to determine the list of valid roles in the deployment plan.'

      set_status_failed_for_each_role:
        on-success: update_message_format
        publish:
          status: FAILED
          # gets the status and message for all roles from task result.
          message: <% task(for_each_role).result.select(dict('role_name' => $.role_name, 'status' => $.get('status', 'SUCCESS'), 'message' => $.get('message', ''))) %>

      update_message_format:
        on-success: send_message
        publish:
          # updates the message format(Role 'role name': message) for each roles which are failed and joins the message list as string with ', ' separator.
          message: <% $.message.where($.get('status', 'SUCCESS') != 'SUCCESS').select(concat("Role '{}':".format($.role_name), " ", $.get('message', '(error unknown)'))).join(', ') %>

      set_status_failed_reset_derive_parameters_in_plan:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(reset_derive_parameters_in_plan).result %>

      set_status_failed_update_derive_parameters_in_plan:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(update_derive_parameters_in_plan).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.derive_params.v1.derive_parameters
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                result: <% $.get('result', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = 'FAILED' %>


  _derive_parameters_per_role:
    description: >
      Workflow which runs per role to get the introspection data on the first matching node assigned to role.
      Once introspection data is fetched, this worklow will trigger the actual derive parameters workflow
    input:
      - plan
      - role_name
      - environment_parameters
      - heat_resource_tree
      - user_inputs

    output:
      derived_parameters: <% $.get('derived_parameters', {}) %>
      # Need role_name in output parameter to display the status for all roles in main workflow when any role fails here.
      role_name: <% $.role_name %>

    tags:
      - tripleo-common-managed

    tasks:
      get_role_info:
        workflow: _get_role_info
        input:
          role_name: <% $.role_name %>
          heat_resource_tree: <% $.heat_resource_tree %>
        publish:
          role_features: <% task().result.get('role_features', []) %>
          role_services: <% task().result.get('role_services', []) %>
        on-success:
          # Continue only if there are features associated with this role. Otherwise, we're done.
          - get_scheduler_hints: <% $.role_features %>
        on-error: set_status_failed_get_role_info

      # Find a node associated with this role. Look for nodes matching any scheduler hints
      # associated with the role, and if there are no scheduler hints then locate nodes
      # with a profile matching the role's flavor.
      get_scheduler_hints:
        publish:
          scheduler_hints: <% let(param_name => concat($.role_name, 'SchedulerHints')) -> $.heat_resource_tree.parameters.get($param_name, {}).get('default', {}) %>
        on-success:
          - get_hint_regex: <% $.scheduler_hints %>
          # If there are no scheduler hints then move on to use the flavor
          - get_flavor_name: <% not $.scheduler_hints %>

      get_hint_regex:
        publish:
          hint_regex: <% $.scheduler_hints.get('capabilities:node', '').replace('%index%', '(\d+)') %>
        on-success:
          - get_node_with_hint: <% $.hint_regex %>
          # If there is no 'capabilities:node' hint then move on to use the flavor
          - get_flavor_name: <% not $.hint_regex %>

      get_node_with_hint:
        workflow: tripleo.baremetal.v1.nodes_with_hint
        input:
          hint_regex: <% concat('^', $.hint_regex, '$') %>
        publish:
          role_node_uuid: <% task().result.matching_nodes.first('') %>
        on-success:
          - get_introspection_data: <% $.role_node_uuid %>
          # If no nodes match the scheduler hint then move on to use the flavor
          - get_flavor_name: <% not $.role_node_uuid %>
        on-error: set_status_failed_on_error_get_node_with_hint

      get_flavor_name:
        publish:
          flavor_name: <% let(param_name => concat('Overcloud', $.role_name, 'Flavor').replace('OvercloudControllerFlavor', 'OvercloudControlFlavor')) -> $.heat_resource_tree.parameters.get($param_name, {}).get('default', '') %>
        on-success:
          - get_profile_name: <% $.flavor_name %>
          - set_status_failed_get_flavor_name: <% not $.flavor_name %>

      get_profile_name:
        action: tripleo.parameters.get_profile_of_flavor flavor_name=<% $.flavor_name %>
        publish:
          profile_name: <% task().result %>
        on-success: get_profile_node
        on-error: set_status_failed_get_profile_name

      get_profile_node:
        workflow: tripleo.baremetal.v1.nodes_with_profile
        input:
          profile: <% $.profile_name %>
        publish:
          role_node_uuid: <% task().result.matching_nodes.first('') %>
        on-success:
          - get_introspection_data: <% $.role_node_uuid %>
          - set_status_failed_no_matching_node_get_profile_node: <% not $.role_node_uuid %>
        on-error: set_status_failed_on_error_get_profile_node

      get_introspection_data:
        action: baremetal_introspection.get_data uuid=<% $.role_node_uuid %>
        publish:
          hw_data: <% task().result %>
          # Establish an empty dictionary of derived_parameters prior to
          # invoking the individual "feature" algorithms
          derived_parameters: <% dict() %>
        on-success: handle_dpdk_feature
        on-error: set_status_failed_get_introspection_data

      handle_dpdk_feature:
        on-success:
          - get_dpdk_derive_params: <% $.role_features.contains('DPDK') %>
          - handle_sriov_feature: <% not $.role_features.contains('DPDK') %>

      get_dpdk_derive_params:
        workflow: tripleo.derive_params_formulas.v1.dpdk_derive_params
        input:
          plan: <% $.plan %>
          role_name: <% $.role_name %>
          hw_data: <% $.hw_data %>
          user_inputs: <% $.user_inputs %>
        publish:
          derived_parameters: <% task().result.get('derived_parameters', {}) %>
        on-success: handle_sriov_feature
        on-error: set_status_failed_get_dpdk_derive_params

      handle_sriov_feature:
        on-success:
          - get_sriov_derive_params: <% $.role_features.contains('SRIOV') %>
          - handle_host_feature: <% not $.role_features.contains('SRIOV') %>

      get_sriov_derive_params:
        workflow: tripleo.derive_params_formulas.v1.sriov_derive_params
        input:
          role_name: <% $.role_name %>
          hw_data: <% $.hw_data %>
          derived_parameters: <% $.derived_parameters %>
        publish:
          derived_parameters: <% task().result.get('derived_parameters', {}) %>
        on-success: handle_host_feature
        on-error: set_status_failed_get_sriov_derive_params

      handle_host_feature:
        on-success:
          - get_host_derive_params: <% $.role_features.contains('HOST') %>
          - handle_hci_feature: <% not $.role_features.contains('HOST') %>

      get_host_derive_params:
        workflow: tripleo.derive_params_formulas.v1.host_derive_params
        input:
          role_name: <% $.role_name %>
          hw_data: <% $.hw_data %>
          user_inputs: <% $.user_inputs %>
          derived_parameters: <% $.derived_parameters %>
        publish:
          derived_parameters: <% task().result.get('derived_parameters', {}) %>
        on-success: handle_hci_feature
        on-error: set_status_failed_get_host_derive_params

      handle_hci_feature:
        on-success:
          - get_hci_derive_params: <% $.role_features.contains('HCI') %>

      get_hci_derive_params:
        workflow: tripleo.derive_params_formulas.v1.hci_derive_params
        input:
          role_name: <% $.role_name %>
          environment_parameters: <% $.environment_parameters %>
          heat_resource_tree: <% $.heat_resource_tree %>
          introspection_data: <% $.hw_data %>
          user_inputs: <% $.user_inputs %>
          derived_parameters: <% $.derived_parameters %>
        publish:
          derived_parameters: <% task().result.get('derived_parameters', {}) %>
        on-error: set_status_failed_get_hci_derive_params
        # Done (no more derived parameter features)

      set_status_failed_get_role_info:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_role_info).result.get('message', '') %>
        on-success: fail

      set_status_failed_get_flavor_name:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% "Unable to determine flavor for role '{0}'".format($.role_name) %>
        on-success: fail

      set_status_failed_get_profile_name:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_profile_name).result %>
        on-success: fail

      set_status_failed_no_matching_node_get_profile_node:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% "Unable to determine matching node for profile '{0}'".format($.profile_name) %>
        on-success: fail

      set_status_failed_on_error_get_profile_node:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_profile_node).result %>
        on-success: fail

      set_status_failed_on_error_get_node_with_hint:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_node_with_hint).result %>
        on-success: fail

      set_status_failed_get_introspection_data:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_introspection_data).result %>
        on-success: fail

      set_status_failed_get_dpdk_derive_params:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_dpdk_derive_params).result.message %>
        on-success: fail

      set_status_failed_get_sriov_derive_params:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_sriov_derive_params).result.message %>
        on-success: fail

      set_status_failed_get_host_derive_params:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_host_derive_params).result.message %>
        on-success: fail

      set_status_failed_get_hci_derive_params:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_hci_derive_params).result.message %>
        on-success: fail


  _get_role_info:
    description: >
      Workflow that determines the list of derived parameter features (DPDK,
      HCI, etc.) for a role based on the services assigned to the role.

    input:
      - role_name
      - heat_resource_tree

    tags:
      - tripleo-common-managed

    tasks:
      get_resource_chains:
        publish:
          resource_chains: <% $.heat_resource_tree.resources.values().where($.get('type', '') = 'OS::Heat::ResourceChain') %>
        on-success:
          - get_role_chain: <% $.resource_chains %>
          - set_status_failed_get_resource_chains: <% not $.resource_chains %>

      get_role_chain:
        publish:
          role_chain: <% let(chain_name => concat($.role_name, 'ServiceChain'))-> $.heat_resource_tree.resources.values().where($.name = $chain_name).first({}) %>
        on-success:
          - get_service_chain: <% $.role_chain %>
          - set_status_failed_get_role_chain: <% not $.role_chain %>

      get_service_chain:
        publish:
          service_chain: <% let(resources => $.role_chain.resources)-> $.resource_chains.where($resources.contains($.id)).first('') %>
        on-success:
          - get_role_services: <% $.service_chain %>
          - set_status_failed_get_service_chain: <% not $.service_chain %>

      get_role_services:
        publish:
          role_services: <% let(resources => $.heat_resource_tree.resources)-> $.service_chain.resources.select($resources.get($)) %>
        on-success:
          - check_features: <% $.role_services %>
          - set_status_failed_get_role_services: <% not $.role_services %>

      check_features:
        on-success: build_feature_dict
        publish:
          # The role supports the DPDK feature if the NeutronDatapathType parameter is present
          dpdk: <% let(resources => $.heat_resource_tree.resources) -> $.role_services.any($.get('parameters', []).contains('NeutronDatapathType') or $.get('resources', []).select($resources.get($)).any($.get('parameters', []).contains('NeutronDatapathType'))) %>

          # The role supports the DPDK feature in ODL if the OvsEnableDpdk parameter value is true in role parameters.
          odl_dpdk: <% let(role => $.role_name) -> $.heat_resource_tree.parameters.get(concat($role, 'Parameters'), {}).get('default', {}).get('OvsEnableDpdk', false) %>

          # The role supports the SRIOV feature if it includes NeutronSriovAgent services.
          sriov: <% $.role_services.any($.get('type', '').endsWith('::NeutronSriovAgent')) %>

          # The role supports the HCI feature if it includes both NovaCompute and CephOSD services.
          hci: <% $.role_services.any($.get('type', '').endsWith('::NovaCompute')) and $.role_services.any($.get('type', '').endsWith('::CephOSD')) %>

      build_feature_dict:
        on-success: filter_features
        publish:
          feature_dict: <% dict(DPDK => ($.dpdk or $.odl_dpdk), SRIOV => $.sriov, HOST => ($.dpdk or $.odl_dpdk or $.sriov), HCI => $.hci) %>

      filter_features:
        publish:
          # The list of features that are enabled (i.e. are true in the feature_dict).
          role_features: <% let(feature_dict => $.feature_dict)-> $feature_dict.keys().where($feature_dict[$]) %>

      set_status_failed_get_resource_chains:
        publish:
          message: <% 'Unable to locate any resource chains in the heat resource tree' %>
        on-success: fail

      set_status_failed_get_role_chain:
        publish:
          message: <% "Unable to determine the service chain resource for role '{0}'".format($.role_name) %>
        on-success: fail

      set_status_failed_get_service_chain:
        publish:
          message: <% "Unable to determine the service chain for role '{0}'".format($.role_name) %>
        on-success: fail

      set_status_failed_get_role_services:
        publish:
          message: <% "Unable to determine list of services for role '{0}'".format($.role_name) %>
        on-success: fail
'
2018-12-03 06:18:58,762 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 20049
2018-12-03 06:18:58,803 DEBUG: RESP: [201] Content-Length: 20049 Content-Type: application/json Date: Mon, 03 Dec 2018 11:18:58 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.derive_params.v1\ndescription: TripleO Workflows to derive deployment parameters from the introspected data\n\nworkflows:\n\n  derive_parameters:\n    description: The main workflow for deriving parameters from the introspected data\n\n    input:\n      - plan: overcloud\n      - queue_name: tripleo\n      - user_inputs: {}\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_flattened_parameters:\n        action: tripleo.parameters.get_flatten container=<% $.plan %>\n        publish:\n          environment_parameters: <% task().result.environment_parameters %>\n          heat_resource_tree: <% task().result.heat_resource_tree %>\n        on-success:\n          - get_roles: <% $.environment_parameters and $.heat_resource_tree %>\n          - set_status_failed_get_flattened_parameters: <% (not $.environment_parameters) or (not $.heat_resource_tree) %>\n        on-error: set_status_failed_get_flattened_parameters\n\n      get_roles:\n        action: tripleo.role.list container=<% $.plan %>\n        publish:\n          role_name_list: <% task().result %>\n        on-success:\n          - get_valid_roles: <% $.role_name_list %>\n          - set_status_failed_get_roles: <% not $.role_name_list %>\n        on-error: set_status_failed_on_error_get_roles\n\n      # Obtain only the roles which has count > 0, by checking <RoleName>Count parameter, like ComputeCount\n      get_valid_roles:\n        publish:\n          valid_role_name_list: <% let(hr => $.heat_resource_tree.parameters) -> $.role_name_list.where(int($hr.get(concat($, 'Count'), {}).get('default', 0)) > 0) %>\n        on-success:\n          - for_each_role: <% $.valid_role_name_list %>\n          - set_status_failed_get_valid_roles: <% not $.valid_role_name_list %>\n\n      # Execute the basic preparation workflow for each role to get introspection data\n      for_each_role:\n        with-items: role_name in <% $.valid_role_name_list %>\n        concurrency: 1\n        workflow: _derive_parameters_per_role\n        input:\n          plan: <% $.plan %>\n          role_name: <% $.role_name %>\n          environment_parameters: <% $.environment_parameters %>\n          heat_resource_tree: <% $.heat_resource_tree %>\n          user_inputs: <% $.user_inputs %>\n        publish:\n          # Gets all the roles derived parameters as dictionary\n          result: <% task().result.select($.get('derived_parameters', {})).sum() %>\n        on-success: reset_derive_parameters_in_plan\n        on-error: set_status_failed_for_each_role\n\n      reset_derive_parameters_in_plan:\n        action: tripleo.parameters.reset\n        input:\n          container: <% $.plan %>\n          key: 'derived_parameters'\n        on-success:\n          # Add the derived parameters to the deployment plan only when $.result\n          # (the derived parameters) is non-empty. Otherwise, we're done.\n          - update_derive_parameters_in_plan: <% $.result %>\n          - send_message: <% not $.result %>\n        on-error: set_status_failed_reset_derive_parameters_in_plan\n\n      update_derive_parameters_in_plan:\n        action: tripleo.parameters.update\n        input:\n          container: <% $.plan %>\n          key: 'derived_parameters'\n          parameters: <% $.get('result', {}) %>\n        on-success: send_message\n        on-error: set_status_failed_update_derive_parameters_in_plan\n\n      set_status_failed_get_flattened_parameters:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_flattened_parameters).result %>\n\n      set_status_failed_get_roles:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: \"Unable to determine the list of roles in the deployment plan\"\n\n      set_status_failed_on_error_get_roles:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_roles).result %>\n\n      set_status_failed_get_valid_roles:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: 'Unable to determine the list of valid roles in the deployment plan.'\n\n      set_status_failed_for_each_role:\n        on-success: update_message_format\n        publish:\n          status: FAILED\n          # gets the status and message for all roles from task result.\n          message: <% task(for_each_role).result.select(dict('role_name' => $.role_name, 'status' => $.get('status', 'SUCCESS'), 'message' => $.get('message', ''))) %>\n\n      update_message_format:\n        on-success: send_message\n        publish:\n          # updates the message format(Role 'role name': message) for each roles which are failed and joins the message list as string with ', ' separator.\n          message: <% $.message.where($.get('status', 'SUCCESS') != 'SUCCESS').select(concat(\"Role '{}':\".format($.role_name), \" \", $.get('message', '(error unknown)'))).join(', ') %>\n\n      set_status_failed_reset_derive_parameters_in_plan:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(reset_derive_parameters_in_plan).result %>\n\n      set_status_failed_update_derive_parameters_in_plan:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(update_derive_parameters_in_plan).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.derive_params.v1.derive_parameters\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                result: <% $.get('result', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n\n  _derive_parameters_per_role:\n    description: >\n      Workflow which runs per role to get the introspection data on the first matching node assigned to role.\n      Once introspection data is fetched, this worklow will trigger the actual derive parameters workflow\n    input:\n      - plan\n      - role_name\n      - environment_parameters\n      - heat_resource_tree\n      - user_inputs\n\n    output:\n      derived_parameters: <% $.get('derived_parameters', {}) %>\n      # Need role_name in output parameter to display the status for all roles in main workflow when any role fails here.\n      role_name: <% $.role_name %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_role_info:\n        workflow: _get_role_info\n        input:\n          role_name: <% $.role_name %>\n          heat_resource_tree: <% $.heat_resource_tree %>\n        publish:\n          role_features: <% task().result.get('role_features', []) %>\n          role_services: <% task().result.get('role_services', []) %>\n        on-success:\n          # Continue only if there are features associated with this role. Otherwise, we're done.\n          - get_scheduler_hints: <% $.role_features %>\n        on-error: set_status_failed_get_role_info\n\n      # Find a node associated with this role. Look for nodes matching any scheduler hints\n      # associated with the role, and if there are no scheduler hints then locate nodes\n      # with a profile matching the role's flavor.\n      get_scheduler_hints:\n        publish:\n          scheduler_hints: <% let(param_name => concat($.role_name, 'SchedulerHints')) -> $.heat_resource_tree.parameters.get($param_name, {}).get('default', {}) %>\n        on-success:\n          - get_hint_regex: <% $.scheduler_hints %>\n          # If there are no scheduler hints then move on to use the flavor\n          - get_flavor_name: <% not $.scheduler_hints %>\n\n      get_hint_regex:\n        publish:\n          hint_regex: <% $.scheduler_hints.get('capabilities:node', '').replace('%index%', '(\\d+)') %>\n        on-success:\n          - get_node_with_hint: <% $.hint_regex %>\n          # If there is no 'capabilities:node' hint then move on to use the flavor\n          - get_flavor_name: <% not $.hint_regex %>\n\n      get_node_with_hint:\n        workflow: tripleo.baremetal.v1.nodes_with_hint\n        input:\n          hint_regex: <% concat('^', $.hint_regex, '$') %>\n        publish:\n          role_node_uuid: <% task().result.matching_nodes.first('') %>\n        on-success:\n          - get_introspection_data: <% $.role_node_uuid %>\n          # If no nodes match the scheduler hint then move on to use the flavor\n          - get_flavor_name: <% not $.role_node_uuid %>\n        on-error: set_status_failed_on_error_get_node_with_hint\n\n      get_flavor_name:\n        publish:\n          flavor_name: <% let(param_name => concat('Overcloud', $.role_name, 'Flavor').replace('OvercloudControllerFlavor', 'OvercloudControlFlavor')) -> $.heat_resource_tree.parameters.get($param_name, {}).get('default', '') %>\n        on-success:\n          - get_profile_name: <% $.flavor_name %>\n          - set_status_failed_get_flavor_name: <% not $.flavor_name %>\n\n      get_profile_name:\n        action: tripleo.parameters.get_profile_of_flavor flavor_name=<% $.flavor_name %>\n        publish:\n          profile_name: <% task().result %>\n        on-success: get_profile_node\n        on-error: set_status_failed_get_profile_name\n\n      get_profile_node:\n        workflow: tripleo.baremetal.v1.nodes_with_profile\n        input:\n          profile: <% $.profile_name %>\n        publish:\n          role_node_uuid: <% task().result.matching_nodes.first('') %>\n        on-success:\n          - get_introspection_data: <% $.role_node_uuid %>\n          - set_status_failed_no_matching_node_get_profile_node: <% not $.role_node_uuid %>\n        on-error: set_status_failed_on_error_get_profile_node\n\n      get_introspection_data:\n        action: baremetal_introspection.get_data uuid=<% $.role_node_uuid %>\n        publish:\n          hw_data: <% task().result %>\n          # Establish an empty dictionary of derived_parameters prior to\n          # invoking the individual \"feature\" algorithms\n          derived_parameters: <% dict() %>\n        on-success: handle_dpdk_feature\n        on-error: set_status_failed_get_introspection_data\n\n      handle_dpdk_feature:\n        on-success:\n          - get_dpdk_derive_params: <% $.role_features.contains('DPDK') %>\n          - handle_sriov_feature: <% not $.role_features.contains('DPDK') %>\n\n      get_dpdk_derive_params:\n        workflow: tripleo.derive_params_formulas.v1.dpdk_derive_params\n        input:\n          plan: <% $.plan %>\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n          user_inputs: <% $.user_inputs %>\n        publish:\n          derived_parameters: <% task().result.get('derived_parameters', {}) %>\n        on-success: handle_sriov_feature\n        on-error: set_status_failed_get_dpdk_derive_params\n\n      handle_sriov_feature:\n        on-success:\n          - get_sriov_derive_params: <% $.role_features.contains('SRIOV') %>\n          - handle_host_feature: <% not $.role_features.contains('SRIOV') %>\n\n      get_sriov_derive_params:\n        workflow: tripleo.derive_params_formulas.v1.sriov_derive_params\n        input:\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n          derived_parameters: <% $.derived_parameters %>\n        publish:\n          derived_parameters: <% task().result.get('derived_parameters', {}) %>\n        on-success: handle_host_feature\n        on-error: set_status_failed_get_sriov_derive_params\n\n      handle_host_feature:\n        on-success:\n          - get_host_derive_params: <% $.role_features.contains('HOST') %>\n          - handle_hci_feature: <% not $.role_features.contains('HOST') %>\n\n      get_host_derive_params:\n        workflow: tripleo.derive_params_formulas.v1.host_derive_params\n        input:\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n          user_inputs: <% $.user_inputs %>\n          derived_parameters: <% $.derived_parameters %>\n        publish:\n          derived_parameters: <% task().result.get('derived_parameters', {}) %>\n        on-success: handle_hci_feature\n        on-error: set_status_failed_get_host_derive_params\n\n      handle_hci_feature:\n        on-success:\n          - get_hci_derive_params: <% $.role_features.contains('HCI') %>\n\n      get_hci_derive_params:\n        workflow: tripleo.derive_params_formulas.v1.hci_derive_params\n        input:\n          role_name: <% $.role_name %>\n          environment_parameters: <% $.environment_parameters %>\n          heat_resource_tree: <% $.heat_resource_tree %>\n          introspection_data: <% $.hw_data %>\n          user_inputs: <% $.user_inputs %>\n          derived_parameters: <% $.derived_parameters %>\n        publish:\n          derived_parameters: <% task().result.get('derived_parameters', {}) %>\n        on-error: set_status_failed_get_hci_derive_params\n        # Done (no more derived parameter features)\n\n      set_status_failed_get_role_info:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_role_info).result.get('message', '') %>\n        on-success: fail\n\n      set_status_failed_get_flavor_name:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% \"Unable to determine flavor for role '{0}'\".format($.role_name) %>\n        on-success: fail\n\n      set_status_failed_get_profile_name:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_profile_name).result %>\n        on-success: fail\n\n      set_status_failed_no_matching_node_get_profile_node:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% \"Unable to determine matching node for profile '{0}'\".format($.profile_name) %>\n        on-success: fail\n\n      set_status_failed_on_error_get_profile_node:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_profile_node).result %>\n        on-success: fail\n\n      set_status_failed_on_error_get_node_with_hint:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_node_with_hint).result %>\n        on-success: fail\n\n      set_status_failed_get_introspection_data:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_introspection_data).result %>\n        on-success: fail\n\n      set_status_failed_get_dpdk_derive_params:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_dpdk_derive_params).result.message %>\n        on-success: fail\n\n      set_status_failed_get_sriov_derive_params:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_sriov_derive_params).result.message %>\n        on-success: fail\n\n      set_status_failed_get_host_derive_params:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_host_derive_params).result.message %>\n        on-success: fail\n\n      set_status_failed_get_hci_derive_params:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_hci_derive_params).result.message %>\n        on-success: fail\n\n\n  _get_role_info:\n    description: >\n      Workflow that determines the list of derived parameter features (DPDK,\n      HCI, etc.) for a role based on the services assigned to the role.\n\n    input:\n      - role_name\n      - heat_resource_tree\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_resource_chains:\n        publish:\n          resource_chains: <% $.heat_resource_tree.resources.values().where($.get('type', '') = 'OS::Heat::ResourceChain') %>\n        on-success:\n          - get_role_chain: <% $.resource_chains %>\n          - set_status_failed_get_resource_chains: <% not $.resource_chains %>\n\n      get_role_chain:\n        publish:\n          role_chain: <% let(chain_name => concat($.role_name, 'ServiceChain'))-> $.heat_resource_tree.resources.values().where($.name = $chain_name).first({}) %>\n        on-success:\n          - get_service_chain: <% $.role_chain %>\n          - set_status_failed_get_role_chain: <% not $.role_chain %>\n\n      get_service_chain:\n        publish:\n          service_chain: <% let(resources => $.role_chain.resources)-> $.resource_chains.where($resources.contains($.id)).first('') %>\n        on-success:\n          - get_role_services: <% $.service_chain %>\n          - set_status_failed_get_service_chain: <% not $.service_chain %>\n\n      get_role_services:\n        publish:\n          role_services: <% let(resources => $.heat_resource_tree.resources)-> $.service_chain.resources.select($resources.get($)) %>\n        on-success:\n          - check_features: <% $.role_services %>\n          - set_status_failed_get_role_services: <% not $.role_services %>\n\n      check_features:\n        on-success: build_feature_dict\n        publish:\n          # The role supports the DPDK feature if the NeutronDatapathType parameter is present\n          dpdk: <% let(resources => $.heat_resource_tree.resources) -> $.role_services.any($.get('parameters', []).contains('NeutronDatapathType') or $.get('resources', []).select($resources.get($)).any($.get('parameters', []).contains('NeutronDatapathType'))) %>\n\n          # The role supports the DPDK feature in ODL if the OvsEnableDpdk parameter value is true in role parameters.\n          odl_dpdk: <% let(role => $.role_name) -> $.heat_resource_tree.parameters.get(concat($role, 'Parameters'), {}).get('default', {}).get('OvsEnableDpdk', false) %>\n\n          # The role supports the SRIOV feature if it includes NeutronSriovAgent services.\n          sriov: <% $.role_services.any($.get('type', '').endsWith('::NeutronSriovAgent')) %>\n\n          # The role supports the HCI feature if it includes both NovaCompute and CephOSD services.\n          hci: <% $.role_services.any($.get('type', '').endsWith('::NovaCompute')) and $.role_services.any($.get('type', '').endsWith('::CephOSD')) %>\n\n      build_feature_dict:\n        on-success: filter_features\n        publish:\n          feature_dict: <% dict(DPDK => ($.dpdk or $.odl_dpdk), SRIOV => $.sriov, HOST => ($.dpdk or $.odl_dpdk or $.sriov), HCI => $.hci) %>\n\n      filter_features:\n        publish:\n          # The list of features that are enabled (i.e. are true in the feature_dict).\n          role_features: <% let(feature_dict => $.feature_dict)-> $feature_dict.keys().where($feature_dict[$]) %>\n\n      set_status_failed_get_resource_chains:\n        publish:\n          message: <% 'Unable to locate any resource chains in the heat resource tree' %>\n        on-success: fail\n\n      set_status_failed_get_role_chain:\n        publish:\n          message: <% \"Unable to determine the service chain resource for role '{0}'\".format($.role_name) %>\n        on-success: fail\n\n      set_status_failed_get_service_chain:\n        publish:\n          message: <% \"Unable to determine the service chain for role '{0}'\".format($.role_name) %>\n        on-success: fail\n\n      set_status_failed_get_role_services:\n        publish:\n          message: <% \"Unable to determine list of services for role '{0}'\".format($.role_name) %>\n        on-success: fail\n", "name": "tripleo.derive_params.v1", "tags": [], "created_at": "2018-12-03 11:18:58", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "fa1df666-182f-4cfd-b9c5-a4c23966aacb"}

2018-12-03 06:18:58,804 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:18:58,806 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.derive_params_formulas.v1
description: TripleO Workflows to derive deployment parameters from the introspected data

workflows:


  dpdk_derive_params:
    description: >
      Workflow to derive parameters for DPDK service.
    input:
      - plan
      - role_name
      - hw_data # introspection data
      - user_inputs
      - derived_parameters: {}

    output:
      derived_parameters: <% $.derived_parameters.mergeWith($.get('dpdk_parameters', {})) %>

    tags:
      - tripleo-common-managed

    tasks:
      get_network_config:
        action: tripleo.parameters.get_network_config
        input:
          container: <% $.plan %>
          role_name: <% $.role_name %>
        publish:
          network_configs: <% task().result.get('network_config', []) %>
        on-success: get_dpdk_nics_numa_info
        on-error: set_status_failed_get_network_config

      get_dpdk_nics_numa_info:
        action: tripleo.derive_params.get_dpdk_nics_numa_info
        input:
          network_configs: <% $.network_configs %>
          inspect_data: <% $.hw_data %>
        publish:
          dpdk_nics_numa_info: <% task().result %>
        on-success:
          # TODO: Need to remove condtions here
          # adding condition and throw error in action for empty check
          - get_dpdk_nics_numa_nodes: <% $.dpdk_nics_numa_info %>
          - set_status_failed_get_dpdk_nics_numa_info: <% not $.dpdk_nics_numa_info %>
        on-error: set_status_failed_on_error_get_dpdk_nics_numa_info

      get_dpdk_nics_numa_nodes:
        publish:
          dpdk_nics_numa_nodes: <% $.dpdk_nics_numa_info.groupBy($.numa_node).select($[0]).orderBy($) %>
        on-success:
          - get_numa_nodes: <% $.dpdk_nics_numa_nodes %>
          - set_status_failed_get_dpdk_nics_numa_nodes: <% not $.dpdk_nics_numa_nodes %>

      get_numa_nodes:
        publish:
          numa_nodes: <% $.hw_data.numa_topology.ram.select($.numa_node).orderBy($) %>
        on-success:
          - get_num_phy_cores_per_numa_for_pmd: <% $.numa_nodes %>
          - set_status_failed_get_numa_nodes: <% not $.numa_nodes %>

      get_num_phy_cores_per_numa_for_pmd:
        publish:
          num_phy_cores_per_numa_node_for_pmd: <% $.user_inputs.get('num_phy_cores_per_numa_node_for_pmd', 0) %>
        on-success:
          - get_num_cores_per_numa_nodes: <% isInteger($.num_phy_cores_per_numa_node_for_pmd) and $.num_phy_cores_per_numa_node_for_pmd > 0 %>
          - set_status_failed_get_num_phy_cores_per_numa_for_pmd_invalid: <% not isInteger($.num_phy_cores_per_numa_node_for_pmd) %>
          - set_status_failed_get_num_phy_cores_per_numa_for_pmd_not_provided: <% $.num_phy_cores_per_numa_node_for_pmd = 0 %>

      # For NUMA node with DPDK nic, number of cores should be used from user input
      # For NUMA node without DPDK nic, number of cores should be 1
      get_num_cores_per_numa_nodes:
        publish:
          num_cores_per_numa_nodes: <% let(dpdk_nics_nodes => $.dpdk_nics_numa_nodes, cores => $.num_phy_cores_per_numa_node_for_pmd) -> $.numa_nodes.select(switch($ in $dpdk_nics_nodes => $cores, not $ in $dpdk_nics_nodes => 1)) %>
        on-success: get_pmd_cpus

      get_pmd_cpus:
        action: tripleo.derive_params.get_dpdk_core_list
        input:
          inspect_data: <% $.hw_data %>
          numa_nodes_cores_count: <% $.num_cores_per_numa_nodes %>
        publish:
          pmd_cpus: <% task().result %>
        on-success:
          - get_pmd_cpus_range_list: <% $.pmd_cpus %>
          - set_status_failed_get_pmd_cpus: <% not $.pmd_cpus %>
        on-error: set_status_failed_on_error_get_pmd_cpus

      get_pmd_cpus_range_list:
        action: tripleo.derive_params.convert_number_to_range_list
        input:
          num_list: <% $.pmd_cpus %>
        publish:
          pmd_cpus: <% task().result %>
        on-success: get_host_cpus
        on-error: set_status_failed_get_pmd_cpus_range_list

      get_host_cpus:
        workflow: tripleo.derive_params_formulas.v1.get_host_cpus
        input:
          role_name: <% $.role_name %>
          hw_data: <% $.hw_data %>
        publish:
          host_cpus: <% task().result.get('host_cpus', '') %>
        on-success: get_sock_mem
        on-error: set_status_failed_get_host_cpus

      get_sock_mem:
        action: tripleo.derive_params.get_dpdk_socket_memory
        input:
          dpdk_nics_numa_info: <% $.dpdk_nics_numa_info %>
          numa_nodes: <% $.numa_nodes %>
          overhead: <% $.user_inputs.get('overhead', 800) %>
          packet_size_in_buffer: <% 4096*64 %>
        publish:
          sock_mem: <% task().result %>
        on-success:
          - get_dpdk_parameters: <% $.sock_mem %>
          - set_status_failed_get_sock_mem: <% not $.sock_mem %>
        on-error: set_status_failed_on_error_get_sock_mem

      get_dpdk_parameters:
        publish:
          dpdk_parameters: <% dict(concat($.role_name, 'Parameters') => dict('OvsPmdCoreList' => $.get('pmd_cpus', ''), 'OvsDpdkCoreList' => $.get('host_cpus', ''), 'OvsDpdkSocketMemory' => $.get('sock_mem', ''))) %>

      set_status_failed_get_network_config:
        publish:
          status: FAILED
          message: <% task(get_network_config).result %>
        on-success: fail

      set_status_failed_get_dpdk_nics_numa_info:
        publish:
          status: FAILED
          message: "Unable to determine DPDK NIC's NUMA information"
        on-success: fail

      set_status_failed_on_error_get_dpdk_nics_numa_info:
        publish:
          status: FAILED
          message: <% task(get_dpdk_nics_numa_info).result %>
        on-success: fail

      set_status_failed_get_dpdk_nics_numa_nodes:
        publish:
          status: FAILED
          message: "Unable to determine DPDK NIC's numa nodes"
        on-success: fail

      set_status_failed_get_numa_nodes:
        publish:
          status: FAILED
          message: 'Unable to determine available NUMA nodes'
        on-success: fail

      set_status_failed_get_num_phy_cores_per_numa_for_pmd_invalid:
        publish:
          status: FAILED
          message: <% "num_phy_cores_per_numa_node_for_pmd user input '{0}' is invalid".format($.num_phy_cores_per_numa_node_for_pmd) %>
        on-success: fail

      set_status_failed_get_num_phy_cores_per_numa_for_pmd_not_provided:
        publish:
          status: FAILED
          message: 'num_phy_cores_per_numa_node_for_pmd user input is not provided'
        on-success: fail

      set_status_failed_get_pmd_cpus:
        publish:
          status: FAILED
          message: 'Unable to determine OvsPmdCoreList parameter'
        on-success: fail

      set_status_failed_on_error_get_pmd_cpus:
        publish:
          status: FAILED
          message: <% task(get_pmd_cpus).result %>
        on-success: fail

      set_status_failed_get_pmd_cpus_range_list:
        publish:
          status: FAILED
          message: <% task(get_pmd_cpus_range_list).result %>
        on-success: fail

      set_status_failed_get_host_cpus:
        publish:
          status: FAILED
          message: <% task(get_host_cpus).result.get('message', '') %>
        on-success: fail

      set_status_failed_get_sock_mem:
        publish:
          status: FAILED
          message: 'Unable to determine OvsDpdkSocketMemory parameter'
        on-success: fail

      set_status_failed_on_error_get_sock_mem:
        publish:
          status: FAILED
          message: <% task(get_sock_mem).result %>
        on-success: fail


  sriov_derive_params:
    description: >
      This workflow derives parameters for the SRIOV feature.

    input:
      - role_name
      - hw_data # introspection data
      - derived_parameters: {}

    output:
      derived_parameters: <% $.derived_parameters.mergeWith($.get('sriov_parameters', {})) %>

    tags:
      - tripleo-common-managed

    tasks:
      get_host_cpus:
        workflow: tripleo.derive_params_formulas.v1.get_host_cpus
        input:
          role_name: <% $.role_name %>
          hw_data: <% $.hw_data %>
        publish:
          host_cpus: <% task().result.get('host_cpus', '') %>
        on-success: get_sriov_parameters
        on-error: set_status_failed_get_host_cpus

      get_sriov_parameters:
        publish:
          # SriovHostCpusList parameter is added temporarily and it's removed later from derived parameters result.
          sriov_parameters: <% dict(concat($.role_name, 'Parameters') => dict('SriovHostCpusList' => $.get('host_cpus', ''))) %>

      set_status_failed_get_host_cpus:
        publish:
          status: FAILED
          message: <% task(get_host_cpus).result.get('message', '') %>
        on-success: fail


  get_host_cpus:
    description: >
      Fetching the host CPU list from the introspection data, and then converting the raw list into a range list.

    input:
      - hw_data # introspection data

    output:
      host_cpus: <% $.get('host_cpus', '') %>

    tags:
      - tripleo-common-managed

    tasks:
      get_host_cpus:
        action: tripleo.derive_params.get_host_cpus_list inspect_data=<% $.hw_data %>
        publish:
          host_cpus: <% task().result  %>
        on-success:
          - get_host_cpus_range_list: <% $.host_cpus %>
          - set_status_failed_get_host_cpus: <% not $.host_cpus %>
        on-error: set_status_failed_on_error_get_host_cpus

      get_host_cpus_range_list:
        action: tripleo.derive_params.convert_number_to_range_list
        input:
          num_list: <% $.host_cpus %>
        publish:
          host_cpus: <% task().result %>
        on-error: set_status_failed_get_host_cpus_range_list

      set_status_failed_get_host_cpus:
        publish:
          status: FAILED
          message: 'Unable to determine host cpus'
        on-success: fail

      set_status_failed_on_error_get_host_cpus:
        publish:
          status: FAILED
          message: <% task(get_host_cpus).result %>
        on-success: fail

      set_status_failed_get_host_cpus_range_list:
        publish:
          status: FAILED
          message: <% task(get_host_cpus_range_list).result %>
        on-success: fail


  host_derive_params:
    description: >
      This workflow derives parameters for the Host process, and is mainly associated with CPU pinning and huge memory pages.
      This workflow can be dependent on any feature or also can be invoked individually as well.

    input:
      - role_name
      - hw_data # introspection data
      - user_inputs
      - derived_parameters: {}

    output:
      derived_parameters: <% $.derived_parameters.mergeWith($.get('host_parameters', {})) %>

    tags:
      - tripleo-common-managed

    tasks:
      get_cpus:
        publish:
          cpus: <% $.hw_data.numa_topology.cpus %>
        on-success:
          - get_role_derive_params: <% $.cpus %>
          - set_status_failed_get_cpus: <% not $.cpus %>

      get_role_derive_params:
        publish:
          role_derive_params: <% $.derived_parameters.get(concat($.role_name, 'Parameters'), {}) %>
          # removing the role parameters (eg. ComputeParameters) in derived_parameters dictionary since already copied in role_derive_params.
          derived_parameters: <% $.derived_parameters.delete(concat($.role_name, 'Parameters')) %>
        on-success: get_host_cpus

      get_host_cpus:
        publish:
          host_cpus: <% $.role_derive_params.get('OvsDpdkCoreList', '') or $.role_derive_params.get('SriovHostCpusList', '') %>
          # SriovHostCpusList parameter is added temporarily for host_cpus and not needed in derived_parameters result.
          # SriovHostCpusList parameter is deleted in derived_parameters list and adding the updated role parameters
          # back in the derived_parameters.
          derived_parameters: <% $.derived_parameters + dict(concat($.role_name, 'Parameters') => $.role_derive_params.delete('SriovHostCpusList')) %>
        on-success: get_host_dpdk_combined_cpus

      get_host_dpdk_combined_cpus:
        publish:
          host_dpdk_combined_cpus: <% let(pmd_cpus => $.role_derive_params.get('OvsPmdCoreList', '')) -> switch($pmd_cpus => concat($pmd_cpus, ',', $.host_cpus), not $pmd_cpus => $.host_cpus) %>
          reserved_cpus: []
        on-success:
          - get_host_dpdk_combined_cpus_num_list: <% $.host_dpdk_combined_cpus %>
          - set_status_failed_get_host_dpdk_combined_cpus: <% not $.host_dpdk_combined_cpus %>

      get_host_dpdk_combined_cpus_num_list:
        action: tripleo.derive_params.convert_range_to_number_list
        input:
          range_list: <% $.host_dpdk_combined_cpus %>
        publish:
          host_dpdk_combined_cpus: <% task().result %>
          reserved_cpus: <% task().result.split(',') %>
        on-success: get_nova_cpus
        on-error: set_status_failed_get_host_dpdk_combined_cpus_num_list

      get_nova_cpus:
        publish:
          nova_cpus: <% let(reserved_cpus => $.reserved_cpus) -> $.cpus.select($.thread_siblings).flatten().where(not (str($) in $reserved_cpus)).join(',') %>
        on-success:
          - get_isol_cpus: <% $.nova_cpus %>
          - set_status_failed_get_nova_cpus: <% not $.nova_cpus %>

      # concatinates OvsPmdCoreList range format and NovaVcpuPinSet in range format. it may not be in perfect range format.
      # example: concatinates '12-15,19' and 16-18' ranges '12-15,19,16-18'
      get_isol_cpus:
        publish:
          isol_cpus: <% let(pmd_cpus => $.role_derive_params.get('OvsPmdCoreList','')) -> switch($pmd_cpus => concat($pmd_cpus, ',', $.nova_cpus), not $pmd_cpus => $.nova_cpus) %>
        on-success: get_isol_cpus_num_list

      # Gets the isol_cpus in the number list
      # example: '12-15,19,16-18' into '12,13,14,15,16,17,18,19'
      get_isol_cpus_num_list:
        action: tripleo.derive_params.convert_range_to_number_list
        input:
          range_list: <% $.isol_cpus %>
        publish:
          isol_cpus: <% task().result %>
        on-success: get_nova_cpus_range_list
        on-error: set_status_failed_get_isol_cpus_num_list

      get_nova_cpus_range_list:
        action: tripleo.derive_params.convert_number_to_range_list
        input:
          num_list: <% $.nova_cpus %>
        publish:
          nova_cpus: <% task().result %>
        on-success: get_isol_cpus_range_list
        on-error: set_status_failed_get_nova_cpus_range_list

      # converts number format isol_cpus into range format
      # example: '12,13,14,15,16,17,18,19' into '12-19'
      get_isol_cpus_range_list:
        action: tripleo.derive_params.convert_number_to_range_list
        input:
          num_list: <% $.isol_cpus %>
        publish:
          isol_cpus: <% task().result %>
        on-success: get_host_mem
        on-error: set_status_failed_get_isol_cpus_range_list

      get_host_mem:
        publish:
          host_mem: <% $.user_inputs.get('host_mem_default', 4096) %>
        on-success: check_default_hugepage_supported

      check_default_hugepage_supported:
        publish:
          default_hugepage_supported: <% $.hw_data.get('inventory', {}).get('cpu', {}).get('flags', []).contains('pdpe1gb') %>
        on-success:
          - get_total_memory: <% $.default_hugepage_supported %>
          - set_status_failed_check_default_hugepage_supported: <% not $.default_hugepage_supported %>

      get_total_memory:
        publish:
          total_memory: <% $.hw_data.get('inventory', {}).get('memory', {}).get('physical_mb', 0) %>
        on-success:
          - get_hugepage_allocation_percentage: <% $.total_memory %>
          - set_status_failed_get_total_memory: <% not $.total_memory %>

      get_hugepage_allocation_percentage:
        publish:
          huge_page_allocation_percentage: <% $.user_inputs.get('huge_page_allocation_percentage', 0) %>
        on-success:
          - get_hugepages: <% isInteger($.huge_page_allocation_percentage) and $.huge_page_allocation_percentage > 0 %>
          - set_status_failed_get_hugepage_allocation_percentage_invalid: <% not isInteger($.huge_page_allocation_percentage) %>
          - set_status_failed_get_hugepage_allocation_percentage_not_provided: <% $.huge_page_allocation_percentage = 0 %>

      get_hugepages:
        publish:
          hugepages: <% let(huge_page_perc => float($.huge_page_allocation_percentage)/100)-> int((($.total_memory/1024)-4) * $huge_page_perc) %>
        on-success:
          - get_cpu_model: <% $.hugepages %>
          - set_status_failed_get_hugepages: <% not $.hugepages %>

      get_cpu_model:
        publish:
          intel_cpu_model: <% $.hw_data.get('inventory', {}).get('cpu', {}).get('model_name', '').startsWith('Intel') %>
        on-success: get_iommu_info

      get_iommu_info:
        publish:
          iommu_info: <% switch($.intel_cpu_model => 'intel_iommu=on iommu=pt', not $.intel_cpu_model => '') %>
        on-success: get_kernel_args

      get_kernel_args:
        publish:
          kernel_args: <% concat('default_hugepagesz=1GB hugepagesz=1G ', 'hugepages=', str($.hugepages), ' ',  $.iommu_info, ' isolcpus=', $.isol_cpus) %>
        on-success: get_host_parameters

      get_host_parameters:
        publish:
          host_parameters: <% dict(concat($.role_name, 'Parameters') => dict('NovaVcpuPinSet' => $.get('nova_cpus', ''), 'NovaReservedHostMemory' => $.get('host_mem', ''), 'KernelArgs' => $.get('kernel_args', ''), 'IsolCpusList' => $.get('isol_cpus', ''))) %>

      set_status_failed_get_cpus:
        publish:
          status: FAILED
          message: "Unable to determine CPU's on NUMA nodes"
        on-success: fail

      set_status_failed_get_host_dpdk_combined_cpus:
        publish:
          status: FAILED
          message: 'Unable to combine host and dpdk cpus list'
        on-success: fail

      set_status_failed_get_host_dpdk_combined_cpus_num_list:
        publish:
          status: FAILED
          message: <% task(get_host_dpdk_combined_cpus_num_list).result %>
        on-success: fail

      set_status_failed_get_nova_cpus:
        publish:
          status: FAILED
          message: 'Unable to determine nova vcpu pin set'
        on-success: fail

      set_status_failed_get_nova_cpus_range_list:
        publish:
          status: FAILED
          message: <% task(get_nova_cpus_range_list).result %>
        on-success: fail

      set_status_failed_get_isol_cpus_num_list:
        publish:
          status: FAILED
          message: <% task(get_isol_cpus_num_list).result %>
        on-success: fail

      set_status_failed_get_isol_cpus_range_list:
        publish:
          status: FAILED
          message: <% task(get_isol_cpus_range_list).result %>
        on-success: fail

      set_status_failed_check_default_hugepage_supported:
        publish:
          status: FAILED
          message: 'default huge page size 1GB is not supported'
        on-success: fail

      set_status_failed_get_total_memory:
        publish:
          status: FAILED
          message: 'Unable to determine total memory'
        on-success: fail

      set_status_failed_get_hugepage_allocation_percentage_invalid:
        publish:
          status: FAILED
          message: <% "huge_page_allocation_percentage user input '{0}' is invalid".format($.huge_page_allocation_percentage) %>
        on-success: fail

      set_status_failed_get_hugepage_allocation_percentage_not_provided:
        publish:
          status: FAILED
          message: 'huge_page_allocation_percentage user input is not provided'
        on-success: fail

      set_status_failed_get_hugepages:
        publish:
          status: FAILED
          message: 'Unable to determine huge pages'
        on-success: fail


  hci_derive_params:
    description: Derive the deployment parameters for HCI
    input:
      - role_name
      - environment_parameters
      - heat_resource_tree
      - introspection_data
      - user_inputs
      - derived_parameters: {}

    output:
      derived_parameters: <% $.derived_parameters.mergeWith($.get('hci_parameters', {})) %>

    tags:
      - tripleo-common-managed

    tasks:
      get_hci_inputs:
        publish:
          hci_profile: <% $.user_inputs.get('hci_profile', '') %>
          hci_profile_config: <% $.user_inputs.get('hci_profile_config', {}) %>
          MB_PER_GB: 1024
        on-success:
          - get_average_guest_memory_size_in_mb: <% $.hci_profile and $.hci_profile_config.get($.hci_profile, {}) %>
          - set_failed_invalid_hci_profile: <% $.hci_profile and not $.hci_profile_config.get($.hci_profile, {}) %>
          # When no hci_profile is specified, the workflow terminates without deriving any HCI parameters.

      get_average_guest_memory_size_in_mb:
        publish:
          average_guest_memory_size_in_mb: <% $.hci_profile_config.get($.hci_profile, {}).get('average_guest_memory_size_in_mb', 0) %>
        on-success:
          - get_average_guest_cpu_utilization_percentage: <% isInteger($.average_guest_memory_size_in_mb) %>
          - set_failed_invalid_average_guest_memory_size_in_mb: <% not isInteger($.average_guest_memory_size_in_mb) %>

      get_average_guest_cpu_utilization_percentage:
        publish:
          average_guest_cpu_utilization_percentage: <% $.hci_profile_config.get($.hci_profile, {}).get('average_guest_cpu_utilization_percentage', 0) %>
        on-success:
          - get_gb_overhead_per_guest: <% isInteger($.average_guest_cpu_utilization_percentage) %>
          - set_failed_invalid_average_guest_cpu_utilization_percentage: <% not isInteger($.average_guest_cpu_utilization_percentage) %>

      get_gb_overhead_per_guest:
        publish:
          gb_overhead_per_guest: <% $.user_inputs.get('gb_overhead_per_guest', 0.5) %>
        on-success:
          - get_gb_per_osd: <% isNumber($.gb_overhead_per_guest) %>
          - set_failed_invalid_gb_overhead_per_guest: <% not isNumber($.gb_overhead_per_guest) %>

      get_gb_per_osd:
        publish:
          gb_per_osd: <% $.user_inputs.get('gb_per_osd', 5) %>
        on-success:
          - get_cores_per_osd: <% isNumber($.gb_per_osd) %>
          - set_failed_invalid_gb_per_osd: <% not isNumber($.gb_per_osd) %>

      get_cores_per_osd:
        publish:
          cores_per_osd: <% $.user_inputs.get('cores_per_osd', 1.0) %>
        on-success:
          - get_extra_configs: <% isNumber($.cores_per_osd) %>
          - set_failed_invalid_cores_per_osd: <% not isNumber($.cores_per_osd) %>

      get_extra_configs:
        publish:
          extra_config: <% $.environment_parameters.get('ExtraConfig', {}) %>
          role_extra_config: <% $.environment_parameters.get(concat($.role_name, 'ExtraConfig'), {}) %>
          role_env_params: <% $.environment_parameters.get(concat($.role_name, 'Parameters'), {}) %>
          role_derive_params: <% $.derived_parameters.get(concat($.role_name, 'Parameters'), {}) %>
        on-success: get_num_osds

      get_num_osds:
        publish:
          num_osds: <% $.heat_resource_tree.parameters.get('CephAnsibleDisksConfig', {}).get('default', {}).get('devices', []).count() %>
        on-success:
          - get_memory_mb: <% $.num_osds %>
          # If there's no CephAnsibleDisksConfig then look for OSD configuration in hiera data
          - get_num_osds_from_hiera: <% not $.num_osds %>

      get_num_osds_from_hiera:
        publish:
          num_osds: <% $.role_extra_config.get('ceph::profile::params::osds', $.extra_config.get('ceph::profile::params::osds', {})).keys().count() %>
        on-success:
          - get_memory_mb: <% $.num_osds %>
          - set_failed_no_osds: <% not $.num_osds %>

      get_memory_mb:
        publish:
          memory_mb: <% $.introspection_data.get('memory_mb', 0) %>
        on-success:
          - get_nova_vcpu_pin_set: <% $.memory_mb %>
          - set_failed_get_memory_mb: <% not $.memory_mb %>

      # Determine the number of CPU cores available to Nova and Ceph. If
      # NovaVcpuPinSet is defined then use the number of vCPUs in the set,
      # otherwise use all of the cores identified in the introspection data.

      get_nova_vcpu_pin_set:
        publish:
          # NovaVcpuPinSet can be defined in multiple locations, and it's
          # important to select the value in order of precedence:
          #
          # 1) User specified value for this role
          # 2) User specified default value for all roles
          # 3) Value derived by another derived parameters workflow
          nova_vcpu_pin_set: <% $.role_env_params.get('NovaVcpuPinSet', $.environment_parameters.get('NovaVcpuPinSet', $.role_derive_params.get('NovaVcpuPinSet', ''))) %>
        on-success:
          - get_nova_vcpu_count: <% $.nova_vcpu_pin_set %>
          - get_num_cores: <% not $.nova_vcpu_pin_set %>

      get_nova_vcpu_count:
        action: tripleo.derive_params.convert_range_to_number_list
        input:
          range_list: <% $.nova_vcpu_pin_set %>
        publish:
          num_cores: <% task().result.split(',').count() %>
        on-success: calculate_nova_parameters
        on-error: set_failed_get_nova_vcpu_count

      get_num_cores:
        publish:
          num_cores: <% $.introspection_data.get('cpus', 0) %>
        on-success:
          - calculate_nova_parameters: <% $.num_cores %>
          - set_failed_get_num_cores: <% not $.num_cores %>

      # HCI calculations are broken into multiple steps. This is necessary
      # because variables published by a Mistral task are not available
      # for use by that same task. Variables computed and published in a task
      # are only available in subsequent tasks.
      #
      # The HCI calculations compute two Nova parameters:
      # - reserved_host_memory
      # - cpu_allocation_ratio
      #
      # The reserved_host_memory calculation computes the amount of memory
      # that needs to be reserved for Ceph and the total amount of "guest
      # overhead" memory that is based on the anticipated number of guests.
      # Psuedo-code for the calculation (disregarding MB and GB units) is
      # as follows:
      #
      #   ceph_memory = mem_per_osd * num_osds
      #   nova_memory = total_memory - ceph_memory
      #   num_guests = nova_memory /
      #                (average_guest_memory_size + overhead_per_guest)
      #   reserved_memory = ceph_memory + (num_guests * overhead_per_guest)
      #
      # The cpu_allocation_ratio calculation is similar in that it takes into
      # account the number of cores that must be reserved for Ceph.
      #
      #   ceph_cores = cores_per_osd * num_osds
      #   guest_cores = num_cores - ceph_cores
      #   guest_vcpus = guest_cores / average_guest_utilization
      #   cpu_allocation_ratio = guest_vcpus / num_cores

      calculate_nova_parameters:
        publish:
          avg_guest_util: <% $.average_guest_cpu_utilization_percentage / 100.0 %>
          avg_guest_size_gb: <% $.average_guest_memory_size_in_mb / float($.MB_PER_GB) %>
          memory_gb: <% $.memory_mb / float($.MB_PER_GB) %>
          ceph_mem_gb: <% $.gb_per_osd * $.num_osds %>
          nonceph_cores: <% $.num_cores - int($.cores_per_osd * $.num_osds) %>
        on-success: calc_step_2

      calc_step_2:
        publish:
          num_guests: <% int(($.memory_gb - $.ceph_mem_gb) / ($.avg_guest_size_gb + $.gb_overhead_per_guest)) %>
          guest_vcpus: <% $.nonceph_cores / $.avg_guest_util %>
        on-success: calc_step_3

      calc_step_3:
        publish:
          reserved_host_memory: <% $.MB_PER_GB * int($.ceph_mem_gb + ($.num_guests * $.gb_overhead_per_guest)) %>
          cpu_allocation_ratio: <% $.guest_vcpus / $.num_cores %>
        on-success: validate_results

      validate_results:
        publish:
          # Verify whether HCI is viable:
          # - At least 80% of the memory is reserved for Ceph and guest overhead
          # - At least half of the CPU cores must be available to Nova
          mem_ok: <% $.reserved_host_memory <= ($.memory_mb * 0.8) %>
          cpu_ok: <% $.cpu_allocation_ratio >= 0.5 %>
        on-success:
          - set_failed_insufficient_mem: <% not $.mem_ok %>
          - set_failed_insufficient_cpu: <% not $.cpu_ok %>
          - publish_hci_parameters: <% $.mem_ok and $.cpu_ok %>

      publish_hci_parameters:
        publish:
          # TODO(abishop): Update this when the cpu_allocation_ratio can be set
          # via a THT parameter (no such parameter currently exists). Until a
          # THT parameter exists, use hiera data to set the cpu_allocation_ratio.
          hci_parameters: <% dict(concat($.role_name, 'Parameters') => dict('NovaReservedHostMemory' => $.reserved_host_memory)) + dict(concat($.role_name, 'ExtraConfig') => dict('nova::cpu_allocation_ratio' => $.cpu_allocation_ratio)) %>

      set_failed_invalid_hci_profile:
        publish:
          message: "'<% $.hci_profile %>' is not a valid HCI profile."
        on-success: fail

      set_failed_invalid_average_guest_memory_size_in_mb:
        publish:
          message: "'<% $.average_guest_memory_size_in_mb %>' is not a valid average_guest_memory_size_in_mb value."
        on-success: fail

      set_failed_invalid_gb_overhead_per_guest:
        publish:
          message: "'<% $.gb_overhead_per_guest %>' is not a valid gb_overhead_per_guest value."
        on-success: fail

      set_failed_invalid_gb_per_osd:
        publish:
          message: "'<% $.gb_per_osd %>' is not a valid gb_per_osd value."
        on-success: fail

      set_failed_invalid_cores_per_osd:
        publish:
          message: "'<% $.cores_per_osd %>' is not a valid cores_per_osd value."
        on-success: fail

      set_failed_invalid_average_guest_cpu_utilization_percentage:
        publish:
          message: "'<% $.average_guest_cpu_utilization_percentage %>' is not a valid average_guest_cpu_utilization_percentage value."
        on-success: fail

      set_failed_no_osds:
        publish:
          message: "No Ceph OSDs found in the overcloud definition ('ceph::profile::params::osds')."
        on-success: fail

      set_failed_get_memory_mb:
        publish:
          message: "Unable to determine the amount of physical memory (no 'memory_mb' found in introspection_data)."
        on-success: fail

      set_failed_get_nova_vcpu_count:
        publish:
          message: <% task(get_nova_vcpu_count).result %>
        on-success: fail

      set_failed_get_num_cores:
        publish:
          message: "Unable to determine the number of CPU cores (no 'cpus' found in introspection_data)."
        on-success: fail

      set_failed_insufficient_mem:
        publish:
          message: "<% $.memory_mb %> MB is not enough memory to run hyperconverged."
        on-success: fail

      set_failed_insufficient_cpu:
        publish:
          message: "<% $.num_cores %> CPU cores are not enough to run hyperconverged."
        on-success: fail
'
2018-12-03 06:19:02,315 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 32010
2018-12-03 06:19:02,355 DEBUG: RESP: [201] Content-Length: 32010 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:02 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.derive_params_formulas.v1\ndescription: TripleO Workflows to derive deployment parameters from the introspected data\n\nworkflows:\n\n\n  dpdk_derive_params:\n    description: >\n      Workflow to derive parameters for DPDK service.\n    input:\n      - plan\n      - role_name\n      - hw_data # introspection data\n      - user_inputs\n      - derived_parameters: {}\n\n    output:\n      derived_parameters: <% $.derived_parameters.mergeWith($.get('dpdk_parameters', {})) %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_network_config:\n        action: tripleo.parameters.get_network_config\n        input:\n          container: <% $.plan %>\n          role_name: <% $.role_name %>\n        publish:\n          network_configs: <% task().result.get('network_config', []) %>\n        on-success: get_dpdk_nics_numa_info\n        on-error: set_status_failed_get_network_config\n\n      get_dpdk_nics_numa_info:\n        action: tripleo.derive_params.get_dpdk_nics_numa_info\n        input:\n          network_configs: <% $.network_configs %>\n          inspect_data: <% $.hw_data %>\n        publish:\n          dpdk_nics_numa_info: <% task().result %>\n        on-success:\n          # TODO: Need to remove condtions here\n          # adding condition and throw error in action for empty check\n          - get_dpdk_nics_numa_nodes: <% $.dpdk_nics_numa_info %>\n          - set_status_failed_get_dpdk_nics_numa_info: <% not $.dpdk_nics_numa_info %>\n        on-error: set_status_failed_on_error_get_dpdk_nics_numa_info\n\n      get_dpdk_nics_numa_nodes:\n        publish:\n          dpdk_nics_numa_nodes: <% $.dpdk_nics_numa_info.groupBy($.numa_node).select($[0]).orderBy($) %>\n        on-success:\n          - get_numa_nodes: <% $.dpdk_nics_numa_nodes %>\n          - set_status_failed_get_dpdk_nics_numa_nodes: <% not $.dpdk_nics_numa_nodes %>\n\n      get_numa_nodes:\n        publish:\n          numa_nodes: <% $.hw_data.numa_topology.ram.select($.numa_node).orderBy($) %>\n        on-success:\n          - get_num_phy_cores_per_numa_for_pmd: <% $.numa_nodes %>\n          - set_status_failed_get_numa_nodes: <% not $.numa_nodes %>\n\n      get_num_phy_cores_per_numa_for_pmd:\n        publish:\n          num_phy_cores_per_numa_node_for_pmd: <% $.user_inputs.get('num_phy_cores_per_numa_node_for_pmd', 0) %>\n        on-success:\n          - get_num_cores_per_numa_nodes: <% isInteger($.num_phy_cores_per_numa_node_for_pmd) and $.num_phy_cores_per_numa_node_for_pmd > 0 %>\n          - set_status_failed_get_num_phy_cores_per_numa_for_pmd_invalid: <% not isInteger($.num_phy_cores_per_numa_node_for_pmd) %>\n          - set_status_failed_get_num_phy_cores_per_numa_for_pmd_not_provided: <% $.num_phy_cores_per_numa_node_for_pmd = 0 %>\n\n      # For NUMA node with DPDK nic, number of cores should be used from user input\n      # For NUMA node without DPDK nic, number of cores should be 1\n      get_num_cores_per_numa_nodes:\n        publish:\n          num_cores_per_numa_nodes: <% let(dpdk_nics_nodes => $.dpdk_nics_numa_nodes, cores => $.num_phy_cores_per_numa_node_for_pmd) -> $.numa_nodes.select(switch($ in $dpdk_nics_nodes => $cores, not $ in $dpdk_nics_nodes => 1)) %>\n        on-success: get_pmd_cpus\n\n      get_pmd_cpus:\n        action: tripleo.derive_params.get_dpdk_core_list\n        input:\n          inspect_data: <% $.hw_data %>\n          numa_nodes_cores_count: <% $.num_cores_per_numa_nodes %>\n        publish:\n          pmd_cpus: <% task().result %>\n        on-success:\n          - get_pmd_cpus_range_list: <% $.pmd_cpus %>\n          - set_status_failed_get_pmd_cpus: <% not $.pmd_cpus %>\n        on-error: set_status_failed_on_error_get_pmd_cpus\n\n      get_pmd_cpus_range_list:\n        action: tripleo.derive_params.convert_number_to_range_list\n        input:\n          num_list: <% $.pmd_cpus %>\n        publish:\n          pmd_cpus: <% task().result %>\n        on-success: get_host_cpus\n        on-error: set_status_failed_get_pmd_cpus_range_list\n\n      get_host_cpus:\n        workflow: tripleo.derive_params_formulas.v1.get_host_cpus\n        input:\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n        publish:\n          host_cpus: <% task().result.get('host_cpus', '') %>\n        on-success: get_sock_mem\n        on-error: set_status_failed_get_host_cpus\n\n      get_sock_mem:\n        action: tripleo.derive_params.get_dpdk_socket_memory\n        input:\n          dpdk_nics_numa_info: <% $.dpdk_nics_numa_info %>\n          numa_nodes: <% $.numa_nodes %>\n          overhead: <% $.user_inputs.get('overhead', 800) %>\n          packet_size_in_buffer: <% 4096*64 %>\n        publish:\n          sock_mem: <% task().result %>\n        on-success:\n          - get_dpdk_parameters: <% $.sock_mem %>\n          - set_status_failed_get_sock_mem: <% not $.sock_mem %>\n        on-error: set_status_failed_on_error_get_sock_mem\n\n      get_dpdk_parameters:\n        publish:\n          dpdk_parameters: <% dict(concat($.role_name, 'Parameters') => dict('OvsPmdCoreList' => $.get('pmd_cpus', ''), 'OvsDpdkCoreList' => $.get('host_cpus', ''), 'OvsDpdkSocketMemory' => $.get('sock_mem', ''))) %>\n\n      set_status_failed_get_network_config:\n        publish:\n          status: FAILED\n          message: <% task(get_network_config).result %>\n        on-success: fail\n\n      set_status_failed_get_dpdk_nics_numa_info:\n        publish:\n          status: FAILED\n          message: \"Unable to determine DPDK NIC's NUMA information\"\n        on-success: fail\n\n      set_status_failed_on_error_get_dpdk_nics_numa_info:\n        publish:\n          status: FAILED\n          message: <% task(get_dpdk_nics_numa_info).result %>\n        on-success: fail\n\n      set_status_failed_get_dpdk_nics_numa_nodes:\n        publish:\n          status: FAILED\n          message: \"Unable to determine DPDK NIC's numa nodes\"\n        on-success: fail\n\n      set_status_failed_get_numa_nodes:\n        publish:\n          status: FAILED\n          message: 'Unable to determine available NUMA nodes'\n        on-success: fail\n\n      set_status_failed_get_num_phy_cores_per_numa_for_pmd_invalid:\n        publish:\n          status: FAILED\n          message: <% \"num_phy_cores_per_numa_node_for_pmd user input '{0}' is invalid\".format($.num_phy_cores_per_numa_node_for_pmd) %>\n        on-success: fail\n\n      set_status_failed_get_num_phy_cores_per_numa_for_pmd_not_provided:\n        publish:\n          status: FAILED\n          message: 'num_phy_cores_per_numa_node_for_pmd user input is not provided'\n        on-success: fail\n\n      set_status_failed_get_pmd_cpus:\n        publish:\n          status: FAILED\n          message: 'Unable to determine OvsPmdCoreList parameter'\n        on-success: fail\n\n      set_status_failed_on_error_get_pmd_cpus:\n        publish:\n          status: FAILED\n          message: <% task(get_pmd_cpus).result %>\n        on-success: fail\n\n      set_status_failed_get_pmd_cpus_range_list:\n        publish:\n          status: FAILED\n          message: <% task(get_pmd_cpus_range_list).result %>\n        on-success: fail\n\n      set_status_failed_get_host_cpus:\n        publish:\n          status: FAILED\n          message: <% task(get_host_cpus).result.get('message', '') %>\n        on-success: fail\n\n      set_status_failed_get_sock_mem:\n        publish:\n          status: FAILED\n          message: 'Unable to determine OvsDpdkSocketMemory parameter'\n        on-success: fail\n\n      set_status_failed_on_error_get_sock_mem:\n        publish:\n          status: FAILED\n          message: <% task(get_sock_mem).result %>\n        on-success: fail\n\n\n  sriov_derive_params:\n    description: >\n      This workflow derives parameters for the SRIOV feature.\n\n    input:\n      - role_name\n      - hw_data # introspection data\n      - derived_parameters: {}\n\n    output:\n      derived_parameters: <% $.derived_parameters.mergeWith($.get('sriov_parameters', {})) %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_host_cpus:\n        workflow: tripleo.derive_params_formulas.v1.get_host_cpus\n        input:\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n        publish:\n          host_cpus: <% task().result.get('host_cpus', '') %>\n        on-success: get_sriov_parameters\n        on-error: set_status_failed_get_host_cpus\n\n      get_sriov_parameters:\n        publish:\n          # SriovHostCpusList parameter is added temporarily and it's removed later from derived parameters result.\n          sriov_parameters: <% dict(concat($.role_name, 'Parameters') => dict('SriovHostCpusList' => $.get('host_cpus', ''))) %>\n\n      set_status_failed_get_host_cpus:\n        publish:\n          status: FAILED\n          message: <% task(get_host_cpus).result.get('message', '') %>\n        on-success: fail\n\n\n  get_host_cpus:\n    description: >\n      Fetching the host CPU list from the introspection data, and then converting the raw list into a range list.\n\n    input:\n      - hw_data # introspection data\n\n    output:\n      host_cpus: <% $.get('host_cpus', '') %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_host_cpus:\n        action: tripleo.derive_params.get_host_cpus_list inspect_data=<% $.hw_data %>\n        publish:\n          host_cpus: <% task().result  %>\n        on-success:\n          - get_host_cpus_range_list: <% $.host_cpus %>\n          - set_status_failed_get_host_cpus: <% not $.host_cpus %>\n        on-error: set_status_failed_on_error_get_host_cpus\n\n      get_host_cpus_range_list:\n        action: tripleo.derive_params.convert_number_to_range_list\n        input:\n          num_list: <% $.host_cpus %>\n        publish:\n          host_cpus: <% task().result %>\n        on-error: set_status_failed_get_host_cpus_range_list\n\n      set_status_failed_get_host_cpus:\n        publish:\n          status: FAILED\n          message: 'Unable to determine host cpus'\n        on-success: fail\n\n      set_status_failed_on_error_get_host_cpus:\n        publish:\n          status: FAILED\n          message: <% task(get_host_cpus).result %>\n        on-success: fail\n\n      set_status_failed_get_host_cpus_range_list:\n        publish:\n          status: FAILED\n          message: <% task(get_host_cpus_range_list).result %>\n        on-success: fail\n\n\n  host_derive_params:\n    description: >\n      This workflow derives parameters for the Host process, and is mainly associated with CPU pinning and huge memory pages.\n      This workflow can be dependent on any feature or also can be invoked individually as well.\n\n    input:\n      - role_name\n      - hw_data # introspection data\n      - user_inputs\n      - derived_parameters: {}\n\n    output:\n      derived_parameters: <% $.derived_parameters.mergeWith($.get('host_parameters', {})) %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_cpus:\n        publish:\n          cpus: <% $.hw_data.numa_topology.cpus %>\n        on-success:\n          - get_role_derive_params: <% $.cpus %>\n          - set_status_failed_get_cpus: <% not $.cpus %>\n\n      get_role_derive_params:\n        publish:\n          role_derive_params: <% $.derived_parameters.get(concat($.role_name, 'Parameters'), {}) %>\n          # removing the role parameters (eg. ComputeParameters) in derived_parameters dictionary since already copied in role_derive_params.\n          derived_parameters: <% $.derived_parameters.delete(concat($.role_name, 'Parameters')) %>\n        on-success: get_host_cpus\n\n      get_host_cpus:\n        publish:\n          host_cpus: <% $.role_derive_params.get('OvsDpdkCoreList', '') or $.role_derive_params.get('SriovHostCpusList', '') %>\n          # SriovHostCpusList parameter is added temporarily for host_cpus and not needed in derived_parameters result.\n          # SriovHostCpusList parameter is deleted in derived_parameters list and adding the updated role parameters\n          # back in the derived_parameters.\n          derived_parameters: <% $.derived_parameters + dict(concat($.role_name, 'Parameters') => $.role_derive_params.delete('SriovHostCpusList')) %>\n        on-success: get_host_dpdk_combined_cpus\n\n      get_host_dpdk_combined_cpus:\n        publish:\n          host_dpdk_combined_cpus: <% let(pmd_cpus => $.role_derive_params.get('OvsPmdCoreList', '')) -> switch($pmd_cpus => concat($pmd_cpus, ',', $.host_cpus), not $pmd_cpus => $.host_cpus) %>\n          reserved_cpus: []\n        on-success:\n          - get_host_dpdk_combined_cpus_num_list: <% $.host_dpdk_combined_cpus %>\n          - set_status_failed_get_host_dpdk_combined_cpus: <% not $.host_dpdk_combined_cpus %>\n\n      get_host_dpdk_combined_cpus_num_list:\n        action: tripleo.derive_params.convert_range_to_number_list\n        input:\n          range_list: <% $.host_dpdk_combined_cpus %>\n        publish:\n          host_dpdk_combined_cpus: <% task().result %>\n          reserved_cpus: <% task().result.split(',') %>\n        on-success: get_nova_cpus\n        on-error: set_status_failed_get_host_dpdk_combined_cpus_num_list\n\n      get_nova_cpus:\n        publish:\n          nova_cpus: <% let(reserved_cpus => $.reserved_cpus) -> $.cpus.select($.thread_siblings).flatten().where(not (str($) in $reserved_cpus)).join(',') %>\n        on-success:\n          - get_isol_cpus: <% $.nova_cpus %>\n          - set_status_failed_get_nova_cpus: <% not $.nova_cpus %>\n\n      # concatinates OvsPmdCoreList range format and NovaVcpuPinSet in range format. it may not be in perfect range format.\n      # example: concatinates '12-15,19' and 16-18' ranges '12-15,19,16-18'\n      get_isol_cpus:\n        publish:\n          isol_cpus: <% let(pmd_cpus => $.role_derive_params.get('OvsPmdCoreList','')) -> switch($pmd_cpus => concat($pmd_cpus, ',', $.nova_cpus), not $pmd_cpus => $.nova_cpus) %>\n        on-success: get_isol_cpus_num_list\n\n      # Gets the isol_cpus in the number list\n      # example: '12-15,19,16-18' into '12,13,14,15,16,17,18,19'\n      get_isol_cpus_num_list:\n        action: tripleo.derive_params.convert_range_to_number_list\n        input:\n          range_list: <% $.isol_cpus %>\n        publish:\n          isol_cpus: <% task().result %>\n        on-success: get_nova_cpus_range_list\n        on-error: set_status_failed_get_isol_cpus_num_list\n\n      get_nova_cpus_range_list:\n        action: tripleo.derive_params.convert_number_to_range_list\n        input:\n          num_list: <% $.nova_cpus %>\n        publish:\n          nova_cpus: <% task().result %>\n        on-success: get_isol_cpus_range_list\n        on-error: set_status_failed_get_nova_cpus_range_list\n\n      # converts number format isol_cpus into range format\n      # example: '12,13,14,15,16,17,18,19' into '12-19'\n      get_isol_cpus_range_list:\n        action: tripleo.derive_params.convert_number_to_range_list\n        input:\n          num_list: <% $.isol_cpus %>\n        publish:\n          isol_cpus: <% task().result %>\n        on-success: get_host_mem\n        on-error: set_status_failed_get_isol_cpus_range_list\n\n      get_host_mem:\n        publish:\n          host_mem: <% $.user_inputs.get('host_mem_default', 4096) %>\n        on-success: check_default_hugepage_supported\n\n      check_default_hugepage_supported:\n        publish:\n          default_hugepage_supported: <% $.hw_data.get('inventory', {}).get('cpu', {}).get('flags', []).contains('pdpe1gb') %>\n        on-success:\n          - get_total_memory: <% $.default_hugepage_supported %>\n          - set_status_failed_check_default_hugepage_supported: <% not $.default_hugepage_supported %>\n\n      get_total_memory:\n        publish:\n          total_memory: <% $.hw_data.get('inventory', {}).get('memory', {}).get('physical_mb', 0) %>\n        on-success:\n          - get_hugepage_allocation_percentage: <% $.total_memory %>\n          - set_status_failed_get_total_memory: <% not $.total_memory %>\n\n      get_hugepage_allocation_percentage:\n        publish:\n          huge_page_allocation_percentage: <% $.user_inputs.get('huge_page_allocation_percentage', 0) %>\n        on-success:\n          - get_hugepages: <% isInteger($.huge_page_allocation_percentage) and $.huge_page_allocation_percentage > 0 %>\n          - set_status_failed_get_hugepage_allocation_percentage_invalid: <% not isInteger($.huge_page_allocation_percentage) %>\n          - set_status_failed_get_hugepage_allocation_percentage_not_provided: <% $.huge_page_allocation_percentage = 0 %>\n\n      get_hugepages:\n        publish:\n          hugepages: <% let(huge_page_perc => float($.huge_page_allocation_percentage)/100)-> int((($.total_memory/1024)-4) * $huge_page_perc) %>\n        on-success:\n          - get_cpu_model: <% $.hugepages %>\n          - set_status_failed_get_hugepages: <% not $.hugepages %>\n\n      get_cpu_model:\n        publish:\n          intel_cpu_model: <% $.hw_data.get('inventory', {}).get('cpu', {}).get('model_name', '').startsWith('Intel') %>\n        on-success: get_iommu_info\n\n      get_iommu_info:\n        publish:\n          iommu_info: <% switch($.intel_cpu_model => 'intel_iommu=on iommu=pt', not $.intel_cpu_model => '') %>\n        on-success: get_kernel_args\n\n      get_kernel_args:\n        publish:\n          kernel_args: <% concat('default_hugepagesz=1GB hugepagesz=1G ', 'hugepages=', str($.hugepages), ' ',  $.iommu_info, ' isolcpus=', $.isol_cpus) %>\n        on-success: get_host_parameters\n\n      get_host_parameters:\n        publish:\n          host_parameters: <% dict(concat($.role_name, 'Parameters') => dict('NovaVcpuPinSet' => $.get('nova_cpus', ''), 'NovaReservedHostMemory' => $.get('host_mem', ''), 'KernelArgs' => $.get('kernel_args', ''), 'IsolCpusList' => $.get('isol_cpus', ''))) %>\n\n      set_status_failed_get_cpus:\n        publish:\n          status: FAILED\n          message: \"Unable to determine CPU's on NUMA nodes\"\n        on-success: fail\n\n      set_status_failed_get_host_dpdk_combined_cpus:\n        publish:\n          status: FAILED\n          message: 'Unable to combine host and dpdk cpus list'\n        on-success: fail\n\n      set_status_failed_get_host_dpdk_combined_cpus_num_list:\n        publish:\n          status: FAILED\n          message: <% task(get_host_dpdk_combined_cpus_num_list).result %>\n        on-success: fail\n\n      set_status_failed_get_nova_cpus:\n        publish:\n          status: FAILED\n          message: 'Unable to determine nova vcpu pin set'\n        on-success: fail\n\n      set_status_failed_get_nova_cpus_range_list:\n        publish:\n          status: FAILED\n          message: <% task(get_nova_cpus_range_list).result %>\n        on-success: fail\n\n      set_status_failed_get_isol_cpus_num_list:\n        publish:\n          status: FAILED\n          message: <% task(get_isol_cpus_num_list).result %>\n        on-success: fail\n\n      set_status_failed_get_isol_cpus_range_list:\n        publish:\n          status: FAILED\n          message: <% task(get_isol_cpus_range_list).result %>\n        on-success: fail\n\n      set_status_failed_check_default_hugepage_supported:\n        publish:\n          status: FAILED\n          message: 'default huge page size 1GB is not supported'\n        on-success: fail\n\n      set_status_failed_get_total_memory:\n        publish:\n          status: FAILED\n          message: 'Unable to determine total memory'\n        on-success: fail\n\n      set_status_failed_get_hugepage_allocation_percentage_invalid:\n        publish:\n          status: FAILED\n          message: <% \"huge_page_allocation_percentage user input '{0}' is invalid\".format($.huge_page_allocation_percentage) %>\n        on-success: fail\n\n      set_status_failed_get_hugepage_allocation_percentage_not_provided:\n        publish:\n          status: FAILED\n          message: 'huge_page_allocation_percentage user input is not provided'\n        on-success: fail\n\n      set_status_failed_get_hugepages:\n        publish:\n          status: FAILED\n          message: 'Unable to determine huge pages'\n        on-success: fail\n\n\n  hci_derive_params:\n    description: Derive the deployment parameters for HCI\n    input:\n      - role_name\n      - environment_parameters\n      - heat_resource_tree\n      - introspection_data\n      - user_inputs\n      - derived_parameters: {}\n\n    output:\n      derived_parameters: <% $.derived_parameters.mergeWith($.get('hci_parameters', {})) %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_hci_inputs:\n        publish:\n          hci_profile: <% $.user_inputs.get('hci_profile', '') %>\n          hci_profile_config: <% $.user_inputs.get('hci_profile_config', {}) %>\n          MB_PER_GB: 1024\n        on-success:\n          - get_average_guest_memory_size_in_mb: <% $.hci_profile and $.hci_profile_config.get($.hci_profile, {}) %>\n          - set_failed_invalid_hci_profile: <% $.hci_profile and not $.hci_profile_config.get($.hci_profile, {}) %>\n          # When no hci_profile is specified, the workflow terminates without deriving any HCI parameters.\n\n      get_average_guest_memory_size_in_mb:\n        publish:\n          average_guest_memory_size_in_mb: <% $.hci_profile_config.get($.hci_profile, {}).get('average_guest_memory_size_in_mb', 0) %>\n        on-success:\n          - get_average_guest_cpu_utilization_percentage: <% isInteger($.average_guest_memory_size_in_mb) %>\n          - set_failed_invalid_average_guest_memory_size_in_mb: <% not isInteger($.average_guest_memory_size_in_mb) %>\n\n      get_average_guest_cpu_utilization_percentage:\n        publish:\n          average_guest_cpu_utilization_percentage: <% $.hci_profile_config.get($.hci_profile, {}).get('average_guest_cpu_utilization_percentage', 0) %>\n        on-success:\n          - get_gb_overhead_per_guest: <% isInteger($.average_guest_cpu_utilization_percentage) %>\n          - set_failed_invalid_average_guest_cpu_utilization_percentage: <% not isInteger($.average_guest_cpu_utilization_percentage) %>\n\n      get_gb_overhead_per_guest:\n        publish:\n          gb_overhead_per_guest: <% $.user_inputs.get('gb_overhead_per_guest', 0.5) %>\n        on-success:\n          - get_gb_per_osd: <% isNumber($.gb_overhead_per_guest) %>\n          - set_failed_invalid_gb_overhead_per_guest: <% not isNumber($.gb_overhead_per_guest) %>\n\n      get_gb_per_osd:\n        publish:\n          gb_per_osd: <% $.user_inputs.get('gb_per_osd', 5) %>\n        on-success:\n          - get_cores_per_osd: <% isNumber($.gb_per_osd) %>\n          - set_failed_invalid_gb_per_osd: <% not isNumber($.gb_per_osd) %>\n\n      get_cores_per_osd:\n        publish:\n          cores_per_osd: <% $.user_inputs.get('cores_per_osd', 1.0) %>\n        on-success:\n          - get_extra_configs: <% isNumber($.cores_per_osd) %>\n          - set_failed_invalid_cores_per_osd: <% not isNumber($.cores_per_osd) %>\n\n      get_extra_configs:\n        publish:\n          extra_config: <% $.environment_parameters.get('ExtraConfig', {}) %>\n          role_extra_config: <% $.environment_parameters.get(concat($.role_name, 'ExtraConfig'), {}) %>\n          role_env_params: <% $.environment_parameters.get(concat($.role_name, 'Parameters'), {}) %>\n          role_derive_params: <% $.derived_parameters.get(concat($.role_name, 'Parameters'), {}) %>\n        on-success: get_num_osds\n\n      get_num_osds:\n        publish:\n          num_osds: <% $.heat_resource_tree.parameters.get('CephAnsibleDisksConfig', {}).get('default', {}).get('devices', []).count() %>\n        on-success:\n          - get_memory_mb: <% $.num_osds %>\n          # If there's no CephAnsibleDisksConfig then look for OSD configuration in hiera data\n          - get_num_osds_from_hiera: <% not $.num_osds %>\n\n      get_num_osds_from_hiera:\n        publish:\n          num_osds: <% $.role_extra_config.get('ceph::profile::params::osds', $.extra_config.get('ceph::profile::params::osds', {})).keys().count() %>\n        on-success:\n          - get_memory_mb: <% $.num_osds %>\n          - set_failed_no_osds: <% not $.num_osds %>\n\n      get_memory_mb:\n        publish:\n          memory_mb: <% $.introspection_data.get('memory_mb', 0) %>\n        on-success:\n          - get_nova_vcpu_pin_set: <% $.memory_mb %>\n          - set_failed_get_memory_mb: <% not $.memory_mb %>\n\n      # Determine the number of CPU cores available to Nova and Ceph. If\n      # NovaVcpuPinSet is defined then use the number of vCPUs in the set,\n      # otherwise use all of the cores identified in the introspection data.\n\n      get_nova_vcpu_pin_set:\n        publish:\n          # NovaVcpuPinSet can be defined in multiple locations, and it's\n          # important to select the value in order of precedence:\n          #\n          # 1) User specified value for this role\n          # 2) User specified default value for all roles\n          # 3) Value derived by another derived parameters workflow\n          nova_vcpu_pin_set: <% $.role_env_params.get('NovaVcpuPinSet', $.environment_parameters.get('NovaVcpuPinSet', $.role_derive_params.get('NovaVcpuPinSet', ''))) %>\n        on-success:\n          - get_nova_vcpu_count: <% $.nova_vcpu_pin_set %>\n          - get_num_cores: <% not $.nova_vcpu_pin_set %>\n\n      get_nova_vcpu_count:\n        action: tripleo.derive_params.convert_range_to_number_list\n        input:\n          range_list: <% $.nova_vcpu_pin_set %>\n        publish:\n          num_cores: <% task().result.split(',').count() %>\n        on-success: calculate_nova_parameters\n        on-error: set_failed_get_nova_vcpu_count\n\n      get_num_cores:\n        publish:\n          num_cores: <% $.introspection_data.get('cpus', 0) %>\n        on-success:\n          - calculate_nova_parameters: <% $.num_cores %>\n          - set_failed_get_num_cores: <% not $.num_cores %>\n\n      # HCI calculations are broken into multiple steps. This is necessary\n      # because variables published by a Mistral task are not available\n      # for use by that same task. Variables computed and published in a task\n      # are only available in subsequent tasks.\n      #\n      # The HCI calculations compute two Nova parameters:\n      # - reserved_host_memory\n      # - cpu_allocation_ratio\n      #\n      # The reserved_host_memory calculation computes the amount of memory\n      # that needs to be reserved for Ceph and the total amount of \"guest\n      # overhead\" memory that is based on the anticipated number of guests.\n      # Psuedo-code for the calculation (disregarding MB and GB units) is\n      # as follows:\n      #\n      #   ceph_memory = mem_per_osd * num_osds\n      #   nova_memory = total_memory - ceph_memory\n      #   num_guests = nova_memory /\n      #                (average_guest_memory_size + overhead_per_guest)\n      #   reserved_memory = ceph_memory + (num_guests * overhead_per_guest)\n      #\n      # The cpu_allocation_ratio calculation is similar in that it takes into\n      # account the number of cores that must be reserved for Ceph.\n      #\n      #   ceph_cores = cores_per_osd * num_osds\n      #   guest_cores = num_cores - ceph_cores\n      #   guest_vcpus = guest_cores / average_guest_utilization\n      #   cpu_allocation_ratio = guest_vcpus / num_cores\n\n      calculate_nova_parameters:\n        publish:\n          avg_guest_util: <% $.average_guest_cpu_utilization_percentage / 100.0 %>\n          avg_guest_size_gb: <% $.average_guest_memory_size_in_mb / float($.MB_PER_GB) %>\n          memory_gb: <% $.memory_mb / float($.MB_PER_GB) %>\n          ceph_mem_gb: <% $.gb_per_osd * $.num_osds %>\n          nonceph_cores: <% $.num_cores - int($.cores_per_osd * $.num_osds) %>\n        on-success: calc_step_2\n\n      calc_step_2:\n        publish:\n          num_guests: <% int(($.memory_gb - $.ceph_mem_gb) / ($.avg_guest_size_gb + $.gb_overhead_per_guest)) %>\n          guest_vcpus: <% $.nonceph_cores / $.avg_guest_util %>\n        on-success: calc_step_3\n\n      calc_step_3:\n        publish:\n          reserved_host_memory: <% $.MB_PER_GB * int($.ceph_mem_gb + ($.num_guests * $.gb_overhead_per_guest)) %>\n          cpu_allocation_ratio: <% $.guest_vcpus / $.num_cores %>\n        on-success: validate_results\n\n      validate_results:\n        publish:\n          # Verify whether HCI is viable:\n          # - At least 80% of the memory is reserved for Ceph and guest overhead\n          # - At least half of the CPU cores must be available to Nova\n          mem_ok: <% $.reserved_host_memory <= ($.memory_mb * 0.8) %>\n          cpu_ok: <% $.cpu_allocation_ratio >= 0.5 %>\n        on-success:\n          - set_failed_insufficient_mem: <% not $.mem_ok %>\n          - set_failed_insufficient_cpu: <% not $.cpu_ok %>\n          - publish_hci_parameters: <% $.mem_ok and $.cpu_ok %>\n\n      publish_hci_parameters:\n        publish:\n          # TODO(abishop): Update this when the cpu_allocation_ratio can be set\n          # via a THT parameter (no such parameter currently exists). Until a\n          # THT parameter exists, use hiera data to set the cpu_allocation_ratio.\n          hci_parameters: <% dict(concat($.role_name, 'Parameters') => dict('NovaReservedHostMemory' => $.reserved_host_memory)) + dict(concat($.role_name, 'ExtraConfig') => dict('nova::cpu_allocation_ratio' => $.cpu_allocation_ratio)) %>\n\n      set_failed_invalid_hci_profile:\n        publish:\n          message: \"'<% $.hci_profile %>' is not a valid HCI profile.\"\n        on-success: fail\n\n      set_failed_invalid_average_guest_memory_size_in_mb:\n        publish:\n          message: \"'<% $.average_guest_memory_size_in_mb %>' is not a valid average_guest_memory_size_in_mb value.\"\n        on-success: fail\n\n      set_failed_invalid_gb_overhead_per_guest:\n        publish:\n          message: \"'<% $.gb_overhead_per_guest %>' is not a valid gb_overhead_per_guest value.\"\n        on-success: fail\n\n      set_failed_invalid_gb_per_osd:\n        publish:\n          message: \"'<% $.gb_per_osd %>' is not a valid gb_per_osd value.\"\n        on-success: fail\n\n      set_failed_invalid_cores_per_osd:\n        publish:\n          message: \"'<% $.cores_per_osd %>' is not a valid cores_per_osd value.\"\n        on-success: fail\n\n      set_failed_invalid_average_guest_cpu_utilization_percentage:\n        publish:\n          message: \"'<% $.average_guest_cpu_utilization_percentage %>' is not a valid average_guest_cpu_utilization_percentage value.\"\n        on-success: fail\n\n      set_failed_no_osds:\n        publish:\n          message: \"No Ceph OSDs found in the overcloud definition ('ceph::profile::params::osds').\"\n        on-success: fail\n\n      set_failed_get_memory_mb:\n        publish:\n          message: \"Unable to determine the amount of physical memory (no 'memory_mb' found in introspection_data).\"\n        on-success: fail\n\n      set_failed_get_nova_vcpu_count:\n        publish:\n          message: <% task(get_nova_vcpu_count).result %>\n        on-success: fail\n\n      set_failed_get_num_cores:\n        publish:\n          message: \"Unable to determine the number of CPU cores (no 'cpus' found in introspection_data).\"\n        on-success: fail\n\n      set_failed_insufficient_mem:\n        publish:\n          message: \"<% $.memory_mb %> MB is not enough memory to run hyperconverged.\"\n        on-success: fail\n\n      set_failed_insufficient_cpu:\n        publish:\n          message: \"<% $.num_cores %> CPU cores are not enough to run hyperconverged.\"\n        on-success: fail\n", "name": "tripleo.derive_params_formulas.v1", "tags": [], "created_at": "2018-12-03 11:19:02", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "bc5dc13a-902d-4396-bd53-ccc503597905"}

2018-12-03 06:19:02,356 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:19:02,357 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.fernet_keys.v1
description: TripleO fernet key rotation workflows

workflows:

  rotate_fernet_keys:

    input:
      - container
      - queue_name: tripleo
      - ansible_extra_env_variables:
            ANSIBLE_HOST_KEY_CHECKING: 'False'

    tags:
      - tripleo-common-managed

    tasks:

      rotate_keys:
        action: tripleo.parameters.rotate_fernet_keys container=<% $.container %>
        on-success: deploy_ssh_key
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      deploy_ssh_key:
        workflow: tripleo.validations.v1.copy_ssh_key
        on-success: get_privkey
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      get_privkey:
        action: tripleo.validations.get_privkey
        on-success: deploy_keys
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      deploy_keys:
        action: tripleo.ansible-playbook
        input:
          hosts: keystone
          inventory: /usr/bin/tripleo-ansible-inventory
          ssh_private_key: <% task(get_privkey).result %>
          extra_env_variables: <% $.ansible_extra_env_variables + dict(TRIPLEO_PLAN_NAME=>$.container) %>
          verbosity: 0
          remote_user: heat-admin
          become: true
          extra_vars:
            fernet_keys: <% task(rotate_keys).result %>
          use_openstack_credentials: true
          playbook: /usr/share/tripleo-common/playbooks/rotate-keys.yaml
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task().result %>
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.fernet_keys.v1.rotate_fernet_keys
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-03 06:19:02,623 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 2609
2018-12-03 06:19:02,624 DEBUG: RESP: [201] Content-Length: 2609 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:02 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.fernet_keys.v1\ndescription: TripleO fernet key rotation workflows\n\nworkflows:\n\n  rotate_fernet_keys:\n\n    input:\n      - container\n      - queue_name: tripleo\n      - ansible_extra_env_variables:\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      rotate_keys:\n        action: tripleo.parameters.rotate_fernet_keys container=<% $.container %>\n        on-success: deploy_ssh_key\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      deploy_ssh_key:\n        workflow: tripleo.validations.v1.copy_ssh_key\n        on-success: get_privkey\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_privkey:\n        action: tripleo.validations.get_privkey\n        on-success: deploy_keys\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      deploy_keys:\n        action: tripleo.ansible-playbook\n        input:\n          hosts: keystone\n          inventory: /usr/bin/tripleo-ansible-inventory\n          ssh_private_key: <% task(get_privkey).result %>\n          extra_env_variables: <% $.ansible_extra_env_variables + dict(TRIPLEO_PLAN_NAME=>$.container) %>\n          verbosity: 0\n          remote_user: heat-admin\n          become: true\n          extra_vars:\n            fernet_keys: <% task(rotate_keys).result %>\n          use_openstack_credentials: true\n          playbook: /usr/share/tripleo-common/playbooks/rotate-keys.yaml\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.fernet_keys.v1.rotate_fernet_keys\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.fernet_keys.v1", "tags": [], "created_at": "2018-12-03 11:19:02", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "973e7fcc-ad26-4580-95a3-f70097396243"}

2018-12-03 06:19:02,624 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:19:02,626 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.networks.v1
description: TripleO Overcloud Networks Workflows v1

workflows:

  validate_networks_input:
    description: >
      Validate that required fields are present.

    input:
      - networks
      - queue_name: tripleo

    output:
      result: <% task(validate_network_names).result %>

    tags:
      - tripleo-common-managed

    tasks:
      validate_network_names:
        publish:
          network_name_present: <% $.networks.all($.containsKey('name')) %>
        on-success:
          - set_status_success: <% $.network_name_present = true %>
          - set_status_error: <% $.network_name_present = false %>
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task(validate_network_names).result %>

      set_status_error:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: "One or more entries did not contain the required field 'name'"

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.networks.v1.validate_networks_input
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  update_networks:
    description: >
      Takes data in networks parameter in json format, validates its contents,
      and persists them in network_data.yaml. After successful update,
      templates are regenerated.

    input:
      - container: overcloud
      - networks
      - network_data_file: 'network_data.yaml'
      - queue_name: tripleo

    output:
      network_data: <% $.network_data %>

    tags:
      - tripleo-common-managed

    tasks:
      validate_input:
        description: >
          validate the format of input (input includes required fields for
          each network)
        workflow: validate_networks_input
        input:
          networks: <% $.networks %>
        on-success: validate_network_files
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      validate_network_files:
        description: >
          validate that Network names exist in Swift container
        workflow: tripleo.plan_management.v1.validate_network_files
        input:
          container: <% $.container %>
          network_data: <% $.networks %>
          queue_name: <% $.queue_name %>
        publish:
          network_data: <% task().network_data %>
        on-success: get_available_networks
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      get_available_networks:
        workflow: tripleo.plan_management.v1.list_available_networks
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>
        publish:
          available_networks: <% task().result.available_networks %>
        on-success: get_current_networks
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      get_current_networks:
        workflow: tripleo.plan_management.v1.get_network_data
        input:
          container: <% $.container %>
          network_data_file: <% $.network_data_file %>
          queue_name: <% $.queue_name %>
        publish:
          current_networks: <% task().result.network_data %>
        on-success: update_network_data
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      update_network_data:
        description: >
          Combine (or replace) the network data
        action: tripleo.plan.update_networks
        input:
          networks: <% $.available_networks %>
          current_networks: <% $.current_networks %>
          remove_all: false
        publish:
          new_network_data: <% task().result.network_data %>
        on-success: update_network_data_in_swift
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      update_network_data_in_swift:
        description: >
          update network_data.yaml object in Swift with data from workflow input
        action: swift.put_object
        input:
          container: <% $.container %>
          obj: <% $.network_data_file %>
          contents: <% yaml_dump($.new_network_data) %>
        on-success: regenerate_templates
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      regenerate_templates:
        action: tripleo.templates.process container=<% $.container %>
        on-success: get_networks
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      get_networks:
        description: >
          run GetNetworksAction to get updated contents of network_data.yaml and
          provide it as output
        workflow: tripleo.plan_management.v1.get_network_data
        input:
          container: <% $.container %>
          network_data_file: <% $.network_data_file %>
          queue_name: <% $.queue_name %>
        publish:
          network_data: <% task().network_data %>
        on-success: set_status_success
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task(get_networks).result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.networks.v1.update_networks
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-03 06:19:03,290 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 6800
2018-12-03 06:19:03,292 DEBUG: RESP: [201] Content-Length: 6800 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:03 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.networks.v1\ndescription: TripleO Overcloud Networks Workflows v1\n\nworkflows:\n\n  validate_networks_input:\n    description: >\n      Validate that required fields are present.\n\n    input:\n      - networks\n      - queue_name: tripleo\n\n    output:\n      result: <% task(validate_network_names).result %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      validate_network_names:\n        publish:\n          network_name_present: <% $.networks.all($.containsKey('name')) %>\n        on-success:\n          - set_status_success: <% $.network_name_present = true %>\n          - set_status_error: <% $.network_name_present = false %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(validate_network_names).result %>\n\n      set_status_error:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: \"One or more entries did not contain the required field 'name'\"\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.networks.v1.validate_networks_input\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_networks:\n    description: >\n      Takes data in networks parameter in json format, validates its contents,\n      and persists them in network_data.yaml. After successful update,\n      templates are regenerated.\n\n    input:\n      - container: overcloud\n      - networks\n      - network_data_file: 'network_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      validate_input:\n        description: >\n          validate the format of input (input includes required fields for\n          each network)\n        workflow: validate_networks_input\n        input:\n          networks: <% $.networks %>\n        on-success: validate_network_files\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      validate_network_files:\n        description: >\n          validate that Network names exist in Swift container\n        workflow: tripleo.plan_management.v1.validate_network_files\n        input:\n          container: <% $.container %>\n          network_data: <% $.networks %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().network_data %>\n        on-success: get_available_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      get_available_networks:\n        workflow: tripleo.plan_management.v1.list_available_networks\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n        publish:\n          available_networks: <% task().result.available_networks %>\n        on-success: get_current_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      get_current_networks:\n        workflow: tripleo.plan_management.v1.get_network_data\n        input:\n          container: <% $.container %>\n          network_data_file: <% $.network_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          current_networks: <% task().result.network_data %>\n        on-success: update_network_data\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      update_network_data:\n        description: >\n          Combine (or replace) the network data\n        action: tripleo.plan.update_networks\n        input:\n          networks: <% $.available_networks %>\n          current_networks: <% $.current_networks %>\n          remove_all: false\n        publish:\n          new_network_data: <% task().result.network_data %>\n        on-success: update_network_data_in_swift\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      update_network_data_in_swift:\n        description: >\n          update network_data.yaml object in Swift with data from workflow input\n        action: swift.put_object\n        input:\n          container: <% $.container %>\n          obj: <% $.network_data_file %>\n          contents: <% yaml_dump($.new_network_data) %>\n        on-success: regenerate_templates\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      regenerate_templates:\n        action: tripleo.templates.process container=<% $.container %>\n        on-success: get_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      get_networks:\n        description: >\n          run GetNetworksAction to get updated contents of network_data.yaml and\n          provide it as output\n        workflow: tripleo.plan_management.v1.get_network_data\n        input:\n          container: <% $.container %>\n          network_data_file: <% $.network_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().network_data %>\n        on-success: set_status_success\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(get_networks).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.networks.v1.update_networks\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.networks.v1", "tags": [], "created_at": "2018-12-03 11:19:03", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "8d5dca8c-ada5-43df-be70-43e6cdc1cef4"}

2018-12-03 06:19:03,292 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:19:03,293 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.octavia_post.v1
description: TripleO Octavia post deployment Workflows

workflows:

  octavia_post_deploy:
    description: Octavia post deployment
    input:
      - amp_image_name
      - amp_image_filename
      - amp_image_tag
      - amp_ssh_key_name
      - amp_ssh_key_path
      - amp_ssh_key_data
      - auth_username
      - auth_password
      - auth_project_name
      - lb_mgmt_net_name
      - lb_mgmt_subnet_name
      - lb_sec_group_name
      - lb_mgmt_subnet_cidr
      - lb_mgmt_subnet_gateway
      - lb_mgmt_subnet_pool_start
      - lb_mgmt_subnet_pool_end
      - generate_certs
      - octavia_ansible_playbook
      - overcloud_admin
      - ca_cert_path
      - ca_private_key_path
      - ca_passphrase
      - client_cert_path
      - mgmt_port_dev
      - overcloud_password
      - overcloud_project
      - overcloud_pub_auth_uri
      - ansible_extra_env_variables:
          ANSIBLE_HOST_KEY_CHECKING: 'False'
          ANSIBLE_SSH_RETRIES: '3'
    tags:
      - tripleo-common-managed
    tasks:
      get_overcloud_stack_details:
        publish:
          # TODO(beagles), we are making an assumption about the octavia heatlh manager and
          # controller worker needing
          #
          octavia_controller_ips: <% env().get('service_ips', {}).get('octavia_worker_ctlplane_node_ips', []) %>
        on-success: enable_ssh_admin

      enable_ssh_admin:
        workflow: tripleo.access.v1.enable_ssh_admin
        input:
          ssh_servers: <% $.octavia_controller_ips %>
        on-success: get_private_key

      get_private_key:
        action: tripleo.validations.get_privkey
        publish:
          private_key: <% task().result %>
        on-success: make_local_temp_directory

      make_local_temp_directory:
        action: tripleo.files.make_temp_dir
        publish:
          undercloud_local_dir: <% task().result.path %>
        on-success: make_remote_temp_directory

      make_remote_temp_directory:
        action: tripleo.files.make_temp_dir
        publish:
          undercloud_remote_dir: <% task().result.path %>
        on-success: build_local_connection_environment_vars

      build_local_connection_environment_vars:
        publish:
          ansible_local_connection_variables: <% dict('ANSIBLE_REMOTE_TEMP' => $.undercloud_remote_dir, 'ANSIBLE_LOCAL_TEMP' => $.undercloud_local_dir) + $.ansible_extra_env_variables %>
        on-success: upload_amphora

      upload_amphora:
        action: tripleo.ansible-playbook
        input:
          inventory:
            undercloud:
              hosts:
                localhost:
                  ansible_connection: local

          playbook: <% $.octavia_ansible_playbook %>
          remote_user: stack
          extra_env_variables: <% $.ansible_local_connection_variables %>
          extra_vars:
            os_password: <% $.overcloud_password %>
            os_username: <% $.overcloud_admin %>
            os_project_name: <% $.overcloud_project %>
            os_auth_url: <% $.overcloud_pub_auth_uri %>
            os_auth_type: "password"
            os_identity_api_version: "3"
            amp_image_name: <% $.amp_image_name %>
            amp_image_filename: <% $.amp_image_filename %>
            amp_image_tag: <% $.amp_image_tag %>
            amp_ssh_key_name: <% $.amp_ssh_key_name %>
            amp_ssh_key_path: <% $.amp_ssh_key_path %>
            amp_ssh_key_data: <% $.amp_ssh_key_data %>
            auth_username: <% $.auth_username %>
            auth_password: <% $.auth_password %>
            auth_project_name: <% $.auth_project_name %>
        on-success: config_octavia

      config_octavia:
        action: tripleo.ansible-playbook
        input:
          inventory:
            octavia_nodes:
              hosts: <% $.octavia_controller_ips.toDict($, {}) %>
          verbosity: 0
          playbook: <% $.octavia_ansible_playbook %>
          remote_user: tripleo-admin
          become: true
          become_user: root
          ssh_private_key: <% $.private_key %>
          ssh_common_args: '-o StrictHostKeyChecking=no'
          ssh_extra_args: '-o UserKnownHostsFile=/dev/null'
          extra_env_variables: <% $.ansible_extra_env_variables %>
          extra_vars:
            os_password: <% $.overcloud_password %>
            os_username: <% $.overcloud_admin %>
            os_project_name: <% $.overcloud_project %>
            os_auth_url: <% $.overcloud_pub_auth_uri %>
            os_auth_type: "password"
            os_identity_api_version: "3"
            amp_image_tag: <% $.amp_image_tag %>
            lb_mgmt_net_name: <% $.lb_mgmt_net_name %>
            lb_mgmt_subnet_name: <% $.lb_mgmt_subnet_name %>
            lb_sec_group_name: <% $.lb_sec_group_name %>
            lb_mgmt_subnet_cidr: <% $.lb_mgmt_subnet_cidr %>
            lb_mgmt_subnet_gateway: <% $.lb_mgmt_subnet_gateway %>
            lb_mgmt_subnet_pool_start: <% $.lb_mgmt_subnet_pool_start %>
            lb_mgmt_subnet_pool_end: <% $.lb_mgmt_subnet_pool_end %>
            ca_cert_path: <% $.ca_cert_path %>
            ca_private_key_path: <% $.ca_private_key_path %>
            ca_passphrase: <% $.ca_passphrase %>
            client_cert_path: <% $.client_cert_path %>
            generate_certs: <% $.generate_certs %>
            mgmt_port_dev: <% $.mgmt_port_dev %>
            auth_project_name: <% $.auth_project_name %>
        on-complete: purge_local_temp_dir
      purge_local_temp_dir:
        action: tripleo.files.remove_temp_dir path=<% $.undercloud_local_dir %>
        on-complete: purge_remote_temp_dir
      purge_remote_temp_dir:
        action: tripleo.files.remove_temp_dir path=<% $.undercloud_remote_dir %>

'
2018-12-03 06:19:03,704 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 6113
2018-12-03 06:19:03,706 DEBUG: RESP: [201] Content-Length: 6113 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:03 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.octavia_post.v1\ndescription: TripleO Octavia post deployment Workflows\n\nworkflows:\n\n  octavia_post_deploy:\n    description: Octavia post deployment\n    input:\n      - amp_image_name\n      - amp_image_filename\n      - amp_image_tag\n      - amp_ssh_key_name\n      - amp_ssh_key_path\n      - amp_ssh_key_data\n      - auth_username\n      - auth_password\n      - auth_project_name\n      - lb_mgmt_net_name\n      - lb_mgmt_subnet_name\n      - lb_sec_group_name\n      - lb_mgmt_subnet_cidr\n      - lb_mgmt_subnet_gateway\n      - lb_mgmt_subnet_pool_start\n      - lb_mgmt_subnet_pool_end\n      - generate_certs\n      - octavia_ansible_playbook\n      - overcloud_admin\n      - ca_cert_path\n      - ca_private_key_path\n      - ca_passphrase\n      - client_cert_path\n      - mgmt_port_dev\n      - overcloud_password\n      - overcloud_project\n      - overcloud_pub_auth_uri\n      - ansible_extra_env_variables:\n          ANSIBLE_HOST_KEY_CHECKING: 'False'\n          ANSIBLE_SSH_RETRIES: '3'\n    tags:\n      - tripleo-common-managed\n    tasks:\n      get_overcloud_stack_details:\n        publish:\n          # TODO(beagles), we are making an assumption about the octavia heatlh manager and\n          # controller worker needing\n          #\n          octavia_controller_ips: <% env().get('service_ips', {}).get('octavia_worker_ctlplane_node_ips', []) %>\n        on-success: enable_ssh_admin\n\n      enable_ssh_admin:\n        workflow: tripleo.access.v1.enable_ssh_admin\n        input:\n          ssh_servers: <% $.octavia_controller_ips %>\n        on-success: get_private_key\n\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: make_local_temp_directory\n\n      make_local_temp_directory:\n        action: tripleo.files.make_temp_dir\n        publish:\n          undercloud_local_dir: <% task().result.path %>\n        on-success: make_remote_temp_directory\n\n      make_remote_temp_directory:\n        action: tripleo.files.make_temp_dir\n        publish:\n          undercloud_remote_dir: <% task().result.path %>\n        on-success: build_local_connection_environment_vars\n\n      build_local_connection_environment_vars:\n        publish:\n          ansible_local_connection_variables: <% dict('ANSIBLE_REMOTE_TEMP' => $.undercloud_remote_dir, 'ANSIBLE_LOCAL_TEMP' => $.undercloud_local_dir) + $.ansible_extra_env_variables %>\n        on-success: upload_amphora\n\n      upload_amphora:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            undercloud:\n              hosts:\n                localhost:\n                  ansible_connection: local\n\n          playbook: <% $.octavia_ansible_playbook %>\n          remote_user: stack\n          extra_env_variables: <% $.ansible_local_connection_variables %>\n          extra_vars:\n            os_password: <% $.overcloud_password %>\n            os_username: <% $.overcloud_admin %>\n            os_project_name: <% $.overcloud_project %>\n            os_auth_url: <% $.overcloud_pub_auth_uri %>\n            os_auth_type: \"password\"\n            os_identity_api_version: \"3\"\n            amp_image_name: <% $.amp_image_name %>\n            amp_image_filename: <% $.amp_image_filename %>\n            amp_image_tag: <% $.amp_image_tag %>\n            amp_ssh_key_name: <% $.amp_ssh_key_name %>\n            amp_ssh_key_path: <% $.amp_ssh_key_path %>\n            amp_ssh_key_data: <% $.amp_ssh_key_data %>\n            auth_username: <% $.auth_username %>\n            auth_password: <% $.auth_password %>\n            auth_project_name: <% $.auth_project_name %>\n        on-success: config_octavia\n\n      config_octavia:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            octavia_nodes:\n              hosts: <% $.octavia_controller_ips.toDict($, {}) %>\n          verbosity: 0\n          playbook: <% $.octavia_ansible_playbook %>\n          remote_user: tripleo-admin\n          become: true\n          become_user: root\n          ssh_private_key: <% $.private_key %>\n          ssh_common_args: '-o StrictHostKeyChecking=no'\n          ssh_extra_args: '-o UserKnownHostsFile=/dev/null'\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          extra_vars:\n            os_password: <% $.overcloud_password %>\n            os_username: <% $.overcloud_admin %>\n            os_project_name: <% $.overcloud_project %>\n            os_auth_url: <% $.overcloud_pub_auth_uri %>\n            os_auth_type: \"password\"\n            os_identity_api_version: \"3\"\n            amp_image_tag: <% $.amp_image_tag %>\n            lb_mgmt_net_name: <% $.lb_mgmt_net_name %>\n            lb_mgmt_subnet_name: <% $.lb_mgmt_subnet_name %>\n            lb_sec_group_name: <% $.lb_sec_group_name %>\n            lb_mgmt_subnet_cidr: <% $.lb_mgmt_subnet_cidr %>\n            lb_mgmt_subnet_gateway: <% $.lb_mgmt_subnet_gateway %>\n            lb_mgmt_subnet_pool_start: <% $.lb_mgmt_subnet_pool_start %>\n            lb_mgmt_subnet_pool_end: <% $.lb_mgmt_subnet_pool_end %>\n            ca_cert_path: <% $.ca_cert_path %>\n            ca_private_key_path: <% $.ca_private_key_path %>\n            ca_passphrase: <% $.ca_passphrase %>\n            client_cert_path: <% $.client_cert_path %>\n            generate_certs: <% $.generate_certs %>\n            mgmt_port_dev: <% $.mgmt_port_dev %>\n            auth_project_name: <% $.auth_project_name %>\n        on-complete: purge_local_temp_dir\n      purge_local_temp_dir:\n        action: tripleo.files.remove_temp_dir path=<% $.undercloud_local_dir %>\n        on-complete: purge_remote_temp_dir\n      purge_remote_temp_dir:\n        action: tripleo.files.remove_temp_dir path=<% $.undercloud_remote_dir %>\n\n", "name": "tripleo.octavia_post.v1", "tags": [], "created_at": "2018-12-03 11:19:03", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "69f3c5a9-bc16-4c51-b834-a466dfc1100a"}

2018-12-03 06:19:03,706 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:19:03,707 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.package_update.v1
description: TripleO update workflows

workflows:

  # Updates a workload cloud stack
  package_update_plan:
    description: Take a container and perform a package update with possible breakpoints

    input:
      - container
      - ceph_ansible_playbook
      - timeout: 240
      - queue_name: tripleo
      - skip_deploy_identifier: False
      - config_dir: '/tmp/'

    tags:
      - tripleo-common-managed

    tasks:
      update:
        action: tripleo.package_update.update_stack
        input:
          timeout: <% $.timeout %>
          container: <% $.container %>
          ceph_ansible_playbook: <% $.ceph_ansible_playbook %>
        on-success: clean_plan
        on-error: set_update_failed

      clean_plan:
        action: tripleo.plan.update_plan_environment
        input:
          container: <% $.container %>
          parameter: CephAnsiblePlaybook
          env_key: parameter_defaults
          delete: true
        on-success: send_message
        on-error: set_update_failed


      set_update_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(update).result %>

      send_message:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.package_update.v1.package_update_plan
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  get_config:
    input:
      - container
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      get_config:
        action: tripleo.config.get_overcloud_config container=<% $.container %>
        publish:
          status: SUCCESS
          message: <% task().result %>
        publish-on-error:
          status: FAILED
          message: Init Minor update failed
        on-complete: send_message

      send_message:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.package_update.v1.package_update_plan
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  update_nodes:
    description: Take a container and perform an update nodes by nodes

    input:
      - node_user: heat-admin
      - nodes
      - playbook
      - inventory_file
      - ansible_queue_name: tripleo
      - module_path: /usr/share/ansible-modules
      - ansible_extra_env_variables:
            ANSIBLE_LOG_PATH: /var/log/mistral/package_update.log
            ANSIBLE_HOST_KEY_CHECKING: 'False'
      - verbosity: 1
      - work_dir: /var/lib/mistral
      - skip_tags: ''

    tags:
      - tripleo-common-managed

    tasks:
      download_config:
        action: tripleo.config.download_config
        input:
          work_dir: <% $.work_dir %>/<% execution().id %>
        on-success: get_private_key
        on-error: node_update_failed

      get_private_key:
        action: tripleo.validations.get_privkey
        publish:
          private_key: <% task().result %>
        on-success: node_update

      node_update:
        action: tripleo.ansible-playbook
        input:
          inventory: <% $.inventory_file %>
          playbook: <% $.work_dir %>/<% execution().id %>/<% $.playbook %>
          remote_user: <% $.node_user %>
          become: true
          become_user: root
          verbosity: <% $.verbosity %>
          ssh_private_key: <% $.private_key %>
          extra_env_variables: <% $.ansible_extra_env_variables %>
          limit_hosts: <% $.nodes %>
          module_path: <% $.module_path %>
          queue_name: <% $.ansible_queue_name %>
          execution_id: <% execution().id %>
          skip_tags: <% $.skip_tags %>
          trash_output: true
        on-success:
          - node_update_passed: <% task().result.returncode = 0 %>
          - node_update_failed: <% task().result.returncode != 0 %>
        on-error: node_update_failed
        publish:
          output: <% task().result %>

      node_update_passed:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: Updated nodes - <% $.nodes %>

      node_update_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: Failed to update nodes - <% $.nodes %>, please see the logs.

      notify_zaqar:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.ansible_queue_name %>
          messages:
            body:
              type: tripleo.package_update.v1.update_nodes
              payload:
                status: <% $.status %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  update_converge_plan:
    description: Take a container and perform the converge for minor update

    input:
      - container
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      remove_noop:
        action: tripleo.plan.remove_noop_deploystep
        input:
          container: <% $.container %>
        on-success: send_message
        on-error: set_update_failed

      set_update_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(remove_noop).result %>

      send_message:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.package_update.v1.update_converge_plan
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  converge_upgrade_plan:
    description: Take a container and perform the converge step of a major upgrade

    input:
      - container
      - timeout: 240
      - queue_name: tripleo
      - skip_deploy_identifier: False

    tags:
      - tripleo-common-managed

    tasks:
      remove_noop:
        action: tripleo.plan.remove_noop_deploystep
        input:
          container: <% $.container %>
        on-success: send_message
        on-error: set_update_failed

      set_update_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(upgrade_converge).result %>

      send_message:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.major_upgrade.v1.converge_upgrade_plan
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  ffwd_upgrade_converge_plan:
    description: ffwd-upgrade converge removes DeploymentSteps no-op from plan

    input:
      - container
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      remove_noop:
        action: tripleo.plan.remove_noop_deploystep
        input:
          container: <% $.container %>
        on-success: send_message
        on-error: set_update_failed

      set_update_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(remove_noop).result %>

      send_message:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.package_update.v1.ffwd_upgrade_converge_plan
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-03 06:19:04,830 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 8860
2018-12-03 06:19:04,832 DEBUG: RESP: [201] Content-Length: 8860 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:04 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.package_update.v1\ndescription: TripleO update workflows\n\nworkflows:\n\n  # Updates a workload cloud stack\n  package_update_plan:\n    description: Take a container and perform a package update with possible breakpoints\n\n    input:\n      - container\n      - ceph_ansible_playbook\n      - timeout: 240\n      - queue_name: tripleo\n      - skip_deploy_identifier: False\n      - config_dir: '/tmp/'\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      update:\n        action: tripleo.package_update.update_stack\n        input:\n          timeout: <% $.timeout %>\n          container: <% $.container %>\n          ceph_ansible_playbook: <% $.ceph_ansible_playbook %>\n        on-success: clean_plan\n        on-error: set_update_failed\n\n      clean_plan:\n        action: tripleo.plan.update_plan_environment\n        input:\n          container: <% $.container %>\n          parameter: CephAnsiblePlaybook\n          env_key: parameter_defaults\n          delete: true\n        on-success: send_message\n        on-error: set_update_failed\n\n\n      set_update_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(update).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.package_update_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  get_config:\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_config:\n        action: tripleo.config.get_overcloud_config container=<% $.container %>\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n        publish-on-error:\n          status: FAILED\n          message: Init Minor update failed\n        on-complete: send_message\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.package_update_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_nodes:\n    description: Take a container and perform an update nodes by nodes\n\n    input:\n      - node_user: heat-admin\n      - nodes\n      - playbook\n      - inventory_file\n      - ansible_queue_name: tripleo\n      - module_path: /usr/share/ansible-modules\n      - ansible_extra_env_variables:\n            ANSIBLE_LOG_PATH: /var/log/mistral/package_update.log\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n      - verbosity: 1\n      - work_dir: /var/lib/mistral\n      - skip_tags: ''\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      download_config:\n        action: tripleo.config.download_config\n        input:\n          work_dir: <% $.work_dir %>/<% execution().id %>\n        on-success: get_private_key\n        on-error: node_update_failed\n\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: node_update\n\n      node_update:\n        action: tripleo.ansible-playbook\n        input:\n          inventory: <% $.inventory_file %>\n          playbook: <% $.work_dir %>/<% execution().id %>/<% $.playbook %>\n          remote_user: <% $.node_user %>\n          become: true\n          become_user: root\n          verbosity: <% $.verbosity %>\n          ssh_private_key: <% $.private_key %>\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          limit_hosts: <% $.nodes %>\n          module_path: <% $.module_path %>\n          queue_name: <% $.ansible_queue_name %>\n          execution_id: <% execution().id %>\n          skip_tags: <% $.skip_tags %>\n          trash_output: true\n        on-success:\n          - node_update_passed: <% task().result.returncode = 0 %>\n          - node_update_failed: <% task().result.returncode != 0 %>\n        on-error: node_update_failed\n        publish:\n          output: <% task().result %>\n\n      node_update_passed:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: Updated nodes - <% $.nodes %>\n\n      node_update_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: Failed to update nodes - <% $.nodes %>, please see the logs.\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.ansible_queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.update_nodes\n              payload:\n                status: <% $.status %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_converge_plan:\n    description: Take a container and perform the converge for minor update\n\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      remove_noop:\n        action: tripleo.plan.remove_noop_deploystep\n        input:\n          container: <% $.container %>\n        on-success: send_message\n        on-error: set_update_failed\n\n      set_update_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(remove_noop).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.update_converge_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  converge_upgrade_plan:\n    description: Take a container and perform the converge step of a major upgrade\n\n    input:\n      - container\n      - timeout: 240\n      - queue_name: tripleo\n      - skip_deploy_identifier: False\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      remove_noop:\n        action: tripleo.plan.remove_noop_deploystep\n        input:\n          container: <% $.container %>\n        on-success: send_message\n        on-error: set_update_failed\n\n      set_update_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(upgrade_converge).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.major_upgrade.v1.converge_upgrade_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  ffwd_upgrade_converge_plan:\n    description: ffwd-upgrade converge removes DeploymentSteps no-op from plan\n\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      remove_noop:\n        action: tripleo.plan.remove_noop_deploystep\n        input:\n          container: <% $.container %>\n        on-success: send_message\n        on-error: set_update_failed\n\n      set_update_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(remove_noop).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.ffwd_upgrade_converge_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.package_update.v1", "tags": [], "created_at": "2018-12-03 11:19:04", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "30274b50-4833-48c3-972a-7520aad2bebf"}

2018-12-03 06:19:04,832 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:19:04,833 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.plan_management.v1
description: TripleO Overcloud Deployment Workflows v1

workflows:

  create_default_deployment_plan:
    description: >
      This workflow exists to maintain backwards compatibility in pike.  This
      workflow will likely be removed in queens in favor of create_deployment_plan.
    input:
      - container
      - queue_name: tripleo
      - generate_passwords: true
    tags:
      - tripleo-common-managed
    tasks:
      call_create_deployment_plan:
        workflow: tripleo.plan_management.v1.create_deployment_plan
        on-success: set_status_success
        on-error: call_create_deployment_plan_set_status_failed
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>
          generate_passwords: <% $.generate_passwords %>
          use_default_templates: true

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task(call_create_deployment_plan).result %>

      call_create_deployment_plan_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(call_create_deployment_plan).result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.create_default_deployment_plan
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  create_deployment_plan:
    description: >
      This workflow provides the capability to create a deployment plan using
      the default heat templates provided in a standard TripleO undercloud
      deployment, heat templates contained in an external git repository, or a
      swift container that already contains templates.
    input:
      - container
      - source_url: null
      - queue_name: tripleo
      - generate_passwords: true
      - use_default_templates: false

    tags:
      - tripleo-common-managed

    tasks:
      container_required_check:
        description: >
          If using the default templates or importing templates from a git
          repository, a new container needs to be created.  If using an existing
          container containing templates, skip straight to create_plan.
        on-success:
          - verify_container_doesnt_exist: <% $.use_default_templates or $.source_url %>
          - create_plan: <% $.use_default_templates = false and $.source_url = null %>

      verify_container_doesnt_exist:
        action: swift.head_container container=<% $.container %>
        on-success: notify_zaqar
        on-error: create_container
        publish:
          status: FAILED
          message: "Unable to create plan. The Swift container already exists"

      create_container:
        action: tripleo.plan.create_container container=<% $.container %>
        on-success: templates_source_check
        on-error: create_container_set_status_failed

      cleanup_temporary_files:
        action: tripleo.git.clean container=<% $.container %>

      templates_source_check:
        on-success:
          - upload_default_templates: <% $.use_default_templates = true %>
          - clone_git_repo: <% $.source_url != null %>

      clone_git_repo:
        action: tripleo.git.clone container=<% $.container %> url=<% $.source_url %>
        on-success: upload_templates_directory
        on-error: clone_git_repo_set_status_failed

      upload_templates_directory:
        action: tripleo.templates.upload container=<% $.container %> templates_path=<% task(clone_git_repo).result %>
        on-success: create_plan
        on-complete: cleanup_temporary_files
        on-error: upload_templates_directory_set_status_failed

      upload_default_templates:
        action: tripleo.templates.upload container=<% $.container %>
        on-success: create_plan
        on-error: upload_to_container_set_status_failed

      create_plan:
        on-success:
          - ensure_passwords_exist: <% $.generate_passwords = true %>
          - add_root_stack_name: <% $.generate_passwords != true %>

      ensure_passwords_exist:
        action: tripleo.parameters.generate_passwords container=<% $.container %>
        on-success: add_root_stack_name
        on-error: ensure_passwords_exist_set_status_failed

      add_root_stack_name:
        action: tripleo.parameters.update
        input:
          container: <% $.container %>
          parameters:
            RootStackName: <% $.container %>
        on-success: container_images_prepare
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      container_images_prepare:
        description: >
          Populate all container image parameters with default values.
        action: tripleo.container_images.prepare container=<% $.container %>
        on-success: process_templates
        on-error: container_images_prepare_set_status_failed

      process_templates:
        action: tripleo.templates.process container=<% $.container %>
        on-success: set_status_success
        on-error: process_templates_set_status_failed

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: 'Plan created.'

      create_container_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(create_container).result %>

      clone_git_repo_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(clone_git_repo).result %>

      upload_templates_directory_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(upload_templates_directory).result %>

      upload_to_container_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(upload_default_templates).result %>

      ensure_passwords_exist_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(ensure_passwords_exist).result %>

      process_templates_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(process_templates).result %>

      container_images_prepare_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(container_images_prepare).result %>

      notify_zaqar:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.create_deployment_plan
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  update_deployment_plan:
    input:
      - container
      - source_url: null
      - queue_name: tripleo
      - generate_passwords: true
      - plan_environment: null
    tags:
      - tripleo-common-managed
    tasks:
      templates_source_check:
        on-success:
          - update_plan: <% $.source_url = null %>
          - clone_git_repo: <% $.source_url != null %>

      clone_git_repo:
        action: tripleo.git.clone container=<% $.container %> url=<% $.source_url %>
        on-success: upload_templates_directory
        on-error: clone_git_repo_set_status_failed

      upload_templates_directory:
        action: tripleo.templates.upload container=<% $.container %> templates_path=<% task(clone_git_repo).result %>
        on-success: create_swift_rings_backup_plan
        on-complete: cleanup_temporary_files
        on-error: upload_templates_directory_set_status_failed

      cleanup_temporary_files:
        action: tripleo.git.clean container=<% $.container %>

      create_swift_rings_backup_plan:
        workflow: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan
        on-success: update_plan
        on-error: create_swift_rings_backup_plan_set_status_failed
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>
          use_default_templates: true

      update_plan:
        on-success:
          - ensure_passwords_exist: <% $.generate_passwords = true %>
          - container_images_prepare: <% $.generate_passwords != true %>

      ensure_passwords_exist:
        action: tripleo.parameters.generate_passwords container=<% $.container %>
        on-success: container_images_prepare
        on-error: ensure_passwords_exist_set_status_failed

      container_images_prepare:
        description: >
          Populate all container image parameters with default values.
        action: tripleo.container_images.prepare container=<% $.container %>
        on-success: process_templates
        on-error: container_images_prepare_set_status_failed

      process_templates:
        action: tripleo.templates.process container=<% $.container %>
        on-success:
          - set_status_success: <% $.plan_environment = null %>
          - upload_plan_environment: <% $.plan_environment != null %>
        on-error: process_templates_set_status_failed

      upload_plan_environment:
        action: tripleo.templates.upload_plan_environment container=<% $.container %> plan_environment=<% $.plan_environment %>
        on-success: set_status_success
        on-error: process_templates_set_status_failed

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: 'Plan updated.'

      create_swift_rings_backup_plan_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(create_swift_rings_backup_plan).result %>

      clone_git_repo_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(clone_git_repo).result %>

      upload_templates_directory_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(upload_templates_directory).result %>

      process_templates_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(process_templates).result %>

      ensure_passwords_exist_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(ensure_passwords_exist).result %>

      container_images_prepare_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(container_images_prepare).result %>

      notify_zaqar:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.update_deployment_plan
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  delete_deployment_plan:
    description: >
      Deletes a plan by deleting the container matching plan_name. It will
      not delete the plan if a stack exists with the same name.

    tags:
      - tripleo-common-managed

    input:
      - container: overcloud
      - queue_name: tripleo

    tasks:
      delete_plan:
        action: tripleo.plan.delete container=<% $.container %>
        on-complete: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        publish:
          status: SUCCESS
          message: <% task().result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.delete_deployment_plan
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>


  get_passwords:
    description: Retrieves passwords for a given plan
    input:
      - container
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      verify_container_exists:
        action: swift.head_container container=<% $.container %>
        on-success: get_environment_passwords
        on-error: verify_container_set_status_failed

      get_environment_passwords:
        action: tripleo.parameters.get_passwords container=<% $.container %>
        on-success: get_passwords_set_status_success
        on-error: get_passwords_set_status_failed

      get_passwords_set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task(get_environment_passwords).result %>

      get_passwords_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(get_environment_passwords).result %>

      verify_container_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(verify_container_exists).result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.get_passwords
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  export_deployment_plan:
    description: Creates an export tarball for a given plan
    input:
      - plan
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      export_plan:
        action: tripleo.plan.export
        input:
          plan: <% $.plan %>
          delete_after: 3600
          exports_container: "plan-exports"
        on-success: create_tempurl
        on-error: export_plan_set_status_failed

      create_tempurl:
        action: tripleo.swift.tempurl
        on-success: set_status_success
        on-error: create_tempurl_set_status_failed
        input:
          container: "plan-exports"
          obj: "<% $.plan %>.tar.gz"
          valid: 3600

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task(create_tempurl).result %>
          tempurl: <% task(create_tempurl).result %>

      export_plan_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(export_plan).result %>

      create_tempurl_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(create_tempurl).result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.export_deployment_plan
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                tempurl: <% $.get('tempurl', '') %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  get_deprecated_parameters:
    description: Gets the list of deprecated parameters in the whole of the plan including nested stack
    input:
      - container: overcloud
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      get_flatten_data:
        action: tripleo.parameters.get_flatten container=<% $.container %>
        on-success: get_deprecated_params
        on-error: set_status_failed_get_flatten_data
        publish:
          user_params: <% task().result.environment_parameters %>
          plan_params: <% task().result.heat_resource_tree.parameters.keys() %>
          parameter_groups: <% task().result.heat_resource_tree.resources.values().where( $.get('parameter_groups') ).select($.parameter_groups).flatten() %>

      get_deprecated_params:
        on-success: check_if_user_param_has_deprecated
        publish:
          deprecated_params: <% $.parameter_groups.where($.get('label') = 'deprecated').select($.parameters).flatten().distinct() %>

      check_if_user_param_has_deprecated:
        on-success: get_unused_params
        publish:
          deprecated_result: <% let(up => $.user_params) -> $.deprecated_params.select( dict('parameter' => $, 'deprecated' => true, 'user_defined' => $up.keys().contains($)) ) %>

      # Get the list of parameters, which are defined by user via environment files's parameter_default, but not part of the plan definition
      # It may be possible that the parameter will be used by a service, but the service is not part of the plan.
      # In such cases, the parameter will be reported as unused, care should be take to understand whether it is really unused or not.
      get_unused_params:
        on-success: send_message
        publish:
          unused_params: <% let(plan_params => $.plan_params) ->  $.user_params.keys().where( not $plan_params.contains($) ) %>

      set_status_failed_get_flatten_data:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_flatten_data).result %>

      send_message:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.get_deprecated_parameters
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                deprecated: <% $.get('deprecated_result', []) %>
                unused: <% $.get('unused_params', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  publish_ui_logs_to_swift:
    description: >
      This workflow drains a zaqar queue, and publish its messages into a log
      file in swift.  This workflow is called by cron trigger.

    input:
      - logging_queue_name: tripleo-ui-logging
      - logging_container: tripleo-ui-logs

    tags:
      - tripleo-common-managed

    tasks:

      # We're using a NoOp action to start the workflow.  The recursive nature
      # of the workflow means that Mistral will refuse to execute it because it
      # doesn't know where to begin.
      start:
        on-success: get_messages

      get_messages:
        action: zaqar.claim_messages
        on-success:
          - format_messages: <% task().result.len() > 0 %>
        input:
          queue_name: <% $.logging_queue_name %>
          ttl: 60
          grace: 60
        publish:
          status: SUCCESS
          messages: <% task().result %>
          message_ids: <% task().result.select($._id) %>

      format_messages:
        action: tripleo.logging_to_swift.format_messages
        on-success: upload_to_swift
        input:
          messages: <% $.messages %>
        publish:
          status: SUCCESS
          formatted_messages: <% task().result %>

      upload_to_swift:
        action: tripleo.logging_to_swift.publish_ui_log_to_swift
        on-success: delete_messages
        input:
          logging_data: <% $.formatted_messages %>
          logging_container: <% $.logging_container %>
        publish:
          status: SUCCESS

      delete_messages:
        action: zaqar.delete_messages
        on-success: get_messages
        input:
          queue_name: <% $.logging_queue_name %>
          messages: <% $.message_ids %>
        publish:
          status: SUCCESS

  download_logs:
    description: Creates a tarball with logging data
    input:
      - queue_name: tripleo
      - logging_container: "tripleo-ui-logs"
      - downloads_container: "tripleo-ui-logs-downloads"
      - delete_after: 3600

    tags:
      - tripleo-common-managed

    tasks:

      publish_logs:
        workflow: tripleo.plan_management.v1.publish_ui_logs_to_swift
        on-success: prepare_log_download
        on-error: publish_logs_set_status_failed

      prepare_log_download:
        action: tripleo.logging_to_swift.prepare_log_download
        input:
          logging_container: <% $.logging_container %>
          downloads_container: <% $.downloads_container %>
          delete_after: <% $.delete_after %>
        on-success: create_tempurl
        on-error: download_logs_set_status_failed
        publish:
          filename: <% task().result %>

      create_tempurl:
        action: tripleo.swift.tempurl
        on-success: set_status_success
        on-error: create_tempurl_set_status_failed
        input:
          container: <% $.downloads_container %>
          obj: <% $.filename %>
          valid: 3600
        publish:
          tempurl: <% task().result %>

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task(create_tempurl).result %>
          tempurl: <% task(create_tempurl).result %>

      publish_logs_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(publish_logs).result %>

      download_logs_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(prepare_log_download).result %>

      create_tempurl_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(create_tempurl).result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.download_logs
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                tempurl: <% $.get('tempurl', '') %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  list_roles:
    description: Retrieve the roles_data.yaml and return a usable object

    input:
      - container: overcloud
      - roles_data_file: 'roles_data.yaml'
      - queue_name: tripleo

    output:
      roles_data: <% $.roles_data %>

    tags:
      - tripleo-common-managed

    tasks:
      get_roles_data:
        action: swift.get_object
        input:
          container: <% $.container %>
          obj: <% $.roles_data_file %>
        publish:
          roles_data: <% yaml_parse(task().result.last()) %>
          status: SUCCESS
        on-success: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.list_roles
              payload:
                status: <% $.status %>
                roles_data: <% $.get('roles_data', {}) %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  list_available_networks:
    input:
      - container
      - queue_name: tripleo

    output:
      available_networks: <% $.available_networks %>

    tags:
      - tripleo-common-managed

    tasks:
      get_network_file_names:
        action: swift.get_container
        input:
          container: <% $.container %>
        publish:
          network_names: <% task().result[1].where($.name.startsWith('networks/')).where($.name.endsWith('.yaml')).name %>
        on-success: get_network_files
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      get_network_files:
        with-items: network_name in <% $.network_names %>
        action: swift.get_object
        on-success: transform_output
        on-error: notify_zaqar
        input:
          container: <% $.container %>
          obj: <% $.network_name %>
        publish:
          status: SUCCESS
          available_yaml_networks: <% task().result.select($[1]) %>
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      transform_output:
        publish:
          status: SUCCESS
          available_networks: <% yaml_parse($.available_yaml_networks.join("\n")) %>
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-complete: notify_zaqar

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.list_available_networks
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                available_networks: <% $.get('available_networks', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  list_networks:
    input:
      - container: 'overcloud'
      - network_data_file: 'network_data.yaml'
      - queue_name: tripleo

    output:
      network_data: <% $.network_data %>

    tags:
      - tripleo-common-managed

    tasks:
      get_networks:
        action: swift.get_object
        input:
          container: <% $.container %>
          obj: <% $.network_data_file %>
        on-success: notify_zaqar
        publish:
          network_data: <% yaml_parse(task().result.last()) %>
          status: SUCCESS
          message: <% task().result %>
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      notify_zaqar:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.list_networks
              payload:
                status: <% $.status %>
                network_data: <% $.get('network_data', {}) %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  validate_network_files:
    description: Validate network files exist
    input:
      - container: overcloud
      - network_data
      - queue_name: tripleo

    output:
      network_data: <% $.network_data %>

    tags:
      - tripleo-common-managed

    tasks:
      get_network_names:
        publish:
          network_names_lower: <% $.network_data.where($.containsKey('name_lower')).name_lower %>
          network_names: <% $.network_data.where(not $.containsKey('name_lower')).name %>
        on-success: validate_networks

      validate_networks:
        with-items: network in <% $.network_names_lower.concat($.network_names) %>
        action: swift.head_object
        input:
          container: <% $.container %>
          obj: network/<% $.network.toLower() %>.yaml
        publish:
          status: SUCCESS
          message: <% task().result %>
        on-success: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      notify_zaqar:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.validate_network_files
              payload:
                status: <% $.status %>
                message: <% $.message %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  validate_networks:
    description: Validate network files were generated properly and exist
    input:
      - container: 'overcloud'
      - network_data_file: 'network_data.yaml'
      - queue_name: tripleo

    output:
      network_data: <% $.network_data %>

    tags:
      - tripleo-common-managed

    tasks:
      get_network_data:
        workflow: list_networks
        input:
          container: <% $.container %>
          network_data_file: <% $.network_data_file %>
          queue_name: <% $.queue_name %>
        publish:
          network_data: <% task().result.network_data %>
        on-success: validate_networks
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error:
          notify_zaqar

      validate_networks:
        workflow: validate_network_files
        input:
          container: <% $.container %>
          network_data: <% $.network_data %>
          queue_name: <% $.queue_name %>
        publish:
          status: SUCCESS
          message: <% task().result %>
        on-success: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.validate_networks
              payload:
                status: <% $.status %>
                network_data: <% $.get('network_data', {}) %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  validate_roles:
    description: Vaildate roles data exists and is parsable

    input:
      - container: overcloud
      - roles_data_file: 'roles_data.yaml'
      - queue_name: tripleo

    output:
      roles_data: <% $.roles_data %>

    tags:
      - tripleo-common-managed

    tasks:
      get_roles_data:
        workflow: list_roles
        input:
          container: <% $.container %>
          roles_data_file: <% $.roles_data_file %>
          queue_name: <% $.queue_name %>
        publish:
          roles_data: <% task().result.roles_data %>
          status: SUCCESS
        on-success: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error:
          notify_zaqar

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.validate_networks
              payload:
                status: <% $.status %>
                roles_data: <% $.get('roles_data', '') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  _validate_networks_from_roles:
    description: Internal workflow for validating a network exists from a role

    input:
      - container: overcloud
      - defined_networks
      - networks_in_roles
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      validate_network_in_network_data:
        publish:
          networks_found: <% $.networks_in_roles.toSet().intersect($.defined_networks.toSet()) %>
          networks_not_found: <% $.networks_in_roles.toSet().difference($.defined_networks.toSet()) %>
        on-success:
          - network_not_found: <% $.networks_not_found %>
          - notify_zaqar: <% not $.networks_not_found %>

      network_not_found:
        publish:
          message: <% "Some networks in roles are not defined, {0}".format($.networks_not_found.join(', ')) %>
          status: FAILED
        on-success: notify_zaqar

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1._validate_networks_from_role
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  validate_roles_and_networks:
    description: Vaidate that roles and network data are valid

    input:
      - container: overcloud
      - roles_data_file: 'roles_data.yaml'
      - network_data_file: 'network_data.yaml'
      - queue_name: tripleo

    output:
      roles_data: <% $.roles_data %>
      network_data: <% $.network_data %>

    tags:
      - tripleo-common-managed

    tasks:
      validate_network_data:
        workflow: validate_networks
        input:
          container: <% $.container %>
          network_data_file: <% $.network_data_file %>
          queue_name: <% $.queue_name %>
        publish:
          network_data: <% task().result.network_data %>
        on-success: validate_roles_data
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      validate_roles_data:
        workflow: validate_roles
        input:
          container: <% $.container %>
          roles_data_file: <% $.roles_data_file %>
          queue_name: <% $.queue_name %>
        publish:
          roles_data: <% task().result.roles_data %>
          role_networks_data: <% task().result.roles_data.networks %>
          networks_in_roles: <% task().result.roles_data.networks.flatten().distinct() %>
        on-success: validate_roles_and_networks
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      validate_roles_and_networks:
        workflow: _validate_networks_from_roles
        input:
          container: <% $.container %>
          defined_networks: <% $.network_data.name %>
          networks_in_roles: <% $.networks_in_roles %>
          queue_name: <% $.queue_name %>
        publish:
          status: SUCCESS
        on-success: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result.message %>
        on-error: notify_zaqar

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.validate_roles_and_networks
              payload:
                status: <% $.status %>
                roles_data: <% $.get('roles_data', {}) %>
                network_data: <% $.get('network_data', {}) %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  list_available_roles:
    input:
      - container: overcloud
      - queue_name: tripleo

    output:
      available_roles: <% $.available_roles %>

    tags:
      - tripleo-common-managed

    tasks:
      get_role_file_names:
        action: swift.get_container
        input:
          container: <% $.container %>
        publish:
          role_names: <% task().result[1].where($.name.startsWith('roles/')).where($.name.endsWith('.yaml')).name %>
        on-success: get_role_files
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      get_role_files:
        with-items: role_name in <% $.role_names %>
        action: swift.get_object
        on-success: transform_output
        on-error: notify_zaqar
        input:
          container: <% $.container %>
          obj: <% $.role_name %>
        publish:
          status: SUCCESS
          available_yaml_roles: <% task().result.select($[1]) %>
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      transform_output:
        publish:
          status: SUCCESS
          available_roles: <% yaml_parse($.available_yaml_roles.join("\n")) %>
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-complete: notify_zaqar

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.list_available_roles
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                available_roles: <% $.get('available_roles', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  update_roles:
    description: >
      takes data in json format validates its contents and persists them in
      roles_data.yaml, after successful update, templates are regenerated.
    input:
      - container
      - roles
      - roles_data_file: 'roles_data.yaml'
      - replace_all: false
      - queue_name: tripleo
    tags:
      - tripleo-common-managed
    tasks:
      get_available_roles:
        workflow: list_available_roles
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name%>
        publish:
          available_roles: <% task().result.available_roles %>
        on-success: validate_input
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      validate_input:
        description: >
          validate the format of input (verify that each role in input has the
          required attributes set. check README in roles directory in t-h-t),
          validate that roles in input exist in roles directory in t-h-t
        action: tripleo.plan.validate_roles
        input:
          container: <% $.container %>
          roles: <% $.roles %>
          available_roles: <% $.available_roles %>
        on-success: get_network_data
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      get_network_data:
        workflow: list_networks
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>
        publish:
          network_data: <% task().result.network_data %>
        on-success: validate_network_names
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      validate_network_names:
        description: >
          validate that Network names assigned to Role exist in
          network-data.yaml object in Swift container
        workflow: _validate_networks_from_roles
        input:
          container: <% $.container %>
          defined_networks: <% $.network_data.name %>
          networks_in_roles: <% $.roles.networks.flatten().distinct() %>
          queue_name: <% $.queue_name %>
        on-success: get_current_roles
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result.message %>

      get_current_roles:
        workflow: list_roles
        input:
          container: <% $.container %>
          roles_data_file: <% $.roles_data_file %>
          queue_name: <% $.queue_name %>
        publish:
            current_roles: <% task().result.roles_data %>
        on-success: update_roles_data
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      update_roles_data:
        description: >
          update roles_data.yaml object in Swift with roles from workflow input
        action: tripleo.plan.update_roles
        input:
          container: <% $.container %>
          roles: <% $.roles %>
          current_roles: <% $.current_roles %>
          replace_all: <% $.replace_all %>
        publish:
          updated_roles_data: <% task().result.roles %>
        on-success: update_roles_data_in_swift
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      update_roles_data_in_swift:
        description: >
          update roles_data.yaml object in Swift with data from workflow input
        action: swift.put_object
        input:
          container: <% $.container %>
          obj: <% $.roles_data_file %>
          contents: <% yaml_dump($.updated_roles_data) %>
        on-success: regenerate_templates
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      regenerate_templates:
        action: tripleo.templates.process container=<% $.container %>
        on-success: get_updated_roles
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      get_updated_roles:
        workflow: list_roles
        input:
          container: <% $.container %>
          roles_data_file:  <% $.roles_data_file %>
        publish:
            updated_roles: <% task().result.roles_data %>
            status: SUCCESS
        on-complete: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.roles.v1.update_roles
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                updated_roles: <% $.get('updated_roles', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  select_roles:
    description: >
      takes a list of role names as input and populates roles_data.yaml in
      container in Swift with respective roles from 'roles directory'
    input:
      - container
      - role_names
      - roles_data_file: 'roles_data.yaml'
      - replace_all: true
      - queue_name: tripleo
    tags:
      - tripleo-common-managed
    tasks:

      get_available_roles:
        workflow: list_available_roles
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>
        publish:
          available_roles: <% task().result.available_roles %>
        on-success: get_current_roles
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      get_current_roles:
        workflow: list_roles
        input:
          container: <% $.container %>
          roles_data_file: <% $.roles_data_file %>
          queue_name: <% $.queue_name %>
        publish:
          current_roles: <% task().result.roles_data %>
        on-success: gather_roles
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      gather_roles:
        description: >
          for each role name from the input, check if it exists in
          roles_data.yaml, if yes, use that role definition, if not, get the
          role definition from roles directory. Use the gathered roles
          definitions as input to updateRolesWorkflow - this ensures
          configuration of the roles which are already in roles_data.yaml
          will not get overridden by data from roles directory
        action: tripleo.plan.gather_roles
        input:
          role_names: <% $.role_names %>
          current_roles: <% $.current_roles %>
          available_roles: <% $.available_roles %>
        publish:
          gathered_roles: <% task().result.gathered_roles %>
        on-success: call_update_roles_workflow
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      call_update_roles_workflow:
        workflow: update_roles
        input:
          container: <% $.container %>
          roles: <% $.gathered_roles %>
          roles_data_file: <% $.roles_data_file %>
          replace_all: <% $.replace_all %>
          queue_name: <% $.queue_name %>
        on-complete: notify_zaqar
        publish:
          selected_roles: <% task().result.updated_roles %>
          status: SUCCESS
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.select_roles
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                selected_roles: <% $.get('selected_roles', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-03 06:19:10,132 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 47190
2018-12-03 06:19:10,173 DEBUG: RESP: [201] Content-Length: 47190 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:10 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.plan_management.v1\ndescription: TripleO Overcloud Deployment Workflows v1\n\nworkflows:\n\n  create_default_deployment_plan:\n    description: >\n      This workflow exists to maintain backwards compatibility in pike.  This\n      workflow will likely be removed in queens in favor of create_deployment_plan.\n    input:\n      - container\n      - queue_name: tripleo\n      - generate_passwords: true\n    tags:\n      - tripleo-common-managed\n    tasks:\n      call_create_deployment_plan:\n        workflow: tripleo.plan_management.v1.create_deployment_plan\n        on-success: set_status_success\n        on-error: call_create_deployment_plan_set_status_failed\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n          generate_passwords: <% $.generate_passwords %>\n          use_default_templates: true\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(call_create_deployment_plan).result %>\n\n      call_create_deployment_plan_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(call_create_deployment_plan).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.create_default_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  create_deployment_plan:\n    description: >\n      This workflow provides the capability to create a deployment plan using\n      the default heat templates provided in a standard TripleO undercloud\n      deployment, heat templates contained in an external git repository, or a\n      swift container that already contains templates.\n    input:\n      - container\n      - source_url: null\n      - queue_name: tripleo\n      - generate_passwords: true\n      - use_default_templates: false\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      container_required_check:\n        description: >\n          If using the default templates or importing templates from a git\n          repository, a new container needs to be created.  If using an existing\n          container containing templates, skip straight to create_plan.\n        on-success:\n          - verify_container_doesnt_exist: <% $.use_default_templates or $.source_url %>\n          - create_plan: <% $.use_default_templates = false and $.source_url = null %>\n\n      verify_container_doesnt_exist:\n        action: swift.head_container container=<% $.container %>\n        on-success: notify_zaqar\n        on-error: create_container\n        publish:\n          status: FAILED\n          message: \"Unable to create plan. The Swift container already exists\"\n\n      create_container:\n        action: tripleo.plan.create_container container=<% $.container %>\n        on-success: templates_source_check\n        on-error: create_container_set_status_failed\n\n      cleanup_temporary_files:\n        action: tripleo.git.clean container=<% $.container %>\n\n      templates_source_check:\n        on-success:\n          - upload_default_templates: <% $.use_default_templates = true %>\n          - clone_git_repo: <% $.source_url != null %>\n\n      clone_git_repo:\n        action: tripleo.git.clone container=<% $.container %> url=<% $.source_url %>\n        on-success: upload_templates_directory\n        on-error: clone_git_repo_set_status_failed\n\n      upload_templates_directory:\n        action: tripleo.templates.upload container=<% $.container %> templates_path=<% task(clone_git_repo).result %>\n        on-success: create_plan\n        on-complete: cleanup_temporary_files\n        on-error: upload_templates_directory_set_status_failed\n\n      upload_default_templates:\n        action: tripleo.templates.upload container=<% $.container %>\n        on-success: create_plan\n        on-error: upload_to_container_set_status_failed\n\n      create_plan:\n        on-success:\n          - ensure_passwords_exist: <% $.generate_passwords = true %>\n          - add_root_stack_name: <% $.generate_passwords != true %>\n\n      ensure_passwords_exist:\n        action: tripleo.parameters.generate_passwords container=<% $.container %>\n        on-success: add_root_stack_name\n        on-error: ensure_passwords_exist_set_status_failed\n\n      add_root_stack_name:\n        action: tripleo.parameters.update\n        input:\n          container: <% $.container %>\n          parameters:\n            RootStackName: <% $.container %>\n        on-success: container_images_prepare\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      container_images_prepare:\n        description: >\n          Populate all container image parameters with default values.\n        action: tripleo.container_images.prepare container=<% $.container %>\n        on-success: process_templates\n        on-error: container_images_prepare_set_status_failed\n\n      process_templates:\n        action: tripleo.templates.process container=<% $.container %>\n        on-success: set_status_success\n        on-error: process_templates_set_status_failed\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: 'Plan created.'\n\n      create_container_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_container).result %>\n\n      clone_git_repo_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(clone_git_repo).result %>\n\n      upload_templates_directory_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(upload_templates_directory).result %>\n\n      upload_to_container_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(upload_default_templates).result %>\n\n      ensure_passwords_exist_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(ensure_passwords_exist).result %>\n\n      process_templates_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(process_templates).result %>\n\n      container_images_prepare_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(container_images_prepare).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.create_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_deployment_plan:\n    input:\n      - container\n      - source_url: null\n      - queue_name: tripleo\n      - generate_passwords: true\n      - plan_environment: null\n    tags:\n      - tripleo-common-managed\n    tasks:\n      templates_source_check:\n        on-success:\n          - update_plan: <% $.source_url = null %>\n          - clone_git_repo: <% $.source_url != null %>\n\n      clone_git_repo:\n        action: tripleo.git.clone container=<% $.container %> url=<% $.source_url %>\n        on-success: upload_templates_directory\n        on-error: clone_git_repo_set_status_failed\n\n      upload_templates_directory:\n        action: tripleo.templates.upload container=<% $.container %> templates_path=<% task(clone_git_repo).result %>\n        on-success: create_swift_rings_backup_plan\n        on-complete: cleanup_temporary_files\n        on-error: upload_templates_directory_set_status_failed\n\n      cleanup_temporary_files:\n        action: tripleo.git.clean container=<% $.container %>\n\n      create_swift_rings_backup_plan:\n        workflow: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan\n        on-success: update_plan\n        on-error: create_swift_rings_backup_plan_set_status_failed\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n          use_default_templates: true\n\n      update_plan:\n        on-success:\n          - ensure_passwords_exist: <% $.generate_passwords = true %>\n          - container_images_prepare: <% $.generate_passwords != true %>\n\n      ensure_passwords_exist:\n        action: tripleo.parameters.generate_passwords container=<% $.container %>\n        on-success: container_images_prepare\n        on-error: ensure_passwords_exist_set_status_failed\n\n      container_images_prepare:\n        description: >\n          Populate all container image parameters with default values.\n        action: tripleo.container_images.prepare container=<% $.container %>\n        on-success: process_templates\n        on-error: container_images_prepare_set_status_failed\n\n      process_templates:\n        action: tripleo.templates.process container=<% $.container %>\n        on-success:\n          - set_status_success: <% $.plan_environment = null %>\n          - upload_plan_environment: <% $.plan_environment != null %>\n        on-error: process_templates_set_status_failed\n\n      upload_plan_environment:\n        action: tripleo.templates.upload_plan_environment container=<% $.container %> plan_environment=<% $.plan_environment %>\n        on-success: set_status_success\n        on-error: process_templates_set_status_failed\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: 'Plan updated.'\n\n      create_swift_rings_backup_plan_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_swift_rings_backup_plan).result %>\n\n      clone_git_repo_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(clone_git_repo).result %>\n\n      upload_templates_directory_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(upload_templates_directory).result %>\n\n      process_templates_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(process_templates).result %>\n\n      ensure_passwords_exist_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(ensure_passwords_exist).result %>\n\n      container_images_prepare_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(container_images_prepare).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.update_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  delete_deployment_plan:\n    description: >\n      Deletes a plan by deleting the container matching plan_name. It will\n      not delete the plan if a stack exists with the same name.\n\n    tags:\n      - tripleo-common-managed\n\n    input:\n      - container: overcloud\n      - queue_name: tripleo\n\n    tasks:\n      delete_plan:\n        action: tripleo.plan.delete container=<% $.container %>\n        on-complete: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.delete_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n\n  get_passwords:\n    description: Retrieves passwords for a given plan\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      verify_container_exists:\n        action: swift.head_container container=<% $.container %>\n        on-success: get_environment_passwords\n        on-error: verify_container_set_status_failed\n\n      get_environment_passwords:\n        action: tripleo.parameters.get_passwords container=<% $.container %>\n        on-success: get_passwords_set_status_success\n        on-error: get_passwords_set_status_failed\n\n      get_passwords_set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(get_environment_passwords).result %>\n\n      get_passwords_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(get_environment_passwords).result %>\n\n      verify_container_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(verify_container_exists).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.get_passwords\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  export_deployment_plan:\n    description: Creates an export tarball for a given plan\n    input:\n      - plan\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      export_plan:\n        action: tripleo.plan.export\n        input:\n          plan: <% $.plan %>\n          delete_after: 3600\n          exports_container: \"plan-exports\"\n        on-success: create_tempurl\n        on-error: export_plan_set_status_failed\n\n      create_tempurl:\n        action: tripleo.swift.tempurl\n        on-success: set_status_success\n        on-error: create_tempurl_set_status_failed\n        input:\n          container: \"plan-exports\"\n          obj: \"<% $.plan %>.tar.gz\"\n          valid: 3600\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(create_tempurl).result %>\n          tempurl: <% task(create_tempurl).result %>\n\n      export_plan_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(export_plan).result %>\n\n      create_tempurl_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_tempurl).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.export_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                tempurl: <% $.get('tempurl', '') %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  get_deprecated_parameters:\n    description: Gets the list of deprecated parameters in the whole of the plan including nested stack\n    input:\n      - container: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_flatten_data:\n        action: tripleo.parameters.get_flatten container=<% $.container %>\n        on-success: get_deprecated_params\n        on-error: set_status_failed_get_flatten_data\n        publish:\n          user_params: <% task().result.environment_parameters %>\n          plan_params: <% task().result.heat_resource_tree.parameters.keys() %>\n          parameter_groups: <% task().result.heat_resource_tree.resources.values().where( $.get('parameter_groups') ).select($.parameter_groups).flatten() %>\n\n      get_deprecated_params:\n        on-success: check_if_user_param_has_deprecated\n        publish:\n          deprecated_params: <% $.parameter_groups.where($.get('label') = 'deprecated').select($.parameters).flatten().distinct() %>\n\n      check_if_user_param_has_deprecated:\n        on-success: get_unused_params\n        publish:\n          deprecated_result: <% let(up => $.user_params) -> $.deprecated_params.select( dict('parameter' => $, 'deprecated' => true, 'user_defined' => $up.keys().contains($)) ) %>\n\n      # Get the list of parameters, which are defined by user via environment files's parameter_default, but not part of the plan definition\n      # It may be possible that the parameter will be used by a service, but the service is not part of the plan.\n      # In such cases, the parameter will be reported as unused, care should be take to understand whether it is really unused or not.\n      get_unused_params:\n        on-success: send_message\n        publish:\n          unused_params: <% let(plan_params => $.plan_params) ->  $.user_params.keys().where( not $plan_params.contains($) ) %>\n\n      set_status_failed_get_flatten_data:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_flatten_data).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.get_deprecated_parameters\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                deprecated: <% $.get('deprecated_result', []) %>\n                unused: <% $.get('unused_params', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  publish_ui_logs_to_swift:\n    description: >\n      This workflow drains a zaqar queue, and publish its messages into a log\n      file in swift.  This workflow is called by cron trigger.\n\n    input:\n      - logging_queue_name: tripleo-ui-logging\n      - logging_container: tripleo-ui-logs\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      # We're using a NoOp action to start the workflow.  The recursive nature\n      # of the workflow means that Mistral will refuse to execute it because it\n      # doesn't know where to begin.\n      start:\n        on-success: get_messages\n\n      get_messages:\n        action: zaqar.claim_messages\n        on-success:\n          - format_messages: <% task().result.len() > 0 %>\n        input:\n          queue_name: <% $.logging_queue_name %>\n          ttl: 60\n          grace: 60\n        publish:\n          status: SUCCESS\n          messages: <% task().result %>\n          message_ids: <% task().result.select($._id) %>\n\n      format_messages:\n        action: tripleo.logging_to_swift.format_messages\n        on-success: upload_to_swift\n        input:\n          messages: <% $.messages %>\n        publish:\n          status: SUCCESS\n          formatted_messages: <% task().result %>\n\n      upload_to_swift:\n        action: tripleo.logging_to_swift.publish_ui_log_to_swift\n        on-success: delete_messages\n        input:\n          logging_data: <% $.formatted_messages %>\n          logging_container: <% $.logging_container %>\n        publish:\n          status: SUCCESS\n\n      delete_messages:\n        action: zaqar.delete_messages\n        on-success: get_messages\n        input:\n          queue_name: <% $.logging_queue_name %>\n          messages: <% $.message_ids %>\n        publish:\n          status: SUCCESS\n\n  download_logs:\n    description: Creates a tarball with logging data\n    input:\n      - queue_name: tripleo\n      - logging_container: \"tripleo-ui-logs\"\n      - downloads_container: \"tripleo-ui-logs-downloads\"\n      - delete_after: 3600\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      publish_logs:\n        workflow: tripleo.plan_management.v1.publish_ui_logs_to_swift\n        on-success: prepare_log_download\n        on-error: publish_logs_set_status_failed\n\n      prepare_log_download:\n        action: tripleo.logging_to_swift.prepare_log_download\n        input:\n          logging_container: <% $.logging_container %>\n          downloads_container: <% $.downloads_container %>\n          delete_after: <% $.delete_after %>\n        on-success: create_tempurl\n        on-error: download_logs_set_status_failed\n        publish:\n          filename: <% task().result %>\n\n      create_tempurl:\n        action: tripleo.swift.tempurl\n        on-success: set_status_success\n        on-error: create_tempurl_set_status_failed\n        input:\n          container: <% $.downloads_container %>\n          obj: <% $.filename %>\n          valid: 3600\n        publish:\n          tempurl: <% task().result %>\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(create_tempurl).result %>\n          tempurl: <% task(create_tempurl).result %>\n\n      publish_logs_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(publish_logs).result %>\n\n      download_logs_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(prepare_log_download).result %>\n\n      create_tempurl_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_tempurl).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.download_logs\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                tempurl: <% $.get('tempurl', '') %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list_roles:\n    description: Retrieve the roles_data.yaml and return a usable object\n\n    input:\n      - container: overcloud\n      - roles_data_file: 'roles_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      roles_data: <% $.roles_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_roles_data:\n        action: swift.get_object\n        input:\n          container: <% $.container %>\n          obj: <% $.roles_data_file %>\n        publish:\n          roles_data: <% yaml_parse(task().result.last()) %>\n          status: SUCCESS\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.list_roles\n              payload:\n                status: <% $.status %>\n                roles_data: <% $.get('roles_data', {}) %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list_available_networks:\n    input:\n      - container\n      - queue_name: tripleo\n\n    output:\n      available_networks: <% $.available_networks %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_network_file_names:\n        action: swift.get_container\n        input:\n          container: <% $.container %>\n        publish:\n          network_names: <% task().result[1].where($.name.startsWith('networks/')).where($.name.endsWith('.yaml')).name %>\n        on-success: get_network_files\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_network_files:\n        with-items: network_name in <% $.network_names %>\n        action: swift.get_object\n        on-success: transform_output\n        on-error: notify_zaqar\n        input:\n          container: <% $.container %>\n          obj: <% $.network_name %>\n        publish:\n          status: SUCCESS\n          available_yaml_networks: <% task().result.select($[1]) %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      transform_output:\n        publish:\n          status: SUCCESS\n          available_networks: <% yaml_parse($.available_yaml_networks.join(\"\\n\")) %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-complete: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.list_available_networks\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                available_networks: <% $.get('available_networks', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list_networks:\n    input:\n      - container: 'overcloud'\n      - network_data_file: 'network_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_networks:\n        action: swift.get_object\n        input:\n          container: <% $.container %>\n          obj: <% $.network_data_file %>\n        on-success: notify_zaqar\n        publish:\n          network_data: <% yaml_parse(task().result.last()) %>\n          status: SUCCESS\n          message: <% task().result %>\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.list_networks\n              payload:\n                status: <% $.status %>\n                network_data: <% $.get('network_data', {}) %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_network_files:\n    description: Validate network files exist\n    input:\n      - container: overcloud\n      - network_data\n      - queue_name: tripleo\n\n    output:\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_network_names:\n        publish:\n          network_names_lower: <% $.network_data.where($.containsKey('name_lower')).name_lower %>\n          network_names: <% $.network_data.where(not $.containsKey('name_lower')).name %>\n        on-success: validate_networks\n\n      validate_networks:\n        with-items: network in <% $.network_names_lower.concat($.network_names) %>\n        action: swift.head_object\n        input:\n          container: <% $.container %>\n          obj: network/<% $.network.toLower() %>.yaml\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.validate_network_files\n              payload:\n                status: <% $.status %>\n                message: <% $.message %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_networks:\n    description: Validate network files were generated properly and exist\n    input:\n      - container: 'overcloud'\n      - network_data_file: 'network_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_network_data:\n        workflow: list_networks\n        input:\n          container: <% $.container %>\n          network_data_file: <% $.network_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().result.network_data %>\n        on-success: validate_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error:\n          notify_zaqar\n\n      validate_networks:\n        workflow: validate_network_files\n        input:\n          container: <% $.container %>\n          network_data: <% $.network_data %>\n          queue_name: <% $.queue_name %>\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.validate_networks\n              payload:\n                status: <% $.status %>\n                network_data: <% $.get('network_data', {}) %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_roles:\n    description: Vaildate roles data exists and is parsable\n\n    input:\n      - container: overcloud\n      - roles_data_file: 'roles_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      roles_data: <% $.roles_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_roles_data:\n        workflow: list_roles\n        input:\n          container: <% $.container %>\n          roles_data_file: <% $.roles_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          roles_data: <% task().result.roles_data %>\n          status: SUCCESS\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error:\n          notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.validate_networks\n              payload:\n                status: <% $.status %>\n                roles_data: <% $.get('roles_data', '') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  _validate_networks_from_roles:\n    description: Internal workflow for validating a network exists from a role\n\n    input:\n      - container: overcloud\n      - defined_networks\n      - networks_in_roles\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      validate_network_in_network_data:\n        publish:\n          networks_found: <% $.networks_in_roles.toSet().intersect($.defined_networks.toSet()) %>\n          networks_not_found: <% $.networks_in_roles.toSet().difference($.defined_networks.toSet()) %>\n        on-success:\n          - network_not_found: <% $.networks_not_found %>\n          - notify_zaqar: <% not $.networks_not_found %>\n\n      network_not_found:\n        publish:\n          message: <% \"Some networks in roles are not defined, {0}\".format($.networks_not_found.join(', ')) %>\n          status: FAILED\n        on-success: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1._validate_networks_from_role\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_roles_and_networks:\n    description: Vaidate that roles and network data are valid\n\n    input:\n      - container: overcloud\n      - roles_data_file: 'roles_data.yaml'\n      - network_data_file: 'network_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      roles_data: <% $.roles_data %>\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      validate_network_data:\n        workflow: validate_networks\n        input:\n          container: <% $.container %>\n          network_data_file: <% $.network_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().result.network_data %>\n        on-success: validate_roles_data\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      validate_roles_data:\n        workflow: validate_roles\n        input:\n          container: <% $.container %>\n          roles_data_file: <% $.roles_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          roles_data: <% task().result.roles_data %>\n          role_networks_data: <% task().result.roles_data.networks %>\n          networks_in_roles: <% task().result.roles_data.networks.flatten().distinct() %>\n        on-success: validate_roles_and_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      validate_roles_and_networks:\n        workflow: _validate_networks_from_roles\n        input:\n          container: <% $.container %>\n          defined_networks: <% $.network_data.name %>\n          networks_in_roles: <% $.networks_in_roles %>\n          queue_name: <% $.queue_name %>\n        publish:\n          status: SUCCESS\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result.message %>\n        on-error: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.validate_roles_and_networks\n              payload:\n                status: <% $.status %>\n                roles_data: <% $.get('roles_data', {}) %>\n                network_data: <% $.get('network_data', {}) %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list_available_roles:\n    input:\n      - container: overcloud\n      - queue_name: tripleo\n\n    output:\n      available_roles: <% $.available_roles %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_role_file_names:\n        action: swift.get_container\n        input:\n          container: <% $.container %>\n        publish:\n          role_names: <% task().result[1].where($.name.startsWith('roles/')).where($.name.endsWith('.yaml')).name %>\n        on-success: get_role_files\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_role_files:\n        with-items: role_name in <% $.role_names %>\n        action: swift.get_object\n        on-success: transform_output\n        on-error: notify_zaqar\n        input:\n          container: <% $.container %>\n          obj: <% $.role_name %>\n        publish:\n          status: SUCCESS\n          available_yaml_roles: <% task().result.select($[1]) %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      transform_output:\n        publish:\n          status: SUCCESS\n          available_roles: <% yaml_parse($.available_yaml_roles.join(\"\\n\")) %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-complete: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.list_available_roles\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                available_roles: <% $.get('available_roles', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_roles:\n    description: >\n      takes data in json format validates its contents and persists them in\n      roles_data.yaml, after successful update, templates are regenerated.\n    input:\n      - container\n      - roles\n      - roles_data_file: 'roles_data.yaml'\n      - replace_all: false\n      - queue_name: tripleo\n    tags:\n      - tripleo-common-managed\n    tasks:\n      get_available_roles:\n        workflow: list_available_roles\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name%>\n        publish:\n          available_roles: <% task().result.available_roles %>\n        on-success: validate_input\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      validate_input:\n        description: >\n          validate the format of input (verify that each role in input has the\n          required attributes set. check README in roles directory in t-h-t),\n          validate that roles in input exist in roles directory in t-h-t\n        action: tripleo.plan.validate_roles\n        input:\n          container: <% $.container %>\n          roles: <% $.roles %>\n          available_roles: <% $.available_roles %>\n        on-success: get_network_data\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_network_data:\n        workflow: list_networks\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().result.network_data %>\n        on-success: validate_network_names\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      validate_network_names:\n        description: >\n          validate that Network names assigned to Role exist in\n          network-data.yaml object in Swift container\n        workflow: _validate_networks_from_roles\n        input:\n          container: <% $.container %>\n          defined_networks: <% $.network_data.name %>\n          networks_in_roles: <% $.roles.networks.flatten().distinct() %>\n          queue_name: <% $.queue_name %>\n        on-success: get_current_roles\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result.message %>\n\n      get_current_roles:\n        workflow: list_roles\n        input:\n          container: <% $.container %>\n          roles_data_file: <% $.roles_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n            current_roles: <% task().result.roles_data %>\n        on-success: update_roles_data\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      update_roles_data:\n        description: >\n          update roles_data.yaml object in Swift with roles from workflow input\n        action: tripleo.plan.update_roles\n        input:\n          container: <% $.container %>\n          roles: <% $.roles %>\n          current_roles: <% $.current_roles %>\n          replace_all: <% $.replace_all %>\n        publish:\n          updated_roles_data: <% task().result.roles %>\n        on-success: update_roles_data_in_swift\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      update_roles_data_in_swift:\n        description: >\n          update roles_data.yaml object in Swift with data from workflow input\n        action: swift.put_object\n        input:\n          container: <% $.container %>\n          obj: <% $.roles_data_file %>\n          contents: <% yaml_dump($.updated_roles_data) %>\n        on-success: regenerate_templates\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      regenerate_templates:\n        action: tripleo.templates.process container=<% $.container %>\n        on-success: get_updated_roles\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_updated_roles:\n        workflow: list_roles\n        input:\n          container: <% $.container %>\n          roles_data_file:  <% $.roles_data_file %>\n        publish:\n            updated_roles: <% task().result.roles_data %>\n            status: SUCCESS\n        on-complete: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.roles.v1.update_roles\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                updated_roles: <% $.get('updated_roles', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  select_roles:\n    description: >\n      takes a list of role names as input and populates roles_data.yaml in\n      container in Swift with respective roles from 'roles directory'\n    input:\n      - container\n      - role_names\n      - roles_data_file: 'roles_data.yaml'\n      - replace_all: true\n      - queue_name: tripleo\n    tags:\n      - tripleo-common-managed\n    tasks:\n\n      get_available_roles:\n        workflow: list_available_roles\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n        publish:\n          available_roles: <% task().result.available_roles %>\n        on-success: get_current_roles\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_current_roles:\n        workflow: list_roles\n        input:\n          container: <% $.container %>\n          roles_data_file: <% $.roles_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          current_roles: <% task().result.roles_data %>\n        on-success: gather_roles\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      gather_roles:\n        description: >\n          for each role name from the input, check if it exists in\n          roles_data.yaml, if yes, use that role definition, if not, get the\n          role definition from roles directory. Use the gathered roles\n          definitions as input to updateRolesWorkflow - this ensures\n          configuration of the roles which are already in roles_data.yaml\n          will not get overridden by data from roles directory\n        action: tripleo.plan.gather_roles\n        input:\n          role_names: <% $.role_names %>\n          current_roles: <% $.current_roles %>\n          available_roles: <% $.available_roles %>\n        publish:\n          gathered_roles: <% task().result.gathered_roles %>\n        on-success: call_update_roles_workflow\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      call_update_roles_workflow:\n        workflow: update_roles\n        input:\n          container: <% $.container %>\n          roles: <% $.gathered_roles %>\n          roles_data_file: <% $.roles_data_file %>\n          replace_all: <% $.replace_all %>\n          queue_name: <% $.queue_name %>\n        on-complete: notify_zaqar\n        publish:\n          selected_roles: <% task().result.updated_roles %>\n          status: SUCCESS\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.select_roles\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                selected_roles: <% $.get('selected_roles', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1", "tags": [], "created_at": "2018-12-03 11:19:09", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "9c82ad70-39c5-48a5-b310-0e0f342222a4"}

2018-12-03 06:19:10,174 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:19:10,176 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.scale.v1
description: TripleO Overcloud Deployment Workflows v1

workflows:

  delete_node:
    description: deletes given overcloud nodes and updates the stack

    input:
      - container
      - nodes
      - timeout: 240
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      delete_node:
        action: tripleo.scale.delete_node nodes=<% $.nodes %> timeout=<% $.timeout %> container=<% $.container %>
        on-success: wait_for_stack_in_progress
        on-error: set_delete_node_failed

      set_delete_node_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(delete_node).result %>

      wait_for_stack_in_progress:
        workflow: tripleo.stack.v1.wait_for_stack_in_progress stack=<% $.container %>
        on-success: wait_for_stack_complete
        on-error: wait_for_stack_in_progress_failed

      wait_for_stack_in_progress_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(wait_for_stack_in_progress).result %>

      wait_for_stack_complete:
        workflow: tripleo.stack.v1.wait_for_stack_complete_or_failed stack=<% $.container %>
        on-success: send_message
        on-error: wait_for_stack_complete_failed

      wait_for_stack_complete_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(wait_for_stack_complete).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.scale.v1.delete_node
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-03 06:19:10,489 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 2258
2018-12-03 06:19:10,490 DEBUG: RESP: [201] Content-Length: 2258 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:10 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.scale.v1\ndescription: TripleO Overcloud Deployment Workflows v1\n\nworkflows:\n\n  delete_node:\n    description: deletes given overcloud nodes and updates the stack\n\n    input:\n      - container\n      - nodes\n      - timeout: 240\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      delete_node:\n        action: tripleo.scale.delete_node nodes=<% $.nodes %> timeout=<% $.timeout %> container=<% $.container %>\n        on-success: wait_for_stack_in_progress\n        on-error: set_delete_node_failed\n\n      set_delete_node_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(delete_node).result %>\n\n      wait_for_stack_in_progress:\n        workflow: tripleo.stack.v1.wait_for_stack_in_progress stack=<% $.container %>\n        on-success: wait_for_stack_complete\n        on-error: wait_for_stack_in_progress_failed\n\n      wait_for_stack_in_progress_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(wait_for_stack_in_progress).result %>\n\n      wait_for_stack_complete:\n        workflow: tripleo.stack.v1.wait_for_stack_complete_or_failed stack=<% $.container %>\n        on-success: send_message\n        on-error: wait_for_stack_complete_failed\n\n      wait_for_stack_complete_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(wait_for_stack_complete).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.scale.v1.delete_node\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.scale.v1", "tags": [], "created_at": "2018-12-03 11:19:10", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "f2834234-458d-424e-8b41-f398dee4a3c8"}

2018-12-03 06:19:10,490 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:19:10,491 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.skydive_ansible.v1
description: TripleO manages Skydive with skydive-ansible

workflows:
  skydive_install:
    # allows for additional extra_vars via workflow input
    input:
      - ansible_playbook_verbosity: 0
      - ansible_extra_env_variables:
          ANSIBLE_ROLES_PATH: /usr/share/skydive-ansible/roles/
          ANSIBLE_RETRY_FILES_ENABLED: 'False'
          ANSIBLE_LOG_PATH: /var/log/mistral/skydive-install-workflow.log
          ANSIBLE_HOST_KEY_CHECKING: 'False'
      - skydive_ansible_extra_vars: {}
      - skydive_ansible_playbook: /usr/share/skydive-ansible/playbook.yml.sample
    tags:
      - tripleo-common-managed
    tasks:
      set_blacklisted_ips:
        publish:
          blacklisted_ips: <% env().get('blacklisted_ip_addresses', []) %>
        on-success: set_ip_lists
      set_ip_lists:
        publish:
          agent_ips: <% let(root => $) -> env().get('service_ips', {}).get('skydive_agent_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          analyzer_ips: <% let(root => $) -> env().get('service_ips', {}).get('skydive_analyzer_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
        on-success: enable_ssh_admin
      enable_ssh_admin:
        workflow: tripleo.access.v1.enable_ssh_admin
        input:
          ssh_servers: <% ($.agent_ips + $.analyzer_ips).toSet() %>
        on-success: get_private_key
      get_private_key:
        action: tripleo.validations.get_privkey
        publish:
          private_key: <% task().result %>
        on-success: set_fork_count
      set_fork_count:
        publish: # unique list of all IPs: make each list a set, take unions and count
          fork_count: <% min($.agent_ips.toSet().union($.analyzer_ips.toSet()).count(), 100) %> # don't use >100 forks
        on-success: set_role_vars
      set_role_vars:
        publish:
          # NOTE(sbaubeau): collect role settings from all tht roles
          agent_vars: <% env().get('role_merged_configs', {}).values().select($.get('skydive_agent_ansible_vars', {})).aggregate($1 + $2) %>
          analyzer_vars: <% env().get('role_merged_configs', {}).values().select($.get('skydive_analyzer_ansible_vars', {})).aggregate($1 + $2) %>
        on-success: build_extra_vars
      build_extra_vars:
        publish:
          # NOTE(sbaubeau): merge vars from all ansible roles
          extra_vars: <% $.agent_vars + $.analyzer_vars + $.skydive_ansible_extra_vars %>
        on-success: skydive_install
      skydive_install:
        action: tripleo.ansible-playbook
        input:
          inventory:
            agents:
              hosts: <% $.agent_ips.toDict($, {}) %>
            analyzers:
              hosts: <% $.analyzer_ips.toDict($, {}) %>
          playbook: <% $.skydive_ansible_playbook %>
          remote_user: tripleo-admin
          become: true
          become_user: root
          verbosity: <% $.ansible_playbook_verbosity %>
          forks: <% $.fork_count %>
          ssh_private_key: <% $.private_key %>
          extra_env_variables: <% $.ansible_extra_env_variables %>
          extra_vars: <% $.extra_vars %>
        publish:
          output: <% task().result %>
'
2018-12-03 06:19:10,869 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 3507
2018-12-03 06:19:10,870 DEBUG: RESP: [201] Content-Length: 3507 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:10 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.skydive_ansible.v1\ndescription: TripleO manages Skydive with skydive-ansible\n\nworkflows:\n  skydive_install:\n    # allows for additional extra_vars via workflow input\n    input:\n      - ansible_playbook_verbosity: 0\n      - ansible_extra_env_variables:\n          ANSIBLE_ROLES_PATH: /usr/share/skydive-ansible/roles/\n          ANSIBLE_RETRY_FILES_ENABLED: 'False'\n          ANSIBLE_LOG_PATH: /var/log/mistral/skydive-install-workflow.log\n          ANSIBLE_HOST_KEY_CHECKING: 'False'\n      - skydive_ansible_extra_vars: {}\n      - skydive_ansible_playbook: /usr/share/skydive-ansible/playbook.yml.sample\n    tags:\n      - tripleo-common-managed\n    tasks:\n      set_blacklisted_ips:\n        publish:\n          blacklisted_ips: <% env().get('blacklisted_ip_addresses', []) %>\n        on-success: set_ip_lists\n      set_ip_lists:\n        publish:\n          agent_ips: <% let(root => $) -> env().get('service_ips', {}).get('skydive_agent_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          analyzer_ips: <% let(root => $) -> env().get('service_ips', {}).get('skydive_analyzer_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n        on-success: enable_ssh_admin\n      enable_ssh_admin:\n        workflow: tripleo.access.v1.enable_ssh_admin\n        input:\n          ssh_servers: <% ($.agent_ips + $.analyzer_ips).toSet() %>\n        on-success: get_private_key\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: set_fork_count\n      set_fork_count:\n        publish: # unique list of all IPs: make each list a set, take unions and count\n          fork_count: <% min($.agent_ips.toSet().union($.analyzer_ips.toSet()).count(), 100) %> # don't use >100 forks\n        on-success: set_role_vars\n      set_role_vars:\n        publish:\n          # NOTE(sbaubeau): collect role settings from all tht roles\n          agent_vars: <% env().get('role_merged_configs', {}).values().select($.get('skydive_agent_ansible_vars', {})).aggregate($1 + $2) %>\n          analyzer_vars: <% env().get('role_merged_configs', {}).values().select($.get('skydive_analyzer_ansible_vars', {})).aggregate($1 + $2) %>\n        on-success: build_extra_vars\n      build_extra_vars:\n        publish:\n          # NOTE(sbaubeau): merge vars from all ansible roles\n          extra_vars: <% $.agent_vars + $.analyzer_vars + $.skydive_ansible_extra_vars %>\n        on-success: skydive_install\n      skydive_install:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            agents:\n              hosts: <% $.agent_ips.toDict($, {}) %>\n            analyzers:\n              hosts: <% $.analyzer_ips.toDict($, {}) %>\n          playbook: <% $.skydive_ansible_playbook %>\n          remote_user: tripleo-admin\n          become: true\n          become_user: root\n          verbosity: <% $.ansible_playbook_verbosity %>\n          forks: <% $.fork_count %>\n          ssh_private_key: <% $.private_key %>\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          extra_vars: <% $.extra_vars %>\n        publish:\n          output: <% task().result %>\n", "name": "tripleo.skydive_ansible.v1", "tags": [], "created_at": "2018-12-03 11:19:10", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "d9eb9cb8-56d0-4006-b72e-ddd07a368e40"}

2018-12-03 06:19:10,870 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:19:10,871 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.stack.v1
description: TripleO Stack Workflows

workflows:

  wait_for_stack_complete_or_failed:
    input:
      - stack
      - timeout: 14400 # 4 hours. Default timeout of stack deployment

    tags:
      - tripleo-common-managed

    tasks:

      wait_for_stack_status:
        action: heat.stacks_get stack_id=<% $.stack %>
        timeout: <% $.timeout %>
        retry:
          delay: 15
          count: <% $.timeout / 15 %>
          continue-on: <% task().result.stack_status in ['CREATE_IN_PROGRESS', 'UPDATE_IN_PROGRESS', 'DELETE_IN_PROGRESS'] %>

  wait_for_stack_in_progress:
    input:
      - stack
      - timeout: 600 # 10 minutes. Should not take much longer for a stack to transition to IN_PROGRESS

    tags:
      - tripleo-common-managed

    tasks:

      wait_for_stack_status:
        action: heat.stacks_get stack_id=<% $.stack %>
        timeout: <% $.timeout %>
        retry:
          delay: 15
          count: <% $.timeout / 15 %>
          continue-on: <% task().result.stack_status in ['CREATE_COMPLETE', 'CREATE_FAILED', 'UPDATE_COMPLETE', 'UPDATE_FAILED', 'DELETE_FAILED'] %>

  wait_for_stack_does_not_exist:
    input:
      - stack
      - timeout: 3600

    tags:
      - tripleo-common-managed

    tasks:
      wait_for_stack_does_not_exist:
        action: heat.stacks_list
        timeout: <% $.timeout %>
        retry:
          delay: 15
          count: <% $.timeout / 15 %>
          continue-on: <% $.stack in task(wait_for_stack_does_not_exist).result.select([$.stack_name, $.id]).flatten() %>

  delete_stack:
    input:
      - stack
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      delete_the_stack:
        action: heat.stacks_delete stack_id=<% $.stack %>
        on-success: wait_for_stack_does_not_exist
        on-error: delete_the_stack_failed

      delete_the_stack_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(delete_the_stack).result %>

      wait_for_stack_does_not_exist:
        workflow: tripleo.stack.v1.wait_for_stack_does_not_exist stack=<% $.stack %>
        on-success: send_message
        on-error: wait_for_stack_does_not_exist_failed

      wait_for_stack_does_not_exist_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(wait_for_stack_does_not_exist).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.scale.v1.delete_stack
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-03 06:19:11,268 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 3236
2018-12-03 06:19:11,269 DEBUG: RESP: [201] Content-Length: 3236 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:11 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.stack.v1\ndescription: TripleO Stack Workflows\n\nworkflows:\n\n  wait_for_stack_complete_or_failed:\n    input:\n      - stack\n      - timeout: 14400 # 4 hours. Default timeout of stack deployment\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      wait_for_stack_status:\n        action: heat.stacks_get stack_id=<% $.stack %>\n        timeout: <% $.timeout %>\n        retry:\n          delay: 15\n          count: <% $.timeout / 15 %>\n          continue-on: <% task().result.stack_status in ['CREATE_IN_PROGRESS', 'UPDATE_IN_PROGRESS', 'DELETE_IN_PROGRESS'] %>\n\n  wait_for_stack_in_progress:\n    input:\n      - stack\n      - timeout: 600 # 10 minutes. Should not take much longer for a stack to transition to IN_PROGRESS\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      wait_for_stack_status:\n        action: heat.stacks_get stack_id=<% $.stack %>\n        timeout: <% $.timeout %>\n        retry:\n          delay: 15\n          count: <% $.timeout / 15 %>\n          continue-on: <% task().result.stack_status in ['CREATE_COMPLETE', 'CREATE_FAILED', 'UPDATE_COMPLETE', 'UPDATE_FAILED', 'DELETE_FAILED'] %>\n\n  wait_for_stack_does_not_exist:\n    input:\n      - stack\n      - timeout: 3600\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      wait_for_stack_does_not_exist:\n        action: heat.stacks_list\n        timeout: <% $.timeout %>\n        retry:\n          delay: 15\n          count: <% $.timeout / 15 %>\n          continue-on: <% $.stack in task(wait_for_stack_does_not_exist).result.select([$.stack_name, $.id]).flatten() %>\n\n  delete_stack:\n    input:\n      - stack\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      delete_the_stack:\n        action: heat.stacks_delete stack_id=<% $.stack %>\n        on-success: wait_for_stack_does_not_exist\n        on-error: delete_the_stack_failed\n\n      delete_the_stack_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(delete_the_stack).result %>\n\n      wait_for_stack_does_not_exist:\n        workflow: tripleo.stack.v1.wait_for_stack_does_not_exist stack=<% $.stack %>\n        on-success: send_message\n        on-error: wait_for_stack_does_not_exist_failed\n\n      wait_for_stack_does_not_exist_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(wait_for_stack_does_not_exist).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.scale.v1.delete_stack\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.stack.v1", "tags": [], "created_at": "2018-12-03 11:19:11", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "fb99e428-9481-4f48-b14b-64197a944fc9"}

2018-12-03 06:19:11,269 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:19:11,271 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.support.v1
description: TripleO support workflows

workflows:

  collect_logs:
    description: >
      This workflow runs sosreport on the servers where their names match the
      provided server_name input. The logs are stored in the provided sos_dir.
    input:
      - server_name
      - sos_dir: /var/tmp/tripleo-sos
      - sos_options: boot,cluster,hardware,kernel,memory,nfs,openstack,packagemanager,performance,services,storage,system,webserver,virt
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      collect_logs_on_servers:
        workflow: tripleo.deployment.v1.deploy_on_servers
        on-success: send_message
        on-error: set_collect_logs_on_servers_failed
        input:
          server_name: <% $.server_name %>
          config_name: 'run_sosreport'
          config: |
            #!/bin/bash
            mkdir -p <% $.sos_dir %>
            sosreport --batch \
            -p <% $.sos_options %> \
            --tmp-dir <% $.sos_dir %>

      set_collect_logs_on_servers_failed:
          on-complete:
           - send_message
          publish:
            type: tripleo.deployment.v1.fetch_logs
            status: FAILED
            message: <% task().result %>

      # status messaging
      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: <% $.get('type', 'tripleo.support.v1.collect_logs') %>
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = 'FAILED' %>

  upload_logs:
    description: >
      This workflow uploads the sosreport files stored in the provide sos_dir
      on the provided host (server_uuid) to a swift container on the undercloud
    input:
      - server_uuid
      - server_name
      - container
      - sos_dir: /var/tmp/tripleo-sos
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      # actions
      get_swift_information:
        action: tripleo.swift.swift_information
        on-success: do_log_upload
        on-error: set_get_swift_information_failed
        input:
          container: <% $.container %>
        publish:
          container_url: <% task().result.container_url %>
          auth_key: <% task().result.auth_key %>

      set_get_swift_information_failed:
        on-complete:
          - send_message
        publish:
          status: FAILED
          message: <% task(get_swift_information).result %>

      do_log_upload:
        action: tripleo.deployment.config
        on-success: send_message
        on-error: set_do_log_upload_failed
        input:
          server_id: <% $.server_uuid %>
          name: "upload_logs"
          config: |
            #!/bin/bash
            CONTAINER_URL="<% $.container_url %>"
            TOKEN="<% $.auth_key %>"
            SOS_DIR="<% $.sos_dir %>"
            for FILE in $(find $SOS_DIR -type f); do
              FILENAME=$(basename $FILE)
              curl -X PUT -i -H "X-Auth-Token: $TOKEN" -T $FILE $CONTAINER_URL/$FILENAME
              if [ $? -eq 0 ]; then
                rm -f $FILE
              fi
            done
          group: "script"
        publish:
          message: "Uploaded logs from <% $.server_name %>"

      set_do_log_upload_failed:
        on-complete:
          - send_message
        publish:
          status: FAILED
          message: <% tag(do_log_upload).result %>

      # status messaging
      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: <% $.get('type', 'tripleo.support.v1.upload_logs') %>
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = 'FAILED' %>

  create_container:
    description: >
      This work flow is used to check if the container exists and creates it
      if it does not exist.
    input:
      - container
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      check_container:
        action: swift.head_container container=<% $.container %>
        on-success: send_message
        on-error: create_container

      create_container:
        action: swift.put_container
        input:
          container: <% $.container %>
          headers:
            x-container-meta-usage-tripleo: support
        on-success: send_message
        on-error: set_create_container_failed

      set_create_container_failed:
        on-complete:
          - send_message
        publish:
          type: tripleo.support.v1.create_container.create_container
          status: FAILED
          message: <% task(create_container).result %>

      # status messaging
      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: <% $.get('type', 'tripleo.support.v1.create_container') %>
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = 'FAILED' %>

  delete_container:
    description: >
      This workflow deletes all the objects in a provided swift container and
      then removes the container itself from the undercloud.
    input:
      - container
      - concurrency: 5
      - timeout: 900
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      # actions
      check_container:
        action: swift.head_container container=<% $.container %>
        on-success: list_objects
        on-error: set_check_container_failure

      set_check_container_failure:
        on-complete: send_message
        publish:
          status: FAILED
          type: tripleo.support.v1.delete_container.check_container
          message: <% task(check_container).result %>

      list_objects:
        action: swift.get_container container=<% $.container %>
        on-success: delete_objects
        on-error: set_list_objects_failure
        publish:
          log_objects: <% task().result[1] %>

      set_list_objects_failure:
        on-complete: send_message
        publish:
          status: FAILED
          type: tripleo.support.v1.delete_container.list_objects
          message: <% task(list_objects).result %>

      delete_objects:
        action: swift.delete_object
        concurrency: <% $.concurrency %>
        timeout: <% $.timeout %>
        with-items: object in <% $.log_objects %>
        input:
          container: <% $.container %>
          obj: <% $.object.name %>
        on-success: remove_container
        on-error: set_delete_objects_failure

      set_delete_objects_failure:
        on-complete: send_message
        publish:
          status: FAILED
          type: tripleo.support.v1.delete_container.delete_objects
          message: <% task(delete_objects).result %>

      remove_container:
        action: swift.delete_container container=<% $.container %>
        on-success: send_message
        on-error: set_remove_container_failure

      set_remove_container_failure:
        on-complete: send_message
        publish:
          status: FAILED
          type: tripleo.support.v1.delete_container.remove_container
          message: <% task(remove_container).result %>

      # status messaging
      send_message:
        action: zaqar.queue_post
        wait-before: 5
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: <% $.get('type', 'tripleo.support.v1.delete_container') %>
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = 'FAILED' %>

  fetch_logs:
    description: >
      This workflow creates a container on the undercloud, executes the log
      collection on the servers whose names match the provided server_name, and
      executes the log upload process on all the servers to the container on
      the undercloud.
    input:
      - server_name
      - container
      - concurrency: 5
      - timeout: 1800
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      # actions
      create_container:
        workflow: tripleo.support.v1.create_container
        on-success: get_servers_matching
        on-error: set_create_container_failed
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>

      set_create_container_failed:
          on-complete: send_message
          publish:
            type: tripleo.support.v1.fetch_logs.create_container
            status: FAILED
            message: <% task(create_container).result %>

      get_servers_matching:
        action: nova.servers_list
        on-success: collect_logs_on_servers
        publish:
          servers_with_name: <% task().result._info.where($.name.indexOf(execution().input.server_name) > -1) %>

      collect_logs_on_servers:
        workflow: tripleo.support.v1.collect_logs
        timeout: <% $.timeout %>
        on-success: upload_logs_on_servers
        on-error: set_collect_logs_on_servers_failed
        input:
          server_name: <% $.server_name %>
          queue_name: <% $.queue_name %>

      set_collect_logs_on_servers_failed:
          on-complete: send_message
          publish:
            type: tripleo.support.v1.fetch_logs.collect_logs_on_servers
            status: FAILED
            message: <% task(collect_logs_on_servers).result %>

      upload_logs_on_servers:
        on-success: send_message
        on-error: set_upload_logs_on_servers_failed
        with-items: server in <% $.servers_with_name %>
        concurrency: <% $.concurrency %>
        workflow: tripleo.support.v1.upload_logs
        input:
          server_name: <% $.server.name %>
          server_uuid: <% $.server.id %>
          container: <% $.container %>
          queue_name: <% $.queue_name %>

      set_upload_logs_on_servers_failed:
          on-complete: send_message
          publish:
            type: tripleo.support.v1.fetch_logs.upload_logs
            status: FAILED
            message: <% task(upload_logs_on_servers).result %>

      # status messaging
      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: <% $.get('type', 'tripleo.support.v1.fetch_logs') %>
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = 'FAILED' %>
'
2018-12-03 06:19:12,550 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 12000
2018-12-03 06:19:12,551 DEBUG: RESP: [201] Content-Length: 12000 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:12 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.support.v1\ndescription: TripleO support workflows\n\nworkflows:\n\n  collect_logs:\n    description: >\n      This workflow runs sosreport on the servers where their names match the\n      provided server_name input. The logs are stored in the provided sos_dir.\n    input:\n      - server_name\n      - sos_dir: /var/tmp/tripleo-sos\n      - sos_options: boot,cluster,hardware,kernel,memory,nfs,openstack,packagemanager,performance,services,storage,system,webserver,virt\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      collect_logs_on_servers:\n        workflow: tripleo.deployment.v1.deploy_on_servers\n        on-success: send_message\n        on-error: set_collect_logs_on_servers_failed\n        input:\n          server_name: <% $.server_name %>\n          config_name: 'run_sosreport'\n          config: |\n            #!/bin/bash\n            mkdir -p <% $.sos_dir %>\n            sosreport --batch \\\n            -p <% $.sos_options %> \\\n            --tmp-dir <% $.sos_dir %>\n\n      set_collect_logs_on_servers_failed:\n          on-complete:\n           - send_message\n          publish:\n            type: tripleo.deployment.v1.fetch_logs\n            status: FAILED\n            message: <% task().result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.collect_logs') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n  upload_logs:\n    description: >\n      This workflow uploads the sosreport files stored in the provide sos_dir\n      on the provided host (server_uuid) to a swift container on the undercloud\n    input:\n      - server_uuid\n      - server_name\n      - container\n      - sos_dir: /var/tmp/tripleo-sos\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      # actions\n      get_swift_information:\n        action: tripleo.swift.swift_information\n        on-success: do_log_upload\n        on-error: set_get_swift_information_failed\n        input:\n          container: <% $.container %>\n        publish:\n          container_url: <% task().result.container_url %>\n          auth_key: <% task().result.auth_key %>\n\n      set_get_swift_information_failed:\n        on-complete:\n          - send_message\n        publish:\n          status: FAILED\n          message: <% task(get_swift_information).result %>\n\n      do_log_upload:\n        action: tripleo.deployment.config\n        on-success: send_message\n        on-error: set_do_log_upload_failed\n        input:\n          server_id: <% $.server_uuid %>\n          name: \"upload_logs\"\n          config: |\n            #!/bin/bash\n            CONTAINER_URL=\"<% $.container_url %>\"\n            TOKEN=\"<% $.auth_key %>\"\n            SOS_DIR=\"<% $.sos_dir %>\"\n            for FILE in $(find $SOS_DIR -type f); do\n              FILENAME=$(basename $FILE)\n              curl -X PUT -i -H \"X-Auth-Token: $TOKEN\" -T $FILE $CONTAINER_URL/$FILENAME\n              if [ $? -eq 0 ]; then\n                rm -f $FILE\n              fi\n            done\n          group: \"script\"\n        publish:\n          message: \"Uploaded logs from <% $.server_name %>\"\n\n      set_do_log_upload_failed:\n        on-complete:\n          - send_message\n        publish:\n          status: FAILED\n          message: <% tag(do_log_upload).result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.upload_logs') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n  create_container:\n    description: >\n      This work flow is used to check if the container exists and creates it\n      if it does not exist.\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_container:\n        action: swift.head_container container=<% $.container %>\n        on-success: send_message\n        on-error: create_container\n\n      create_container:\n        action: swift.put_container\n        input:\n          container: <% $.container %>\n          headers:\n            x-container-meta-usage-tripleo: support\n        on-success: send_message\n        on-error: set_create_container_failed\n\n      set_create_container_failed:\n        on-complete:\n          - send_message\n        publish:\n          type: tripleo.support.v1.create_container.create_container\n          status: FAILED\n          message: <% task(create_container).result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.create_container') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n  delete_container:\n    description: >\n      This workflow deletes all the objects in a provided swift container and\n      then removes the container itself from the undercloud.\n    input:\n      - container\n      - concurrency: 5\n      - timeout: 900\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      # actions\n      check_container:\n        action: swift.head_container container=<% $.container %>\n        on-success: list_objects\n        on-error: set_check_container_failure\n\n      set_check_container_failure:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          type: tripleo.support.v1.delete_container.check_container\n          message: <% task(check_container).result %>\n\n      list_objects:\n        action: swift.get_container container=<% $.container %>\n        on-success: delete_objects\n        on-error: set_list_objects_failure\n        publish:\n          log_objects: <% task().result[1] %>\n\n      set_list_objects_failure:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          type: tripleo.support.v1.delete_container.list_objects\n          message: <% task(list_objects).result %>\n\n      delete_objects:\n        action: swift.delete_object\n        concurrency: <% $.concurrency %>\n        timeout: <% $.timeout %>\n        with-items: object in <% $.log_objects %>\n        input:\n          container: <% $.container %>\n          obj: <% $.object.name %>\n        on-success: remove_container\n        on-error: set_delete_objects_failure\n\n      set_delete_objects_failure:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          type: tripleo.support.v1.delete_container.delete_objects\n          message: <% task(delete_objects).result %>\n\n      remove_container:\n        action: swift.delete_container container=<% $.container %>\n        on-success: send_message\n        on-error: set_remove_container_failure\n\n      set_remove_container_failure:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          type: tripleo.support.v1.delete_container.remove_container\n          message: <% task(remove_container).result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        wait-before: 5\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.delete_container') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n  fetch_logs:\n    description: >\n      This workflow creates a container on the undercloud, executes the log\n      collection on the servers whose names match the provided server_name, and\n      executes the log upload process on all the servers to the container on\n      the undercloud.\n    input:\n      - server_name\n      - container\n      - concurrency: 5\n      - timeout: 1800\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      # actions\n      create_container:\n        workflow: tripleo.support.v1.create_container\n        on-success: get_servers_matching\n        on-error: set_create_container_failed\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n\n      set_create_container_failed:\n          on-complete: send_message\n          publish:\n            type: tripleo.support.v1.fetch_logs.create_container\n            status: FAILED\n            message: <% task(create_container).result %>\n\n      get_servers_matching:\n        action: nova.servers_list\n        on-success: collect_logs_on_servers\n        publish:\n          servers_with_name: <% task().result._info.where($.name.indexOf(execution().input.server_name) > -1) %>\n\n      collect_logs_on_servers:\n        workflow: tripleo.support.v1.collect_logs\n        timeout: <% $.timeout %>\n        on-success: upload_logs_on_servers\n        on-error: set_collect_logs_on_servers_failed\n        input:\n          server_name: <% $.server_name %>\n          queue_name: <% $.queue_name %>\n\n      set_collect_logs_on_servers_failed:\n          on-complete: send_message\n          publish:\n            type: tripleo.support.v1.fetch_logs.collect_logs_on_servers\n            status: FAILED\n            message: <% task(collect_logs_on_servers).result %>\n\n      upload_logs_on_servers:\n        on-success: send_message\n        on-error: set_upload_logs_on_servers_failed\n        with-items: server in <% $.servers_with_name %>\n        concurrency: <% $.concurrency %>\n        workflow: tripleo.support.v1.upload_logs\n        input:\n          server_name: <% $.server.name %>\n          server_uuid: <% $.server.id %>\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n\n      set_upload_logs_on_servers_failed:\n          on-complete: send_message\n          publish:\n            type: tripleo.support.v1.fetch_logs.upload_logs\n            status: FAILED\n            message: <% task(upload_logs_on_servers).result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.fetch_logs') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n", "name": "tripleo.support.v1", "tags": [], "created_at": "2018-12-03 11:19:12", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "dd244f84-80aa-4511-b2cf-78211a3c1c50"}

2018-12-03 06:19:12,552 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:19:12,553 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.swift_ring.v1
description: Rebalance and distribute Swift rings using Ansible


workflows:
  rebalance:
    tags:
      - tripleo-common-managed

    tasks:
      get_private_key:
        action: tripleo.validations.get_privkey
        on-success: deploy_rings

      deploy_rings:
        action: tripleo.ansible-playbook
        publish:
          output: <% task().result %>
        input:
          ssh_private_key: <% task(get_private_key).result %>
          ssh_common_args: '-o StrictHostKeyChecking=no'
          ssh_extra_args: '-o UserKnownHostsFile=/dev/null'
          verbosity: 1
          remote_user: heat-admin
          become: true
          become_user: root
          playbook: /usr/share/tripleo-common/playbooks/swift_ring_rebalance.yaml
          inventory: /usr/bin/tripleo-ansible-inventory
          use_openstack_credentials: true
'
2018-12-03 06:19:12,658 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 1140
2018-12-03 06:19:12,659 DEBUG: RESP: [201] Content-Length: 1140 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:12 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.swift_ring.v1\ndescription: Rebalance and distribute Swift rings using Ansible\n\n\nworkflows:\n  rebalance:\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        on-success: deploy_rings\n\n      deploy_rings:\n        action: tripleo.ansible-playbook\n        publish:\n          output: <% task().result %>\n        input:\n          ssh_private_key: <% task(get_private_key).result %>\n          ssh_common_args: '-o StrictHostKeyChecking=no'\n          ssh_extra_args: '-o UserKnownHostsFile=/dev/null'\n          verbosity: 1\n          remote_user: heat-admin\n          become: true\n          become_user: root\n          playbook: /usr/share/tripleo-common/playbooks/swift_ring_rebalance.yaml\n          inventory: /usr/bin/tripleo-ansible-inventory\n          use_openstack_credentials: true\n", "name": "tripleo.swift_ring.v1", "tags": [], "created_at": "2018-12-03 11:19:12", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "6f3140be-f8aa-470b-a519-695eaae1d6c1"}

2018-12-03 06:19:12,659 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:19:12,660 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.swift_rings_backup.v1
description: TripleO Swift Rings backup container Deployment Workflow v1

workflows:

  create_swift_rings_backup_container_plan:
    description: >
      This plan ensures existence of container for Swift Rings backup.
    input:
      - container
      - queue_name: tripleo
    tags:
      - tripleo-common-managed
    tasks:

      swift_rings_container:
        publish:
          swift_rings_container: "<% $.container %>-swift-rings"
          swift_rings_tar: "swift-rings.tar.gz"
        on-complete: check_container

      check_container:
        action: swift.head_container container=<% $.swift_rings_container %>
        on-success: get_tempurl
        on-error: create_container

      create_container:
        action: swift.put_container container=<% $.swift_rings_container %>
        on-error: set_create_container_failed
        on-success: get_tempurl

      get_tempurl:
        action: tripleo.swift.tempurl
        on-success: set_get_tempurl
        input:
          container: <% $.swift_rings_container %>
          obj: <% $.swift_rings_tar %>

      set_get_tempurl:
        action: tripleo.parameters.update
        input:
          parameters:
            SwiftRingGetTempurl: <% task(get_tempurl).result %>
          container: <% $.container %>
        on-success: put_tempurl

      put_tempurl:
        action: tripleo.swift.tempurl
        on-success: set_put_tempurl
        input:
          container: <% $.swift_rings_container %>
          obj: <% $.swift_rings_tar %>
          method: "PUT"

      set_put_tempurl:
        action: tripleo.parameters.update
        input:
          parameters:
            SwiftRingPutTempurl: <% task(put_tempurl).result %>
          container: <% $.container %>
        on-success: set_status_success
        on-error: set_put_tempurl_failed

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task(set_put_tempurl).result %>

      set_put_tempurl_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(set_put_tempurl).result %>

      set_create_container_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(create_container).result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-03 06:19:13,094 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 3154
2018-12-03 06:19:13,095 DEBUG: RESP: [201] Content-Length: 3154 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:13 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.swift_rings_backup.v1\ndescription: TripleO Swift Rings backup container Deployment Workflow v1\n\nworkflows:\n\n  create_swift_rings_backup_container_plan:\n    description: >\n      This plan ensures existence of container for Swift Rings backup.\n    input:\n      - container\n      - queue_name: tripleo\n    tags:\n      - tripleo-common-managed\n    tasks:\n\n      swift_rings_container:\n        publish:\n          swift_rings_container: \"<% $.container %>-swift-rings\"\n          swift_rings_tar: \"swift-rings.tar.gz\"\n        on-complete: check_container\n\n      check_container:\n        action: swift.head_container container=<% $.swift_rings_container %>\n        on-success: get_tempurl\n        on-error: create_container\n\n      create_container:\n        action: swift.put_container container=<% $.swift_rings_container %>\n        on-error: set_create_container_failed\n        on-success: get_tempurl\n\n      get_tempurl:\n        action: tripleo.swift.tempurl\n        on-success: set_get_tempurl\n        input:\n          container: <% $.swift_rings_container %>\n          obj: <% $.swift_rings_tar %>\n\n      set_get_tempurl:\n        action: tripleo.parameters.update\n        input:\n          parameters:\n            SwiftRingGetTempurl: <% task(get_tempurl).result %>\n          container: <% $.container %>\n        on-success: put_tempurl\n\n      put_tempurl:\n        action: tripleo.swift.tempurl\n        on-success: set_put_tempurl\n        input:\n          container: <% $.swift_rings_container %>\n          obj: <% $.swift_rings_tar %>\n          method: \"PUT\"\n\n      set_put_tempurl:\n        action: tripleo.parameters.update\n        input:\n          parameters:\n            SwiftRingPutTempurl: <% task(put_tempurl).result %>\n          container: <% $.container %>\n        on-success: set_status_success\n        on-error: set_put_tempurl_failed\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(set_put_tempurl).result %>\n\n      set_put_tempurl_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(set_put_tempurl).result %>\n\n      set_create_container_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_container).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.swift_rings_backup.v1", "tags": [], "created_at": "2018-12-03 11:19:13", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "846f1be7-0559-43c1-9c16-b7e395da8430"}

2018-12-03 06:19:13,095 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:19:13,096 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.undercloud_backup.v1
description: TripleO Undercloud backup workflows

workflows:

  backup:
    description: This workflow will launch the Undercloud backup
    tags:
      - tripleo-common-managed
    input:
      - sources_path: '/home/stack/'
      - queue_name: tripleo
    tasks:
      # Action to know if there is enough available space
      # to run the Undercloud backup
      get_free_space:
        action: tripleo.undercloud.get_free_space
        publish:
            status: SUCCESS
            message: <% task().result %>
            free_space: <% task().result %>
        on-success: create_backup_dir
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      # We create a temp directory to store the Undercloud
      # backup
      create_backup_dir:
        action: tripleo.undercloud.create_backup_dir
        publish:
            status: SUCCESS
            message: <% task().result %>
            backup_path: <% task().result %>
        on-success: get_database_credentials
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      # The Undercloud database password for the root
      # user is stored in a Mistral environment, we
      # need the password in order to run the database dump
      get_database_credentials:
        action: mistral.environments_get name='tripleo.undercloud-config'
        publish:
            status: SUCCESS
            message: <% task().result %>
            undercloud_db_password: <% task(get_database_credentials).result.variables.undercloud_db_password %>
        on-success: create_database_backup
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      # Run the DB dump of all the databases and store the result
      # in the temporary folder
      create_database_backup:
        input:
            path: <% $.backup_path.path %>
            dbuser: root
            dbpassword: <% $.undercloud_db_password %>
        action: tripleo.undercloud.create_database_backup
        publish:
            status: SUCCESS
            message: <% task().result %>
        on-success: create_fs_backup
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      # This action will run the fs backup
      create_fs_backup:
        input:
            sources_path: <% $.sources_path %>
            path: <% $.backup_path.path %>
        action: tripleo.undercloud.create_file_system_backup
        publish:
            status: SUCCESS
            message: <% task().result %>
        on-success: upload_backup
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      # This action will push the backup to swift
      upload_backup:
        input:
            backup_path: <% $.backup_path.path %>
        action: tripleo.undercloud.upload_backup_to_swift
        publish:
            status: SUCCESS
            message: <% task().result %>
        on-success: cleanup_backup
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      # This action will remove the backup temp folder
      cleanup_backup:
        input:
            path: <% $.backup_path.path %>
        action: tripleo.undercloud.remove_temp_dir
        publish:
            status: SUCCESS
            message: <% task().result %>
        on-success: send_message
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      # Sending a message to show that the backup finished
      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.undercloud_backup.v1.launch
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                execution: <% execution() %>
                message: <% $.get('message', '') %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-03 06:19:13,633 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 4669
2018-12-03 06:19:13,635 DEBUG: RESP: [201] Content-Length: 4669 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:13 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.undercloud_backup.v1\ndescription: TripleO Undercloud backup workflows\n\nworkflows:\n\n  backup:\n    description: This workflow will launch the Undercloud backup\n    tags:\n      - tripleo-common-managed\n    input:\n      - sources_path: '/home/stack/'\n      - queue_name: tripleo\n    tasks:\n      # Action to know if there is enough available space\n      # to run the Undercloud backup\n      get_free_space:\n        action: tripleo.undercloud.get_free_space\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n            free_space: <% task().result %>\n        on-success: create_backup_dir\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # We create a temp directory to store the Undercloud\n      # backup\n      create_backup_dir:\n        action: tripleo.undercloud.create_backup_dir\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n            backup_path: <% task().result %>\n        on-success: get_database_credentials\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # The Undercloud database password for the root\n      # user is stored in a Mistral environment, we\n      # need the password in order to run the database dump\n      get_database_credentials:\n        action: mistral.environments_get name='tripleo.undercloud-config'\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n            undercloud_db_password: <% task(get_database_credentials).result.variables.undercloud_db_password %>\n        on-success: create_database_backup\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # Run the DB dump of all the databases and store the result\n      # in the temporary folder\n      create_database_backup:\n        input:\n            path: <% $.backup_path.path %>\n            dbuser: root\n            dbpassword: <% $.undercloud_db_password %>\n        action: tripleo.undercloud.create_database_backup\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n        on-success: create_fs_backup\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # This action will run the fs backup\n      create_fs_backup:\n        input:\n            sources_path: <% $.sources_path %>\n            path: <% $.backup_path.path %>\n        action: tripleo.undercloud.create_file_system_backup\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n        on-success: upload_backup\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # This action will push the backup to swift\n      upload_backup:\n        input:\n            backup_path: <% $.backup_path.path %>\n        action: tripleo.undercloud.upload_backup_to_swift\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n        on-success: cleanup_backup\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # This action will remove the backup temp folder\n      cleanup_backup:\n        input:\n            path: <% $.backup_path.path %>\n        action: tripleo.undercloud.remove_temp_dir\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n        on-success: send_message\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # Sending a message to show that the backup finished\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.undercloud_backup.v1.launch\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                execution: <% execution() %>\n                message: <% $.get('message', '') %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.undercloud_backup.v1", "tags": [], "created_at": "2018-12-03 11:19:13", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "8e569687-eb18-4386-9230-65effd901b25"}

2018-12-03 06:19:13,635 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:19:13,636 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '---
version: '2.0'
name: tripleo.validations.v1
description: TripleO Validations Workflows v1

workflows:

  run_validation:
    input:
      - validation_name
      - plan: overcloud
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      notify_running:
        on-complete: run_validation
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.run_validation
              payload:
                validation_name: <% $.validation_name %>
                plan: <% $.plan %>
                status: RUNNING
                execution: <% execution() %>

      run_validation:
        on-success: send_message
        on-error: set_status_failed
        action: tripleo.validations.run_validation validation=<% $.validation_name %> plan=<% $.plan %>
        publish:
          status: SUCCESS
          stdout: <% task().result.stdout %>
          stderr: <% task().result.stderr %>

      set_status_failed:
        on-complete: send_message
        publish:
          status: FAILED
          stdout: <% task(run_validation).result.stdout %>
          stderr: <% task(run_validation).result.stderr %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.run_validation
              payload:
                validation_name: <% $.validation_name %>
                plan: <% $.plan %>
                status: <% $.get('status', 'SUCCESS') %>
                stdout: <% $.stdout %>
                stderr: <% $.stderr %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  run_validations:
    input:
      - validation_names: []
      - plan: overcloud
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      notify_running:
        on-complete: run_validations
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.run_validations
              payload:
                validation_names: <% $.validation_names %>
                plan: <% $.plan %>
                status: RUNNING
                execution: <% execution() %>

      run_validations:
        on-success: send_message
        on-error: set_status_failed
        workflow: tripleo.validations.v1.run_validation validation_name=<% $.validation %> plan=<% $.plan %> queue_name=<% $.queue_name %>
        with-items: validation in <% $.validation_names %>
        publish:
          status: SUCCESS

      set_status_failed:
        on-complete: send_message
        publish:
          status: FAILED

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.run_validations
              payload:
                validation_names: <% $.validation_names %>
                plan: <% $.plan %>
                status: <% $.get('status', 'SUCCESS') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  run_groups:
    input:
      - group_names: []
      - plan: overcloud
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      find_validations:
        on-success: notify_running
        action: tripleo.validations.list_validations groups=<% $.group_names %>
        publish:
          validations: <% task().result %>

      notify_running:
        on-complete: run_validation_group
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.run_validations
              payload:
                group_names: <% $.group_names %>
                validation_names: <% $.validations.id %>
                plan: <% $.plan %>
                status: RUNNING
                execution: <% execution() %>

      run_validation_group:
        on-success: send_message
        on-error: set_status_failed
        workflow: tripleo.validations.v1.run_validation validation_name=<% $.validation %> plan=<% $.plan %> queue_name=<% $.queue_name %>
        with-items: validation in <% $.validations.id %>
        publish:
          status: SUCCESS

      set_status_failed:
        on-complete: send_message
        publish:
          status: FAILED

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.run_groups
              payload:
                group_names: <% $.group_names %>
                validation_names: <% $.validations.id %>
                plan: <% $.plan %>
                status: <% $.get('status', 'SUCCESS') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  list:
    input:
      - group_names: []
    tags:
      - tripleo-common-managed
    tasks:
      find_validations:
        action: tripleo.validations.list_validations groups=<% $.group_names %>

  list_groups:
    tags:
      - tripleo-common-managed
    tasks:
      find_groups:
        action: tripleo.validations.list_groups

  add_validation_ssh_key_parameter:
    input:
     - container
     - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      test_validations_enabled:
        action: tripleo.validations.enabled
        on-success: get_pubkey
        on-error: unset_validation_key_parameter

      get_pubkey:
        action: tripleo.validations.get_pubkey
        on-success: set_validation_key_parameter
        publish:
          pubkey: <% task().result %>

      set_validation_key_parameter:
        action: tripleo.parameters.update
        input:
          parameters:
            node_admin_extra_ssh_keys: <% $.pubkey %>
          container: <% $.container %>

      # NOTE(shadower): We need to clear keys from a previous deployment
      unset_validation_key_parameter:
        action: tripleo.parameters.update
        input:
          parameters:
            node_admin_extra_ssh_keys: ""
          container: <% $.container %>

  copy_ssh_key:
    input:
      # FIXME: we should stop using heat-admin as e.g. split-stack
      # environments (where Nova didn't create overcloud nodes) don't
      # have it present
      - overcloud_admin: heat-admin
      - queue_name: tripleo
    tags:
      - tripleo-common-managed
    tasks:
      get_servers:
        action: nova.servers_list
        on-success: get_pubkey
        publish:
          servers: <% task().result._info %>

      get_pubkey:
        action: tripleo.validations.get_pubkey
        on-success: deploy_ssh_key
        publish:
          pubkey: <% task().result %>

      deploy_ssh_key:
        workflow: tripleo.deployment.v1.deploy_on_server
        with-items: server in <% $.servers %>
        input:
          server_name: <% $.server.name %>
          server_uuid: <% $.server.id %>
          config: |
            #!/bin/bash
            if ! grep "<% $.pubkey %>" /home/<% $.overcloud_admin %>/.ssh/authorized_keys; then
              echo "<% $.pubkey %>" >> /home/<% $.overcloud_admin %>/.ssh/authorized_keys
            fi
          config_name: copy_ssh_key
          group: script
          queue_name: <% $.queue_name %>

  check_boot_images:
    input:
      - deploy_kernel_name: 'bm-deploy-kernel'
      - deploy_ramdisk_name: 'bm-deploy-ramdisk'
      - run_validations: true
      - queue_name: tripleo
    output:
      errors: <% $.errors %>
      warnings: <% $.warnings %>
      kernel_id: <% $.kernel_id %>
      ramdisk_id: <% $.ramdisk_id %>
    tags:
      - tripleo-common-managed
    tasks:
      check_run_validations:
        on-complete:
          - get_images: <% $.run_validations %>
          - send_message: <% not $.run_validations %>

      get_images:
        action: glance.images_list
        on-success: check_images
        publish:
          images: <% task().result %>

      check_images:
        action: tripleo.validations.check_boot_images
        input:
          images: <% $.images %>
          deploy_kernel_name: <% $.deploy_kernel_name %>
          deploy_ramdisk_name: <% $.deploy_ramdisk_name %>
        on-success: send_message
        publish:
          kernel_id: <% task().result.kernel_id %>
          ramdisk_id: <% task().result.ramdisk_id %>
          warnings: <% task().result.warnings %>
          errors: <% task().result.errors %>
        on-error: send_message
        publish-on-error:
          kernel_id: <% task().result.kernel_id %>
          ramdisk_id: <% task().result.ramdisk_id %>
          warnings: <% task().result.warnings %>
          errors: <% task().result.errors %>
          status: FAILED
          message: <% task().result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.check_boot_images
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                kernel_id: <% $.kernel_id %>
                ramdisk_id: <% $.ramdisk_id %>
                errors: <% $.errors %>
                warnings: <% $.warnings %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  collect_flavors:
    input:
      - roles_info: {}
      - run_validations: true
      - queue_name: tripleo
    output:
      errors: <% $.errors %>
      warnings: <% $.warnings %>
      flavors: <% $.flavors %>

    tags:
      - tripleo-common-managed

    tasks:
      check_run_validations:
        on-complete:
          - check_flavors: <% $.run_validations %>
          - send_message: <% not $.run_validations %>

      check_flavors:
        action: tripleo.validations.check_flavors
        input:
          roles_info: <% $.roles_info %>
        on-success: send_message
        publish:
          flavors: <% task().result.flavors %>
          errors: <% task().result.errors %>
          warnings: <% task().result.warnings %>
        on-error: send_message
        publish-on-error:
          flavors: {}
          errors: <% task().result.errors %>
          warnings: <% task().result.warnings %>
          status: FAILED
          message: <% task().result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.collect_flavors
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                flavors: <% $.flavors %>
                errors: <% $.errors %>
                warnings: <% $.warnings %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  check_ironic_boot_configuration:
    input:
      - kernel_id: null
      - ramdisk_id: null
      - run_validations: true
      - queue_name: tripleo
    output:
      errors: <% $.errors %>
      warnings: <% $.warnings %>

    tags:
      - tripleo-common-managed

    tasks:
      check_run_validations:
        on-complete:
          - get_ironic_nodes: <% $.run_validations %>
          - send_message: <% not $.run_validations %>

      get_ironic_nodes:
        action: ironic.node_list
        input:
          provision_state: available
          maintenance: false
          detail: true
        on-success: check_node_boot_configuration
        publish:
          nodes: <% task().result %>
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      check_node_boot_configuration:
        action: tripleo.validations.check_node_boot_configuration
        input:
          node: <% $.node %>
          kernel_id: <% $.kernel_id %>
          ramdisk_id: <% $.ramdisk_id %>
        with-items: node in <% $.nodes %>
        on-success: send_message
        publish:
          errors: <% task().result.errors.flatten() %>
          warnings: <% task().result.warnings.flatten() %>
        on-error: send_message
        publish-on-error:
          errors: <% task().result.errors.flatten() %>
          warnings: <% task().result.warnings.flatten() %>
          status: FAILED
          message: <% task().result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.check_ironic_boot_configuration
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                errors: <% $.errors %>
                warnings: <% $.warnings %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  verify_profiles:
    input:
      - flavors: []
      - run_validations: true
      - queue_name: tripleo
    output:
      errors: <% $.errors %>
      warnings: <% $.warnings %>

    tags:
      - tripleo-common-managed

    tasks:
      check_run_validations:
        on-complete:
          - get_ironic_nodes: <% $.run_validations %>
          - send_message: <% not $.run_validations %>

      get_ironic_nodes:
        action: ironic.node_list
        input:
          maintenance: false
          detail: true
        on-success: verify_profiles
        publish:
          nodes: <% task().result %>
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      verify_profiles:
        action: tripleo.validations.verify_profiles
        input:
          nodes: <% $.nodes %>
          flavors: <% $.flavors %>
        on-success: send_message
        publish:
          errors: <% task().result.errors %>
          warnings: <% task().result.warnings %>
        on-error: send_message
        publish-on-error:
          errors: <% task().result.errors %>
          warnings: <% task().result.warnings %>
          status: FAILED
          message: <% task().result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.verify_profiles
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                errors: <% $.errors %>
                warnings: <% $.warnings %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  check_default_nodes_count:
    input:
      - stack_id: overcloud
      - parameters: {}
      - default_role_counts: {}
      - run_validations: true
      - queue_name: tripleo
    output:
      statistics: <% $.statistics %>
      errors: <% $.errors %>
      warnings: <% $.warnings %>

    tags:
      - tripleo-common-managed

    tasks:
      check_run_validations:
        on-complete:
          - get_hypervisor_statistics: <% $.run_validations %>
          - send_message: <% not $.run_validations %>

      get_hypervisor_statistics:
        action: nova.hypervisors_statistics
        on-success: get_stack
        publish:
          statistics: <% task().result %>
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>
          errors: []
          warnings: []
          statistics: null

      get_stack:
        action: heat.stacks_get
        input:
          stack_id: <% $.stack_id %>
        on-success: get_associated_nodes
        publish:
          stack: <% task().result %>
        on-error: get_associated_nodes
        publish-on-error:
          stack: null

      get_associated_nodes:
        action: ironic.node_list
        input:
          associated: true
        on-success: get_available_nodes
        publish:
          associated_nodes: <% task().result %>
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>
          errors: []
          warnings: []

      get_available_nodes:
        action: ironic.node_list
        input:
          provision_state: available
          associated: false
          maintenance: false
        on-success: check_nodes_count
        publish:
          available_nodes: <% task().result %>
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>
          errors: []
          warnings: []

      check_nodes_count:
        action: tripleo.validations.check_nodes_count
        input:
          statistics: <% $.statistics %>
          stack: <% $.stack %>
          associated_nodes: <% $.associated_nodes %>
          available_nodes: <% $.available_nodes %>
          parameters: <% $.parameters %>
          default_role_counts: <% $.default_role_counts %>
        on-success: send_message
        publish:
          errors: <% task().result.errors %>
          warnings: <% task().result.warnings %>
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>
          statistics: null
          errors: <% task().result.errors %>
          warnings: <% task().result.warnings %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.check_hypervisor_stats
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                statistics: <% $.statistics %>
                errors: <% $.errors %>
                warnings: <% $.warnings %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  check_pre_deployment_validations:
    input:
      - deploy_kernel_name: 'bm-deploy-kernel'
      - deploy_ramdisk_name: 'bm-deploy-ramdisk'
      - roles_info: {}
      - stack_id: overcloud
      - parameters: {}
      - default_role_counts: {}
      - run_validations: true
      - queue_name: tripleo

    output:
      errors: <% $.errors %>
      warnings: <% $.warnings %>
      kernel_id: <% $.kernel_id %>
      ramdisk_id: <% $.ramdisk_id %>
      flavors: <% $.flavors %>
      statistics: <% $.statistics %>
    tags:
      - tripleo-common-managed
    tasks:
      init_messages:
        on-success: check_boot_images
        publish:
          errors: []
          warnings: []

      check_boot_images:
        workflow: check_boot_images
        input:
          deploy_kernel_name: <% $.deploy_kernel_name %>
          deploy_ramdisk_name: <% $.deploy_ramdisk_name %>
          run_validations: <% $.run_validations %>
          queue_name: <% $.queue_name %>
        publish:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          kernel_id: <% task().result.get('kernel_id') %>
          ramdisk_id: <% task().result.get('ramdisk_id') %>
        publish-on-error:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          kernel_id: <% task().result.get('kernel_id') %>
          ramdisk_id: <% task().result.get('ramdisk_id') %>
          status: FAILED
        on-success: collect_flavors
        on-error: collect_flavors

      collect_flavors:
        workflow: collect_flavors
        input:
          roles_info: <% $.roles_info %>
          run_validations: <% $.run_validations %>
          queue_name: <% $.queue_name %>
        publish:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          flavors: <% task().result.get('flavors') %>
        publish-on-error:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          flavors: <% task().result.get('flavors') %>
          status: FAILED
        on-success: check_ironic_boot_configuration
        on-error: check_ironic_boot_configuration

      check_ironic_boot_configuration:
        workflow: check_ironic_boot_configuration
        input:
          kernel_id: <% $.kernel_id %>
          ramdisk_id: <% $.ramdisk_id %>
          run_validations: <% $.run_validations %>
          queue_name: <% $.queue_name %>
        publish:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
        publish-on-error:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          status: FAILED
        on-success: check_default_nodes_count
        on-error: check_default_nodes_count

      check_default_nodes_count:
        workflow: check_default_nodes_count
        # ironic-nova sync happens once in two minutes
        retry: count=12 delay=10
        input:
          stack_id: <% $.stack_id %>
          parameters: <% $.parameters %>
          default_role_counts: <% $.default_role_counts %>
          run_validations: <% $.run_validations %>
          queue_name: <% $.queue_name %>
        publish:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          statistics: <% task().result.get('statistics') %>
        publish-on-error:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          statistics: <% task().result.get('statistics') %>
          status: FAILED
        on-success: verify_profiles
        # Do not confuse user with info about profiles if the nodes
        # count is off in the first place. Skip directly to
        # send_message. (bug 1703942)
        on-error: send_message

      verify_profiles:
        workflow: verify_profiles
        input:
          flavors: <% $.flavors %>
          run_validations: <% $.run_validations %>
          queue_name: <% $.queue_name %>
        publish:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
        publish-on-error:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          status: FAILED
        on-success: send_message
        on-error: send_message

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.check_hypervisor_stats
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                kernel_id: <% $.kernel_id %>
                ramdisk_id: <% $.ramdisk_id %>
                flavors: <% $.flavors %>
                statistics: <% $.statistics %>
                errors: <% $.errors %>
                warnings: <% $.warnings %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-03 06:19:16,058 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 25434
2018-12-03 06:19:16,098 DEBUG: RESP: [201] Content-Length: 25434 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:16 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.validations.v1\ndescription: TripleO Validations Workflows v1\n\nworkflows:\n\n  run_validation:\n    input:\n      - validation_name\n      - plan: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      notify_running:\n        on-complete: run_validation\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validation\n              payload:\n                validation_name: <% $.validation_name %>\n                plan: <% $.plan %>\n                status: RUNNING\n                execution: <% execution() %>\n\n      run_validation:\n        on-success: send_message\n        on-error: set_status_failed\n        action: tripleo.validations.run_validation validation=<% $.validation_name %> plan=<% $.plan %>\n        publish:\n          status: SUCCESS\n          stdout: <% task().result.stdout %>\n          stderr: <% task().result.stderr %>\n\n      set_status_failed:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          stdout: <% task(run_validation).result.stdout %>\n          stderr: <% task(run_validation).result.stderr %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validation\n              payload:\n                validation_name: <% $.validation_name %>\n                plan: <% $.plan %>\n                status: <% $.get('status', 'SUCCESS') %>\n                stdout: <% $.stdout %>\n                stderr: <% $.stderr %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  run_validations:\n    input:\n      - validation_names: []\n      - plan: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      notify_running:\n        on-complete: run_validations\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validations\n              payload:\n                validation_names: <% $.validation_names %>\n                plan: <% $.plan %>\n                status: RUNNING\n                execution: <% execution() %>\n\n      run_validations:\n        on-success: send_message\n        on-error: set_status_failed\n        workflow: tripleo.validations.v1.run_validation validation_name=<% $.validation %> plan=<% $.plan %> queue_name=<% $.queue_name %>\n        with-items: validation in <% $.validation_names %>\n        publish:\n          status: SUCCESS\n\n      set_status_failed:\n        on-complete: send_message\n        publish:\n          status: FAILED\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validations\n              payload:\n                validation_names: <% $.validation_names %>\n                plan: <% $.plan %>\n                status: <% $.get('status', 'SUCCESS') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  run_groups:\n    input:\n      - group_names: []\n      - plan: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      find_validations:\n        on-success: notify_running\n        action: tripleo.validations.list_validations groups=<% $.group_names %>\n        publish:\n          validations: <% task().result %>\n\n      notify_running:\n        on-complete: run_validation_group\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validations\n              payload:\n                group_names: <% $.group_names %>\n                validation_names: <% $.validations.id %>\n                plan: <% $.plan %>\n                status: RUNNING\n                execution: <% execution() %>\n\n      run_validation_group:\n        on-success: send_message\n        on-error: set_status_failed\n        workflow: tripleo.validations.v1.run_validation validation_name=<% $.validation %> plan=<% $.plan %> queue_name=<% $.queue_name %>\n        with-items: validation in <% $.validations.id %>\n        publish:\n          status: SUCCESS\n\n      set_status_failed:\n        on-complete: send_message\n        publish:\n          status: FAILED\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_groups\n              payload:\n                group_names: <% $.group_names %>\n                validation_names: <% $.validations.id %>\n                plan: <% $.plan %>\n                status: <% $.get('status', 'SUCCESS') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list:\n    input:\n      - group_names: []\n    tags:\n      - tripleo-common-managed\n    tasks:\n      find_validations:\n        action: tripleo.validations.list_validations groups=<% $.group_names %>\n\n  list_groups:\n    tags:\n      - tripleo-common-managed\n    tasks:\n      find_groups:\n        action: tripleo.validations.list_groups\n\n  add_validation_ssh_key_parameter:\n    input:\n     - container\n     - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      test_validations_enabled:\n        action: tripleo.validations.enabled\n        on-success: get_pubkey\n        on-error: unset_validation_key_parameter\n\n      get_pubkey:\n        action: tripleo.validations.get_pubkey\n        on-success: set_validation_key_parameter\n        publish:\n          pubkey: <% task().result %>\n\n      set_validation_key_parameter:\n        action: tripleo.parameters.update\n        input:\n          parameters:\n            node_admin_extra_ssh_keys: <% $.pubkey %>\n          container: <% $.container %>\n\n      # NOTE(shadower): We need to clear keys from a previous deployment\n      unset_validation_key_parameter:\n        action: tripleo.parameters.update\n        input:\n          parameters:\n            node_admin_extra_ssh_keys: \"\"\n          container: <% $.container %>\n\n  copy_ssh_key:\n    input:\n      # FIXME: we should stop using heat-admin as e.g. split-stack\n      # environments (where Nova didn't create overcloud nodes) don't\n      # have it present\n      - overcloud_admin: heat-admin\n      - queue_name: tripleo\n    tags:\n      - tripleo-common-managed\n    tasks:\n      get_servers:\n        action: nova.servers_list\n        on-success: get_pubkey\n        publish:\n          servers: <% task().result._info %>\n\n      get_pubkey:\n        action: tripleo.validations.get_pubkey\n        on-success: deploy_ssh_key\n        publish:\n          pubkey: <% task().result %>\n\n      deploy_ssh_key:\n        workflow: tripleo.deployment.v1.deploy_on_server\n        with-items: server in <% $.servers %>\n        input:\n          server_name: <% $.server.name %>\n          server_uuid: <% $.server.id %>\n          config: |\n            #!/bin/bash\n            if ! grep \"<% $.pubkey %>\" /home/<% $.overcloud_admin %>/.ssh/authorized_keys; then\n              echo \"<% $.pubkey %>\" >> /home/<% $.overcloud_admin %>/.ssh/authorized_keys\n            fi\n          config_name: copy_ssh_key\n          group: script\n          queue_name: <% $.queue_name %>\n\n  check_boot_images:\n    input:\n      - deploy_kernel_name: 'bm-deploy-kernel'\n      - deploy_ramdisk_name: 'bm-deploy-ramdisk'\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n      kernel_id: <% $.kernel_id %>\n      ramdisk_id: <% $.ramdisk_id %>\n    tags:\n      - tripleo-common-managed\n    tasks:\n      check_run_validations:\n        on-complete:\n          - get_images: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      get_images:\n        action: glance.images_list\n        on-success: check_images\n        publish:\n          images: <% task().result %>\n\n      check_images:\n        action: tripleo.validations.check_boot_images\n        input:\n          images: <% $.images %>\n          deploy_kernel_name: <% $.deploy_kernel_name %>\n          deploy_ramdisk_name: <% $.deploy_ramdisk_name %>\n        on-success: send_message\n        publish:\n          kernel_id: <% task().result.kernel_id %>\n          ramdisk_id: <% task().result.ramdisk_id %>\n          warnings: <% task().result.warnings %>\n          errors: <% task().result.errors %>\n        on-error: send_message\n        publish-on-error:\n          kernel_id: <% task().result.kernel_id %>\n          ramdisk_id: <% task().result.ramdisk_id %>\n          warnings: <% task().result.warnings %>\n          errors: <% task().result.errors %>\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.check_boot_images\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                kernel_id: <% $.kernel_id %>\n                ramdisk_id: <% $.ramdisk_id %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  collect_flavors:\n    input:\n      - roles_info: {}\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n      flavors: <% $.flavors %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_run_validations:\n        on-complete:\n          - check_flavors: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      check_flavors:\n        action: tripleo.validations.check_flavors\n        input:\n          roles_info: <% $.roles_info %>\n        on-success: send_message\n        publish:\n          flavors: <% task().result.flavors %>\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n        on-error: send_message\n        publish-on-error:\n          flavors: {}\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.collect_flavors\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                flavors: <% $.flavors %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  check_ironic_boot_configuration:\n    input:\n      - kernel_id: null\n      - ramdisk_id: null\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_run_validations:\n        on-complete:\n          - get_ironic_nodes: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      get_ironic_nodes:\n        action: ironic.node_list\n        input:\n          provision_state: available\n          maintenance: false\n          detail: true\n        on-success: check_node_boot_configuration\n        publish:\n          nodes: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      check_node_boot_configuration:\n        action: tripleo.validations.check_node_boot_configuration\n        input:\n          node: <% $.node %>\n          kernel_id: <% $.kernel_id %>\n          ramdisk_id: <% $.ramdisk_id %>\n        with-items: node in <% $.nodes %>\n        on-success: send_message\n        publish:\n          errors: <% task().result.errors.flatten() %>\n          warnings: <% task().result.warnings.flatten() %>\n        on-error: send_message\n        publish-on-error:\n          errors: <% task().result.errors.flatten() %>\n          warnings: <% task().result.warnings.flatten() %>\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.check_ironic_boot_configuration\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  verify_profiles:\n    input:\n      - flavors: []\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_run_validations:\n        on-complete:\n          - get_ironic_nodes: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      get_ironic_nodes:\n        action: ironic.node_list\n        input:\n          maintenance: false\n          detail: true\n        on-success: verify_profiles\n        publish:\n          nodes: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      verify_profiles:\n        action: tripleo.validations.verify_profiles\n        input:\n          nodes: <% $.nodes %>\n          flavors: <% $.flavors %>\n        on-success: send_message\n        publish:\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n        on-error: send_message\n        publish-on-error:\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.verify_profiles\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  check_default_nodes_count:\n    input:\n      - stack_id: overcloud\n      - parameters: {}\n      - default_role_counts: {}\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      statistics: <% $.statistics %>\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_run_validations:\n        on-complete:\n          - get_hypervisor_statistics: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      get_hypervisor_statistics:\n        action: nova.hypervisors_statistics\n        on-success: get_stack\n        publish:\n          statistics: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n          errors: []\n          warnings: []\n          statistics: null\n\n      get_stack:\n        action: heat.stacks_get\n        input:\n          stack_id: <% $.stack_id %>\n        on-success: get_associated_nodes\n        publish:\n          stack: <% task().result %>\n        on-error: get_associated_nodes\n        publish-on-error:\n          stack: null\n\n      get_associated_nodes:\n        action: ironic.node_list\n        input:\n          associated: true\n        on-success: get_available_nodes\n        publish:\n          associated_nodes: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n          errors: []\n          warnings: []\n\n      get_available_nodes:\n        action: ironic.node_list\n        input:\n          provision_state: available\n          associated: false\n          maintenance: false\n        on-success: check_nodes_count\n        publish:\n          available_nodes: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n          errors: []\n          warnings: []\n\n      check_nodes_count:\n        action: tripleo.validations.check_nodes_count\n        input:\n          statistics: <% $.statistics %>\n          stack: <% $.stack %>\n          associated_nodes: <% $.associated_nodes %>\n          available_nodes: <% $.available_nodes %>\n          parameters: <% $.parameters %>\n          default_role_counts: <% $.default_role_counts %>\n        on-success: send_message\n        publish:\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n          statistics: null\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.check_hypervisor_stats\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                statistics: <% $.statistics %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  check_pre_deployment_validations:\n    input:\n      - deploy_kernel_name: 'bm-deploy-kernel'\n      - deploy_ramdisk_name: 'bm-deploy-ramdisk'\n      - roles_info: {}\n      - stack_id: overcloud\n      - parameters: {}\n      - default_role_counts: {}\n      - run_validations: true\n      - queue_name: tripleo\n\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n      kernel_id: <% $.kernel_id %>\n      ramdisk_id: <% $.ramdisk_id %>\n      flavors: <% $.flavors %>\n      statistics: <% $.statistics %>\n    tags:\n      - tripleo-common-managed\n    tasks:\n      init_messages:\n        on-success: check_boot_images\n        publish:\n          errors: []\n          warnings: []\n\n      check_boot_images:\n        workflow: check_boot_images\n        input:\n          deploy_kernel_name: <% $.deploy_kernel_name %>\n          deploy_ramdisk_name: <% $.deploy_ramdisk_name %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          kernel_id: <% task().result.get('kernel_id') %>\n          ramdisk_id: <% task().result.get('ramdisk_id') %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          kernel_id: <% task().result.get('kernel_id') %>\n          ramdisk_id: <% task().result.get('ramdisk_id') %>\n          status: FAILED\n        on-success: collect_flavors\n        on-error: collect_flavors\n\n      collect_flavors:\n        workflow: collect_flavors\n        input:\n          roles_info: <% $.roles_info %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          flavors: <% task().result.get('flavors') %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          flavors: <% task().result.get('flavors') %>\n          status: FAILED\n        on-success: check_ironic_boot_configuration\n        on-error: check_ironic_boot_configuration\n\n      check_ironic_boot_configuration:\n        workflow: check_ironic_boot_configuration\n        input:\n          kernel_id: <% $.kernel_id %>\n          ramdisk_id: <% $.ramdisk_id %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          status: FAILED\n        on-success: check_default_nodes_count\n        on-error: check_default_nodes_count\n\n      check_default_nodes_count:\n        workflow: check_default_nodes_count\n        # ironic-nova sync happens once in two minutes\n        retry: count=12 delay=10\n        input:\n          stack_id: <% $.stack_id %>\n          parameters: <% $.parameters %>\n          default_role_counts: <% $.default_role_counts %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          statistics: <% task().result.get('statistics') %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          statistics: <% task().result.get('statistics') %>\n          status: FAILED\n        on-success: verify_profiles\n        # Do not confuse user with info about profiles if the nodes\n        # count is off in the first place. Skip directly to\n        # send_message. (bug 1703942)\n        on-error: send_message\n\n      verify_profiles:\n        workflow: verify_profiles\n        input:\n          flavors: <% $.flavors %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          status: FAILED\n        on-success: send_message\n        on-error: send_message\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.check_hypervisor_stats\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                kernel_id: <% $.kernel_id %>\n                ramdisk_id: <% $.ramdisk_id %>\n                flavors: <% $.flavors %>\n                statistics: <% $.statistics %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.validations.v1", "tags": [], "created_at": "2018-12-03 11:19:15", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "799d1f1f-5bdd-49a9-b114-7aa3ebb06f1f"}

2018-12-03 06:19:16,099 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-03 06:19:16,099 INFO: Mistral workbooks configured successfully
2018-12-03 06:19:16,104 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-03 06:19:17,124 DEBUG: https://172.16.0.10:13808 "GET /v1/AUTH_f41e7348daff4254892e5cd759aa8806?format=json HTTP/1.1" 200 2
2018-12-03 06:19:17,125 DEBUG: REQ: curl -i https://172.16.0.10:13808/v1/AUTH_f41e7348daff4254892e5cd759aa8806?format=json -X GET -H "Accept-Encoding: gzip" -H "X-Auth-Token: gAAAAABcBREPZNie..."
2018-12-03 06:19:17,125 DEBUG: RESP STATUS: 200 OK
2018-12-03 06:19:17,125 DEBUG: RESP HEADERS: {u'Content-Length': u'2', u'X-Put-Timestamp': u'1543835957.12133', u'X-Account-Object-Count': u'0', u'X-Timestamp': u'1543835957.12133', u'X-Trans-Id': u'txdf14abc6215848fe93221-005c051134', u'Date': u'Mon, 03 Dec 2018 11:19:17 GMT', u'X-Account-Bytes-Used': u'0', u'X-Account-Container-Count': u'0', u'Content-Type': u'application/json; charset=utf-8', u'X-Openstack-Request-Id': u'txdf14abc6215848fe93221-005c051134'}
2018-12-03 06:19:17,125 DEBUG: RESP BODY: []
2018-12-03 06:19:17,126 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/environments/tripleo.undercloud-config -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:19:17,142 DEBUG: https://172.16.0.10:13989 "GET /v2/environments/tripleo.undercloud-config HTTP/1.1" 404 115
2018-12-03 06:19:17,143 DEBUG: RESP: [404] Content-Length: 115 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:17 GMT Connection: keep-alive 
RESP BODY: {"debuginfo": null, "faultcode": "Client", "faultstring": "Environment not found [name=tripleo.undercloud-config]"}

2018-12-03 06:19:17,144 DEBUG: Request returned failure status: 404
2018-12-03 06:19:17,144 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/environments -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"variables": "{\"undercloud_ceilometer_snmpd_password\": \"fb321565da9bc0ef27c8324166ac09e4d889dc5d\", \"undercloud_db_password\": \"a9a10b0e40e705f2cb9c9d14baf2f49e5ce94c04\"}", "name": "tripleo.undercloud-config", "description": "Undercloud configuration parameters"}'
2018-12-03 06:19:17,162 DEBUG: https://172.16.0.10:13989 "POST /v2/environments HTTP/1.1" 201 423
2018-12-03 06:19:17,163 DEBUG: RESP: [201] Content-Length: 423 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:17 GMT Connection: keep-alive 
RESP BODY: {"created_at": "2018-12-03 11:19:17", "description": "Undercloud configuration parameters", "variables": "{\"undercloud_ceilometer_snmpd_password\": \"fb321565da9bc0ef27c8324166ac09e4d889dc5d\", \"undercloud_db_password\": \"a9a10b0e40e705f2cb9c9d14baf2f49e5ce94c04\"}", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "22e08377-2cd4-40c6-93f5-cd30deea7fc0", "name": "tripleo.undercloud-config"}

2018-12-03 06:19:17,163 DEBUG: HTTP POST https://172.16.0.10:13989/v2/environments 201
2018-12-03 06:19:17,164 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/executions -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"input": "{\"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\"}", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "description": ""}'
2018-12-03 06:19:18,159 DEBUG: https://172.16.0.10:13989 "POST /v2/executions HTTP/1.1" 201 684
2018-12-03 06:19:18,160 DEBUG: RESP: [201] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:18 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:19:18,160 DEBUG: HTTP POST https://172.16.0.10:13989/v2/executions 201
2018-12-03 06:19:18,161 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:19:18,184 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:19:18,186 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:18 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:19:18,186 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:19:23,191 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:19:23,213 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:19:23,214 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:23 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:19:23,214 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:19:28,220 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:19:28,240 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:19:28,241 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:28 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:19:28,242 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:19:33,246 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:19:33,268 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:19:33,269 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:33 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:19:33,269 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:19:38,274 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:19:38,295 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:19:38,296 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:38 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:19:38,296 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:19:43,302 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:19:43,321 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:19:43,322 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:43 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:19:43,322 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:19:48,328 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:19:48,350 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:19:48,351 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:48 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:19:48,351 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:19:53,357 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:19:53,376 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:19:53,377 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:53 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:19:53,377 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:19:58,382 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:19:58,403 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:19:58,404 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:19:58 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:19:58,404 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:20:03,409 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:20:03,431 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:20:03,432 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:20:03 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:20:03,432 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:20:08,436 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:20:08,456 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:20:08,457 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:20:08 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:20:08,457 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:20:13,462 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:20:13,482 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:20:13,483 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:20:13 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:20:13,483 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:20:18,487 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:20:18,507 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:20:18,508 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:20:18 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:20:18,508 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:20:23,514 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:20:23,536 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:20:23,537 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:20:23 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:20:23,537 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:20:28,540 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:20:28,559 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:20:28,560 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:20:28 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:20:28,561 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:20:33,567 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:20:33,587 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:20:33,588 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:20:33 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:20:33,588 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:20:38,592 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:20:38,611 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:20:38,612 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:20:38 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:20:38,612 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:20:43,617 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:20:43,636 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:20:43,637 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:20:43 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:20:43,637 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:20:48,639 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:20:48,660 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:20:48,661 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:20:48 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:20:48,661 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:20:53,664 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:20:53,683 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:20:53,685 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:20:53 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:20:53,685 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:20:58,690 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:20:58,722 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:20:58,723 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:20:58 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:20:58,723 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:21:03,729 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:21:03,749 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:21:03,750 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:21:03 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:21:03,750 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:21:08,756 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:21:08,776 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 684
2018-12-03 06:21:08,777 DEBUG: RESP: [200] Content-Length: 684 Content-Type: application/json Date: Mon, 03 Dec 2018 11:21:08 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:19:18", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:21:08,777 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:21:13,783 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa"
2018-12-03 06:21:13,802 DEBUG: https://172.16.0.10:13989 "GET /v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 HTTP/1.1" 200 739
2018-12-03 06:21:13,803 DEBUG: RESP: [200] Content-Length: 739 Content-Type: application/json Date: Mon, 03 Dec 2018 11:21:13 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "SUCCESS", "workflow_name": "tripleo.plan_management.v1.create_deployment_plan", "task_execution_id": null, "updated_at": "2018-12-03 11:21:12", "workflow_id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{\"status\": \"SUCCESS\", \"message\": \"Plan created.\"}", "input": "{\"generate_passwords\": true, \"use_default_templates\": true, \"queue_name\": \"718cd655-fb9e-4a85-8566-dd1c81557047\", \"container\": \"overcloud\", \"source_url\": null}", "created_at": "2018-12-03 11:19:18", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7ecf3004-1078-4c55-88b8-f79ef3900220"}

2018-12-03 06:21:13,803 DEBUG: HTTP GET https://172.16.0.10:13989/v2/executions/7ecf3004-1078-4c55-88b8-f79ef3900220 200
2018-12-03 06:21:13,804 INFO: Configuring an hourly cron trigger for tripleo-ui logging
2018-12-03 06:21:13,804 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/cron_triggers -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"pattern": "0 * * * *", "workflow_name": "tripleo.plan_management.v1.publish_ui_logs_to_swift", "first_execution_time": null, "name": "publish-ui-logs-hourly", "remaining_executions": null}'
2018-12-03 06:21:15,380 DEBUG: https://172.16.0.10:13989 "POST /v2/cron_triggers HTTP/1.1" 201 493
2018-12-03 06:21:15,381 DEBUG: RESP: [201] Content-Length: 493 Content-Type: application/json Date: Mon, 03 Dec 2018 11:21:15 GMT Connection: keep-alive 
RESP BODY: {"created_at": "2018-12-03 11:21:15", "project_id": "f41e7348daff4254892e5cd759aa8806", "name": "publish-ui-logs-hourly", "pattern": "0 * * * *", "workflow_name": "tripleo.plan_management.v1.publish_ui_logs_to_swift", "workflow_input": "{}", "workflow_id": "392c6fbb-b06e-4734-97e1-aae7ad23345b", "first_execution_time": null, "remaining_executions": null, "scope": "private", "workflow_params": "{}", "id": "7ba61b6c-8685-40a9-802a-a329a1973c1b", "next_execution_time": "2018-12-03 12:00:00"}

2018-12-03 06:21:15,382 DEBUG: HTTP POST https://172.16.0.10:13989/v2/cron_triggers 201
2018-12-03 06:21:15,382 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/executions -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: application/json" -H "X-Auth-Token: {SHA1}59647c8666e26c744ac0c25fb47b7fd272219afa" -d '{"workflow_name": "tripleo.validations.v1.copy_ssh_key", "description": ""}'
2018-12-03 06:21:15,569 DEBUG: https://172.16.0.10:13989 "POST /v2/executions HTTP/1.1" 201 563
2018-12-03 06:21:15,570 DEBUG: RESP: [201] Content-Length: 563 Content-Type: application/json Date: Mon, 03 Dec 2018 11:21:15 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.validations.v1.copy_ssh_key", "task_execution_id": null, "updated_at": "2018-12-03 11:21:15", "workflow_id": "5eea10c6-3279-469d-9c80-9feef8096e66", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"overcloud_admin\": \"heat-admin\", \"queue_name\": \"tripleo\"}", "created_at": "2018-12-03 11:21:15", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "74c08a61-62e6-4094-b24e-d5a8a89f260e"}

2018-12-03 06:21:15,570 DEBUG: HTTP POST https://172.16.0.10:13989/v2/executions 201
2018-12-03 06:21:15,790 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13000/ -H "Accept: application/json" -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5"
2018-12-03 06:21:15,791 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-03 06:21:15,812 DEBUG: https://172.16.0.10:13000 "GET / HTTP/1.1" 300 601
2018-12-03 06:21:15,813 DEBUG: RESP: [300] Date: Mon, 03 Dec 2018 11:21:15 GMT Server: Apache Vary: X-Auth-Token Content-Length: 601 Content-Type: application/json 
RESP BODY: {"versions": {"values": [{"status": "stable", "updated": "2018-02-28T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v3+json"}], "id": "v3.10", "links": [{"href": "https://172.16.0.10:13000/v3/", "rel": "self"}]}, {"status": "deprecated", "updated": "2016-08-04T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v2.0+json"}], "id": "v2.0", "links": [{"href": "https://172.16.0.10:13000/v2.0/", "rel": "self"}, {"href": "https://docs.openstack.org/", "type": "text/html", "rel": "describedby"}]}]}}

2018-12-03 06:21:15,814 DEBUG: Making authentication request to https://172.16.0.10:13000/v3/auth/tokens
2018-12-03 06:21:16,551 DEBUG: https://172.16.0.10:13000 "POST /v3/auth/tokens HTTP/1.1" 201 8096
2018-12-03 06:21:16,553 DEBUG: {"token": {"is_domain": false, "methods": ["password"], "roles": [{"id": "faa7ae54793f4277a142cde9530e003f", "name": "admin"}], "expires_at": "2018-12-03T15:21:16.000000Z", "project": {"domain": {"id": "default", "name": "Default"}, "id": "f41e7348daff4254892e5cd759aa8806", "name": "admin"}, "catalog": [{"endpoints": [{"url": "http://172.16.0.11:6385", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "0790be2fdc3b49d08f54f8afdd4a13f5"}, {"url": "https://172.16.0.10:13385", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "4d1975c16140453f8778e1fb9ac95da8"}, {"url": "http://172.16.0.11:6385", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "6d2c8e36ee28476ba57a9055e744d319"}], "type": "baremetal", "id": "2631f10c56d84d278c6347ffd617c17e", "name": "ironic"}, {"endpoints": [{"url": "http://172.16.0.11:8778/placement", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "17ffbfdf60bb4b1fa47d54afdfd65841"}, {"url": "http://172.16.0.11:8778/placement", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "4f70d07307cc47f7bef0a5e3b8fe3a67"}, {"url": "https://172.16.0.10:13778/placement", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "89249f96d951451f922f786665cd8ac0"}], "type": "placement", "id": "33d36ef2b525461882431a456c280ca4", "name": "placement"}, {"endpoints": [{"url": "http://172.16.0.11:35357", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "339fd082d7a348e0883ed89ccaf17e64"}, {"url": "http://172.16.0.11:5000", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "38e31d810fa946de8ccfbf1f7c3046b8"}, {"url": "https://172.16.0.10:13000", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "760586442ad141a8bb0f1ebc452fbf23"}], "type": "identity", "id": "3ecdc56d07224eac9452c019cb2faf2c", "name": "keystone"}, {"endpoints": [{"url": "http://172.16.0.11:8004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "19b04fec944847e3ae21b20f16b49cd1"}, {"url": "https://172.16.0.10:13004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "1bd9e950b8f64c758f56e16c1e416eb9"}, {"url": "http://172.16.0.11:8004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "f207a1d68f524c00be7cd409f67eb602"}], "type": "orchestration", "id": "6b802215e15547b4933f6ad11ff28780", "name": "heat"}, {"endpoints": [{"url": "http://172.16.0.11:8774/v2.1", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "04bf5d0432f24b62b7db4d32eb6e9185"}, {"url": "https://172.16.0.10:13774/v2.1", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "8e337cbbb3504e5e9f93d010f2a920f4"}, {"url": "http://172.16.0.11:8774/v2.1", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "97a0c76942474f85b0fc3128fe3d349c"}], "type": "compute", "id": "6c68f42f71d5448eb11cc7cd8a8b2b30", "name": "nova"}, {"endpoints": [{"url": "http://172.16.0.11:9696", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "8e04bd214b8f46b993b9f8b13f8ec47f"}, {"url": "http://172.16.0.11:9696", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "a133c18dcc214604a3bb74d5dc2e5c5f"}, {"url": "https://172.16.0.10:13696", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "df06914815264173860e975b90e1569c"}], "type": "network", "id": "acf2ad1a66ec4ae09be00688d95b112c", "name": "neutron"}, {"endpoints": [{"url": "http://172.16.0.11:5050", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "9d61189438cb45e3ac270c69205221ca"}, {"url": "http://172.16.0.11:5050", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "c2d37cd497924ce59d5bac0690d9e64b"}, {"url": "https://172.16.0.10:13050", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "d0b3f48230e349edafeac286fdeb8252"}], "type": "baremetal-introspection", "id": "b7b8e51747704c76888dcd514b7addf9", "name": "ironic-inspector"}, {"endpoints": [{"url": "http://172.16.0.11:8000/v1/f41e7348daff4254892e5cd759aa8806", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "848bf35e6ba8481fb799cc941e4c33de"}, {"url": "http://172.16.0.11:8000/v1/f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "a160fbb2b9a046f6902860dbf9e7900c"}, {"url": "https://172.16.0.10:13800/v1/f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "a314c5a74b3b4ca0ad7294b66e726139"}], "type": "cloudformation", "id": "cb16845f1f7c483e9775a0080437bfe9", "name": "heat-cfn"}, {"endpoints": [{"url": "http://172.16.0.11:9292", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "3561bb34989645fea114a7e5b4e443c7"}, {"url": "http://172.16.0.11:9292", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "8d54c0c6f22d43658500ee4b14d0d925"}, {"url": "https://172.16.0.10:13292", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "ee991dbd122a46aa90bb402e88d2cc12"}], "type": "image", "id": "cbe7192c36d147ceb3a550dce9ff3586", "name": "glance"}, {"endpoints": [{"url": "http://172.16.0.11:8888", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "068c25c0200a4a8aba4b6f6f6947a0f5"}, {"url": "http://172.16.0.11:8888", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "98d3307f2ce44059ac228d580f163ca1"}, {"url": "https://172.16.0.10:13888", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "a3978de44cb84351874ab38de3b38c15"}], "type": "messaging", "id": "d90c70e049d14593b7b666788ee43b6e", "name": "zaqar"}, {"endpoints": [{"url": "wss://172.16.0.10:9000", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "2eabe4530b0f41a9a78684eaffc82974"}, {"url": "ws://172.16.0.11:9000", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "300aa4234cf04b44b3de4d8604e6a75d"}, {"url": "ws://172.16.0.11:9000", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "bdaf3488dbab44a1ba43364b5d974e83"}], "type": "messaging-websocket", "id": "dc2d191d5b0e4491a88b1e372b18dbfe", "name": "zaqar-websocket"}, {"endpoints": [{"url": "http://172.16.0.11:8080", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "1e4eab20558248289100f101cfcf4e1e"}, {"url": "http://172.16.0.11:8080/v1/AUTH_f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "6ee57e5f8aec4f3fa81e3ffef1b6f261"}, {"url": "https://172.16.0.10:13808/v1/AUTH_f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "e4c21afceb3f4a8da0f2d75ba050dcdd"}], "type": "object-store", "id": "ea9a151f0e6342dbb1aeba5c885e208f", "name": "swift"}, {"endpoints": [{"url": "https://172.16.0.10:13989/v2", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "15a1660af0f2476cbda3fe163c56f368"}, {"url": "http://172.16.0.11:8989/v2", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "63fadaa6590b47a3bb91a7c789dce370"}, {"url": "http://172.16.0.11:8989/v2", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "b3b5817a37ef4565b92c2fc34b95f250"}], "type": "workflowv2", "id": "f7c8197dd44f4085baddd516aa6e7d7e", "name": "mistral"}], "user": {"domain": {"id": "default", "name": "Default"}, "password_expires_at": null, "name": "admin", "id": "b0c11cd6e04b4e08ac31e050977f22cf"}, "audit_ids": ["oiRiGBu0TSCq2xjLdoJnxw"], "issued_at": "2018-12-03T11:21:16.000000Z"}}
2018-12-03 06:21:16,553 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13000/ -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}5a928a61acabd25ed1dc80e2b19e34115e583ffa"
2018-12-03 06:21:16,559 DEBUG: https://172.16.0.10:13000 "GET / HTTP/1.1" 300 601
2018-12-03 06:21:16,560 DEBUG: RESP: [300] Date: Mon, 03 Dec 2018 11:21:16 GMT Server: Apache Vary: X-Auth-Token Content-Length: 601 Content-Type: application/json 
RESP BODY: {"versions": {"values": [{"status": "stable", "updated": "2018-02-28T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v3+json"}], "id": "v3.10", "links": [{"href": "https://172.16.0.10:13000/v3/", "rel": "self"}]}, {"status": "deprecated", "updated": "2016-08-04T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v2.0+json"}], "id": "v2.0", "links": [{"href": "https://172.16.0.10:13000/v2.0/", "rel": "self"}, {"href": "https://docs.openstack.org/", "type": "text/html", "rel": "describedby"}]}]}}

2018-12-03 06:21:16,564 DEBUG: REQ: curl -g -i -X GET http://172.16.0.11:35357 -H "Accept: application/json" -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5"
2018-12-03 06:21:16,565 DEBUG: Starting new HTTP connection (1): 172.16.0.11
2018-12-03 06:21:16,572 DEBUG: http://172.16.0.11:35357 "GET / HTTP/1.1" 300 599
2018-12-03 06:21:16,573 DEBUG: RESP: [300] Date: Mon, 03 Dec 2018 11:21:16 GMT Server: Apache Vary: X-Auth-Token Content-Length: 599 Content-Type: application/json 
RESP BODY: {"versions": {"values": [{"status": "stable", "updated": "2018-02-28T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v3+json"}], "id": "v3.10", "links": [{"href": "http://172.16.0.11:35357/v3/", "rel": "self"}]}, {"status": "deprecated", "updated": "2016-08-04T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v2.0+json"}], "id": "v2.0", "links": [{"href": "http://172.16.0.11:35357/v2.0/", "rel": "self"}, {"href": "https://docs.openstack.org/", "type": "text/html", "rel": "describedby"}]}]}}

2018-12-03 06:21:16,575 DEBUG: REQ: curl -g -i -X GET http://172.16.0.11:35357/v3/roles? -H "User-Agent: python-keystoneclient" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}5a928a61acabd25ed1dc80e2b19e34115e583ffa"
2018-12-03 06:21:16,834 DEBUG: http://172.16.0.11:35357 "GET /v3/roles HTTP/1.1" 200 289
2018-12-03 06:21:16,836 DEBUG: RESP: [200] Date: Mon, 03 Dec 2018 11:21:16 GMT Server: Apache Vary: X-Auth-Token,Accept-Encoding x-openstack-request-id: req-d8362778-8246-4c92-8335-b013ed0baf66 Content-Encoding: gzip Content-Length: 289 Content-Type: application/json 
RESP BODY: {"links": {"self": "https://172.16.0.10:13000/v3/roles", "previous": null, "next": null}, "roles": [{"domain_id": null, "id": "5998b5f829fc401d8c428c41e0e0c562", "links": {"self": "https://172.16.0.10:13000/v3/roles/5998b5f829fc401d8c428c41e0e0c562"}, "name": "ResellerAdmin"}, {"domain_id": null, "id": "90067790123e41c0ba405c375a7e5d47", "links": {"self": "https://172.16.0.10:13000/v3/roles/90067790123e41c0ba405c375a7e5d47"}, "name": "swiftoperator"}, {"domain_id": null, "id": "cdb2bc33124e4ee6859f75543f4a126e", "links": {"self": "https://172.16.0.10:13000/v3/roles/cdb2bc33124e4ee6859f75543f4a126e"}, "name": "heat_stack_user"}, {"domain_id": null, "id": "faa7ae54793f4277a142cde9530e003f", "links": {"self": "https://172.16.0.10:13000/v3/roles/faa7ae54793f4277a142cde9530e003f"}, "name": "admin"}]}

2018-12-03 06:21:16,836 DEBUG: GET call to identity for http://172.16.0.11:35357/v3/roles used request id req-d8362778-8246-4c92-8335-b013ed0baf66
2018-12-03 06:21:16,871 INFO: 
#############################################################################
Undercloud install complete.

The file containing this installation's passwords is at
/home/stack/undercloud-passwords.conf.

There is also a stackrc file at /home/stack/stackrc.

These files are needed to interact with the OpenStack services, and should be
secured.

#############################################################################

2018-12-04 05:57:55,786 INFO: Logging to /home/stack/.instack/install-undercloud.log
2018-12-04 05:57:55,857 INFO: Checking for a FQDN hostname...
2018-12-04 05:57:55,933 INFO: Static hostname detected as undercloud.example.com
2018-12-04 05:57:55,963 INFO: Transient hostname detected as undercloud.example.com
2018-12-04 05:57:55,994 INFO: Added hostname undercloud.example.com to /etc/hosts
2018-12-04 05:57:55,996 WARNING: Option "undercloud_public_vip" from group "DEFAULT" is deprecated. Use option "undercloud_public_host" from group "DEFAULT".
2018-12-04 05:57:55,996 WARNING: Option "undercloud_admin_vip" from group "DEFAULT" is deprecated. Use option "undercloud_admin_host" from group "DEFAULT".
2018-12-04 05:57:56,068 INFO: Running yum clean all
2018-12-04 05:57:56,347 INFO: Loaded plugins: search-disabled-repos
2018-12-04 05:57:56,370 INFO: Cleaning repos: rhel-7-server-extras-rpms
2018-12-04 05:57:56,371 INFO:               : rhel-7-server-openstack-13-devtools-rpms
2018-12-04 05:57:56,371 INFO:               : rhel-7-server-openstack-13-optools-rpms
2018-12-04 05:57:56,371 INFO:               : rhel-7-server-openstack-13-rpms rhel-7-server-rh-common-rpms
2018-12-04 05:57:56,371 INFO:               : rhel-7-server-rpms rhel-ha-for-rhel-7-server-rpms
2018-12-04 05:57:56,412 INFO: yum-clean-all completed successfully
2018-12-04 05:57:56,412 INFO: Running yum update
2018-12-04 05:57:56,684 INFO: Loaded plugins: search-disabled-repos
2018-12-04 05:57:59,084 INFO: Resolving Dependencies
2018-12-04 05:57:59,087 INFO: --> Running transaction check
2018-12-04 05:57:59,087 INFO: ---> Package NetworkManager.x86_64 1:1.12.0-6.el7 will be updated
2018-12-04 05:57:59,419 INFO: ---> Package NetworkManager.x86_64 1:1.12.0-8.el7_6 will be an update
2018-12-04 05:57:59,452 INFO: ---> Package NetworkManager-config-server.noarch 1:1.12.0-6.el7 will be updated
2018-12-04 05:57:59,453 INFO: ---> Package NetworkManager-config-server.noarch 1:1.12.0-8.el7_6 will be an update
2018-12-04 05:57:59,454 INFO: ---> Package NetworkManager-libnm.x86_64 1:1.12.0-6.el7 will be updated
2018-12-04 05:57:59,455 INFO: ---> Package NetworkManager-libnm.x86_64 1:1.12.0-8.el7_6 will be an update
2018-12-04 05:57:59,457 INFO: ---> Package NetworkManager-team.x86_64 1:1.12.0-6.el7 will be updated
2018-12-04 05:57:59,457 INFO: ---> Package NetworkManager-team.x86_64 1:1.12.0-8.el7_6 will be an update
2018-12-04 05:57:59,460 INFO: ---> Package NetworkManager-tui.x86_64 1:1.12.0-6.el7 will be updated
2018-12-04 05:57:59,460 INFO: ---> Package NetworkManager-tui.x86_64 1:1.12.0-8.el7_6 will be an update
2018-12-04 05:57:59,464 INFO: ---> Package cronie.x86_64 0:1.4.11-19.el7 will be updated
2018-12-04 05:57:59,466 INFO: ---> Package cronie.x86_64 0:1.4.11-20.el7_6 will be an update
2018-12-04 05:57:59,472 INFO: ---> Package cronie-anacron.x86_64 0:1.4.11-19.el7 will be updated
2018-12-04 05:57:59,473 INFO: ---> Package cronie-anacron.x86_64 0:1.4.11-20.el7_6 will be an update
2018-12-04 05:57:59,475 INFO: ---> Package device-mapper.x86_64 7:1.02.149-8.el7 will be updated
2018-12-04 05:57:59,478 INFO: ---> Package device-mapper.x86_64 7:1.02.149-10.el7_6.2 will be an update
2018-12-04 05:57:59,485 INFO: ---> Package device-mapper-libs.x86_64 7:1.02.149-8.el7 will be updated
2018-12-04 05:57:59,486 INFO: ---> Package device-mapper-libs.x86_64 7:1.02.149-10.el7_6.2 will be an update
2018-12-04 05:57:59,488 INFO: ---> Package heat-cfntools.noarch 0:1.2.6-5.el7 will be updated
2018-12-04 05:57:59,489 INFO: ---> Package heat-cfntools.noarch 0:1.3.0-2.el7ost will be an update
2018-12-04 05:57:59,498 INFO: ---> Package kernel.x86_64 0:3.10.0-957.1.3.el7 will be installed
2018-12-04 05:57:59,528 INFO: --> Processing Dependency: linux-firmware >= 20180911-68 for package: kernel-3.10.0-957.1.3.el7.x86_64
2018-12-04 05:57:59,532 INFO: ---> Package kernel-tools.x86_64 0:3.10.0-957.el7 will be updated
2018-12-04 05:57:59,566 INFO: ---> Package kernel-tools.x86_64 0:3.10.0-957.1.3.el7 will be an update
2018-12-04 05:57:59,610 INFO: ---> Package kernel-tools-libs.x86_64 0:3.10.0-957.el7 will be updated
2018-12-04 05:57:59,621 INFO: ---> Package kernel-tools-libs.x86_64 0:3.10.0-957.1.3.el7 will be an update
2018-12-04 05:57:59,621 INFO: ---> Package python-jsonpatch.noarch 0:1.2-4.el7 will be obsoleted
2018-12-04 05:57:59,623 INFO: ---> Package python-jsonpointer.noarch 0:1.9-2.el7 will be updated
2018-12-04 05:57:59,624 INFO: ---> Package python-jsonpointer.noarch 0:1.10-4.1.el7ost will be an update
2018-12-04 05:57:59,624 INFO: ---> Package python-lxml.x86_64 0:3.2.1-4.el7 will be updated
2018-12-04 05:57:59,628 INFO: ---> Package python-lxml.x86_64 0:3.2.1-6.el7ost will be an update
2018-12-04 05:57:59,641 INFO: ---> Package python-perf.x86_64 0:3.10.0-957.el7 will be updated
2018-12-04 05:57:59,661 INFO: ---> Package python-perf.x86_64 0:3.10.0-957.1.3.el7 will be an update
2018-12-04 05:57:59,661 INFO: ---> Package python2-jsonpatch.noarch 0:1.21-1.el7ost will be obsoleting
2018-12-04 05:57:59,662 INFO: ---> Package selinux-policy.noarch 0:3.13.1-229.el7 will be updated
2018-12-04 05:57:59,667 INFO: ---> Package selinux-policy.noarch 0:3.13.1-229.el7_6.6 will be an update
2018-12-04 05:57:59,671 INFO: ---> Package selinux-policy-targeted.noarch 0:3.13.1-229.el7 will be updated
2018-12-04 05:57:59,673 INFO: ---> Package selinux-policy-targeted.noarch 0:3.13.1-229.el7_6.6 will be an update
2018-12-04 05:57:59,674 INFO: ---> Package subscription-manager.x86_64 0:1.21.10-2.el7 will be updated
2018-12-04 05:57:59,676 INFO: ---> Package subscription-manager.x86_64 0:1.21.10-3.el7_6 will be an update
2018-12-04 05:57:59,687 INFO: ---> Package subscription-manager-rhsm.x86_64 0:1.21.10-2.el7 will be updated
2018-12-04 05:57:59,689 INFO: ---> Package subscription-manager-rhsm.x86_64 0:1.21.10-3.el7_6 will be an update
2018-12-04 05:57:59,693 INFO: ---> Package subscription-manager-rhsm-certificates.x86_64 0:1.21.10-2.el7 will be updated
2018-12-04 05:57:59,693 INFO: ---> Package subscription-manager-rhsm-certificates.x86_64 0:1.21.10-3.el7_6 will be an update
2018-12-04 05:57:59,694 INFO: ---> Package tzdata.noarch 0:2018e-3.el7 will be updated
2018-12-04 05:57:59,697 INFO: ---> Package tzdata.noarch 0:2018g-1.el7 will be an update
2018-12-04 05:57:59,697 INFO: --> Running transaction check
2018-12-04 05:57:59,697 INFO: ---> Package linux-firmware.noarch 0:20180911-69.git85c5d90.el7 will be installed
2018-12-04 05:58:00,726 INFO: --> Finished Dependency Resolution
2018-12-04 05:58:00,835 INFO: 
2018-12-04 05:58:00,835 INFO: Dependencies Resolved
2018-12-04 05:58:00,846 INFO: 
2018-12-04 05:58:00,846 INFO: ================================================================================
2018-12-04 05:58:00,846 INFO:  Package                 Arch   Version                Repository          Size
2018-12-04 05:58:00,846 INFO: ================================================================================
2018-12-04 05:58:00,847 INFO: Installing:
2018-12-04 05:58:00,847 INFO:  kernel                  x86_64 3.10.0-957.1.3.el7     rhel-7-server-rpms  48 M
2018-12-04 05:58:00,847 INFO:  python2-jsonpatch       noarch 1.21-1.el7ost          rhel-7-server-openstack-13-rpms
2018-12-04 05:58:00,848 INFO:                                                                            22 k
2018-12-04 05:58:00,848 INFO:      replacing  python-jsonpatch.noarch 1.2-4.el7
2018-12-04 05:58:00,848 INFO: Updating:
2018-12-04 05:58:00,848 INFO:  NetworkManager          x86_64 1:1.12.0-8.el7_6       rhel-7-server-rpms 1.7 M
2018-12-04 05:58:00,848 INFO:  NetworkManager-config-server
2018-12-04 05:58:00,849 INFO:                          noarch 1:1.12.0-8.el7_6       rhel-7-server-rpms 145 k
2018-12-04 05:58:00,849 INFO:  NetworkManager-libnm    x86_64 1:1.12.0-8.el7_6       rhel-7-server-rpms 1.4 M
2018-12-04 05:58:00,849 INFO:  NetworkManager-team     x86_64 1:1.12.0-8.el7_6       rhel-7-server-rpms 159 k
2018-12-04 05:58:00,850 INFO:  NetworkManager-tui      x86_64 1:1.12.0-8.el7_6       rhel-7-server-rpms 239 k
2018-12-04 05:58:00,850 INFO:  cronie                  x86_64 1.4.11-20.el7_6        rhel-7-server-rpms  91 k
2018-12-04 05:58:00,850 INFO:  cronie-anacron          x86_64 1.4.11-20.el7_6        rhel-7-server-rpms  36 k
2018-12-04 05:58:00,850 INFO:  device-mapper           x86_64 7:1.02.149-10.el7_6.2  rhel-7-server-rpms 292 k
2018-12-04 05:58:00,851 INFO:  device-mapper-libs      x86_64 7:1.02.149-10.el7_6.2  rhel-7-server-rpms 320 k
2018-12-04 05:58:00,851 INFO:  heat-cfntools           noarch 1.3.0-2.el7ost         rhel-7-server-openstack-13-rpms
2018-12-04 05:58:00,851 INFO:                                                                            70 k
2018-12-04 05:58:00,851 INFO:  kernel-tools            x86_64 3.10.0-957.1.3.el7     rhel-7-server-rpms 7.1 M
2018-12-04 05:58:00,852 INFO:  kernel-tools-libs       x86_64 3.10.0-957.1.3.el7     rhel-7-server-rpms 7.0 M
2018-12-04 05:58:00,852 INFO:  python-jsonpointer      noarch 1.10-4.1.el7ost        rhel-7-server-openstack-13-rpms
2018-12-04 05:58:00,852 INFO:                                                                            14 k
2018-12-04 05:58:00,852 INFO:  python-lxml             x86_64 3.2.1-6.el7ost         rhel-7-server-openstack-13-rpms
2018-12-04 05:58:00,853 INFO:                                                                           957 k
2018-12-04 05:58:00,853 INFO:  python-perf             x86_64 3.10.0-957.1.3.el7     rhel-7-server-rpms 7.1 M
2018-12-04 05:58:00,853 INFO:  selinux-policy          noarch 3.13.1-229.el7_6.6     rhel-7-server-rpms 483 k
2018-12-04 05:58:00,854 INFO:  selinux-policy-targeted noarch 3.13.1-229.el7_6.6     rhel-7-server-rpms 6.9 M
2018-12-04 05:58:00,854 INFO:  subscription-manager    x86_64 1.21.10-3.el7_6        rhel-7-server-rpms 1.0 M
2018-12-04 05:58:00,854 INFO:  subscription-manager-rhsm
2018-12-04 05:58:00,855 INFO:                          x86_64 1.21.10-3.el7_6        rhel-7-server-rpms 297 k
2018-12-04 05:58:00,855 INFO:  subscription-manager-rhsm-certificates
2018-12-04 05:58:00,855 INFO:                          x86_64 1.21.10-3.el7_6        rhel-7-server-rpms 212 k
2018-12-04 05:58:00,856 INFO:  tzdata                  noarch 2018g-1.el7            rhel-7-server-rpms 484 k
2018-12-04 05:58:00,856 INFO: Installing for dependencies:
2018-12-04 05:58:00,856 INFO:  linux-firmware          noarch 20180911-69.git85c5d90.el7
2018-12-04 05:58:00,856 INFO:                                                        rhel-7-server-rpms  49 M
2018-12-04 05:58:00,857 INFO: 
2018-12-04 05:58:00,857 INFO: Transaction Summary
2018-12-04 05:58:00,857 INFO: ================================================================================
2018-12-04 05:58:00,857 INFO: Install   2 Packages (+1 Dependent package)
2018-12-04 05:58:00,858 INFO: Upgrade  21 Packages
2018-12-04 05:58:00,858 INFO: 
2018-12-04 05:58:00,858 INFO: Total download size: 133 M
2018-12-04 05:58:00,858 INFO: Downloading packages:
2018-12-04 05:58:00,858 INFO: Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
2018-12-04 05:58:12,851 INFO: --------------------------------------------------------------------------------
2018-12-04 05:58:12,851 INFO: Total                                               11 MB/s | 133 MB  00:11
2018-12-04 05:58:13,492 INFO: Running transaction check
2018-12-04 05:58:13,828 INFO: Running transaction test
2018-12-04 05:58:14,484 INFO: Transaction test succeeded
2018-12-04 05:58:14,484 INFO: Running transaction
2018-12-04 05:58:15,774 INFO:   Updating   : 1:NetworkManager-libnm-1.12.0-8.el7_6.x86_64                1/46
2018-12-04 05:58:15,876 INFO:   Updating   : 1:NetworkManager-1.12.0-8.el7_6.x86_64                      2/46
2018-12-04 05:58:15,953 INFO:   Updating   : cronie-1.4.11-20.el7_6.x86_64                               3/46
2018-12-04 05:58:16,011 INFO:   Updating   : cronie-anacron-1.4.11-20.el7_6.x86_64                       4/46
2018-12-04 05:58:16,088 INFO:   Updating   : 7:device-mapper-libs-1.02.149-10.el7_6.2.x86_64             5/46
2018-12-04 05:58:16,125 INFO:   Updating   : 7:device-mapper-1.02.149-10.el7_6.2.x86_64                  6/46
2018-12-04 05:58:16,188 INFO:   Updating   : subscription-manager-rhsm-certificates-1.21.10-3.el7_6.x    7/46
2018-12-04 05:58:16,218 INFO:   Updating   : subscription-manager-rhsm-1.21.10-3.el7_6.x86_64            8/46
2018-12-04 05:58:16,307 INFO:   Updating   : python-jsonpointer-1.10-4.1.el7ost.noarch                   9/46
2018-12-04 05:58:25,593 INFO:   Updating   : kernel-tools-libs-3.10.0-957.1.3.el7.x86_64                10/46
2018-12-04 05:58:25,663 INFO:   Installing : linux-firmware-20180911-69.git85c5d90.el7.noarch           11/46
2018-12-04 05:58:39,542 INFO:   Updating   : selinux-policy-3.13.1-229.el7_6.6.noarch                   12/46
2018-12-04 05:59:00,612 INFO:   Updating   : selinux-policy-targeted-3.13.1-229.el7_6.6.noarch          13/46
2018-12-04 05:59:02,584 INFO:   Installing : kernel-3.10.0-957.1.3.el7.x86_64                           14/46
2018-12-04 05:59:02,677 INFO:   Updating   : kernel-tools-3.10.0-957.1.3.el7.x86_64                     15/46
2018-12-04 05:59:03,155 INFO:   Installing : python2-jsonpatch-1.21-1.el7ost.noarch                     16/46
2018-12-04 05:59:03,268 INFO:   Updating   : subscription-manager-1.21.10-3.el7_6.x86_64                17/46
2018-12-04 05:59:03,293 INFO:   Updating   : 1:NetworkManager-tui-1.12.0-8.el7_6.x86_64                 18/46
2018-12-04 05:59:04,310 INFO:   Updating   : 1:NetworkManager-team-1.12.0-8.el7_6.x86_64                19/46
2018-12-04 05:59:04,584 INFO:   Updating   : tzdata-2018g-1.el7.noarch                                  20/46
2018-12-04 05:59:04,607 INFO:   Updating   : python-lxml-3.2.1-6.el7ost.x86_64                          21/46
2018-12-04 05:59:04,711 INFO:   Updating   : 1:NetworkManager-config-server-1.12.0-8.el7_6.noarch       22/46
2018-12-04 05:59:04,805 INFO:   Updating   : python-perf-3.10.0-957.1.3.el7.x86_64                      23/46
2018-12-04 05:59:04,821 INFO:   Updating   : heat-cfntools-1.3.0-2.el7ost.noarch                        24/46
2018-12-04 05:59:04,833 INFO:   Cleanup    : 1:NetworkManager-tui-1.12.0-6.el7.x86_64                   25/46
2018-12-04 05:59:04,859 INFO:   Erasing    : python-jsonpatch-1.2-4.el7.noarch                          26/46
2018-12-04 05:59:04,916 INFO:   Cleanup    : selinux-policy-targeted-3.13.1-229.el7.noarch              27/46
2018-12-04 05:59:05,047 INFO:   Cleanup    : cronie-1.4.11-19.el7.x86_64                                28/46
2018-12-04 05:59:05,061 INFO:   Cleanup    : cronie-anacron-1.4.11-19.el7.x86_64                        29/46
2018-12-04 05:59:05,107 INFO:   Cleanup    : 7:device-mapper-libs-1.02.149-8.el7.x86_64                 30/46
2018-12-04 05:59:05,143 INFO:   Cleanup    : 7:device-mapper-1.02.149-8.el7.x86_64                      31/46
2018-12-04 05:59:05,294 INFO:   Cleanup    : subscription-manager-1.21.10-2.el7.x86_64                  32/46
2018-12-04 05:59:05,310 INFO:   Cleanup    : subscription-manager-rhsm-1.21.10-2.el7.x86_64             33/46
2018-12-04 05:59:05,344 INFO:   Cleanup    : 1:NetworkManager-team-1.12.0-6.el7.x86_64                  34/46
2018-12-04 05:59:05,518 INFO:   Cleanup    : 1:NetworkManager-1.12.0-6.el7.x86_64                       35/46
2018-12-04 05:59:05,631 INFO:   Cleanup    : kernel-tools-3.10.0-957.el7.x86_64                         36/46
2018-12-04 05:59:05,655 INFO:   Cleanup    : subscription-manager-rhsm-certificates-1.21.10-2.el7.x86   37/46
2018-12-04 05:59:05,680 INFO:   Cleanup    : selinux-policy-3.13.1-229.el7.noarch                       38/46
2018-12-04 05:59:05,705 INFO:   Cleanup    : python-jsonpointer-1.9-2.el7.noarch                        39/46
2018-12-04 05:59:05,727 INFO:   Cleanup    : tzdata-2018e-3.el7.noarch                                  40/46
2018-12-04 05:59:05,737 INFO:   Cleanup    : 1:NetworkManager-config-server-1.12.0-6.el7.noarch         41/46
2018-12-04 05:59:05,769 INFO:   Cleanup    : heat-cfntools-1.2.6-5.el7.noarch                           42/46
2018-12-04 05:59:05,854 INFO:   Cleanup    : kernel-tools-libs-3.10.0-957.el7.x86_64                    43/46
2018-12-04 05:59:05,884 INFO:   Cleanup    : 1:NetworkManager-libnm-1.12.0-6.el7.x86_64                 44/46
2018-12-04 05:59:05,919 INFO:   Cleanup    : python-lxml-3.2.1-4.el7.x86_64                             45/46
2018-12-04 06:00:55,344 INFO:   Cleanup    : python-perf-3.10.0-957.el7.x86_64                          46/46
2018-12-04 06:00:55,395 INFO:   Verifying  : kernel-3.10.0-957.1.3.el7.x86_64                            1/46
2018-12-04 06:00:55,408 INFO:   Verifying  : selinux-policy-3.13.1-229.el7_6.6.noarch                    2/46
2018-12-04 06:00:55,417 INFO:   Verifying  : 1:NetworkManager-libnm-1.12.0-8.el7_6.x86_64                3/46
2018-12-04 06:00:55,424 INFO:   Verifying  : linux-firmware-20180911-69.git85c5d90.el7.noarch            4/46
2018-12-04 06:00:55,469 INFO:   Verifying  : kernel-tools-libs-3.10.0-957.1.3.el7.x86_64                 5/46
2018-12-04 06:00:55,514 INFO:   Verifying  : kernel-tools-3.10.0-957.1.3.el7.x86_64                      6/46
2018-12-04 06:00:55,522 INFO:   Verifying  : heat-cfntools-1.3.0-2.el7ost.noarch                         7/46
2018-12-04 06:00:55,568 INFO:   Verifying  : python-perf-3.10.0-957.1.3.el7.x86_64                       8/46
2018-12-04 06:00:55,575 INFO:   Verifying  : python-jsonpointer-1.10-4.1.el7ost.noarch                   9/46
2018-12-04 06:00:55,583 INFO:   Verifying  : 7:device-mapper-1.02.149-10.el7_6.2.x86_64                 10/46
2018-12-04 06:00:55,592 INFO:   Verifying  : 7:device-mapper-libs-1.02.149-10.el7_6.2.x86_64            11/46
2018-12-04 06:00:55,601 INFO:   Verifying  : 1:NetworkManager-tui-1.12.0-8.el7_6.x86_64                 12/46
2018-12-04 06:00:55,610 INFO:   Verifying  : 1:NetworkManager-1.12.0-8.el7_6.x86_64                     13/46
2018-12-04 06:00:55,618 INFO:   Verifying  : subscription-manager-rhsm-1.21.10-3.el7_6.x86_64           14/46
2018-12-04 06:00:55,627 INFO:   Verifying  : 1:NetworkManager-team-1.12.0-8.el7_6.x86_64                15/46
2018-12-04 06:00:55,636 INFO:   Verifying  : 1:NetworkManager-config-server-1.12.0-8.el7_6.noarch       16/46
2018-12-04 06:00:55,644 INFO:   Verifying  : subscription-manager-1.21.10-3.el7_6.x86_64                17/46
2018-12-04 06:00:55,651 INFO:   Verifying  : cronie-anacron-1.4.11-20.el7_6.x86_64                      18/46
2018-12-04 06:00:55,659 INFO:   Verifying  : python-lxml-3.2.1-6.el7ost.x86_64                          19/46
2018-12-04 06:00:55,671 INFO:   Verifying  : selinux-policy-targeted-3.13.1-229.el7_6.6.noarch          20/46
2018-12-04 06:00:55,679 INFO:   Verifying  : tzdata-2018g-1.el7.noarch                                  21/46
2018-12-04 06:00:55,687 INFO:   Verifying  : subscription-manager-rhsm-certificates-1.21.10-3.el7_6.x   22/46
2018-12-04 06:00:55,694 INFO:   Verifying  : python2-jsonpatch-1.21-1.el7ost.noarch                     23/46
2018-12-04 06:00:55,701 INFO:   Verifying  : cronie-1.4.11-20.el7_6.x86_64                              24/46
2018-12-04 06:00:55,702 INFO:   Verifying  : 1:NetworkManager-1.12.0-6.el7.x86_64                       25/46
2018-12-04 06:00:55,702 INFO:   Verifying  : 1:NetworkManager-team-1.12.0-6.el7.x86_64                  26/46
2018-12-04 06:00:55,703 INFO:   Verifying  : python-lxml-3.2.1-4.el7.x86_64                             27/46
2018-12-04 06:00:55,703 INFO:   Verifying  : subscription-manager-rhsm-1.21.10-2.el7.x86_64             28/46
2018-12-04 06:00:55,704 INFO:   Verifying  : selinux-policy-3.13.1-229.el7.noarch                       29/46
2018-12-04 06:00:55,704 INFO:   Verifying  : kernel-tools-libs-3.10.0-957.el7.x86_64                    30/46
2018-12-04 06:00:55,704 INFO:   Verifying  : 7:device-mapper-1.02.149-8.el7.x86_64                      31/46
2018-12-04 06:00:55,705 INFO:   Verifying  : selinux-policy-targeted-3.13.1-229.el7.noarch              32/46
2018-12-04 06:00:55,706 INFO:   Verifying  : subscription-manager-rhsm-certificates-1.21.10-2.el7.x86   33/46
2018-12-04 06:00:55,706 INFO:   Verifying  : 1:NetworkManager-config-server-1.12.0-6.el7.noarch         34/46
2018-12-04 06:00:55,706 INFO:   Verifying  : python-jsonpointer-1.9-2.el7.noarch                        35/46
2018-12-04 06:00:55,707 INFO:   Verifying  : python-jsonpatch-1.2-4.el7.noarch                          36/46
2018-12-04 06:00:55,707 INFO:   Verifying  : tzdata-2018e-3.el7.noarch                                  37/46
2018-12-04 06:00:55,708 INFO:   Verifying  : 1:NetworkManager-libnm-1.12.0-6.el7.x86_64                 38/46
2018-12-04 06:00:55,708 INFO:   Verifying  : kernel-tools-3.10.0-957.el7.x86_64                         39/46
2018-12-04 06:00:55,709 INFO:   Verifying  : 1:NetworkManager-tui-1.12.0-6.el7.x86_64                   40/46
2018-12-04 06:00:55,709 INFO:   Verifying  : cronie-1.4.11-19.el7.x86_64                                41/46
2018-12-04 06:00:55,710 INFO:   Verifying  : subscription-manager-1.21.10-2.el7.x86_64                  42/46
2018-12-04 06:00:55,710 INFO:   Verifying  : python-perf-3.10.0-957.el7.x86_64                          43/46
2018-12-04 06:00:55,711 INFO:   Verifying  : cronie-anacron-1.4.11-19.el7.x86_64                        44/46
2018-12-04 06:00:55,711 INFO:   Verifying  : 7:device-mapper-libs-1.02.149-8.el7.x86_64                 45/46
2018-12-04 06:00:55,876 INFO:   Verifying  : heat-cfntools-1.2.6-5.el7.noarch                           46/46
2018-12-04 06:00:55,876 INFO: 
2018-12-04 06:00:55,876 INFO: Installed:
2018-12-04 06:00:55,877 INFO:   kernel.x86_64 0:3.10.0-957.1.3.el7  python2-jsonpatch.noarch 0:1.21-1.el7ost
2018-12-04 06:00:55,877 INFO: 
2018-12-04 06:00:55,877 INFO: Dependency Installed:
2018-12-04 06:00:55,877 INFO:   linux-firmware.noarch 0:20180911-69.git85c5d90.el7
2018-12-04 06:00:55,877 INFO: 
2018-12-04 06:00:55,878 INFO: Updated:
2018-12-04 06:00:55,878 INFO:   NetworkManager.x86_64 1:1.12.0-8.el7_6
2018-12-04 06:00:55,878 INFO:   NetworkManager-config-server.noarch 1:1.12.0-8.el7_6
2018-12-04 06:00:55,879 INFO:   NetworkManager-libnm.x86_64 1:1.12.0-8.el7_6
2018-12-04 06:00:55,879 INFO:   NetworkManager-team.x86_64 1:1.12.0-8.el7_6
2018-12-04 06:00:55,879 INFO:   NetworkManager-tui.x86_64 1:1.12.0-8.el7_6
2018-12-04 06:00:55,879 INFO:   cronie.x86_64 0:1.4.11-20.el7_6
2018-12-04 06:00:55,880 INFO:   cronie-anacron.x86_64 0:1.4.11-20.el7_6
2018-12-04 06:00:55,880 INFO:   device-mapper.x86_64 7:1.02.149-10.el7_6.2
2018-12-04 06:00:55,880 INFO:   device-mapper-libs.x86_64 7:1.02.149-10.el7_6.2
2018-12-04 06:00:55,881 INFO:   heat-cfntools.noarch 0:1.3.0-2.el7ost
2018-12-04 06:00:55,881 INFO:   kernel-tools.x86_64 0:3.10.0-957.1.3.el7
2018-12-04 06:00:55,881 INFO:   kernel-tools-libs.x86_64 0:3.10.0-957.1.3.el7
2018-12-04 06:00:55,881 INFO:   python-jsonpointer.noarch 0:1.10-4.1.el7ost
2018-12-04 06:00:55,882 INFO:   python-lxml.x86_64 0:3.2.1-6.el7ost
2018-12-04 06:00:55,882 INFO:   python-perf.x86_64 0:3.10.0-957.1.3.el7
2018-12-04 06:00:55,882 INFO:   selinux-policy.noarch 0:3.13.1-229.el7_6.6
2018-12-04 06:00:55,882 INFO:   selinux-policy-targeted.noarch 0:3.13.1-229.el7_6.6
2018-12-04 06:00:55,883 INFO:   subscription-manager.x86_64 0:1.21.10-3.el7_6
2018-12-04 06:00:55,883 INFO:   subscription-manager-rhsm.x86_64 0:1.21.10-3.el7_6
2018-12-04 06:00:55,883 INFO:   subscription-manager-rhsm-certificates.x86_64 0:1.21.10-3.el7_6
2018-12-04 06:00:55,884 INFO:   tzdata.noarch 0:2018g-1.el7
2018-12-04 06:00:55,884 INFO: 
2018-12-04 06:00:55,884 INFO: Replaced:
2018-12-04 06:00:55,884 INFO:   python-jsonpatch.noarch 0:1.2-4.el7
2018-12-04 06:00:55,884 INFO: 
2018-12-04 06:00:55,885 INFO: Complete!
2018-12-04 06:00:55,925 INFO: yum-update completed successfully
2018-12-04 06:00:55,993 INFO: Running instack
2018-12-04 06:00:56,209 INFO: INFO: 2018-12-04 06:00:56,209 -- Starting run of instack
2018-12-04 06:00:56,223 INFO: INFO: 2018-12-04 06:00:56,222 -- Using json file: /usr/share/instack-undercloud/json-files/rhel-7-undercloud-packages.json
2018-12-04 06:00:56,223 INFO: INFO: 2018-12-04 06:00:56,223 -- Running Installation
2018-12-04 06:00:56,224 INFO: INFO: 2018-12-04 06:00:56,224 -- Initialized with elements path: /usr/share/tripleo-puppet-elements /usr/share/instack-undercloud /usr/share/tripleo-image-elements /usr/share/diskimage-builder/elements
2018-12-04 06:00:56,249 INFO: WARNING: 2018-12-04 06:00:56,248 -- expand_dependencies() deprecated, use get_elements
2018-12-04 06:00:56,283 INFO: INFO: 2018-12-04 06:00:56,283 -- List of all elements and dependencies: epel undercloud-install dib-python source-repositories install-types puppet-modules install-bin pip-manifest puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url pkg-map enable-packages-install puppet os-apply-config hiera package-installs
2018-12-04 06:00:56,284 INFO: INFO: 2018-12-04 06:00:56,283 -- Excluding element pip-and-virtualenv
2018-12-04 06:00:56,284 INFO: INFO: 2018-12-04 06:00:56,283 -- Excluding element epel
2018-12-04 06:00:56,284 INFO: INFO: 2018-12-04 06:00:56,283 -- Excluding element pip-manifest
2018-12-04 06:00:56,285 INFO: INFO: 2018-12-04 06:00:56,283 -- Excluding element package-installs
2018-12-04 06:00:56,285 INFO: INFO: 2018-12-04 06:00:56,283 -- Excluding element pkg-map
2018-12-04 06:00:56,285 INFO: INFO: 2018-12-04 06:00:56,284 -- Excluding element puppet
2018-12-04 06:00:56,285 INFO: INFO: 2018-12-04 06:00:56,284 -- Excluding element cache-url
2018-12-04 06:00:56,286 INFO: INFO: 2018-12-04 06:00:56,284 -- Excluding element dib-python
2018-12-04 06:00:56,286 INFO: INFO: 2018-12-04 06:00:56,284 -- Excluding element install-bin
2018-12-04 06:00:56,286 INFO: INFO: 2018-12-04 06:00:56,284 -- List of all elements and dependencies after excludes: undercloud-install source-repositories install-types puppet-modules puppet-stack-config os-refresh-config element-manifest manifests enable-packages-install os-apply-config hiera
2018-12-04 06:00:56,534 INFO: INFO: 2018-12-04 06:00:56,534 --   Running hook extra-data
2018-12-04 06:00:56,535 INFO: INFO: 2018-12-04 06:00:56,534 -- ############### Begin stdout/stderr logging ###############
2018-12-04 06:00:56,553 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/../environment.d/00-dib-v2-env
2018-12-04 06:00:56,557 INFO: + source /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/../environment.d/00-dib-v2-env
2018-12-04 06:00:56,557 INFO: ++ export 'IMAGE_ELEMENT=epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs'
2018-12-04 06:00:56,558 INFO: ++ IMAGE_ELEMENT='epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs'
2018-12-04 06:00:56,558 INFO: ++ export 'IMAGE_ELEMENT_YAML={cache-url: /usr/share/diskimage-builder/elements/cache-url, dib-python: /usr/share/diskimage-builder/elements/dib-python,
2018-12-04 06:00:56,559 INFO:   element-manifest: /usr/share/diskimage-builder/elements/element-manifest, enable-packages-install: /usr/share/tripleo-image-elements/enable-packages-install,
2018-12-04 06:00:56,559 INFO:   epel: /usr/share/diskimage-builder/elements/epel, hiera: /usr/share/tripleo-puppet-elements/hiera,
2018-12-04 06:00:56,560 INFO:   install-bin: /usr/share/diskimage-builder/elements/install-bin, install-types: /usr/share/diskimage-builder/elements/install-types,
2018-12-04 06:00:56,560 INFO:   manifests: /usr/share/diskimage-builder/elements/manifests, os-apply-config: /usr/share/tripleo-image-elements/os-apply-config,
2018-12-04 06:00:56,561 INFO:   os-refresh-config: /usr/share/tripleo-image-elements/os-refresh-config, package-installs: /usr/share/diskimage-builder/elements/package-installs,
2018-12-04 06:00:56,561 INFO:   pip-and-virtualenv: /usr/share/diskimage-builder/elements/pip-and-virtualenv, pip-manifest: /usr/share/tripleo-image-elements/pip-manifest,
2018-12-04 06:00:56,562 INFO:   pkg-map: /usr/share/diskimage-builder/elements/pkg-map, puppet: /usr/share/tripleo-puppet-elements/puppet,
2018-12-04 06:00:56,562 INFO:   puppet-modules: /usr/share/tripleo-puppet-elements/puppet-modules, puppet-stack-config: /usr/share/instack-undercloud/puppet-stack-config,
2018-12-04 06:00:56,562 INFO:   source-repositories: /usr/share/diskimage-builder/elements/source-repositories,
2018-12-04 06:00:56,563 INFO:   undercloud-install: /usr/share/instack-undercloud/undercloud-install}
2018-12-04 06:00:56,563 INFO: '
2018-12-04 06:00:56,563 INFO: ++ IMAGE_ELEMENT_YAML='{cache-url: /usr/share/diskimage-builder/elements/cache-url, dib-python: /usr/share/diskimage-builder/elements/dib-python,
2018-12-04 06:00:56,564 INFO:   element-manifest: /usr/share/diskimage-builder/elements/element-manifest, enable-packages-install: /usr/share/tripleo-image-elements/enable-packages-install,
2018-12-04 06:00:56,564 INFO:   epel: /usr/share/diskimage-builder/elements/epel, hiera: /usr/share/tripleo-puppet-elements/hiera,
2018-12-04 06:00:56,564 INFO:   install-bin: /usr/share/diskimage-builder/elements/install-bin, install-types: /usr/share/diskimage-builder/elements/install-types,
2018-12-04 06:00:56,565 INFO:   manifests: /usr/share/diskimage-builder/elements/manifests, os-apply-config: /usr/share/tripleo-image-elements/os-apply-config,
2018-12-04 06:00:56,565 INFO:   os-refresh-config: /usr/share/tripleo-image-elements/os-refresh-config, package-installs: /usr/share/diskimage-builder/elements/package-installs,
2018-12-04 06:00:56,566 INFO:   pip-and-virtualenv: /usr/share/diskimage-builder/elements/pip-and-virtualenv, pip-manifest: /usr/share/tripleo-image-elements/pip-manifest,
2018-12-04 06:00:56,566 INFO:   pkg-map: /usr/share/diskimage-builder/elements/pkg-map, puppet: /usr/share/tripleo-puppet-elements/puppet,
2018-12-04 06:00:56,567 INFO:   puppet-modules: /usr/share/tripleo-puppet-elements/puppet-modules, puppet-stack-config: /usr/share/instack-undercloud/puppet-stack-config,
2018-12-04 06:00:56,567 INFO:   source-repositories: /usr/share/diskimage-builder/elements/source-repositories,
2018-12-04 06:00:56,567 INFO:   undercloud-install: /usr/share/instack-undercloud/undercloud-install}
2018-12-04 06:00:56,567 INFO: '
2018-12-04 06:00:56,568 INFO: ++ export -f get_image_element_array
2018-12-04 06:00:56,568 INFO: + set +o xtrace
2018-12-04 06:00:56,568 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/../environment.d/01-export-install-types.bash
2018-12-04 06:00:56,569 INFO: + source /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/../environment.d/01-export-install-types.bash
2018-12-04 06:00:56,569 INFO: ++ export DIB_DEFAULT_INSTALLTYPE=package
2018-12-04 06:00:56,570 INFO: ++ DIB_DEFAULT_INSTALLTYPE=package
2018-12-04 06:00:56,570 INFO: + set +o xtrace
2018-12-04 06:00:56,570 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/../environment.d/01-puppet-module-pins.sh
2018-12-04 06:00:56,571 INFO: + source /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/../environment.d/01-puppet-module-pins.sh
2018-12-04 06:00:56,571 INFO: ++ export DIB_REPOREF_puppetlabs_ntp=4.2.x
2018-12-04 06:00:56,571 INFO: ++ DIB_REPOREF_puppetlabs_ntp=4.2.x
2018-12-04 06:00:56,572 INFO: + set +o xtrace
2018-12-04 06:00:56,572 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/../environment.d/02-puppet-modules-install-types.sh
2018-12-04 06:00:56,572 INFO: + source /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/../environment.d/02-puppet-modules-install-types.sh
2018-12-04 06:00:56,572 INFO: ++ DIB_DEFAULT_INSTALLTYPE=package
2018-12-04 06:00:56,573 INFO: ++ DIB_INSTALLTYPE_puppet_modules=package
2018-12-04 06:00:56,573 INFO: ++ '[' package = source ']'
2018-12-04 06:00:56,573 INFO: + set +o xtrace
2018-12-04 06:00:56,574 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/../environment.d/10-os-apply-config-venv-dir.bash
2018-12-04 06:00:56,574 INFO: + source /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/../environment.d/10-os-apply-config-venv-dir.bash
2018-12-04 06:00:56,574 INFO: ++ '[' -z '' ']'
2018-12-04 06:00:56,574 INFO: ++ export OS_APPLY_CONFIG_VENV_DIR=/opt/stack/venvs/os-apply-config
2018-12-04 06:00:56,575 INFO: ++ OS_APPLY_CONFIG_VENV_DIR=/opt/stack/venvs/os-apply-config
2018-12-04 06:00:56,575 INFO: + set +o xtrace
2018-12-04 06:00:56,575 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/../environment.d/14-manifests
2018-12-04 06:00:56,576 INFO: + source /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/../environment.d/14-manifests
2018-12-04 06:00:56,576 INFO: ++ export DIB_MANIFEST_IMAGE_DIR=/etc/dib-manifests
2018-12-04 06:00:56,576 INFO: ++ DIB_MANIFEST_IMAGE_DIR=/etc/dib-manifests
2018-12-04 06:00:56,576 INFO: ++ export DIB_MANIFEST_SAVE_DIR=instack.d/
2018-12-04 06:00:56,577 INFO: ++ DIB_MANIFEST_SAVE_DIR=instack.d/
2018-12-04 06:00:56,577 INFO: + set +o xtrace
2018-12-04 06:00:56,577 INFO: dib-run-parts Running /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/10-install-git
2018-12-04 06:00:56,578 INFO: + yum -y install git
2018-12-04 06:00:56,833 INFO: Loaded plugins: search-disabled-repos
2018-12-04 06:00:57,002 INFO: Package git-1.8.3.1-20.el7.x86_64 already installed and latest version
2018-12-04 06:00:57,003 INFO: Nothing to do
2018-12-04 06:00:57,026 INFO: dib-run-parts 10-install-git completed
2018-12-04 06:00:57,027 INFO: dib-run-parts Running /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/20-manifest-dir
2018-12-04 06:00:57,031 INFO: + set -eu
2018-12-04 06:00:57,031 INFO: + set -o pipefail
2018-12-04 06:00:57,032 INFO: + sudo mkdir -p /root/.instack/tmp/instack.l9o3wc/mnt//etc/dib-manifests
2018-12-04 06:00:57,059 INFO: dib-run-parts 20-manifest-dir completed
2018-12-04 06:00:57,059 INFO: dib-run-parts Running /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/75-inject-element-manifest
2018-12-04 06:00:57,064 INFO: + set -eu
2018-12-04 06:00:57,064 INFO: + set -o pipefail
2018-12-04 06:00:57,064 INFO: + DIB_ELEMENT_MANIFEST_PATH=/etc/dib-manifests/dib-element-manifest
2018-12-04 06:00:57,064 INFO: ++ dirname /etc/dib-manifests/dib-element-manifest
2018-12-04 06:00:57,066 INFO: + sudo mkdir -p /root/.instack/tmp/instack.l9o3wc/mnt//etc/dib-manifests
2018-12-04 06:00:57,091 INFO: + sudo /bin/bash -c 'echo epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs | tr '\'' '\'' '\''\n'\'' > /root/.instack/tmp/instack.l9o3wc/mnt//etc/dib-manifests/dib-element-manifest'
2018-12-04 06:00:57,119 INFO: dib-run-parts 75-inject-element-manifest completed
2018-12-04 06:00:57,119 INFO: dib-run-parts Running /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/98-source-repositories
2018-12-04 06:00:57,138 INFO: Getting /root/.cache/image-create/source-repositories/repositories_flock: Tue Dec  4 06:00:57 EST 2018 for /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/source-repository-puppet-modules
2018-12-04 06:00:57,144 INFO: (0001 / 0081)
2018-12-04 06:00:57,150 INFO: puppetlabs-apache install type not set to source
2018-12-04 06:00:57,151 INFO: (0002 / 0081)
2018-12-04 06:00:57,157 INFO: puppet-aodh install type not set to source
2018-12-04 06:00:57,158 INFO: (0003 / 0081)
2018-12-04 06:00:57,163 INFO: puppet-auditd install type not set to source
2018-12-04 06:00:57,164 INFO: (0004 / 0081)
2018-12-04 06:00:57,170 INFO: puppet-barbican install type not set to source
2018-12-04 06:00:57,171 INFO: (0005 / 0081)
2018-12-04 06:00:57,177 INFO: puppet-cassandra install type not set to source
2018-12-04 06:00:57,178 INFO: (0006 / 0081)
2018-12-04 06:00:57,184 INFO: puppet-ceph install type not set to source
2018-12-04 06:00:57,185 INFO: (0007 / 0081)
2018-12-04 06:00:57,191 INFO: puppet-ceilometer install type not set to source
2018-12-04 06:00:57,192 INFO: (0008 / 0081)
2018-12-04 06:00:57,198 INFO: puppet-congress install type not set to source
2018-12-04 06:00:57,198 INFO: (0009 / 0081)
2018-12-04 06:00:57,204 INFO: puppet-gnocchi install type not set to source
2018-12-04 06:00:57,205 INFO: (0010 / 0081)
2018-12-04 06:00:57,211 INFO: puppet-certmonger install type not set to source
2018-12-04 06:00:57,212 INFO: (0011 / 0081)
2018-12-04 06:00:57,218 INFO: puppet-cinder install type not set to source
2018-12-04 06:00:57,219 INFO: (0012 / 0081)
2018-12-04 06:00:57,224 INFO: puppet-common install type not set to source
2018-12-04 06:00:57,225 INFO: (0013 / 0081)
2018-12-04 06:00:57,231 INFO: puppet-contrail install type not set to source
2018-12-04 06:00:57,232 INFO: (0014 / 0081)
2018-12-04 06:00:57,238 INFO: puppetlabs-concat install type not set to source
2018-12-04 06:00:57,239 INFO: (0015 / 0081)
2018-12-04 06:00:57,245 INFO: puppetlabs-firewall install type not set to source
2018-12-04 06:00:57,246 INFO: (0016 / 0081)
2018-12-04 06:00:57,251 INFO: puppet-glance install type not set to source
2018-12-04 06:00:57,252 INFO: (0017 / 0081)
2018-12-04 06:00:57,258 INFO: puppet-gluster install type not set to source
2018-12-04 06:00:57,259 INFO: (0018 / 0081)
2018-12-04 06:00:57,265 INFO: puppetlabs-haproxy install type not set to source
2018-12-04 06:00:57,266 INFO: (0019 / 0081)
2018-12-04 06:00:57,272 INFO: puppet-heat install type not set to source
2018-12-04 06:00:57,273 INFO: (0020 / 0081)
2018-12-04 06:00:57,278 INFO: puppet-healthcheck install type not set to source
2018-12-04 06:00:57,279 INFO: (0021 / 0081)
2018-12-04 06:00:57,285 INFO: puppet-horizon install type not set to source
2018-12-04 06:00:57,286 INFO: (0022 / 0081)
2018-12-04 06:00:57,292 INFO: puppetlabs-inifile install type not set to source
2018-12-04 06:00:57,293 INFO: (0023 / 0081)
2018-12-04 06:00:57,299 INFO: puppet-kafka install type not set to source
2018-12-04 06:00:57,300 INFO: (0024 / 0081)
2018-12-04 06:00:57,305 INFO: puppet-keystone install type not set to source
2018-12-04 06:00:57,306 INFO: (0025 / 0081)
2018-12-04 06:00:57,312 INFO: puppet-manila install type not set to source
2018-12-04 06:00:57,313 INFO: (0026 / 0081)
2018-12-04 06:00:57,319 INFO: puppet-memcached install type not set to source
2018-12-04 06:00:57,320 INFO: (0027 / 0081)
2018-12-04 06:00:57,326 INFO: puppet-mistral install type not set to source
2018-12-04 06:00:57,327 INFO: (0028 / 0081)
2018-12-04 06:00:57,333 INFO: puppetlabs-mongodb install type not set to source
2018-12-04 06:00:57,334 INFO: (0029 / 0081)
2018-12-04 06:00:57,341 INFO: puppetlabs-mysql install type not set to source
2018-12-04 06:00:57,342 INFO: (0030 / 0081)
2018-12-04 06:00:57,348 INFO: puppet-neutron install type not set to source
2018-12-04 06:00:57,349 INFO: (0031 / 0081)
2018-12-04 06:00:57,355 INFO: puppet-nova install type not set to source
2018-12-04 06:00:57,356 INFO: (0032 / 0081)
2018-12-04 06:00:57,362 INFO: puppet-octavia install type not set to source
2018-12-04 06:00:57,363 INFO: (0033 / 0081)
2018-12-04 06:00:57,369 INFO: puppet-oslo install type not set to source
2018-12-04 06:00:57,370 INFO: (0034 / 0081)
2018-12-04 06:00:57,376 INFO: puppet-nssdb install type not set to source
2018-12-04 06:00:57,377 INFO: (0035 / 0081)
2018-12-04 06:00:57,383 INFO: puppet-opendaylight install type not set to source
2018-12-04 06:00:57,384 INFO: (0036 / 0081)
2018-12-04 06:00:57,390 INFO: puppet-ovn install type not set to source
2018-12-04 06:00:57,391 INFO: (0037 / 0081)
2018-12-04 06:00:57,397 INFO: puppet-panko install type not set to source
2018-12-04 06:00:57,398 INFO: (0038 / 0081)
2018-12-04 06:00:57,405 INFO: puppet-puppet install type not set to source
2018-12-04 06:00:57,406 INFO: (0039 / 0081)
2018-12-04 06:00:57,412 INFO: puppetlabs-rabbitmq install type not set to source
2018-12-04 06:00:57,413 INFO: (0040 / 0081)
2018-12-04 06:00:57,419 INFO: puppet-redis install type not set to source
2018-12-04 06:00:57,420 INFO: (0041 / 0081)
2018-12-04 06:00:57,426 INFO: puppetlabs-rsync install type not set to source
2018-12-04 06:00:57,427 INFO: (0042 / 0081)
2018-12-04 06:00:57,433 INFO: puppet-sahara install type not set to source
2018-12-04 06:00:57,434 INFO: (0043 / 0081)
2018-12-04 06:00:57,440 INFO: sensu-puppet install type not set to source
2018-12-04 06:00:57,441 INFO: (0044 / 0081)
2018-12-04 06:00:57,447 INFO: puppet-tacker install type not set to source
2018-12-04 06:00:57,448 INFO: (0045 / 0081)
2018-12-04 06:00:57,454 INFO: puppet-trove install type not set to source
2018-12-04 06:00:57,455 INFO: (0046 / 0081)
2018-12-04 06:00:57,461 INFO: puppet-ssh install type not set to source
2018-12-04 06:00:57,462 INFO: (0047 / 0081)
2018-12-04 06:00:57,468 INFO: puppet-staging install type not set to source
2018-12-04 06:00:57,469 INFO: (0048 / 0081)
2018-12-04 06:00:57,475 INFO: puppetlabs-stdlib install type not set to source
2018-12-04 06:00:57,476 INFO: (0049 / 0081)
2018-12-04 06:00:57,483 INFO: puppet-swift install type not set to source
2018-12-04 06:00:57,484 INFO: (0050 / 0081)
2018-12-04 06:00:57,490 INFO: puppetlabs-sysctl install type not set to source
2018-12-04 06:00:57,491 INFO: (0051 / 0081)
2018-12-04 06:00:57,497 INFO: puppet-timezone install type not set to source
2018-12-04 06:00:57,498 INFO: (0052 / 0081)
2018-12-04 06:00:57,504 INFO: puppet-uchiwa install type not set to source
2018-12-04 06:00:57,505 INFO: (0053 / 0081)
2018-12-04 06:00:57,511 INFO: puppetlabs-vcsrepo install type not set to source
2018-12-04 06:00:57,512 INFO: (0054 / 0081)
2018-12-04 06:00:57,518 INFO: puppet-vlan install type not set to source
2018-12-04 06:00:57,519 INFO: (0055 / 0081)
2018-12-04 06:00:57,525 INFO: puppet-vswitch install type not set to source
2018-12-04 06:00:57,527 INFO: (0056 / 0081)
2018-12-04 06:00:57,533 INFO: puppetlabs-xinetd install type not set to source
2018-12-04 06:00:57,534 INFO: (0057 / 0081)
2018-12-04 06:00:57,540 INFO: puppet-zookeeper install type not set to source
2018-12-04 06:00:57,541 INFO: (0058 / 0081)
2018-12-04 06:00:57,547 INFO: puppet-openstacklib install type not set to source
2018-12-04 06:00:57,548 INFO: (0059 / 0081)
2018-12-04 06:00:57,554 INFO: puppet-module-keepalived install type not set to source
2018-12-04 06:00:57,555 INFO: (0060 / 0081)
2018-12-04 06:00:57,561 INFO: puppetlabs-ntp install type not set to source
2018-12-04 06:00:57,562 INFO: (0061 / 0081)
2018-12-04 06:00:57,568 INFO: puppet-snmp install type not set to source
2018-12-04 06:00:57,569 INFO: (0062 / 0081)
2018-12-04 06:00:57,575 INFO: puppet-tripleo install type not set to source
2018-12-04 06:00:57,576 INFO: (0063 / 0081)
2018-12-04 06:00:57,582 INFO: puppet-ironic install type not set to source
2018-12-04 06:00:57,583 INFO: (0064 / 0081)
2018-12-04 06:00:57,589 INFO: puppet-ipaclient install type not set to source
2018-12-04 06:00:57,590 INFO: (0065 / 0081)
2018-12-04 06:00:57,597 INFO: puppetlabs-corosync install type not set to source
2018-12-04 06:00:57,598 INFO: (0066 / 0081)
2018-12-04 06:00:57,604 INFO: puppet-pacemaker install type not set to source
2018-12-04 06:00:57,605 INFO: (0067 / 0081)
2018-12-04 06:00:57,611 INFO: puppet_aviator install type not set to source
2018-12-04 06:00:57,612 INFO: (0068 / 0081)
2018-12-04 06:00:57,619 INFO: puppet-openstack_extras install type not set to source
2018-12-04 06:00:57,619 INFO: (0069 / 0081)
2018-12-04 06:00:57,626 INFO: konstantin-fluentd install type not set to source
2018-12-04 06:00:57,627 INFO: (0070 / 0081)
2018-12-04 06:00:57,633 INFO: puppet-elasticsearch install type not set to source
2018-12-04 06:00:57,634 INFO: (0071 / 0081)
2018-12-04 06:00:57,640 INFO: puppet-kibana3 install type not set to source
2018-12-04 06:00:57,641 INFO: (0072 / 0081)
2018-12-04 06:00:57,647 INFO: puppetlabs-git install type not set to source
2018-12-04 06:00:57,648 INFO: (0073 / 0081)
2018-12-04 06:00:57,654 INFO: puppet-datacat install type not set to source
2018-12-04 06:00:57,655 INFO: (0074 / 0081)
2018-12-04 06:00:57,661 INFO: puppet-kmod install type not set to source
2018-12-04 06:00:57,662 INFO: (0075 / 0081)
2018-12-04 06:00:57,669 INFO: puppet-zaqar install type not set to source
2018-12-04 06:00:57,669 INFO: (0076 / 0081)
2018-12-04 06:00:57,676 INFO: puppet-ec2api install type not set to source
2018-12-04 06:00:57,677 INFO: (0077 / 0081)
2018-12-04 06:00:57,683 INFO: puppet-qdr install type not set to source
2018-12-04 06:00:57,684 INFO: (0078 / 0081)
2018-12-04 06:00:57,690 INFO: puppet-systemd install type not set to source
2018-12-04 06:00:57,691 INFO: (0079 / 0081)
2018-12-04 06:00:57,697 INFO: puppet-etcd install type not set to source
2018-12-04 06:00:57,698 INFO: (0080 / 0081)
2018-12-04 06:00:57,705 INFO: puppet-veritas_hyperscale install type not set to source
2018-12-04 06:00:57,705 INFO: (0081 / 0081)
2018-12-04 06:00:57,712 INFO: puppet-ptp install type not set to source
2018-12-04 06:00:57,714 INFO: dib-run-parts 98-source-repositories completed
2018-12-04 06:00:57,715 INFO: dib-run-parts Running /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/extra-data.d/99-enable-install-types
2018-12-04 06:00:57,719 INFO: + set -eu
2018-12-04 06:00:57,719 INFO: + set -o pipefail
2018-12-04 06:00:57,720 INFO: + declare -a SPECIFIED_ELEMS
2018-12-04 06:00:57,720 INFO: + SPECIFIED_ELEMS[0]=
2018-12-04 06:00:57,721 INFO: + PREFIX=DIB_INSTALLTYPE_
2018-12-04 06:00:57,721 INFO: ++ env
2018-12-04 06:00:57,721 INFO: ++ grep '^DIB_INSTALLTYPE_'
2018-12-04 06:00:57,721 INFO: ++ cut -d= -f1
2018-12-04 06:00:57,723 INFO: ++ echo ''
2018-12-04 06:00:57,724 INFO: + INSTALL_TYPE_VARS=
2018-12-04 06:00:57,725 INFO: ++ find /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d -maxdepth 1 -name '*-package-install' -type d
2018-12-04 06:00:57,727 INFO: + default_install_type_dirs=/root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/puppet-modules-package-install
2018-12-04 06:00:57,728 INFO: + for _install_dir in '$default_install_type_dirs'
2018-12-04 06:00:57,728 INFO: + SUFFIX=-package-install
2018-12-04 06:00:57,729 INFO: ++ basename /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/puppet-modules-package-install
2018-12-04 06:00:57,730 INFO: + _install_dir=puppet-modules-package-install
2018-12-04 06:00:57,730 INFO: + INSTALLDIRPREFIX=puppet-modules
2018-12-04 06:00:57,730 INFO: + found=0
2018-12-04 06:00:57,730 INFO: + '[' 0 = 0 ']'
2018-12-04 06:00:57,731 INFO: + pushd /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d
2018-12-04 06:00:57,731 INFO: ~/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d /home/stack
2018-12-04 06:00:57,731 INFO: + ln -sf puppet-modules-package-install/75-puppet-modules-package .
2018-12-04 06:00:57,731 INFO: + popd
2018-12-04 06:00:57,731 INFO: /home/stack
2018-12-04 06:00:57,733 INFO: dib-run-parts 99-enable-install-types completed
2018-12-04 06:00:57,734 INFO: dib-run-parts ----------------------- PROFILING -----------------------
2018-12-04 06:00:57,734 INFO: dib-run-parts
2018-12-04 06:00:57,736 INFO: dib-run-parts Target: extra-data.d
2018-12-04 06:00:57,736 INFO: dib-run-parts
2018-12-04 06:00:57,736 INFO: dib-run-parts Script                                     Seconds
2018-12-04 06:00:57,736 INFO: dib-run-parts ---------------------------------------  ----------
2018-12-04 06:00:57,737 INFO: dib-run-parts
2018-12-04 06:00:57,746 INFO: dib-run-parts 10-install-git                                0.452
2018-12-04 06:00:57,753 INFO: dib-run-parts 20-manifest-dir                               0.030
2018-12-04 06:00:57,760 INFO: dib-run-parts 75-inject-element-manifest                    0.058
2018-12-04 06:00:57,767 INFO: dib-run-parts 98-source-repositories                        0.594
2018-12-04 06:00:57,773 INFO: dib-run-parts 99-enable-install-types                       0.017
2018-12-04 06:00:57,776 INFO: dib-run-parts
2018-12-04 06:00:57,776 INFO: dib-run-parts --------------------- END PROFILING ---------------------
2018-12-04 06:00:57,777 INFO: INFO: 2018-12-04 06:00:57,776 -- ############### End stdout/stderr logging ###############
2018-12-04 06:00:57,777 INFO: INFO: 2018-12-04 06:00:57,777 --   Running hook pre-install
2018-12-04 06:00:57,777 INFO: INFO: 2018-12-04 06:00:57,777 --     Skipping hook pre-install, the hook directory doesn't exist at /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/pre-install.d
2018-12-04 06:00:57,777 INFO: INFO: 2018-12-04 06:00:57,777 --   Running hook install
2018-12-04 06:00:57,778 INFO: INFO: 2018-12-04 06:00:57,778 -- ############### Begin stdout/stderr logging ###############
2018-12-04 06:00:57,795 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../environment.d/00-dib-v2-env
2018-12-04 06:00:57,797 INFO: + source /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../environment.d/00-dib-v2-env
2018-12-04 06:00:57,798 INFO: ++ export 'IMAGE_ELEMENT=epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs'
2018-12-04 06:00:57,798 INFO: ++ IMAGE_ELEMENT='epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs'
2018-12-04 06:00:57,799 INFO: ++ export 'IMAGE_ELEMENT_YAML={cache-url: /usr/share/diskimage-builder/elements/cache-url, dib-python: /usr/share/diskimage-builder/elements/dib-python,
2018-12-04 06:00:57,799 INFO:   element-manifest: /usr/share/diskimage-builder/elements/element-manifest, enable-packages-install: /usr/share/tripleo-image-elements/enable-packages-install,
2018-12-04 06:00:57,800 INFO:   epel: /usr/share/diskimage-builder/elements/epel, hiera: /usr/share/tripleo-puppet-elements/hiera,
2018-12-04 06:00:57,800 INFO:   install-bin: /usr/share/diskimage-builder/elements/install-bin, install-types: /usr/share/diskimage-builder/elements/install-types,
2018-12-04 06:00:57,800 INFO:   manifests: /usr/share/diskimage-builder/elements/manifests, os-apply-config: /usr/share/tripleo-image-elements/os-apply-config,
2018-12-04 06:00:57,801 INFO:   os-refresh-config: /usr/share/tripleo-image-elements/os-refresh-config, package-installs: /usr/share/diskimage-builder/elements/package-installs,
2018-12-04 06:00:57,801 INFO:   pip-and-virtualenv: /usr/share/diskimage-builder/elements/pip-and-virtualenv, pip-manifest: /usr/share/tripleo-image-elements/pip-manifest,
2018-12-04 06:00:57,801 INFO:   pkg-map: /usr/share/diskimage-builder/elements/pkg-map, puppet: /usr/share/tripleo-puppet-elements/puppet,
2018-12-04 06:00:57,802 INFO:   puppet-modules: /usr/share/tripleo-puppet-elements/puppet-modules, puppet-stack-config: /usr/share/instack-undercloud/puppet-stack-config,
2018-12-04 06:00:57,802 INFO:   source-repositories: /usr/share/diskimage-builder/elements/source-repositories,
2018-12-04 06:00:57,802 INFO:   undercloud-install: /usr/share/instack-undercloud/undercloud-install}
2018-12-04 06:00:57,802 INFO: '
2018-12-04 06:00:57,803 INFO: ++ IMAGE_ELEMENT_YAML='{cache-url: /usr/share/diskimage-builder/elements/cache-url, dib-python: /usr/share/diskimage-builder/elements/dib-python,
2018-12-04 06:00:57,803 INFO:   element-manifest: /usr/share/diskimage-builder/elements/element-manifest, enable-packages-install: /usr/share/tripleo-image-elements/enable-packages-install,
2018-12-04 06:00:57,803 INFO:   epel: /usr/share/diskimage-builder/elements/epel, hiera: /usr/share/tripleo-puppet-elements/hiera,
2018-12-04 06:00:57,804 INFO:   install-bin: /usr/share/diskimage-builder/elements/install-bin, install-types: /usr/share/diskimage-builder/elements/install-types,
2018-12-04 06:00:57,804 INFO:   manifests: /usr/share/diskimage-builder/elements/manifests, os-apply-config: /usr/share/tripleo-image-elements/os-apply-config,
2018-12-04 06:00:57,804 INFO:   os-refresh-config: /usr/share/tripleo-image-elements/os-refresh-config, package-installs: /usr/share/diskimage-builder/elements/package-installs,
2018-12-04 06:00:57,805 INFO:   pip-and-virtualenv: /usr/share/diskimage-builder/elements/pip-and-virtualenv, pip-manifest: /usr/share/tripleo-image-elements/pip-manifest,
2018-12-04 06:00:57,805 INFO:   pkg-map: /usr/share/diskimage-builder/elements/pkg-map, puppet: /usr/share/tripleo-puppet-elements/puppet,
2018-12-04 06:00:57,805 INFO:   puppet-modules: /usr/share/tripleo-puppet-elements/puppet-modules, puppet-stack-config: /usr/share/instack-undercloud/puppet-stack-config,
2018-12-04 06:00:57,806 INFO:   source-repositories: /usr/share/diskimage-builder/elements/source-repositories,
2018-12-04 06:00:57,806 INFO:   undercloud-install: /usr/share/instack-undercloud/undercloud-install}
2018-12-04 06:00:57,806 INFO: '
2018-12-04 06:00:57,806 INFO: ++ export -f get_image_element_array
2018-12-04 06:00:57,806 INFO: + set +o xtrace
2018-12-04 06:00:57,807 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../environment.d/01-export-install-types.bash
2018-12-04 06:00:57,807 INFO: + source /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../environment.d/01-export-install-types.bash
2018-12-04 06:00:57,808 INFO: ++ export DIB_DEFAULT_INSTALLTYPE=package
2018-12-04 06:00:57,808 INFO: ++ DIB_DEFAULT_INSTALLTYPE=package
2018-12-04 06:00:57,808 INFO: + set +o xtrace
2018-12-04 06:00:57,808 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../environment.d/01-puppet-module-pins.sh
2018-12-04 06:00:57,809 INFO: + source /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../environment.d/01-puppet-module-pins.sh
2018-12-04 06:00:57,809 INFO: ++ export DIB_REPOREF_puppetlabs_ntp=4.2.x
2018-12-04 06:00:57,809 INFO: ++ DIB_REPOREF_puppetlabs_ntp=4.2.x
2018-12-04 06:00:57,809 INFO: + set +o xtrace
2018-12-04 06:00:57,810 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../environment.d/02-puppet-modules-install-types.sh
2018-12-04 06:00:57,810 INFO: + source /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../environment.d/02-puppet-modules-install-types.sh
2018-12-04 06:00:57,810 INFO: ++ DIB_DEFAULT_INSTALLTYPE=package
2018-12-04 06:00:57,810 INFO: ++ DIB_INSTALLTYPE_puppet_modules=package
2018-12-04 06:00:57,811 INFO: ++ '[' package = source ']'
2018-12-04 06:00:57,811 INFO: + set +o xtrace
2018-12-04 06:00:57,811 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../environment.d/10-os-apply-config-venv-dir.bash
2018-12-04 06:00:57,811 INFO: + source /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../environment.d/10-os-apply-config-venv-dir.bash
2018-12-04 06:00:57,812 INFO: ++ '[' -z '' ']'
2018-12-04 06:00:57,812 INFO: ++ export OS_APPLY_CONFIG_VENV_DIR=/opt/stack/venvs/os-apply-config
2018-12-04 06:00:57,812 INFO: ++ OS_APPLY_CONFIG_VENV_DIR=/opt/stack/venvs/os-apply-config
2018-12-04 06:00:57,812 INFO: + set +o xtrace
2018-12-04 06:00:57,813 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../environment.d/14-manifests
2018-12-04 06:00:57,814 INFO: + source /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../environment.d/14-manifests
2018-12-04 06:00:57,815 INFO: ++ export DIB_MANIFEST_IMAGE_DIR=/etc/dib-manifests
2018-12-04 06:00:57,815 INFO: ++ DIB_MANIFEST_IMAGE_DIR=/etc/dib-manifests
2018-12-04 06:00:57,815 INFO: ++ export DIB_MANIFEST_SAVE_DIR=instack.d/
2018-12-04 06:00:57,815 INFO: ++ DIB_MANIFEST_SAVE_DIR=instack.d/
2018-12-04 06:00:57,816 INFO: + set +o xtrace
2018-12-04 06:00:57,816 INFO: dib-run-parts Running /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/02-puppet-stack-config
2018-12-04 06:00:58,818 INFO: dib-run-parts 02-puppet-stack-config completed
2018-12-04 06:00:58,818 INFO: dib-run-parts Running /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/10-hiera-yaml-symlink
2018-12-04 06:00:58,823 INFO: + set -o pipefail
2018-12-04 06:00:58,823 INFO: + ln -f -s /etc/puppet/hiera.yaml /etc/hiera.yaml
2018-12-04 06:00:58,827 INFO: dib-run-parts 10-hiera-yaml-symlink completed
2018-12-04 06:00:58,827 INFO: dib-run-parts Running /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/10-puppet-stack-config-puppet-module
2018-12-04 06:00:58,831 INFO: + set -o pipefail
2018-12-04 06:00:58,832 INFO: + mkdir -p /etc/puppet/manifests
2018-12-04 06:00:58,834 INFO: ++ dirname /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/10-puppet-stack-config-puppet-module
2018-12-04 06:00:58,836 INFO: + cp /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../puppet-stack-config.pp /etc/puppet/manifests/puppet-stack-config.pp
2018-12-04 06:00:58,840 INFO: dib-run-parts 10-puppet-stack-config-puppet-module completed
2018-12-04 06:00:58,840 INFO: dib-run-parts Running /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/11-create-template-root
2018-12-04 06:00:58,845 INFO: ++ os-apply-config --print-templates
2018-12-04 06:00:59,029 INFO: + TEMPLATE_ROOT=/usr/libexec/os-apply-config/templates
2018-12-04 06:00:59,030 INFO: + mkdir -p /usr/libexec/os-apply-config/templates
2018-12-04 06:00:59,034 INFO: dib-run-parts 11-create-template-root completed
2018-12-04 06:00:59,034 INFO: dib-run-parts Running /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/11-hiera-orc-install
2018-12-04 06:00:59,039 INFO: + set -o pipefail
2018-12-04 06:00:59,039 INFO: + mkdir -p /usr/libexec/os-refresh-config/configure.d/
2018-12-04 06:00:59,042 INFO: ++ dirname /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/11-hiera-orc-install
2018-12-04 06:00:59,043 INFO: + install -m 0755 -o root -g root /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../10-hiera-disable /usr/libexec/os-refresh-config/configure.d/10-hiera-disable
2018-12-04 06:00:59,053 INFO: ++ dirname /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/11-hiera-orc-install
2018-12-04 06:00:59,054 INFO: + install -m 0755 -o root -g root /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../40-hiera-datafiles /usr/libexec/os-refresh-config/configure.d/40-hiera-datafiles
2018-12-04 06:00:59,063 INFO: dib-run-parts 11-hiera-orc-install completed
2018-12-04 06:00:59,063 INFO: dib-run-parts Running /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/75-puppet-modules-package
2018-12-04 06:00:59,068 INFO: + find /opt/stack/puppet-modules/ -mindepth 1
2018-12-04 06:00:59,068 INFO: + read
2018-12-04 06:00:59,076 INFO: + ln -f -s /usr/share/openstack-puppet/modules/aodh /usr/share/openstack-puppet/modules/apache /usr/share/openstack-puppet/modules/archive /usr/share/openstack-puppet/modules/auditd /usr/share/openstack-puppet/modules/barbican /usr/share/openstack-puppet/modules/cassandra /usr/share/openstack-puppet/modules/ceilometer /usr/share/openstack-puppet/modules/ceph /usr/share/openstack-puppet/modules/certmonger /usr/share/openstack-puppet/modules/cinder /usr/share/openstack-puppet/modules/collectd /usr/share/openstack-puppet/modules/concat /usr/share/openstack-puppet/modules/contrail /usr/share/openstack-puppet/modules/corosync /usr/share/openstack-puppet/modules/datacat /usr/share/openstack-puppet/modules/designate /usr/share/openstack-puppet/modules/dns /usr/share/openstack-puppet/modules/ec2api /usr/share/openstack-puppet/modules/elasticsearch /usr/share/openstack-puppet/modules/fdio /usr/share/openstack-puppet/modules/firewall /usr/share/openstack-puppet/modules/fluentd /usr/share/openstack-puppet/modules/git /usr/share/openstack-puppet/modules/glance /usr/share/openstack-puppet/modules/gnocchi /usr/share/openstack-puppet/modules/haproxy /usr/share/openstack-puppet/modules/heat /usr/share/openstack-puppet/modules/horizon /usr/share/openstack-puppet/modules/inifile /usr/share/openstack-puppet/modules/ipaclient /usr/share/openstack-puppet/modules/ironic /usr/share/openstack-puppet/modules/java /usr/share/openstack-puppet/modules/kafka /usr/share/openstack-puppet/modules/keepalived /usr/share/openstack-puppet/modules/keystone /usr/share/openstack-puppet/modules/kibana3 /usr/share/openstack-puppet/modules/kmod /usr/share/openstack-puppet/modules/manila /usr/share/openstack-puppet/modules/memcached /usr/share/openstack-puppet/modules/midonet /usr/share/openstack-puppet/modules/mistral /usr/share/openstack-puppet/modules/module-data /usr/share/openstack-puppet/modules/mysql /usr/share/openstack-puppet/modules/n1k_vsm /usr/share/openstack-puppet/modules/neutron /usr/share/openstack-puppet/modules/nova /usr/share/openstack-puppet/modules/nssdb /usr/share/openstack-puppet/modules/ntp /usr/share/openstack-puppet/modules/octavia /usr/share/openstack-puppet/modules/opendaylight /usr/share/openstack-puppet/modules/openstack_extras /usr/share/openstack-puppet/modules/openstacklib /usr/share/openstack-puppet/modules/oslo /usr/share/openstack-puppet/modules/ovn /usr/share/openstack-puppet/modules/pacemaker /usr/share/openstack-puppet/modules/panko /usr/share/openstack-puppet/modules/rabbitmq /usr/share/openstack-puppet/modules/redis /usr/share/openstack-puppet/modules/remote /usr/share/openstack-puppet/modules/rsync /usr/share/openstack-puppet/modules/sahara /usr/share/openstack-puppet/modules/sensu /usr/share/openstack-puppet/modules/snmp /usr/share/openstack-puppet/modules/ssh /usr/share/openstack-puppet/modules/staging /usr/share/openstack-puppet/modules/stdlib /usr/share/openstack-puppet/modules/swift /usr/share/openstack-puppet/modules/sysctl /usr/share/openstack-puppet/modules/systemd /usr/share/openstack-puppet/modules/timezone /usr/share/openstack-puppet/modules/tomcat /usr/share/openstack-puppet/modules/tripleo /usr/share/openstack-puppet/modules/trove /usr/share/openstack-puppet/modules/uchiwa /usr/share/openstack-puppet/modules/vcsrepo /usr/share/openstack-puppet/modules/veritas_hyperscale /usr/share/openstack-puppet/modules/vswitch /usr/share/openstack-puppet/modules/xinetd /usr/share/openstack-puppet/modules/zaqar /usr/share/openstack-puppet/modules/zookeeper /etc/puppet/modules/
2018-12-04 06:00:59,078 INFO: dib-run-parts 75-puppet-modules-package completed
2018-12-04 06:00:59,078 INFO: dib-run-parts Running /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/99-install-config-templates
2018-12-04 06:00:59,083 INFO: ++ os-apply-config --print-templates
2018-12-04 06:00:59,269 INFO: + TEMPLATE_ROOT=/usr/libexec/os-apply-config/templates
2018-12-04 06:00:59,269 INFO: ++ dirname /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/99-install-config-templates
2018-12-04 06:00:59,271 INFO: + TEMPLATE_SOURCE=/root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../os-apply-config
2018-12-04 06:00:59,271 INFO: + mkdir -p /usr/libexec/os-apply-config/templates
2018-12-04 06:00:59,272 INFO: + '[' -d /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../os-apply-config ']'
2018-12-04 06:00:59,273 INFO: + rsync '--exclude=.*.swp' -Cr /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../os-apply-config/ /usr/libexec/os-apply-config/templates/
2018-12-04 06:00:59,281 INFO: dib-run-parts 99-install-config-templates completed
2018-12-04 06:00:59,281 INFO: dib-run-parts Running /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/99-os-refresh-config-install-scripts
2018-12-04 06:00:59,286 INFO: ++ os-refresh-config --print-base
2018-12-04 06:00:59,367 INFO: + SCRIPT_BASE=/usr/libexec/os-refresh-config
2018-12-04 06:00:59,367 INFO: ++ dirname /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/99-os-refresh-config-install-scripts
2018-12-04 06:00:59,368 INFO: + SCRIPT_SOURCE=/root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../os-refresh-config
2018-12-04 06:00:59,369 INFO: + rsync -r /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/install.d/../os-refresh-config/ /usr/libexec/os-refresh-config/
2018-12-04 06:00:59,375 INFO: dib-run-parts 99-os-refresh-config-install-scripts completed
2018-12-04 06:00:59,376 INFO: dib-run-parts ----------------------- PROFILING -----------------------
2018-12-04 06:00:59,376 INFO: dib-run-parts
2018-12-04 06:00:59,378 INFO: dib-run-parts Target: install.d
2018-12-04 06:00:59,378 INFO: dib-run-parts
2018-12-04 06:00:59,378 INFO: dib-run-parts Script                                     Seconds
2018-12-04 06:00:59,378 INFO: dib-run-parts ---------------------------------------  ----------
2018-12-04 06:00:59,379 INFO: dib-run-parts
2018-12-04 06:00:59,388 INFO: dib-run-parts 02-puppet-stack-config                        1.001
2018-12-04 06:00:59,395 INFO: dib-run-parts 10-hiera-yaml-symlink                         0.007
2018-12-04 06:00:59,402 INFO: dib-run-parts 10-puppet-stack-config-puppet-module          0.011
2018-12-04 06:00:59,409 INFO: dib-run-parts 11-create-template-root                       0.192
2018-12-04 06:00:59,416 INFO: dib-run-parts 11-hiera-orc-install                          0.027
2018-12-04 06:00:59,423 INFO: dib-run-parts 75-puppet-modules-package                     0.013
2018-12-04 06:00:59,429 INFO: dib-run-parts 99-install-config-templates                   0.201
2018-12-04 06:00:59,436 INFO: dib-run-parts 99-os-refresh-config-install-scripts          0.092
2018-12-04 06:00:59,438 INFO: dib-run-parts
2018-12-04 06:00:59,439 INFO: dib-run-parts --------------------- END PROFILING ---------------------
2018-12-04 06:00:59,439 INFO: INFO: 2018-12-04 06:00:59,439 -- ############### End stdout/stderr logging ###############
2018-12-04 06:00:59,439 INFO: INFO: 2018-12-04 06:00:59,439 --   Running hook post-install
2018-12-04 06:00:59,440 INFO: INFO: 2018-12-04 06:00:59,440 --     Skipping hook post-install, the hook directory doesn't exist at /root/.instack/tmp/instack.l9o3wc/hookqFMKg4/post-install.d
2018-12-04 06:00:59,444 INFO: INFO: 2018-12-04 06:00:59,444 -- Ending run of instack.
2018-12-04 06:00:59,458 INFO: Instack completed successfully
2018-12-04 06:00:59,458 INFO: Running os-refresh-config
2018-12-04 06:00:59,559 INFO: [2018-12-04 06:00:59,558] (os-refresh-config) [INFO] Starting phase configure
2018-12-04 06:00:59,573 INFO: dib-run-parts Tue Dec  4 06:00:59 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/10-hiera-disable
2018-12-04 06:00:59,577 INFO: + '[' -f /etc/puppet/hiera.yaml ']'
2018-12-04 06:00:59,580 INFO: dib-run-parts Tue Dec  4 06:00:59 EST 2018 10-hiera-disable completed
2018-12-04 06:00:59,582 INFO: dib-run-parts Tue Dec  4 06:00:59 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/20-os-apply-config
2018-12-04 06:00:59,764 INFO: [2018/12/04 06:00:59 AM] [WARNING] DEPRECATED: falling back to /var/run/os-collect-config/os_config_files.json
2018-12-04 06:00:59,773 INFO: [2018/12/04 06:00:59 AM] [INFO] writing /etc/os-net-config/config.json
2018-12-04 06:00:59,773 INFO: [2018/12/04 06:00:59 AM] [INFO] writing /root/stackrc
2018-12-04 06:00:59,774 INFO: [2018/12/04 06:00:59 AM] [INFO] writing /root/tripleo-undercloud-passwords
2018-12-04 06:00:59,775 INFO: [2018/12/04 06:00:59 AM] [INFO] writing /etc/puppet/hiera.yaml
2018-12-04 06:00:59,775 INFO: [2018/12/04 06:00:59 AM] [INFO] writing /var/opt/undercloud-stack/masquerade
2018-12-04 06:00:59,776 INFO: [2018/12/04 06:00:59 AM] [INFO] writing /etc/puppet/hieradata/RedHat.yaml
2018-12-04 06:00:59,776 INFO: [2018/12/04 06:00:59 AM] [INFO] writing /etc/puppet/hieradata/CentOS.yaml
2018-12-04 06:00:59,777 INFO: [2018/12/04 06:00:59 AM] [INFO] writing /var/run/heat-config/heat-config
2018-12-04 06:00:59,778 INFO: [2018/12/04 06:00:59 AM] [INFO] writing /etc/os-collect-config.conf
2018-12-04 06:00:59,778 INFO: [2018/12/04 06:00:59 AM] [INFO] success
2018-12-04 06:00:59,790 INFO: dib-run-parts Tue Dec  4 06:00:59 EST 2018 20-os-apply-config completed
2018-12-04 06:00:59,792 INFO: dib-run-parts Tue Dec  4 06:00:59 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/30-reload-keepalived
2018-12-04 06:00:59,795 INFO: + systemctl is-enabled keepalived
2018-12-04 06:00:59,806 INFO: Failed to get unit file state for keepalived.service: No such file or directory
2018-12-04 06:00:59,810 INFO: dib-run-parts Tue Dec  4 06:00:59 EST 2018 30-reload-keepalived completed
2018-12-04 06:00:59,811 INFO: dib-run-parts Tue Dec  4 06:00:59 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/40-hiera-datafiles
2018-12-04 06:00:59,994 INFO: [2018/12/04 06:00:59 AM] [WARNING] DEPRECATED: falling back to /var/run/os-collect-config/os_config_files.json
2018-12-04 06:01:00,015 INFO: dib-run-parts Tue Dec  4 06:01:00 EST 2018 40-hiera-datafiles completed
2018-12-04 06:01:00,016 INFO: dib-run-parts Tue Dec  4 06:01:00 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/50-puppet-stack-config
2018-12-04 06:01:00,020 INFO: + set -o pipefail
2018-12-04 06:01:00,020 INFO: + puppet_apply puppet apply --summarize --detailed-exitcodes /etc/puppet/manifests/puppet-stack-config.pp
2018-12-04 06:01:00,021 INFO: + set +e
2018-12-04 06:01:00,021 INFO: + puppet apply --summarize --detailed-exitcodes /etc/puppet/manifests/puppet-stack-config.pp
2018-12-04 06:01:07,230 INFO: [mNotice: hiera(): Cannot load backend module_data: cannot load such file -- hiera/backend/module_data_backend[0m
2018-12-04 06:01:07,424 INFO: [1;33mWarning: ModuleLoader: module 'openstacklib' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:01:07,424 INFO:    (file & line not available)[0m
2018-12-04 06:01:07,909 INFO: [mNotice: hiera(): Cannot load backend module_data: cannot load such file -- hiera/backend/module_data_backend[0m
2018-12-04 06:01:08,030 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:01:08,031 INFO:                     with Stdlib::Compat::Bool. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 54]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-04 06:01:08,031 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:01:08,038 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:01:08,039 INFO:                     with Stdlib::Compat::Absolute_Path. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 55]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-04 06:01:08,039 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:01:08,160 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:01:08,160 INFO:                     with Stdlib::Compat::String. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 56]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-04 06:01:08,161 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:01:08,188 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:01:08,188 INFO:                     with Stdlib::Compat::Array. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 66]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-04 06:01:08,189 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:01:08,193 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:01:08,194 INFO:                     with Pattern[]. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 68]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-04 06:01:08,194 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:01:08,204 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:01:08,205 INFO:                     with Stdlib::Compat::Numeric. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 76]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-04 06:01:08,205 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:01:08,754 INFO: [1;33mWarning: This method is deprecated, please use match expressions with Stdlib::Compat::Ipv6 instead. They are described at https://docs.puppet.com/puppet/latest/reference/lang_data_type.html#match-expressions. at ["/etc/puppet/modules/rabbitmq/manifests/install/rabbitmqadmin.pp", 37]:["/etc/puppet/modules/rabbitmq/manifests/init.pp", 316]
2018-12-04 06:01:08,754 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:01:08,910 INFO: [1;33mWarning: tag is a metaparam; this value will inherit to all contained resources in the tripleo::firewall::rule definition[0m
2018-12-04 06:01:09,037 INFO: [mNotice: Scope(Class[Tripleo::Firewall::Post]): At this stage, all network traffic is blocked.[0m
2018-12-04 06:01:10,494 INFO: [1;33mWarning: notify is a metaparam; this value will inherit to all contained resources in the keepalived::instance definition[0m
2018-12-04 06:01:10,546 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:01:10,547 INFO:                     with Stdlib::Compat::Hash. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/tripleo/manifests/profile/base/database/mysql.pp", 103]:["/etc/puppet/manifests/puppet-stack-config.pp", 97]
2018-12-04 06:01:10,547 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:01:10,682 INFO: [1;33mWarning: ModuleLoader: module 'mysql' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:01:10,683 INFO:    (file & line not available)[0m
2018-12-04 06:01:11,085 INFO: [1;33mWarning: ModuleLoader: module 'keystone' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:01:11,086 INFO:    (file & line not available)[0m
2018-12-04 06:01:11,976 INFO: [1;33mWarning: ModuleLoader: module 'glance' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:01:11,977 INFO:    (file & line not available)[0m
2018-12-04 06:01:12,301 INFO: [1;33mWarning: ModuleLoader: module 'nova' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:01:12,301 INFO:    (file & line not available)[0m
2018-12-04 06:01:12,518 INFO: [1;33mWarning: Unknown variable: '::nova::db::mysql_api::setup_cell0'. at /etc/puppet/modules/nova/manifests/db/mysql.pp:53:28[0m
2018-12-04 06:01:12,584 INFO: [1;33mWarning: ModuleLoader: module 'neutron' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:01:12,585 INFO:    (file & line not available)[0m
2018-12-04 06:01:13,581 INFO: [1;33mWarning: ModuleLoader: module 'heat' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:01:13,582 INFO:    (file & line not available)[0m
2018-12-04 06:01:13,677 INFO: [1;33mWarning: ModuleLoader: module 'ironic' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:01:13,678 INFO:    (file & line not available)[0m
2018-12-04 06:01:13,897 INFO: [1;33mWarning: ModuleLoader: module 'swift' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:01:13,897 INFO:    (file & line not available)[0m
2018-12-04 06:01:14,383 INFO: [1;33mWarning: Scope(Class[Keystone]): keystone::rabbit_host, keystone::rabbit_hosts, keystone::rabbit_password, keystone::rabbit_port, keystone::rabbit_userid and keystone::rabbit_virtual_host are deprecated. Please use keystone::default_transport_url instead.[0m
2018-12-04 06:01:16,604 INFO: [1;33mWarning: Scope(Class[Glance::Notify::Rabbitmq]): glance::notify::rabbitmq::rabbit_host, glance::notify::rabbitmq::rabbit_hosts, glance::notify::rabbitmq::rabbit_password, glance::notify::rabbitmq::rabbit_port, glance::notify::rabbitmq::rabbit_userid and glance::notify::rabbitmq::rabbit_virtual_host are deprecated. Please use glance::notify::rabbitmq::default_transport_url instead.[0m
2018-12-04 06:01:16,760 INFO: [1;33mWarning: Scope(Class[Nova::Db]): placement_database_connection has no effect as of pike, and may be removed in a future release[0m
2018-12-04 06:01:16,760 INFO: [1;33mWarning: Scope(Class[Nova::Db]): placement_slave_connection has no effect as of pike, and may be removed in a future release[0m
2018-12-04 06:01:17,215 INFO: [1;33mWarning: ModuleLoader: module 'cinder' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:01:17,215 INFO:    (file & line not available)[0m
2018-12-04 06:01:17,693 INFO: [1;33mWarning: Unknown variable: 'until_complete_real'. at /etc/puppet/modules/nova/manifests/cron/archive_deleted_rows.pp:91:90[0m
2018-12-04 06:01:17,862 INFO: [1;33mWarning: This method is deprecated, please use match expressions with Stdlib::Compat::Array instead. They are described at https://docs.puppet.com/puppet/latest/reference/lang_data_type.html#match-expressions. at ["/etc/puppet/modules/nova/manifests/scheduler/filter.pp", 147]:["/etc/puppet/manifests/puppet-stack-config.pp", 400]
2018-12-04 06:01:17,862 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:01:18,143 INFO: [1;33mWarning: Scope(Class[Neutron]): neutron::rabbit_host, neutron::rabbit_hosts, neutron::rabbit_password, neutron::rabbit_port, neutron::rabbit_user, neutron::rabbit_virtual_host and neutron::rpc_backend are deprecated. Please use neutron::default_transport_url instead.[0m
2018-12-04 06:01:20,036 INFO: [1;33mWarning: Unknown variable: 'methods_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:100:56[0m
2018-12-04 06:01:20,038 INFO: [1;33mWarning: Unknown variable: 'incoming_remove_headers_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:101:56[0m
2018-12-04 06:01:20,038 INFO: [1;33mWarning: Unknown variable: 'incoming_allow_headers_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:102:56[0m
2018-12-04 06:01:20,039 INFO: [1;33mWarning: Unknown variable: 'outgoing_remove_headers_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:103:56[0m
2018-12-04 06:01:20,039 INFO: [1;33mWarning: Unknown variable: 'outgoing_allow_headers_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:104:56[0m
2018-12-04 06:01:20,294 INFO: [1;33mWarning: Scope(Class[Swift::Storage::All]): The default port for the object storage server has changed from 6000 to 6200 and will be changed in a later release[0m
2018-12-04 06:01:20,295 INFO: [1;33mWarning: Scope(Class[Swift::Storage::All]): The default port for the container storage server has changed from 6001 to 6201 and will be changed in a later release[0m
2018-12-04 06:01:20,295 INFO: [1;33mWarning: Scope(Class[Swift::Storage::All]): The default port for the account storage server has changed from 6002 to 6202 and will be changed in a later release[0m
2018-12-04 06:01:21,259 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:01:21,260 INFO:                     with Stdlib::Compat::Integer. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/heat/manifests/wsgi/apache_api_cfn.pp", 125]:["/etc/puppet/manifests/puppet-stack-config.pp", 528]
2018-12-04 06:01:21,260 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:01:21,902 INFO: [1;33mWarning: Unknown variable: '::ironic::conductor::swift_account'. at /etc/puppet/modules/ironic/manifests/glance.pp:117:30[0m
2018-12-04 06:01:21,906 INFO: [1;33mWarning: Unknown variable: '::ironic::conductor::swift_temp_url_key'. at /etc/puppet/modules/ironic/manifests/glance.pp:118:35[0m
2018-12-04 06:01:21,906 INFO: [1;33mWarning: Unknown variable: '::ironic::conductor::swift_temp_url_duration'. at /etc/puppet/modules/ironic/manifests/glance.pp:119:40[0m
2018-12-04 06:01:21,943 INFO: [1;33mWarning: Unknown variable: '::ironic::api::neutron_url'. at /etc/puppet/modules/ironic/manifests/neutron.pp:58:29[0m
2018-12-04 06:01:23,493 INFO: [1;33mWarning: ModuleLoader: module 'mistral' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:01:23,493 INFO:    (file & line not available)[0m
2018-12-04 06:01:23,586 INFO: [1;33mWarning: Unknown variable: '::mistral::database_idle_timeout'. at /etc/puppet/modules/mistral/manifests/db.pp:57:40[0m
2018-12-04 06:01:23,590 INFO: [1;33mWarning: Unknown variable: '::mistral::database_min_pool_size'. at /etc/puppet/modules/mistral/manifests/db.pp:58:40[0m
2018-12-04 06:01:23,590 INFO: [1;33mWarning: Unknown variable: '::mistral::database_max_pool_size'. at /etc/puppet/modules/mistral/manifests/db.pp:59:40[0m
2018-12-04 06:01:23,591 INFO: [1;33mWarning: Unknown variable: '::mistral::database_max_retries'. at /etc/puppet/modules/mistral/manifests/db.pp:60:40[0m
2018-12-04 06:01:23,591 INFO: [1;33mWarning: Unknown variable: '::mistral::database_retry_interval'. at /etc/puppet/modules/mistral/manifests/db.pp:61:40[0m
2018-12-04 06:01:23,591 INFO: [1;33mWarning: Unknown variable: '::mistral::database_max_overflow'. at /etc/puppet/modules/mistral/manifests/db.pp:62:40[0m
2018-12-04 06:01:23,690 INFO: [1;33mWarning: Scope(Class[Mistral]): mistral::rabbit_host, mistral::rabbit_hosts, mistral::rabbit_password, mistral::rabbit_port, mistral::rabbit_userid, mistral::rabbit_virtual_host and mistral::rpc_backend are deprecated. Please use mistral::default_transport_url instead.[0m
2018-12-04 06:01:24,156 INFO: [1;33mWarning: ModuleLoader: module 'zaqar' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:01:24,156 INFO:    (file & line not available)[0m
2018-12-04 06:01:25,475 INFO: [1;33mWarning: Unknown variable: 'ca_pem'. at /etc/puppet/modules/tripleo/manifests/certmonger/haproxy.pp:95:22[0m
2018-12-04 06:01:26,274 INFO: [1;33mWarning: ModuleLoader: module 'oslo' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:01:26,274 INFO:    (file & line not available)[0m
2018-12-04 06:01:26,414 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[keystone_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-04 06:01:27,763 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[glance_api_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-04 06:01:27,787 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[glance_registry_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-04 06:01:28,028 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[neutron_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-04 06:01:28,130 INFO: [1;33mWarning: Scope(Neutron::Plugins::Ml2::Type_driver[local]): local type_driver is useful only for single-box, because it provides no connectivity between hosts[0m
2018-12-04 06:01:29,227 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[mistral_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-04 06:01:30,074 INFO: [1;33mWarning: Scope(Haproxy::Config[haproxy]): haproxy: The $merge_options parameter will default to true in the next major release. Please review the documentation regarding the implications.[0m
2018-12-04 06:01:36,392 INFO: [mNotice: Compiled catalog for undercloud.example.com in environment production in 29.55 seconds[0m
2018-12-04 06:02:08,422 INFO: [mNotice: /Stage[setup]/Vswitch::Ovs/Package[openvswitch]/ensure: created[0m
2018-12-04 06:02:09,083 INFO: [mNotice: /Stage[setup]/Vswitch::Ovs/Service[openvswitch]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:02:15,013 INFO: [mNotice: /Stage[setup]/Tripleo::Network::Os_net_config/Exec[os-net-config]/returns: executed successfully[0m
2018-12-04 06:02:15,027 INFO: [mNotice: /Stage[setup]/Tripleo::Network::Os_net_config/Exec[trigger-keepalived-restart]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:02:15,330 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Time::Ntp/Service[chronyd]/ensure: ensure changed 'running' to 'stopped'[0m
2018-12-04 06:02:17,943 INFO: [mNotice: /Stage[main]/Ntp::Install/Package[ntp]/ensure: created[0m
2018-12-04 06:02:18,231 INFO: [mNotice: /Stage[main]/Ntp::Config/File[/etc/ntp.conf]/content: content changed '{md5}913c85f0fde85f83c2d6c030ecf259e9' to '{md5}e4539afcd4e03217b6bde799c8c0e337'[0m
2018-12-04 06:02:18,565 INFO: [mNotice: /Stage[main]/Ntp::Service/Service[ntp]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:02:20,850 INFO: [mNotice: /Stage[main]/Firewall::Linux::Redhat/Package[iptables-services]/ensure: created[0m
2018-12-04 06:02:20,936 INFO: [mNotice: /Stage[main]/Firewall::Linux::Redhat/Exec[/usr/bin/systemctl daemon-reload]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:02:21,320 INFO: [mNotice: /Stage[main]/Firewall::Linux::Redhat/Service[iptables]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:02:21,688 INFO: [mNotice: /Stage[main]/Firewall::Linux::Redhat/Service[ip6tables]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:02:21,702 INFO: [mNotice: /Stage[main]/Tripleo::Selinux/File[/etc/selinux/config]/content: content changed '{md5}ff8d2663b88f1a20c8489fdf152e6b31' to '{md5}1b476ce188acf89a99c4da6ae6f0e57f'[0m
2018-12-04 06:02:21,703 INFO: [mNotice: /Stage[main]/Tripleo::Selinux/File[/etc/selinux/config]/mode: mode changed '0644' to '0444'[0m
2018-12-04 06:02:24,695 INFO: [mNotice: /Stage[main]/Keepalived::Install/Package[keepalived]/ensure: created[0m
2018-12-04 06:02:24,711 INFO: [mNotice: /Stage[main]/Mysql::Server::Config/File[mysql-config-file]/ensure: defined content as '{md5}4377846377dc6b9d197ab7d4a6311e1d'[0m
2018-12-04 06:02:24,727 INFO: [mNotice: /Stage[main]/Main/File[/etc/systemd/system/mariadb.service.d]/ensure: created[0m
2018-12-04 06:02:24,733 INFO: [mNotice: /Stage[main]/Main/File[/etc/systemd/system/mariadb.service.d/limits.conf]/ensure: defined content as '{md5}8eb9ff6c576b9869944215af3a568c2e'[0m
2018-12-04 06:02:24,816 INFO: [mNotice: /Stage[main]/Main/Exec[systemctl-daemon-reload]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:02:25,000 INFO: [mNotice: /Stage[main]/Mysql::Server::Service/Service[mysqld]/enable: enable changed 'false' to 'true'[0m
2018-12-04 06:02:28,141 INFO: [mNotice: /Stage[main]/Mysql::Server::Service/Service[mysqld]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:02:28,170 INFO: [mNotice: /Stage[main]/Mysql::Server::Service/Exec[wait_for_mysql_socket_to_open]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:02:29,118 INFO: [mNotice: /Stage[main]/Mysql::Server::Root_password/Mysql_user[root@localhost]/password_hash: defined 'password_hash' as '*E72379D56202A763B4BCB2759774759B406403DC'[0m
2018-12-04 06:02:29,130 INFO: [mNotice: /Stage[main]/Mysql::Server::Root_password/File[/root/.my.cnf]/ensure: defined content as '{md5}acf31fdceadf10502b92e00a64405f88'[0m
2018-12-04 06:02:29,162 INFO: [mNotice: /Stage[main]/Mysql::Server::Account_security/Mysql_user[root@undercloud.example.com]/ensure: removed[0m
2018-12-04 06:02:29,184 INFO: [mNotice: /Stage[main]/Mysql::Server::Account_security/Mysql_user[@undercloud.example.com]/ensure: removed[0m
2018-12-04 06:02:29,533 INFO: [mNotice: /Stage[main]/Mysql::Server::Account_security/Mysql_database[test]/ensure: removed[0m
2018-12-04 06:02:29,541 INFO: [mNotice: /Stage[main]/Main/File[/var/log/journal]/ensure: created[0m
2018-12-04 06:02:29,788 INFO: [mNotice: /Stage[main]/Main/Service[systemd-journald]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:02:32,649 INFO: [mNotice: /Stage[main]/Apache::Mod::Mime/Package[mailcap]/ensure: created[0m
2018-12-04 06:02:32,728 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[DEFAULT/bind_host]/ensure: created[0m
2018-12-04 06:02:32,760 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[DEFAULT/bind_port]/ensure: created[0m
2018-12-04 06:02:32,827 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[DEFAULT/workers]/ensure: created[0m
2018-12-04 06:02:32,934 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[DEFAULT/image_cache_dir]/ensure: created[0m
2018-12-04 06:02:33,248 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[DEFAULT/enable_v1_api]/ensure: created[0m
2018-12-04 06:02:33,279 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[DEFAULT/enable_v2_api]/ensure: created[0m
2018-12-04 06:02:33,343 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[glance_store/os_region_name]/ensure: created[0m
2018-12-04 06:02:33,461 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[glance_store/stores]/ensure: created[0m
2018-12-04 06:02:33,495 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_cache_config[glance_store/os_region_name]/ensure: created[0m
2018-12-04 06:02:33,528 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[DEFAULT/registry_host]/ensure: created[0m
2018-12-04 06:02:33,576 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_cache_config[DEFAULT/registry_host]/ensure: created[0m
2018-12-04 06:02:33,617 INFO: [mNotice: /Stage[main]/Glance::Api/Glance_api_config[paste_deploy/flavor]/ensure: created[0m
2018-12-04 06:02:33,986 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_api_config[glance_store/swift_store_create_container_on_put]/ensure: created[0m
2018-12-04 06:02:34,046 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_api_config[glance_store/swift_store_endpoint_type]/ensure: created[0m
2018-12-04 06:02:34,088 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_api_config[glance_store/swift_store_config_file]/ensure: created[0m
2018-12-04 06:02:34,125 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_api_config[glance_store/default_swift_reference]/ensure: created[0m
2018-12-04 06:02:34,155 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_api_config[glance_store/default_store]/ensure: created[0m
2018-12-04 06:02:34,156 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_swift_config[ref1/user]/ensure: created[0m
2018-12-04 06:02:34,158 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_swift_config[ref1/key]/ensure: created[0m
2018-12-04 06:02:34,160 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_swift_config[ref1/auth_address]/ensure: created[0m
2018-12-04 06:02:34,163 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_swift_config[ref1/auth_version]/ensure: created[0m
2018-12-04 06:02:34,166 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_swift_config[ref1/user_domain_id]/ensure: created[0m
2018-12-04 06:02:34,167 INFO: [mNotice: /Stage[main]/Glance::Backend::Swift/Glance_swift_config[ref1/project_domain_id]/ensure: created[0m
2018-12-04 06:02:40,892 INFO: [mNotice: /Stage[main]/Nova/Package[python-nova]/ensure: created[0m
2018-12-04 06:02:43,022 INFO: [mNotice: /Stage[main]/Nova/Package[nova-common]/ensure: created[0m
2018-12-04 06:02:45,442 INFO: [mNotice: /Stage[main]/Nova::Compute/Package[genisoimage]/ensure: created[0m
2018-12-04 06:02:56,548 INFO: [mNotice: /Stage[main]/Neutron/Package[neutron]/ensure: created[0m
2018-12-04 06:02:59,113 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Package[neutron-plugin-ml2]/ensure: created[0m
2018-12-04 06:03:01,131 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2::Networking_baremetal/Package[python2-networking-baremetal]/ensure: created[0m
2018-12-04 06:03:03,178 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Package[python2-ironic-neutron-agent]/ensure: created[0m
2018-12-04 06:03:05,293 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Package[neutron-ovs-agent]/ensure: created[0m
2018-12-04 06:03:05,298 INFO: [mNotice: /Stage[main]/Neutron::Deps/Anchor[neutron::install::end]: Triggered 'refresh' from 5 events[0m
2018-12-04 06:03:05,312 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[DEFAULT/bind_host]/ensure: created[0m
2018-12-04 06:03:05,327 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[DEFAULT/auth_strategy]/ensure: created[0m
2018-12-04 06:03:05,665 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[DEFAULT/core_plugin]/ensure: created[0m
2018-12-04 06:03:05,715 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[DEFAULT/dns_domain]/ensure: created[0m
2018-12-04 06:03:05,731 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[DEFAULT/dhcp_agents_per_network]/ensure: created[0m
2018-12-04 06:03:05,799 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[DEFAULT/global_physnet_mtu]/ensure: created[0m
2018-12-04 06:03:05,815 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[agent/root_helper]/ensure: created[0m
2018-12-04 06:03:05,837 INFO: [mNotice: /Stage[main]/Neutron/Neutron_config[DEFAULT/service_plugins]/ensure: created[0m
2018-12-04 06:03:05,870 INFO: [mNotice: /Stage[main]/Neutron::Server/Neutron_config[DEFAULT/l3_ha]/ensure: created[0m
2018-12-04 06:03:05,879 INFO: [mNotice: /Stage[main]/Neutron::Server/Neutron_config[DEFAULT/max_l3_agents_per_router]/ensure: created[0m
2018-12-04 06:03:05,895 INFO: [mNotice: /Stage[main]/Neutron::Server/Neutron_config[DEFAULT/api_workers]/ensure: created[0m
2018-12-04 06:03:05,911 INFO: [mNotice: /Stage[main]/Neutron::Server/Neutron_config[DEFAULT/rpc_workers]/ensure: created[0m
2018-12-04 06:03:05,942 INFO: [mNotice: /Stage[main]/Neutron::Server/Neutron_config[DEFAULT/router_scheduler_driver]/ensure: created[0m
2018-12-04 06:03:06,239 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/auth_url]/ensure: created[0m
2018-12-04 06:03:06,250 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/username]/ensure: created[0m
2018-12-04 06:03:06,259 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/password]/ensure: created[0m
2018-12-04 06:03:06,267 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/project_domain_id]/ensure: created[0m
2018-12-04 06:03:06,277 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/project_domain_name]/ensure: created[0m
2018-12-04 06:03:06,290 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/project_name]/ensure: created[0m
2018-12-04 06:03:06,294 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/user_domain_id]/ensure: created[0m
2018-12-04 06:03:06,303 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/user_domain_name]/ensure: created[0m
2018-12-04 06:03:06,324 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/auth_type]/ensure: created[0m
2018-12-04 06:03:06,333 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[nova/tenant_name]/ensure: created[0m
2018-12-04 06:03:06,342 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[DEFAULT/notify_nova_on_port_status_changes]/ensure: created[0m
2018-12-04 06:03:06,351 INFO: [mNotice: /Stage[main]/Neutron::Server::Notifications/Neutron_config[DEFAULT/notify_nova_on_port_data_changes]/ensure: created[0m
2018-12-04 06:03:06,385 INFO: [mNotice: /Stage[main]/Neutron::Quota/Neutron_config[quotas/quota_port]/ensure: created[0m
2018-12-04 06:03:06,438 INFO: [mNotice: /Stage[main]/Neutron::Quota/Neutron_config[quotas/quota_firewall_rule]/ensure: created[0m
2018-12-04 06:03:06,459 INFO: [mNotice: /Stage[main]/Neutron::Quota/Neutron_config[quotas/quota_network_gateway]/ensure: created[0m
2018-12-04 06:03:06,471 INFO: [mNotice: /Stage[main]/Neutron::Quota/Neutron_config[quotas/quota_packet_filter]/ensure: created[0m
2018-12-04 06:03:06,504 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/File[/etc/neutron/plugin.ini]/ensure: created[0m
2018-12-04 06:03:06,507 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/File[/etc/default/neutron-server]/ensure: created[0m
2018-12-04 06:03:06,512 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron_plugin_ml2[ml2/type_drivers]/ensure: created[0m
2018-12-04 06:03:06,516 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron_plugin_ml2[ml2/tenant_network_types]/ensure: created[0m
2018-12-04 06:03:06,519 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron_plugin_ml2[ml2/mechanism_drivers]/ensure: created[0m
2018-12-04 06:03:06,521 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron_plugin_ml2[ml2/path_mtu]/ensure: created[0m
2018-12-04 06:03:06,525 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron_plugin_ml2[ml2/extension_drivers]/ensure: created[0m
2018-12-04 06:03:06,762 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/auth_type]/ensure: created[0m
2018-12-04 06:03:06,764 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/auth_url]/ensure: created[0m
2018-12-04 06:03:06,766 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/username]/ensure: created[0m
2018-12-04 06:03:06,766 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/password]/ensure: created[0m
2018-12-04 06:03:06,768 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/project_domain_id]/ensure: created[0m
2018-12-04 06:03:06,770 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/project_domain_name]/ensure: created[0m
2018-12-04 06:03:06,772 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/project_name]/ensure: created[0m
2018-12-04 06:03:06,775 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/user_domain_id]/ensure: created[0m
2018-12-04 06:03:06,777 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/user_domain_name]/ensure: created[0m
2018-12-04 06:03:06,780 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Ironic_neutron_agent_config[ironic/region_name]/ensure: created[0m
2018-12-04 06:03:06,786 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Neutron_dhcp_agent_config[DEFAULT/enable_isolated_metadata]/ensure: created[0m
2018-12-04 06:03:06,792 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Neutron_dhcp_agent_config[DEFAULT/enable_metadata_network]/ensure: created[0m
2018-12-04 06:03:06,798 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Neutron_dhcp_agent_config[DEFAULT/state_path]/ensure: created[0m
2018-12-04 06:03:06,801 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Neutron_dhcp_agent_config[DEFAULT/resync_interval]/ensure: created[0m
2018-12-04 06:03:06,804 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Neutron_dhcp_agent_config[DEFAULT/interface_driver]/ensure: created[0m
2018-12-04 06:03:06,812 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Neutron_dhcp_agent_config[DEFAULT/root_helper]/ensure: created[0m
2018-12-04 06:03:06,817 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Neutron_dhcp_agent_config[DEFAULT/dnsmasq_config_file]/ensure: created[0m
2018-12-04 06:03:06,851 INFO: [mNotice: /Stage[main]/Neutron::Agents::L3/Neutron_l3_agent_config[DEFAULT/interface_driver]/ensure: created[0m
2018-12-04 06:03:06,872 INFO: [mNotice: /Stage[main]/Neutron::Agents::L3/Neutron_l3_agent_config[DEFAULT/agent_mode]/ensure: created[0m
2018-12-04 06:03:06,881 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Neutron_agent_ovs[ovs/bridge_mappings]/ensure: created[0m
2018-12-04 06:03:06,891 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Neutron_agent_ovs[agent/drop_flows_on_start]/ensure: created[0m
2018-12-04 06:03:06,899 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Neutron_agent_ovs[ovs/integration_bridge]/ensure: created[0m
2018-12-04 06:03:06,919 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Neutron_agent_ovs[securitygroup/firewall_driver]/ensure: created[0m
2018-12-04 06:03:07,138 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Service[ovs-cleanup-service]/enable: enable changed 'false' to 'true'[0m
2018-12-04 06:03:07,150 INFO: [mNotice: /Stage[main]/Main/Neutron_config[DEFAULT/notification_driver]/ensure: created[0m
2018-12-04 06:03:09,328 INFO: [mNotice: /Stage[main]/Memcached/Package[memcached]/ensure: created[0m
2018-12-04 06:03:09,336 INFO: [mNotice: /Stage[main]/Memcached/File[/etc/sysconfig/memcached]/content: content changed '{md5}a50ed62e82d31fb4cb2de2226650c545' to '{md5}d8425d450cd9f050a2601befed0ce63a'[0m
2018-12-04 06:03:09,740 INFO: [mNotice: /Stage[main]/Memcached/Service[memcached]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:03:12,117 INFO: [mNotice: /Stage[main]/Swift::Proxy/Package[swift-proxy]/ensure: created[0m
2018-12-04 06:03:12,126 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[pipeline:main/pipeline]/value: value changed 'catch_errors cache proxy-server' to 'catch_errors proxy-server'[0m
2018-12-04 06:03:12,128 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/auto_create_account_prefix]/ensure: created[0m
2018-12-04 06:03:12,129 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/concurrency]/ensure: created[0m
2018-12-04 06:03:12,133 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/expiring_objects_account_name]/ensure: created[0m
2018-12-04 06:03:12,135 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/interval]/ensure: created[0m
2018-12-04 06:03:12,137 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/process]/ensure: created[0m
2018-12-04 06:03:12,138 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/processes]/ensure: created[0m
2018-12-04 06:03:12,140 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/reclaim_age]/ensure: created[0m
2018-12-04 06:03:12,143 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/recon_cache_path]/ensure: created[0m
2018-12-04 06:03:12,144 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/report_interval]/ensure: created[0m
2018-12-04 06:03:12,146 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/log_facility]/ensure: created[0m
2018-12-04 06:03:12,148 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift_object_expirer_config[object-expirer/log_level]/ensure: created[0m
2018-12-04 06:03:14,306 INFO: [mNotice: /Stage[main]/Xinetd/Package[xinetd]/ensure: created[0m
2018-12-04 06:03:14,317 INFO: [mNotice: /Stage[main]/Xinetd/File[/etc/xinetd.conf]/content: content changed '{md5}9ff8cc688dd9f0dfc45e5afd25c427a7' to '{md5}7d37008224e71625019cb48768f267e7'[0m
2018-12-04 06:03:14,325 INFO: [mNotice: /Stage[main]/Xinetd/File[/etc/xinetd.conf]/mode: mode changed '0600' to '0644'[0m
2018-12-04 06:03:14,407 INFO: [mNotice: /Stage[main]/Main/Exec[restart rsyslog]/returns: executed successfully[0m
2018-12-04 06:03:20,562 INFO: [mNotice: /Stage[main]/Heat/Package[heat-common]/ensure: created[0m
2018-12-04 06:03:22,775 INFO: [mNotice: /Stage[main]/Heat::Api/Package[heat-api]/ensure: created[0m
2018-12-04 06:03:24,851 INFO: [mNotice: /Stage[main]/Heat::Api_cfn/Package[heat-api-cfn]/ensure: created[0m
2018-12-04 06:03:27,874 INFO: [mNotice: /Stage[main]/Heat::Engine/Package[heat-engine]/ensure: created[0m
2018-12-04 06:03:27,874 INFO: [mNotice: /Stage[main]/Heat::Deps/Anchor[heat::install::end]: Triggered 'refresh' from 4 events[0m
2018-12-04 06:03:27,902 INFO: [mNotice: /Stage[main]/Heat/Heat_config[trustee/auth_type]/ensure: created[0m
2018-12-04 06:03:27,921 INFO: [mNotice: /Stage[main]/Heat/Heat_config[trustee/auth_url]/ensure: created[0m
2018-12-04 06:03:27,947 INFO: [mNotice: /Stage[main]/Heat/Heat_config[trustee/username]/ensure: created[0m
2018-12-04 06:03:27,971 INFO: [mNotice: /Stage[main]/Heat/Heat_config[trustee/password]/ensure: created[0m
2018-12-04 06:03:27,994 INFO: [mNotice: /Stage[main]/Heat/Heat_config[trustee/project_domain_name]/ensure: created[0m
2018-12-04 06:03:28,013 INFO: [mNotice: /Stage[main]/Heat/Heat_config[trustee/user_domain_name]/ensure: created[0m
2018-12-04 06:03:28,031 INFO: [mNotice: /Stage[main]/Heat/Heat_config[clients_keystone/auth_uri]/ensure: created[0m
2018-12-04 06:03:28,057 INFO: [mNotice: /Stage[main]/Heat/Heat_config[clients/endpoint_type]/ensure: created[0m
2018-12-04 06:03:28,095 INFO: [mNotice: /Stage[main]/Heat/Heat_config[DEFAULT/max_json_body_size]/ensure: created[0m
2018-12-04 06:03:28,140 INFO: [mNotice: /Stage[main]/Heat/Heat_config[ec2authtoken/auth_uri]/ensure: created[0m
2018-12-04 06:03:28,167 INFO: [mNotice: /Stage[main]/Heat/Heat_config[yaql/limit_iterators]/ensure: created[0m
2018-12-04 06:03:28,185 INFO: [mNotice: /Stage[main]/Heat/Heat_config[yaql/memory_quota]/ensure: created[0m
2018-12-04 06:03:28,207 INFO: [mNotice: /Stage[main]/Heat::Api/Heat_config[heat_api/bind_host]/ensure: created[0m
2018-12-04 06:03:28,235 INFO: [mNotice: /Stage[main]/Heat::Api/Heat_config[heat_api/workers]/ensure: created[0m
2018-12-04 06:03:28,253 INFO: [mNotice: /Stage[main]/Heat::Api_cfn/Heat_config[heat_api_cfn/bind_host]/ensure: created[0m
2018-12-04 06:03:28,280 INFO: [mNotice: /Stage[main]/Heat::Api_cfn/Heat_config[heat_api_cfn/workers]/ensure: created[0m
2018-12-04 06:03:28,302 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/auth_encryption_key]/ensure: created[0m
2018-12-04 06:03:28,321 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/heat_stack_user_role]/ensure: created[0m
2018-12-04 06:03:28,339 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/heat_metadata_server_url]/ensure: created[0m
2018-12-04 06:03:28,358 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/heat_waitcondition_server_url]/ensure: created[0m
2018-12-04 06:03:28,378 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/heat_watch_server_url]/ensure: created[0m
2018-12-04 06:03:28,695 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/max_resources_per_stack]/ensure: created[0m
2018-12-04 06:03:28,739 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/num_engine_workers]/ensure: created[0m
2018-12-04 06:03:28,749 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/convergence_engine]/ensure: created[0m
2018-12-04 06:03:28,768 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/reauthentication_auth_method]/ensure: created[0m
2018-12-04 06:03:28,805 INFO: [mNotice: /Stage[main]/Heat::Engine/Heat_config[DEFAULT/max_nested_stack_depth]/ensure: created[0m
2018-12-04 06:03:28,832 INFO: [mNotice: /Stage[main]/Heat::Keystone::Domain/Heat_config[DEFAULT/stack_domain_admin]/ensure: created[0m
2018-12-04 06:03:28,850 INFO: [mNotice: /Stage[main]/Heat::Keystone::Domain/Heat_config[DEFAULT/stack_domain_admin_password]/ensure: created[0m
2018-12-04 06:03:28,869 INFO: [mNotice: /Stage[main]/Heat::Keystone::Domain/Heat_config[DEFAULT/stack_user_domain_name]/ensure: created[0m
2018-12-04 06:03:28,927 INFO: [mNotice: /Stage[main]/Heat::Cron::Purge_deleted/Cron[heat-manage purge_deleted]/ensure: created[0m
2018-12-04 06:03:36,351 INFO: [mNotice: /Stage[main]/Ironic/Package[ironic-common]/ensure: created[0m
2018-12-04 06:03:36,359 INFO: [mNotice: /Stage[main]/Main/File[dnsmasq-ironic.conf]/ensure: defined content as '{md5}1c23f6b2b9a0910c3e32f02970493f00'[0m
2018-12-04 06:03:38,474 INFO: [mNotice: /Stage[main]/Ironic::Api/Package[ironic-api]/ensure: created[0m
2018-12-04 06:03:40,629 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Package[ironic-conductor]/ensure: created[0m
2018-12-04 06:03:42,693 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Ansible/Package[systemd-python]/ensure: created[0m
2018-12-04 06:03:45,103 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Staging/Package[ironic-staging-drivers]/ensure: created[0m
2018-12-04 06:03:48,408 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Package[ironic-inspector]/ensure: created[0m
2018-12-04 06:03:50,470 INFO: [mNotice: /Stage[main]/Ironic::Pxe/Package[tftp-server]/ensure: created[0m
2018-12-04 06:03:53,439 INFO: [mNotice: /Stage[main]/Ironic::Pxe/Package[syslinux]/ensure: created[0m
2018-12-04 06:03:55,668 INFO: [mNotice: /Stage[main]/Ironic::Pxe/Package[ipxe]/ensure: created[0m
2018-12-04 06:03:55,676 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::install::end]: Triggered 'refresh' from 4 events[0m
2018-12-04 06:03:55,678 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic-inspector::install::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:03:55,710 INFO: [mNotice: /Stage[main]/Ironic::Glance/Ironic_config[glance/auth_type]/ensure: created[0m
2018-12-04 06:03:55,739 INFO: [mNotice: /Stage[main]/Ironic::Glance/Ironic_config[glance/username]/ensure: created[0m
2018-12-04 06:03:55,767 INFO: [mNotice: /Stage[main]/Ironic::Glance/Ironic_config[glance/password]/ensure: created[0m
2018-12-04 06:03:56,051 INFO: [mNotice: /Stage[main]/Ironic::Glance/Ironic_config[glance/auth_url]/ensure: created[0m
2018-12-04 06:03:56,084 INFO: [mNotice: /Stage[main]/Ironic::Glance/Ironic_config[glance/project_name]/ensure: created[0m
2018-12-04 06:03:56,114 INFO: [mNotice: /Stage[main]/Ironic::Glance/Ironic_config[glance/user_domain_name]/ensure: created[0m
2018-12-04 06:03:56,147 INFO: [mNotice: /Stage[main]/Ironic::Glance/Ironic_config[glance/project_domain_name]/ensure: created[0m
2018-12-04 06:03:56,299 INFO: [mNotice: /Stage[main]/Ironic::Neutron/Ironic_config[neutron/auth_type]/ensure: created[0m
2018-12-04 06:03:56,330 INFO: [mNotice: /Stage[main]/Ironic::Neutron/Ironic_config[neutron/username]/ensure: created[0m
2018-12-04 06:03:56,361 INFO: [mNotice: /Stage[main]/Ironic::Neutron/Ironic_config[neutron/password]/ensure: created[0m
2018-12-04 06:03:56,389 INFO: [mNotice: /Stage[main]/Ironic::Neutron/Ironic_config[neutron/auth_url]/ensure: created[0m
2018-12-04 06:03:56,417 INFO: [mNotice: /Stage[main]/Ironic::Neutron/Ironic_config[neutron/project_name]/ensure: created[0m
2018-12-04 06:03:56,449 INFO: [mNotice: /Stage[main]/Ironic::Neutron/Ironic_config[neutron/user_domain_name]/ensure: created[0m
2018-12-04 06:03:56,479 INFO: [mNotice: /Stage[main]/Ironic::Neutron/Ironic_config[neutron/project_domain_name]/ensure: created[0m
2018-12-04 06:03:56,512 INFO: [mNotice: /Stage[main]/Ironic/Ironic_config[DEFAULT/auth_strategy]/ensure: created[0m
2018-12-04 06:03:56,537 INFO: [mNotice: /Stage[main]/Ironic/Ironic_config[DEFAULT/my_ip]/ensure: created[0m
2018-12-04 06:03:56,568 INFO: [mNotice: /Stage[main]/Ironic/Ironic_config[DEFAULT/default_resource_class]/ensure: created[0m
2018-12-04 06:03:56,573 INFO: [mNotice: /Stage[main]/Ironic::Db::Sync/File[/var/log/ironic/ironic-dbsync.log]/ensure: created[0m
2018-12-04 06:03:56,602 INFO: [mNotice: /Stage[main]/Ironic::Api/Ironic_config[api/host_ip]/ensure: created[0m
2018-12-04 06:03:56,874 INFO: [mNotice: /Stage[main]/Ironic::Api/Ironic_config[api/port]/ensure: created[0m
2018-12-04 06:03:56,902 INFO: [mNotice: /Stage[main]/Ironic::Api/Ironic_config[api/max_limit]/ensure: created[0m
2018-12-04 06:03:56,933 INFO: [mNotice: /Stage[main]/Ironic::Api/Ironic_config[api/api_workers]/ensure: created[0m
2018-12-04 06:03:57,020 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Agent/Ironic_config[agent/deploy_logs_collect]/ensure: created[0m
2018-12-04 06:03:57,048 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Agent/Ironic_config[agent/deploy_logs_storage_backend]/ensure: created[0m
2018-12-04 06:03:57,076 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Agent/Ironic_config[agent/deploy_logs_local_path]/ensure: created[0m
2018-12-04 06:03:57,134 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[DEFAULT/enabled_drivers]/ensure: created[0m
2018-12-04 06:03:57,163 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[DEFAULT/enabled_hardware_types]/ensure: created[0m
2018-12-04 06:03:57,191 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[conductor/max_time_interval]/ensure: created[0m
2018-12-04 06:03:57,219 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[conductor/force_power_state_during_sync]/ensure: created[0m
2018-12-04 06:03:57,249 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[conductor/automated_clean]/ensure: created[0m
2018-12-04 06:03:57,278 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[conductor/api_url]/ensure: created[0m
2018-12-04 06:03:57,306 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[deploy/http_url]/ensure: created[0m
2018-12-04 06:03:57,349 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[deploy/erase_devices_priority]/ensure: created[0m
2018-12-04 06:03:57,381 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[deploy/erase_devices_metadata_priority]/ensure: created[0m
2018-12-04 06:03:57,709 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[deploy/default_boot_option]/ensure: created[0m
2018-12-04 06:03:57,794 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[neutron/cleaning_network]/ensure: created[0m
2018-12-04 06:03:57,825 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Ironic_config[neutron/provisioning_network]/ensure: created[0m
2018-12-04 06:03:58,054 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Ilo/Ironic_config[ilo/default_boot_mode]/ensure: created[0m
2018-12-04 06:03:58,085 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/enabled]/ensure: created[0m
2018-12-04 06:03:58,376 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/auth_type]/ensure: created[0m
2018-12-04 06:03:58,419 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/username]/ensure: created[0m
2018-12-04 06:03:58,452 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/password]/ensure: created[0m
2018-12-04 06:03:58,501 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/auth_url]/ensure: created[0m
2018-12-04 06:03:58,562 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/project_name]/ensure: created[0m
2018-12-04 06:03:58,622 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/user_domain_name]/ensure: created[0m
2018-12-04 06:03:58,685 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Inspector/Ironic_config[inspector/project_domain_name]/ensure: created[0m
2018-12-04 06:03:58,749 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/ipxe_enabled]/ensure: created[0m
2018-12-04 06:03:58,794 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/pxe_bootfile_name]/ensure: created[0m
2018-12-04 06:03:58,846 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/pxe_config_template]/ensure: created[0m
2018-12-04 06:03:58,895 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/tftp_root]/ensure: created[0m
2018-12-04 06:03:58,941 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/tftp_master_path]/ensure: created[0m
2018-12-04 06:03:59,022 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/uefi_pxe_bootfile_name]/ensure: created[0m
2018-12-04 06:03:59,058 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/uefi_pxe_config_template]/ensure: created[0m
2018-12-04 06:03:59,111 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Pxe/Ironic_config[pxe/ipxe_timeout]/ensure: created[0m
2018-12-04 06:03:59,147 INFO: [mNotice: /Stage[main]/Ironic::Inspector/File[/etc/ironic-inspector/inspector.conf]/owner: owner changed 'root' to 'ironic-inspector'[0m
2018-12-04 06:03:59,158 INFO: [mNotice: /Stage[main]/Ironic::Inspector/File[/etc/ironic-inspector/dnsmasq.conf]/content: content changed '{md5}9eabe6f969928fde6524d0dd00781479' to '{md5}b84675588f83c34e4b0c71e04f3c7bbc'[0m
2018-12-04 06:03:59,419 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[DEFAULT/listen_address]/ensure: created[0m
2018-12-04 06:03:59,428 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[DEFAULT/auth_strategy]/ensure: created[0m
2018-12-04 06:03:59,449 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[capabilities/boot_mode]/ensure: created[0m
2018-12-04 06:03:59,464 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[iptables/dnsmasq_interface]/ensure: created[0m
2018-12-04 06:03:59,477 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[processing/ramdisk_logs_dir]/ensure: created[0m
2018-12-04 06:03:59,501 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[processing/keep_ports]/ensure: created[0m
2018-12-04 06:03:59,515 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[processing/store_data]/ensure: created[0m
2018-12-04 06:03:59,531 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/auth_type]/ensure: created[0m
2018-12-04 06:03:59,537 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/username]/ensure: created[0m
2018-12-04 06:03:59,547 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/password]/ensure: created[0m
2018-12-04 06:03:59,562 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/project_name]/ensure: created[0m
2018-12-04 06:03:59,576 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/project_domain_name]/ensure: created[0m
2018-12-04 06:03:59,586 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/user_domain_name]/ensure: created[0m
2018-12-04 06:03:59,595 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/auth_url]/ensure: created[0m
2018-12-04 06:03:59,613 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/max_retries]/ensure: created[0m
2018-12-04 06:03:59,629 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[ironic/retry_interval]/ensure: created[0m
2018-12-04 06:03:59,637 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[swift/auth_type]/ensure: created[0m
2018-12-04 06:03:59,647 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[swift/username]/ensure: created[0m
2018-12-04 06:03:59,656 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[swift/password]/ensure: created[0m
2018-12-04 06:03:59,673 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[swift/project_name]/ensure: created[0m
2018-12-04 06:03:59,686 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[swift/project_domain_name]/ensure: created[0m
2018-12-04 06:03:59,696 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[swift/user_domain_name]/ensure: created[0m
2018-12-04 06:03:59,706 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[swift/auth_url]/ensure: created[0m
2018-12-04 06:03:59,722 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[processing/processing_hooks]/ensure: created[0m
2018-12-04 06:03:59,748 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Ironic_inspector_config[discovery/enroll_node_driver]/ensure: created[0m
2018-12-04 06:03:59,764 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Pxe_filter/Ironic_inspector_config[pxe_filter/driver]/ensure: created[0m
2018-12-04 06:03:59,779 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Pxe_filter::Dnsmasq/Ironic_inspector_config[dnsmasq_pxe_filter/dhcp_hostsdir]/ensure: created[0m
2018-12-04 06:03:59,788 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Pxe_filter::Dnsmasq/Ironic_inspector_config[dnsmasq_pxe_filter/dnsmasq_start_command]/ensure: created[0m
2018-12-04 06:03:59,802 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Pxe_filter::Dnsmasq/Ironic_inspector_config[dnsmasq_pxe_filter/dnsmasq_stop_command]/ensure: created[0m
2018-12-04 06:03:59,809 INFO: [mNotice: /Stage[main]/Ironic::Pxe/File[/tftpboot]/ensure: created[0m
2018-12-04 06:03:59,816 INFO: [mNotice: /Stage[main]/Ironic::Pxe/File[/tftpboot/pxelinux.cfg]/ensure: created[0m
2018-12-04 06:03:59,821 INFO: [mNotice: /Stage[main]/Ironic::Pxe/File[/httpboot]/ensure: created[0m
2018-12-04 06:03:59,830 INFO: [mNotice: /Stage[main]/Ironic::Inspector/File[/httpboot/inspector.ipxe]/ensure: defined content as '{md5}5c182f06e5a69809ad971c47da276122'[0m
2018-12-04 06:03:59,839 INFO: [mNotice: /Stage[main]/Ironic::Pxe/File[/tftpboot/map-file]/ensure: defined content as '{md5}1c4343c656b7f7b9de48495fdc2b6c5e'[0m
2018-12-04 06:04:00,266 INFO: [mNotice: /Stage[main]/Ironic::Pxe/File[/tftpboot/undionly.kpxe]/ensure: defined content as '{md5}60d84c8e9035fac59c73ed4cee8dc82c'[0m
2018-12-04 06:04:00,287 INFO: [mNotice: /Stage[main]/Ironic::Pxe/File[/tftpboot/ipxe.efi]/ensure: defined content as '{md5}8f49ea062dadf0290b5f8b7e5f42a9b9'[0m
2018-12-04 06:04:00,318 INFO: [mNotice: /Stage[main]/Ironic::Service_catalog/Ironic_config[service_catalog/auth_type]/ensure: created[0m
2018-12-04 06:04:00,591 INFO: [mNotice: /Stage[main]/Ironic::Service_catalog/Ironic_config[service_catalog/username]/ensure: created[0m
2018-12-04 06:04:00,623 INFO: [mNotice: /Stage[main]/Ironic::Service_catalog/Ironic_config[service_catalog/password]/ensure: created[0m
2018-12-04 06:04:00,653 INFO: [mNotice: /Stage[main]/Ironic::Service_catalog/Ironic_config[service_catalog/auth_url]/ensure: created[0m
2018-12-04 06:04:00,685 INFO: [mNotice: /Stage[main]/Ironic::Service_catalog/Ironic_config[service_catalog/project_name]/ensure: created[0m
2018-12-04 06:04:00,716 INFO: [mNotice: /Stage[main]/Ironic::Service_catalog/Ironic_config[service_catalog/user_domain_name]/ensure: created[0m
2018-12-04 06:04:00,746 INFO: [mNotice: /Stage[main]/Ironic::Service_catalog/Ironic_config[service_catalog/project_domain_name]/ensure: created[0m
2018-12-04 06:04:00,776 INFO: [mNotice: /Stage[main]/Ironic::Swift/Ironic_config[swift/auth_type]/ensure: created[0m
2018-12-04 06:04:00,807 INFO: [mNotice: /Stage[main]/Ironic::Swift/Ironic_config[swift/username]/ensure: created[0m
2018-12-04 06:04:00,837 INFO: [mNotice: /Stage[main]/Ironic::Swift/Ironic_config[swift/password]/ensure: created[0m
2018-12-04 06:04:00,865 INFO: [mNotice: /Stage[main]/Ironic::Swift/Ironic_config[swift/auth_url]/ensure: created[0m
2018-12-04 06:04:00,897 INFO: [mNotice: /Stage[main]/Ironic::Swift/Ironic_config[swift/project_name]/ensure: created[0m
2018-12-04 06:04:00,929 INFO: [mNotice: /Stage[main]/Ironic::Swift/Ironic_config[swift/user_domain_name]/ensure: created[0m
2018-12-04 06:04:00,957 INFO: [mNotice: /Stage[main]/Ironic::Swift/Ironic_config[swift/project_domain_name]/ensure: created[0m
2018-12-04 06:04:06,197 INFO: [mNotice: /Stage[main]/Main/Package[openstack-tempest]/ensure: created[0m
2018-12-04 06:04:08,466 INFO: [mNotice: /Stage[main]/Main/Package[subunit-filters]/ensure: created[0m
2018-12-04 06:04:08,496 INFO: [mNotice: /Stage[main]/Main/Group[docker]/ensure: created[0m
2018-12-04 06:04:08,529 INFO: [mNotice: /Stage[main]/Main/User[docker_user]/groups: groups changed '' to ['docker'][0m
2018-12-04 06:04:11,672 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker_registry/Package[docker-distribution]/ensure: created[0m
2018-12-04 06:04:11,683 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker_registry/File[/etc/docker-distribution/registry/config.yml]/content: content changed '{md5}fcc7b86bd3a8b9b41577e3af434de461' to '{md5}7bec1d7568b3fda481c67c509545f1f4'[0m
2018-12-04 06:04:12,110 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker_registry/Service[docker-distribution]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:04:15,443 INFO: [mNotice: /Stage[main]/Mistral/Package[mistral-common]/ensure: created[0m
2018-12-04 06:04:17,484 INFO: [mNotice: /Stage[main]/Mistral::Api/Package[mistral-api]/ensure: created[0m
2018-12-04 06:04:19,528 INFO: [mNotice: /Stage[main]/Mistral::Engine/Package[mistral-engine]/ensure: created[0m
2018-12-04 06:04:21,536 INFO: [mNotice: /Stage[main]/Mistral::Executor/Package[mistral-executor]/ensure: created[0m
2018-12-04 06:04:21,538 INFO: [mNotice: /Stage[main]/Mistral::Deps/Anchor[mistral::install::end]: Triggered 'refresh' from 4 events[0m
2018-12-04 06:04:21,547 INFO: [mNotice: /Stage[main]/Mistral::Api/Mistral_config[api/api_workers]/ensure: created[0m
2018-12-04 06:04:21,549 INFO: [mNotice: /Stage[main]/Mistral::Api/Mistral_config[api/host]/ensure: created[0m
2018-12-04 06:04:21,554 INFO: [mNotice: /Stage[main]/Mistral::Engine/Mistral_config[engine/execution_field_size_limit_kb]/ensure: created[0m
2018-12-04 06:04:21,556 INFO: [mNotice: /Stage[main]/Mistral::Engine/Mistral_config[execution_expiration_policy/evaluation_interval]/ensure: created[0m
2018-12-04 06:04:21,558 INFO: [mNotice: /Stage[main]/Mistral::Engine/Mistral_config[execution_expiration_policy/older_than]/ensure: created[0m
2018-12-04 06:04:21,889 INFO: [mNotice: /Stage[main]/Mistral::Cron_trigger/Mistral_config[cron_trigger/execution_interval]/ensure: created[0m
2018-12-04 06:04:25,785 INFO: [mNotice: /Stage[main]/Tripleo::Ui/Package[openstack-tripleo-ui]/ensure: created[0m
2018-12-04 06:04:28,030 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Validations/Package[openstack-tripleo-validations]/ensure: created[0m
2018-12-04 06:04:28,071 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Validations/User[validations]/ensure: created[0m
2018-12-04 06:04:36,076 INFO: [mNotice: /Stage[main]/Zaqar/Package[zaqar-common]/ensure: created[0m
2018-12-04 06:04:36,076 INFO: [mNotice: /Stage[main]/Zaqar::Deps/Anchor[zaqar::install::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:04:36,108 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Trust/Zaqar_config[trustee/username]/ensure: created[0m
2018-12-04 06:04:36,129 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Trust/Zaqar_config[trustee/user_domain_name]/ensure: created[0m
2018-12-04 06:04:36,147 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Trust/Zaqar_config[trustee/auth_url]/ensure: created[0m
2018-12-04 06:04:36,173 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Trust/Zaqar_config[trustee/auth_type]/ensure: created[0m
2018-12-04 06:04:36,191 INFO: [mNotice: /Stage[main]/Zaqar/Zaqar_config[DEFAULT/auth_strategy]/ensure: created[0m
2018-12-04 06:04:36,217 INFO: [mNotice: /Stage[main]/Zaqar/Zaqar_config[DEFAULT/unreliable]/ensure: created[0m
2018-12-04 06:04:36,250 INFO: [mNotice: /Stage[main]/Zaqar/Zaqar_config[storage/message_pipeline]/ensure: created[0m
2018-12-04 06:04:36,283 INFO: [mNotice: /Stage[main]/Zaqar/Zaqar_config[transport/max_messages_post_size]/ensure: created[0m
2018-12-04 06:04:36,298 INFO: [mNotice: /Stage[main]/Zaqar/Zaqar_config[drivers/message_store]/ensure: created[0m
2018-12-04 06:04:36,314 INFO: [mNotice: /Stage[main]/Zaqar/Zaqar_config[drivers/management_store]/ensure: created[0m
2018-12-04 06:04:36,331 INFO: [mNotice: /Stage[main]/Zaqar::Management::Sqlalchemy/Zaqar_config[drivers:management_store:sqlalchemy/uri]/ensure: created[0m
2018-12-04 06:04:36,347 INFO: [mNotice: /Stage[main]/Zaqar::Messaging::Swift/Zaqar_config[drivers:message_store:swift/uri]/ensure: created[0m
2018-12-04 06:04:36,363 INFO: [mNotice: /Stage[main]/Zaqar::Messaging::Swift/Zaqar_config[drivers:message_store:swift/auth_url]/ensure: created[0m
2018-12-04 06:04:36,381 INFO: [mNotice: /Stage[main]/Zaqar::Transport::Websocket/Zaqar_config[drivers:transport:websocket/bind]/ensure: created[0m
2018-12-04 06:04:36,412 INFO: [mNotice: /Stage[main]/Zaqar::Transport::Websocket/Zaqar_config[drivers:transport:websocket/notification_bind]/ensure: created[0m
2018-12-04 06:04:36,565 INFO: [mNotice: /Stage[main]/Main/Sysctl::Value[net.ipv4.ip_forward]/Sysctl[net.ipv4.ip_forward]/ensure: created[0m
2018-12-04 06:04:36,724 INFO: [mNotice: /Stage[main]/Main/Sysctl::Value[net.ipv4.ip_forward]/Sysctl_runtime[net.ipv4.ip_forward]/val: val changed '0' to '1'[0m
2018-12-04 06:04:47,950 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Package[docker]/ensure: created[0m
2018-12-04 06:04:47,956 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/File[/etc/systemd/system/docker.service.d]/ensure: created[0m
2018-12-04 06:04:48,244 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/File[/etc/systemd/system/docker.service.d/99-unset-mountflags.conf]/ensure: defined content as '{md5}b984426de0b5978853686a649b64e4b8'[0m
2018-12-04 06:04:48,347 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Exec[systemd daemon-reload]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:04:48,518 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Augeas[docker-sysconfig-options]/returns: executed successfully[0m
2018-12-04 06:04:48,663 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Augeas[docker-sysconfig-registry]/returns: executed successfully[0m
2018-12-04 06:04:48,698 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Augeas[docker-daemon.json-debug]/returns: executed successfully[0m
2018-12-04 06:04:48,834 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Augeas[docker-sysconfig-storage]/returns: executed successfully[0m
2018-12-04 06:04:48,970 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Augeas[docker-sysconfig-network]/returns: executed successfully[0m
2018-12-04 06:04:51,467 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Docker/Service[docker]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:04:51,709 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[000 accept related established rules]/Firewall[000 accept related established rules ipv4]/ensure: created[0m
2018-12-04 06:04:52,198 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[000 accept related established rules]/Firewall[000 accept related established rules ipv6]/ensure: created[0m
2018-12-04 06:04:52,425 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[001 accept all icmp]/Firewall[001 accept all icmp ipv4]/ensure: created[0m
2018-12-04 06:04:52,912 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[001 accept all icmp]/Firewall[001 accept all icmp ipv6]/ensure: created[0m
2018-12-04 06:04:53,141 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[002 accept all to lo interface]/Firewall[002 accept all to lo interface ipv4]/ensure: created[0m
2018-12-04 06:04:53,582 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[002 accept all to lo interface]/Firewall[002 accept all to lo interface ipv6]/ensure: created[0m
2018-12-04 06:04:53,822 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[003 accept ssh]/Firewall[003 accept ssh ipv4]/ensure: created[0m
2018-12-04 06:04:54,272 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[003 accept ssh]/Firewall[003 accept ssh ipv6]/ensure: created[0m
2018-12-04 06:04:54,454 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Pre/Tripleo::Firewall::Rule[004 accept ipv6 dhcpv6]/Firewall[004 accept ipv6 dhcpv6 ipv6]/ensure: created[0m
2018-12-04 06:04:54,967 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[105 ntp]/Firewall[105 ntp ipv4]/ensure: created[0m
2018-12-04 06:04:55,153 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[105 ntp]/Firewall[105 ntp ipv6]/ensure: created[0m
2018-12-04 06:04:55,691 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[106 vrrp]/Firewall[106 vrrp ipv4]/ensure: created[0m
2018-12-04 06:04:55,902 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[106 vrrp]/Firewall[106 vrrp ipv6]/ensure: created[0m
2018-12-04 06:04:56,437 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[107 haproxy stats]/Firewall[107 haproxy stats ipv4]/ensure: created[0m
2018-12-04 06:04:56,634 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[107 haproxy stats]/Firewall[107 haproxy stats ipv6]/ensure: created[0m
2018-12-04 06:04:57,176 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[108 redis]/Firewall[108 redis ipv4]/ensure: created[0m
2018-12-04 06:04:57,672 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[108 redis]/Firewall[108 redis ipv6]/ensure: created[0m
2018-12-04 06:04:58,255 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[110 ceph]/Firewall[110 ceph ipv4]/ensure: created[0m
2018-12-04 06:04:58,474 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[110 ceph]/Firewall[110 ceph ipv6]/ensure: created[0m
2018-12-04 06:04:59,027 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[111 keystone]/Firewall[111 keystone ipv4]/ensure: created[0m
2018-12-04 06:04:59,517 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[111 keystone]/Firewall[111 keystone ipv6]/ensure: created[0m
2018-12-04 06:04:59,802 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[112 glance]/Firewall[112 glance ipv4]/ensure: created[0m
2018-12-04 06:05:00,316 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[112 glance]/Firewall[112 glance ipv6]/ensure: created[0m
2018-12-04 06:05:00,887 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[113 nova]/Firewall[113 nova ipv4]/ensure: created[0m
2018-12-04 06:05:01,403 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[113 nova]/Firewall[113 nova ipv6]/ensure: created[0m
2018-12-04 06:05:01,981 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[114 neutron server]/Firewall[114 neutron server ipv4]/ensure: created[0m
2018-12-04 06:05:02,224 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[114 neutron server]/Firewall[114 neutron server ipv6]/ensure: created[0m
2018-12-04 06:05:02,793 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[115 neutron dhcp input]/Firewall[115 neutron dhcp input ipv4]/ensure: created[0m
2018-12-04 06:05:03,323 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[115 neutron dhcp input]/Firewall[115 neutron dhcp input ipv6]/ensure: created[0m
2018-12-04 06:05:03,915 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[116 neutron dhcp output]/Firewall[116 neutron dhcp output ipv4]/ensure: created[0m
2018-12-04 06:05:04,438 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[116 neutron dhcp output]/Firewall[116 neutron dhcp output ipv6]/ensure: created[0m
2018-12-04 06:05:05,019 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[118 neutron vxlan networks]/Firewall[118 neutron vxlan networks ipv4]/ensure: created[0m
2018-12-04 06:05:05,346 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[118 neutron vxlan networks]/Firewall[118 neutron vxlan networks ipv6]/ensure: created[0m
2018-12-04 06:05:06,222 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[119 cinder]/Firewall[119 cinder ipv4]/ensure: created[0m
2018-12-04 06:05:06,489 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[119 cinder]/Firewall[119 cinder ipv6]/ensure: created[0m
2018-12-04 06:05:07,083 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[120 iscsi initiator]/Firewall[120 iscsi initiator ipv4]/ensure: created[0m
2018-12-04 06:05:07,622 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[120 iscsi initiator]/Firewall[120 iscsi initiator ipv6]/ensure: created[0m
2018-12-04 06:05:08,250 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[121 memcached]/Firewall[121 memcached ipv4]/ensure: created[0m
2018-12-04 06:05:09,131 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[122 swift proxy]/Firewall[122 swift proxy ipv4]/ensure: created[0m
2018-12-04 06:05:09,686 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[122 swift proxy]/Firewall[122 swift proxy ipv6]/ensure: created[0m
2018-12-04 06:05:10,325 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[123 swift storage]/Firewall[123 swift storage ipv4]/ensure: created[0m
2018-12-04 06:05:10,881 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[123 swift storage]/Firewall[123 swift storage ipv6]/ensure: created[0m
2018-12-04 06:05:11,507 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[125 heat]/Firewall[125 heat ipv4]/ensure: created[0m
2018-12-04 06:05:12,068 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[125 heat]/Firewall[125 heat ipv6]/ensure: created[0m
2018-12-04 06:05:12,701 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[126 horizon]/Firewall[126 horizon ipv4]/ensure: created[0m
2018-12-04 06:05:13,286 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[126 horizon]/Firewall[126 horizon ipv6]/ensure: created[0m
2018-12-04 06:05:13,927 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[127 snmp]/Firewall[127 snmp ipv4]/ensure: created[0m
2018-12-04 06:05:14,496 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[127 snmp]/Firewall[127 snmp ipv6]/ensure: created[0m
2018-12-04 06:05:15,412 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[128 aodh]/Firewall[128 aodh ipv4]/ensure: created[0m
2018-12-04 06:05:15,988 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[128 aodh]/Firewall[128 aodh ipv6]/ensure: created[0m
2018-12-04 06:05:16,643 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[129 gnocchi-api]/Firewall[129 gnocchi-api ipv4]/ensure: created[0m
2018-12-04 06:05:17,235 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[129 gnocchi-api]/Firewall[129 gnocchi-api ipv6]/ensure: created[0m
2018-12-04 06:05:18,171 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[130 tftp]/Firewall[130 tftp ipv4]/ensure: created[0m
2018-12-04 06:05:18,856 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[130 tftp]/Firewall[130 tftp ipv6]/ensure: created[0m
2018-12-04 06:05:19,545 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[131 novnc]/Firewall[131 novnc ipv4]/ensure: created[0m
2018-12-04 06:05:20,330 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[131 novnc]/Firewall[131 novnc ipv6]/ensure: created[0m
2018-12-04 06:05:21,284 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[132 mistral]/Firewall[132 mistral ipv4]/ensure: created[0m
2018-12-04 06:05:21,889 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[132 mistral]/Firewall[132 mistral ipv6]/ensure: created[0m
2018-12-04 06:05:23,018 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[133 zaqar]/Firewall[133 zaqar ipv4]/ensure: created[0m
2018-12-04 06:05:23,632 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[133 zaqar]/Firewall[133 zaqar ipv6]/ensure: created[0m
2018-12-04 06:05:24,318 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[134 zaqar websockets]/Firewall[134 zaqar websockets ipv4]/ensure: created[0m
2018-12-04 06:05:24,934 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[134 zaqar websockets]/Firewall[134 zaqar websockets ipv6]/ensure: created[0m
2018-12-04 06:05:25,898 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[135 ironic]/Firewall[135 ironic ipv4]/ensure: created[0m
2018-12-04 06:05:26,531 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[135 ironic]/Firewall[135 ironic ipv6]/ensure: created[0m
2018-12-04 06:05:27,492 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[136 trove]/Firewall[136 trove ipv4]/ensure: created[0m
2018-12-04 06:05:28,126 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[136 trove]/Firewall[136 trove ipv6]/ensure: created[0m
2018-12-04 06:05:29,127 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[137 ironic-inspector]/Firewall[137 ironic-inspector ipv4]/ensure: created[0m
2018-12-04 06:05:29,772 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[137 ironic-inspector]/Firewall[137 ironic-inspector ipv6]/ensure: created[0m
2018-12-04 06:05:30,765 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[138 docker registry]/Firewall[138 docker registry ipv4]/ensure: created[0m
2018-12-04 06:05:31,428 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[138 docker registry]/Firewall[138 docker registry ipv6]/ensure: created[0m
2018-12-04 06:05:32,436 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[139 apache vhost]/Firewall[139 apache vhost ipv4]/ensure: created[0m
2018-12-04 06:05:33,165 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[139 apache vhost]/Firewall[139 apache vhost ipv6]/ensure: created[0m
2018-12-04 06:05:34,171 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[140 destination ctlplane-subnet cidr nat]/Firewall[140 destination ctlplane-subnet cidr nat ipv4]/ensure: created[0m
2018-12-04 06:05:35,213 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[140 source ctlplane-subnet cidr nat]/Firewall[140 source ctlplane-subnet cidr nat ipv4]/ensure: created[0m
2018-12-04 06:05:36,220 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[142 tripleo-ui]/Firewall[142 tripleo-ui ipv4]/ensure: created[0m
2018-12-04 06:05:36,879 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[142 tripleo-ui]/Firewall[142 tripleo-ui ipv6]/ensure: created[0m
2018-12-04 06:05:37,889 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[143 panko-api]/Firewall[143 panko-api ipv4]/ensure: created[0m
2018-12-04 06:05:38,829 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Tripleo::Firewall::Rule[143 panko-api]/Firewall[143 panko-api ipv6]/ensure: created[0m
2018-12-04 06:05:38,896 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Kernel/Sysctl::Value[net.ipv4.ip_nonlocal_bind]/Sysctl[net.ipv4.ip_nonlocal_bind]/ensure: created[0m
2018-12-04 06:05:38,922 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Kernel/Sysctl::Value[net.ipv4.ip_nonlocal_bind]/Sysctl_runtime[net.ipv4.ip_nonlocal_bind]/val: val changed '0' to '1'[0m
2018-12-04 06:05:38,922 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Kernel/Sysctl::Value[net.ipv6.ip_nonlocal_bind]/Sysctl[net.ipv6.ip_nonlocal_bind]/ensure: created[0m
2018-12-04 06:05:38,945 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Kernel/Sysctl::Value[net.ipv6.ip_nonlocal_bind]/Sysctl_runtime[net.ipv6.ip_nonlocal_bind]/val: val changed '0' to '1'[0m
2018-12-04 06:05:42,025 INFO: [mNotice: /Stage[main]/Certmonger/Package[certmonger]/ensure: created[0m
2018-12-04 06:05:42,483 INFO: [mNotice: /Stage[main]/Certmonger/Service[certmonger]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:05:42,556 INFO: [mNotice: /Stage[main]/Tripleo::Certmonger::Ca::Local/Exec[extract-and-trust-ca]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:05:42,624 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Certmonger_user/Tripleo::Certmonger::Haproxy[undercloud-haproxy-public]/Certmonger_certificate[undercloud-haproxy-public-cert]/ensure: created[0m
2018-12-04 06:05:42,955 INFO: [mNotice: /Stage[main]/Haproxy/Haproxy::Instance[haproxy]/Anchor[haproxy::haproxy::begin]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:05:43,045 INFO: [mNotice: /Stage[main]/Keepalived::Config/Concat[/etc/keepalived/keepalived.conf]/File[/etc/keepalived/keepalived.conf]/content: content changed '{md5}968d3992f248f7c764c5d91c84db4f3f' to '{md5}675c103ce74ddf3a143bd45eb172b77f'[0m
2018-12-04 06:05:43,150 INFO: [mNotice: /Stage[main]/Glance::Policy/Oslo::Policy[glance_api_config]/Glance_api_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-04 06:05:43,205 INFO: [mNotice: /Stage[main]/Glance::Policy/Oslo::Policy[glance_registry_config]/Glance_registry_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-04 06:05:43,287 INFO: [mNotice: /Stage[main]/Glance::Api::Db/Oslo::Db[glance_api_config]/Glance_api_config[database/connection]/ensure: created[0m
2018-12-04 06:05:43,847 INFO: [mNotice: /Stage[main]/Glance::Api::Logging/Oslo::Log[glance_api_config]/Glance_api_config[DEFAULT/debug]/ensure: created[0m
2018-12-04 06:05:43,911 INFO: [mNotice: /Stage[main]/Glance::Api::Logging/Oslo::Log[glance_api_config]/Glance_api_config[DEFAULT/log_file]/ensure: created[0m
2018-12-04 06:05:43,943 INFO: [mNotice: /Stage[main]/Glance::Api::Logging/Oslo::Log[glance_api_config]/Glance_api_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-04 06:05:44,454 INFO: [mNotice: /Stage[main]/Glance::Cache::Logging/Oslo::Log[glance_cache_config]/Glance_cache_config[DEFAULT/debug]/ensure: created[0m
2018-12-04 06:05:44,488 INFO: [mNotice: /Stage[main]/Glance::Cache::Logging/Oslo::Log[glance_cache_config]/Glance_cache_config[DEFAULT/log_file]/ensure: created[0m
2018-12-04 06:05:44,507 INFO: [mNotice: /Stage[main]/Glance::Cache::Logging/Oslo::Log[glance_cache_config]/Glance_cache_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-04 06:05:44,677 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-04 06:05:44,707 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-04 06:05:45,329 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-04 06:05:45,360 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/username]/ensure: created[0m
2018-12-04 06:05:45,401 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/password]/ensure: created[0m
2018-12-04 06:05:45,432 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-04 06:05:45,463 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-04 06:05:45,494 INFO: [mNotice: /Stage[main]/Glance::Api::Authtoken/Keystone::Resource::Authtoken[glance_api_config]/Glance_api_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-04 06:05:45,816 INFO: [mNotice: /Stage[main]/Glance::Api/Oslo::Middleware[glance_api_config]/Glance_api_config[oslo_middleware/enable_proxy_headers_parsing]/ensure: created[0m
2018-12-04 06:05:45,977 INFO: [mNotice: /Stage[main]/Glance::Notify::Rabbitmq/Oslo::Messaging::Rabbit[glance_api_config]/Glance_api_config[oslo_messaging_rabbit/rabbit_password]/ensure: created[0m
2018-12-04 06:05:46,066 INFO: [mNotice: /Stage[main]/Glance::Notify::Rabbitmq/Oslo::Messaging::Rabbit[glance_api_config]/Glance_api_config[oslo_messaging_rabbit/rabbit_userid]/ensure: created[0m
2018-12-04 06:05:46,155 INFO: [mNotice: /Stage[main]/Glance::Notify::Rabbitmq/Oslo::Messaging::Rabbit[glance_api_config]/Glance_api_config[oslo_messaging_rabbit/rabbit_host]/ensure: created[0m
2018-12-04 06:05:46,584 INFO: [mNotice: /Stage[main]/Glance::Notify::Rabbitmq/Oslo::Messaging::Rabbit[glance_registry_config]/Glance_registry_config[oslo_messaging_rabbit/rabbit_password]/ensure: created[0m
2018-12-04 06:05:46,632 INFO: [mNotice: /Stage[main]/Glance::Notify::Rabbitmq/Oslo::Messaging::Rabbit[glance_registry_config]/Glance_registry_config[oslo_messaging_rabbit/rabbit_userid]/ensure: created[0m
2018-12-04 06:05:46,680 INFO: [mNotice: /Stage[main]/Glance::Notify::Rabbitmq/Oslo::Messaging::Rabbit[glance_registry_config]/Glance_registry_config[oslo_messaging_rabbit/rabbit_host]/ensure: created[0m
2018-12-04 06:05:46,861 INFO: [mNotice: /Stage[main]/Glance::Deps/Anchor[glance::config::end]: Triggered 'refresh' from 47 events[0m
2018-12-04 06:05:48,954 INFO: [mNotice: /Stage[main]/Nova::Api/Nova::Generic_service[api]/Package[nova-api]/ensure: created[0m
2018-12-04 06:05:55,847 INFO: [mNotice: /Stage[main]/Nova::Wsgi::Apache_placement/Nova::Generic_service[placement-api]/Package[nova-placement-api]/ensure: created[0m
2018-12-04 06:05:57,988 INFO: [mNotice: /Stage[main]/Nova::Conductor/Nova::Generic_service[conductor]/Package[nova-conductor]/ensure: created[0m
2018-12-04 06:06:00,089 INFO: [mNotice: /Stage[main]/Nova::Scheduler/Nova::Generic_service[scheduler]/Package[nova-scheduler]/ensure: created[0m
2018-12-04 06:06:15,526 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova::Generic_service[compute]/Package[nova-compute]/ensure: created[0m
2018-12-04 06:06:15,528 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::install::end]: Triggered 'refresh' from 7 events[0m
2018-12-04 06:06:15,598 INFO: [mNotice: /Stage[main]/Nova::Db/Nova_config[api_database/connection]/ensure: created[0m
2018-12-04 06:06:15,660 INFO: [mNotice: /Stage[main]/Nova::Db/Nova_config[placement_database/connection]/ensure: created[0m
2018-12-04 06:06:15,734 INFO: [mNotice: /Stage[main]/Nova/Nova_config[glance/api_servers]/ensure: created[0m
2018-12-04 06:06:15,782 INFO: [mNotice: /Stage[main]/Nova/Nova_config[DEFAULT/ssl_only]/ensure: created[0m
2018-12-04 06:06:16,231 INFO: [mNotice: /Stage[main]/Nova/Nova_config[api/auth_strategy]/ensure: created[0m
2018-12-04 06:06:16,324 INFO: [mNotice: /Stage[main]/Nova/Nova_config[DEFAULT/ram_allocation_ratio]/ensure: created[0m
2018-12-04 06:06:16,830 INFO: [mNotice: /Stage[main]/Nova/Nova_config[notifications/notify_api_faults]/ensure: created[0m
2018-12-04 06:06:17,137 INFO: [mNotice: /Stage[main]/Nova/Nova_config[notifications/notification_format]/ensure: created[0m
2018-12-04 06:06:17,177 INFO: [mNotice: /Stage[main]/Nova/Nova_config[DEFAULT/state_path]/ensure: created[0m
2018-12-04 06:06:17,217 INFO: [mNotice: /Stage[main]/Nova/Nova_config[DEFAULT/service_down_time]/ensure: created[0m
2018-12-04 06:06:17,261 INFO: [mNotice: /Stage[main]/Nova/Nova_config[DEFAULT/rootwrap_config]/ensure: created[0m
2018-12-04 06:06:17,296 INFO: [mNotice: /Stage[main]/Nova/Nova_config[DEFAULT/report_interval]/ensure: created[0m
2018-12-04 06:06:17,387 INFO: [mNotice: /Stage[main]/Nova/Nova_config[notifications/notify_on_state_change]/ensure: created[0m
2018-12-04 06:06:18,276 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[wsgi/api_paste_config]/ensure: created[0m
2018-12-04 06:06:18,317 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/enabled_apis]/ensure: created[0m
2018-12-04 06:06:18,360 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/osapi_compute_listen]/ensure: created[0m
2018-12-04 06:06:18,419 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/metadata_listen]/ensure: created[0m
2018-12-04 06:06:18,740 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/metadata_listen_port]/ensure: created[0m
2018-12-04 06:06:18,780 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/osapi_compute_listen_port]/ensure: created[0m
2018-12-04 06:06:18,820 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/osapi_compute_workers]/ensure: created[0m
2018-12-04 06:06:18,861 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/metadata_workers]/ensure: created[0m
2018-12-04 06:06:19,357 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[api/use_forwarded_for]/ensure: created[0m
2018-12-04 06:06:19,400 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[api/fping_path]/ensure: created[0m
2018-12-04 06:06:20,438 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[vendordata_dynamic_auth/project_domain_name]/ensure: created[0m
2018-12-04 06:06:20,501 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[vendordata_dynamic_auth/user_domain_name]/ensure: created[0m
2018-12-04 06:06:20,566 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[neutron/service_metadata_proxy]/ensure: created[0m
2018-12-04 06:06:20,959 INFO: [mNotice: /Stage[main]/Nova::Api/Nova_config[DEFAULT/allow_resize_to_same_host]/ensure: created[0m
2018-12-04 06:06:20,993 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/auth_type]/ensure: created[0m
2018-12-04 06:06:21,027 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/auth_url]/ensure: created[0m
2018-12-04 06:06:21,062 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/password]/ensure: created[0m
2018-12-04 06:06:21,098 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/project_domain_name]/ensure: created[0m
2018-12-04 06:06:21,133 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/project_name]/ensure: created[0m
2018-12-04 06:06:21,477 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/user_domain_name]/ensure: created[0m
2018-12-04 06:06:21,511 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/username]/ensure: created[0m
2018-12-04 06:06:21,553 INFO: [mNotice: /Stage[main]/Nova::Placement/Nova_config[placement/os_region_name]/ensure: created[0m
2018-12-04 06:06:21,608 INFO: [mNotice: /Stage[main]/Nova::Conductor/Nova_config[conductor/workers]/ensure: created[0m
2018-12-04 06:06:21,670 INFO: [mNotice: /Stage[main]/Nova::Scheduler/Nova_config[scheduler/driver]/ensure: created[0m
2018-12-04 06:06:22,028 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[scheduler/host_manager]/ensure: created[0m
2018-12-04 06:06:22,062 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[scheduler/max_attempts]/ensure: created[0m
2018-12-04 06:06:22,126 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[filter_scheduler/host_subset_size]/ensure: created[0m
2018-12-04 06:06:22,161 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[filter_scheduler/max_io_ops_per_host]/ensure: created[0m
2018-12-04 06:06:22,197 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[filter_scheduler/max_instances_per_host]/ensure: created[0m
2018-12-04 06:06:22,565 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[filter_scheduler/available_filters]/ensure: created[0m
2018-12-04 06:06:22,600 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[filter_scheduler/weight_classes]/ensure: created[0m
2018-12-04 06:06:22,636 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[filter_scheduler/use_baremetal_filters]/ensure: created[0m
2018-12-04 06:06:22,678 INFO: [mNotice: /Stage[main]/Nova::Scheduler::Filter/Nova_config[filter_scheduler/enabled_filters]/ensure: created[0m
2018-12-04 06:06:23,776 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[DEFAULT/reserved_host_memory_mb]/ensure: created[0m
2018-12-04 06:06:23,844 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[DEFAULT/heal_instance_info_cache_interval]/ensure: created[0m
2018-12-04 06:06:24,291 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[key_manager/backend]/ensure: created[0m
2018-12-04 06:06:24,439 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[compute/consecutive_build_service_disable_threshold]/ensure: created[0m
2018-12-04 06:06:24,835 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[vnc/enabled]/ensure: created[0m
2018-12-04 06:06:24,875 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[DEFAULT/force_config_drive]/ensure: created[0m
2018-12-04 06:06:24,915 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[DEFAULT/instance_usage_audit]/ensure: created[0m
2018-12-04 06:06:24,956 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[DEFAULT/instance_usage_audit_period]/ensure: created[0m
2018-12-04 06:06:24,996 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova_config[DEFAULT/force_raw_images]/ensure: created[0m
2018-12-04 06:06:25,405 INFO: [mNotice: /Stage[main]/Main/Nova_config[DEFAULT/sync_power_state_interval]/ensure: created[0m
2018-12-04 06:06:25,440 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/auth_plugin]/ensure: created[0m
2018-12-04 06:06:25,474 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/username]/ensure: created[0m
2018-12-04 06:06:25,509 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/password]/ensure: created[0m
2018-12-04 06:06:25,552 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/auth_url]/ensure: created[0m
2018-12-04 06:06:25,579 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/project_name]/ensure: created[0m
2018-12-04 06:06:25,623 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/api_endpoint]/ensure: created[0m
2018-12-04 06:06:26,021 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/user_domain_name]/ensure: created[0m
2018-12-04 06:06:26,056 INFO: [mNotice: /Stage[main]/Nova::Ironic::Common/Nova_config[ironic/project_domain_name]/ensure: created[0m
2018-12-04 06:06:26,111 INFO: [mNotice: /Stage[main]/Nova::Compute::Ironic/Nova_config[DEFAULT/compute_driver]/ensure: created[0m
2018-12-04 06:06:26,181 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[DEFAULT/dhcp_domain]/ensure: created[0m
2018-12-04 06:06:26,546 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[DEFAULT/firewall_driver]/ensure: created[0m
2018-12-04 06:06:26,584 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[DEFAULT/vif_plugging_is_fatal]/ensure: created[0m
2018-12-04 06:06:26,631 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[DEFAULT/vif_plugging_timeout]/ensure: created[0m
2018-12-04 06:06:26,658 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/default_floating_pool]/ensure: created[0m
2018-12-04 06:06:26,693 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/url]/ensure: created[0m
2018-12-04 06:06:26,737 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/timeout]/ensure: created[0m
2018-12-04 06:06:26,766 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/project_name]/ensure: created[0m
2018-12-04 06:06:26,802 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/project_domain_name]/ensure: created[0m
2018-12-04 06:06:27,158 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/region_name]/ensure: created[0m
2018-12-04 06:06:27,194 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/username]/ensure: created[0m
2018-12-04 06:06:27,232 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/user_domain_name]/ensure: created[0m
2018-12-04 06:06:27,267 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/password]/ensure: created[0m
2018-12-04 06:06:27,301 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/auth_url]/ensure: created[0m
2018-12-04 06:06:27,336 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/ovs_bridge]/ensure: created[0m
2018-12-04 06:06:27,387 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/extension_sync_interval]/ensure: created[0m
2018-12-04 06:06:27,738 INFO: [mNotice: /Stage[main]/Nova::Network::Neutron/Nova_config[neutron/auth_type]/ensure: created[0m
2018-12-04 06:06:28,670 INFO: [mNotice: /Stage[main]/Main/Augeas[lvm.conf]/returns: executed successfully[0m
2018-12-04 06:06:28,764 INFO: [mNotice: /Stage[main]/Nova::Db/Oslo::Db[nova_config]/Nova_config[database/connection]/ensure: created[0m
2018-12-04 06:06:29,926 INFO: [mNotice: /Stage[main]/Nova::Logging/Oslo::Log[nova_config]/Nova_config[DEFAULT/debug]/ensure: created[0m
2018-12-04 06:06:30,377 INFO: [mNotice: /Stage[main]/Nova::Logging/Oslo::Log[nova_config]/Nova_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-04 06:06:35,457 INFO: [mNotice: /Stage[main]/Nova/Oslo::Messaging::Default[nova_config]/Nova_config[DEFAULT/rpc_response_timeout]/ensure: created[0m
2018-12-04 06:06:35,500 INFO: [mNotice: /Stage[main]/Nova/Oslo::Messaging::Default[nova_config]/Nova_config[DEFAULT/transport_url]/ensure: created[0m
2018-12-04 06:06:35,565 INFO: [mNotice: /Stage[main]/Nova/Oslo::Messaging::Notifications[nova_config]/Nova_config[oslo_messaging_notifications/driver]/ensure: created[0m
2018-12-04 06:06:36,051 INFO: [mNotice: /Stage[main]/Nova/Oslo::Concurrency[nova_config]/Nova_config[oslo_concurrency/lock_path]/ensure: created[0m
2018-12-04 06:06:36,086 INFO: [mNotice: /Stage[main]/Nova::Policy/Oslo::Policy[nova_config]/Nova_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-04 06:06:36,213 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-04 06:06:36,251 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-04 06:06:38,051 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-04 06:06:38,088 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/username]/ensure: created[0m
2018-12-04 06:06:38,600 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/password]/ensure: created[0m
2018-12-04 06:06:38,636 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-04 06:06:38,672 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-04 06:06:38,708 INFO: [mNotice: /Stage[main]/Nova::Keystone::Authtoken/Keystone::Resource::Authtoken[nova_config]/Nova_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-04 06:06:38,803 INFO: [mNotice: /Stage[main]/Nova::Api/Oslo::Middleware[nova_config]/Nova_config[oslo_middleware/enable_proxy_headers_parsing]/ensure: created[0m
2018-12-04 06:06:38,815 INFO: [mNotice: /Stage[main]/Neutron::Logging/Oslo::Log[neutron_config]/Neutron_config[DEFAULT/debug]/ensure: created[0m
2018-12-04 06:06:38,844 INFO: [mNotice: /Stage[main]/Neutron::Logging/Oslo::Log[neutron_config]/Neutron_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-04 06:06:39,349 INFO: [mNotice: /Stage[main]/Neutron/Oslo::Messaging::Default[neutron_config]/Neutron_config[DEFAULT/control_exchange]/ensure: created[0m
2018-12-04 06:06:39,364 INFO: [mNotice: /Stage[main]/Neutron/Oslo::Concurrency[neutron_config]/Neutron_config[oslo_concurrency/lock_path]/ensure: created[0m
2018-12-04 06:06:39,458 INFO: [mNotice: /Stage[main]/Neutron/Oslo::Messaging::Rabbit[neutron_config]/Neutron_config[oslo_messaging_rabbit/rabbit_password]/ensure: created[0m
2018-12-04 06:06:39,495 INFO: [mNotice: /Stage[main]/Neutron/Oslo::Messaging::Rabbit[neutron_config]/Neutron_config[oslo_messaging_rabbit/rabbit_userid]/ensure: created[0m
2018-12-04 06:06:39,510 INFO: [mNotice: /Stage[main]/Neutron/Oslo::Messaging::Rabbit[neutron_config]/Neutron_config[oslo_messaging_rabbit/rabbit_hosts]/ensure: created[0m
2018-12-04 06:06:40,150 INFO: [mNotice: /Stage[main]/Neutron::Db/Oslo::Db[neutron_config]/Neutron_config[database/connection]/ensure: created[0m
2018-12-04 06:06:40,652 INFO: [mNotice: /Stage[main]/Neutron::Policy/Oslo::Policy[neutron_config]/Neutron_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-04 06:06:40,681 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-04 06:06:40,690 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-04 06:06:41,250 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-04 06:06:41,260 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/username]/ensure: created[0m
2018-12-04 06:06:41,269 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/password]/ensure: created[0m
2018-12-04 06:06:41,278 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-04 06:06:41,287 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-04 06:06:41,297 INFO: [mNotice: /Stage[main]/Neutron::Keystone::Authtoken/Keystone::Resource::Authtoken[neutron_config]/Neutron_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-04 06:06:41,322 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron::Plugins::Ml2::Type_driver[flat]/Neutron_plugin_ml2[ml2_type_flat/flat_networks]/ensure: created[0m
2018-12-04 06:06:41,326 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron::Plugins::Ml2::Type_driver[vlan]/Neutron_plugin_ml2[ml2_type_vlan/network_vlan_ranges]/ensure: created[0m
2018-12-04 06:06:41,331 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron::Plugins::Ml2::Type_driver[gre]/Neutron_plugin_ml2[ml2_type_gre/tunnel_id_ranges]/ensure: created[0m
2018-12-04 06:06:41,335 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron::Plugins::Ml2::Type_driver[vxlan]/Neutron_plugin_ml2[ml2_type_vxlan/vxlan_group]/ensure: created[0m
2018-12-04 06:06:41,339 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron::Plugins::Ml2::Type_driver[vxlan]/Neutron_plugin_ml2[ml2_type_vxlan/vni_ranges]/ensure: created[0m
2018-12-04 06:06:41,346 INFO: [mNotice: /Stage[main]/Neutron::Plugins::Ml2/Neutron::Plugins::Ml2::Type_driver[geneve]/Neutron_plugin_ml2[ml2_type_geneve/vni_ranges]/ensure: created[0m
2018-12-04 06:06:41,350 INFO: [mNotice: /Stage[main]/Neutron::Deps/Anchor[neutron::config::end]: Triggered 'refresh' from 83 events[0m
2018-12-04 06:06:41,425 INFO: [mNotice: /Stage[main]/Rsync::Server/Xinetd::Service[rsync]/File[/etc/xinetd.d/rsync]/ensure: defined content as '{md5}07f7d21614492b12e647b7140939f2c0'[0m
2018-12-04 06:06:41,454 INFO: [mNotice: /Stage[main]/Rsync::Server/Concat[/etc/rsyncd.conf]/File[/etc/rsyncd.conf]/content: content changed '{md5}c63fccb45c0dcbbbe17d0f4bdba920ec' to '{md5}bb435847238b233af8de6f9c8b9b7f5b'[0m
2018-12-04 06:06:41,484 INFO: [mNotice: /Stage[main]/Heat::Logging/Oslo::Log[heat_config]/Heat_config[DEFAULT/debug]/ensure: created[0m
2018-12-04 06:06:41,533 INFO: [mNotice: /Stage[main]/Heat::Logging/Oslo::Log[heat_config]/Heat_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-04 06:06:42,150 INFO: [mNotice: /Stage[main]/Heat::Db/Oslo::Db[heat_config]/Heat_config[database/connection]/ensure: created[0m
2018-12-04 06:06:42,330 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-04 06:06:42,352 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-04 06:06:42,993 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-04 06:06:43,014 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/username]/ensure: created[0m
2018-12-04 06:06:43,033 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/password]/ensure: created[0m
2018-12-04 06:06:43,052 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-04 06:06:43,075 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-04 06:06:43,105 INFO: [mNotice: /Stage[main]/Heat::Keystone::Authtoken/Keystone::Resource::Authtoken[heat_config]/Heat_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-04 06:06:44,083 INFO: [mNotice: /Stage[main]/Heat/Oslo::Messaging::Notifications[heat_config]/Heat_config[oslo_messaging_notifications/driver]/ensure: created[0m
2018-12-04 06:06:44,122 INFO: [mNotice: /Stage[main]/Heat/Oslo::Messaging::Default[heat_config]/Heat_config[DEFAULT/rpc_response_timeout]/ensure: created[0m
2018-12-04 06:06:44,146 INFO: [mNotice: /Stage[main]/Heat/Oslo::Messaging::Default[heat_config]/Heat_config[DEFAULT/transport_url]/ensure: created[0m
2018-12-04 06:06:44,184 INFO: [mNotice: /Stage[main]/Heat/Oslo::Middleware[heat_config]/Heat_config[oslo_middleware/enable_proxy_headers_parsing]/ensure: created[0m
2018-12-04 06:06:44,200 INFO: [mNotice: /Stage[main]/Heat::Policy/Oslo::Policy[heat_config]/Heat_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-04 06:06:44,358 INFO: [mNotice: /Stage[main]/Heat::Cors/Oslo::Cors[heat_config]/Heat_config[cors/allowed_origin]/ensure: created[0m
2018-12-04 06:06:44,387 INFO: [mNotice: /Stage[main]/Heat::Cors/Oslo::Cors[heat_config]/Heat_config[cors/expose_headers]/ensure: created[0m
2018-12-04 06:06:44,405 INFO: [mNotice: /Stage[main]/Heat::Cors/Oslo::Cors[heat_config]/Heat_config[cors/max_age]/ensure: created[0m
2018-12-04 06:06:44,434 INFO: [mNotice: /Stage[main]/Heat::Cors/Oslo::Cors[heat_config]/Heat_config[cors/allow_headers]/ensure: created[0m
2018-12-04 06:06:44,435 INFO: [mNotice: /Stage[main]/Heat::Deps/Anchor[heat::config::end]: Triggered 'refresh' from 49 events[0m
2018-12-04 06:06:44,893 INFO: [mNotice: /Stage[main]/Nova::Cors/Oslo::Cors[nova_config]/Nova_config[cors/allowed_origin]/ensure: created[0m
2018-12-04 06:06:44,957 INFO: [mNotice: /Stage[main]/Nova::Cors/Oslo::Cors[nova_config]/Nova_config[cors/expose_headers]/ensure: created[0m
2018-12-04 06:06:44,992 INFO: [mNotice: /Stage[main]/Nova::Cors/Oslo::Cors[nova_config]/Nova_config[cors/max_age]/ensure: created[0m
2018-12-04 06:06:45,027 INFO: [mNotice: /Stage[main]/Nova::Cors/Oslo::Cors[nova_config]/Nova_config[cors/allow_methods]/ensure: created[0m
2018-12-04 06:06:45,062 INFO: [mNotice: /Stage[main]/Nova::Cors/Oslo::Cors[nova_config]/Nova_config[cors/allow_headers]/ensure: created[0m
2018-12-04 06:06:45,065 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::config::end]: Triggered 'refresh' from 104 events[0m
2018-12-04 06:06:45,106 INFO: [mNotice: /Stage[main]/Ironic::Logging/Oslo::Log[ironic_config]/Ironic_config[DEFAULT/debug]/ensure: created[0m
2018-12-04 06:06:45,606 INFO: [mNotice: /Stage[main]/Ironic::Logging/Oslo::Log[ironic_config]/Ironic_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-04 06:06:45,899 INFO: [mNotice: /Stage[main]/Ironic::Db/Oslo::Db[ironic_config]/Ironic_config[database/connection]/ensure: created[0m
2018-12-04 06:06:46,608 INFO: [mNotice: /Stage[main]/Ironic/Oslo::Messaging::Default[ironic_config]/Ironic_config[DEFAULT/rpc_response_timeout]/ensure: created[0m
2018-12-04 06:06:46,648 INFO: [mNotice: /Stage[main]/Ironic/Oslo::Messaging::Default[ironic_config]/Ironic_config[DEFAULT/transport_url]/ensure: created[0m
2018-12-04 06:06:48,557 INFO: [mNotice: /Stage[main]/Ironic::Policy/Oslo::Policy[ironic_config]/Ironic_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-04 06:06:48,633 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-04 06:06:48,663 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-04 06:06:49,477 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-04 06:06:49,506 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/username]/ensure: created[0m
2018-12-04 06:06:49,546 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/password]/ensure: created[0m
2018-12-04 06:06:49,576 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-04 06:06:49,605 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-04 06:06:49,641 INFO: [mNotice: /Stage[main]/Ironic::Api::Authtoken/Keystone::Resource::Authtoken[ironic_config]/Ironic_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-04 06:06:49,712 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[boot]/Ironic_config[DEFAULT/enabled_boot_interfaces]/ensure: created[0m
2018-12-04 06:06:49,759 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[console]/Ironic_config[DEFAULT/enabled_console_interfaces]/ensure: created[0m
2018-12-04 06:06:49,809 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[deploy]/Ironic_config[DEFAULT/enabled_deploy_interfaces]/ensure: created[0m
2018-12-04 06:06:49,859 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[inspect]/Ironic_config[DEFAULT/enabled_inspect_interfaces]/ensure: created[0m
2018-12-04 06:06:49,890 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[inspect]/Ironic_config[DEFAULT/default_inspect_interface]/ensure: created[0m
2018-12-04 06:06:50,376 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[management]/Ironic_config[DEFAULT/enabled_management_interfaces]/ensure: created[0m
2018-12-04 06:06:50,450 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[power]/Ironic_config[DEFAULT/enabled_power_interfaces]/ensure: created[0m
2018-12-04 06:06:50,497 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[raid]/Ironic_config[DEFAULT/enabled_raid_interfaces]/ensure: created[0m
2018-12-04 06:06:50,606 INFO: [mNotice: /Stage[main]/Ironic::Drivers::Interfaces/Ironic::Drivers::Hardware_interface[vendor]/Ironic_config[DEFAULT/enabled_vendor_interfaces]/ensure: created[0m
2018-12-04 06:06:50,632 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Logging/Oslo::Log[ironic_inspector_config]/Ironic_inspector_config[DEFAULT/debug]/ensure: created[0m
2018-12-04 06:06:50,654 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Logging/Oslo::Log[ironic_inspector_config]/Ironic_inspector_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-04 06:06:50,745 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Db/Oslo::Db[ironic_inspector_config]/Ironic_inspector_config[database/connection]/ensure: created[0m
2018-12-04 06:06:51,293 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-04 06:06:51,304 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-04 06:06:51,417 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-04 06:06:51,426 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/username]/ensure: created[0m
2018-12-04 06:06:51,435 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/password]/ensure: created[0m
2018-12-04 06:06:51,445 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-04 06:06:51,455 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-04 06:06:51,465 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Authtoken/Keystone::Resource::Authtoken[ironic_inspector_config]/Ironic_inspector_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-04 06:06:51,484 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Cors/Oslo::Cors[ironic_inspector_config]/Ironic_inspector_config[cors/allowed_origin]/ensure: created[0m
2018-12-04 06:06:51,498 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Cors/Oslo::Cors[ironic_inspector_config]/Ironic_inspector_config[cors/expose_headers]/ensure: created[0m
2018-12-04 06:06:51,503 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Cors/Oslo::Cors[ironic_inspector_config]/Ironic_inspector_config[cors/max_age]/ensure: created[0m
2018-12-04 06:06:51,513 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Cors/Oslo::Cors[ironic_inspector_config]/Ironic_inspector_config[cors/allow_methods]/ensure: created[0m
2018-12-04 06:06:51,523 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Cors/Oslo::Cors[ironic_inspector_config]/Ironic_inspector_config[cors/allow_headers]/ensure: created[0m
2018-12-04 06:06:51,524 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic-inspector::config::end]: Triggered 'refresh' from 45 events[0m
2018-12-04 06:06:51,534 INFO: [mNotice: /Stage[main]/Ironic::Pxe/Xinetd::Service[tftp]/File[/etc/xinetd.d/tftp]/content: content changed '{md5}678efd3887a91cd4e0955aa6c8b12257' to '{md5}50a3a2ab0bde157472ab030b96fb23e4'[0m
2018-12-04 06:06:51,858 INFO: [mNotice: /Stage[main]/Xinetd/Service[xinetd]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:06:51,871 INFO: [mNotice: /Stage[main]/Ironic::Pxe/Ironic::Pxe::Tftpboot_file[pxelinux.0]/File[/tftpboot/pxelinux.0]/ensure: defined content as '{md5}e28413b36fee074c30554f8c9e54ca2c'[0m
2018-12-04 06:06:51,881 INFO: [mNotice: /Stage[main]/Ironic::Pxe/Ironic::Pxe::Tftpboot_file[chain.c32]/File[/tftpboot/chain.c32]/ensure: defined content as '{md5}b1b9c82af57fffaf21612dbac538b5ac'[0m
2018-12-04 06:06:51,908 INFO: [mNotice: /Stage[main]/Ironic::Cors/Oslo::Cors[ironic_config]/Ironic_config[cors/allowed_origin]/ensure: created[0m
2018-12-04 06:06:51,952 INFO: [mNotice: /Stage[main]/Ironic::Cors/Oslo::Cors[ironic_config]/Ironic_config[cors/expose_headers]/ensure: created[0m
2018-12-04 06:06:51,982 INFO: [mNotice: /Stage[main]/Ironic::Cors/Oslo::Cors[ironic_config]/Ironic_config[cors/max_age]/ensure: created[0m
2018-12-04 06:06:52,011 INFO: [mNotice: /Stage[main]/Ironic::Cors/Oslo::Cors[ironic_config]/Ironic_config[cors/allow_methods]/ensure: created[0m
2018-12-04 06:06:52,527 INFO: [mNotice: /Stage[main]/Ironic::Cors/Oslo::Cors[ironic_config]/Ironic_config[cors/allow_headers]/ensure: created[0m
2018-12-04 06:06:52,529 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::config::end]: Triggered 'refresh' from 95 events[0m
2018-12-04 06:06:52,540 INFO: [mNotice: /Stage[main]/Mistral::Db/Oslo::Db[mistral_config]/Mistral_config[database/connection]/ensure: created[0m
2018-12-04 06:06:52,556 INFO: [mNotice: /Stage[main]/Mistral::Logging/Oslo::Log[mistral_config]/Mistral_config[DEFAULT/debug]/ensure: created[0m
2018-12-04 06:06:52,560 INFO: [mNotice: /Stage[main]/Mistral::Logging/Oslo::Log[mistral_config]/Mistral_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-04 06:06:52,579 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-04 06:06:52,583 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-04 06:06:52,603 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-04 06:06:52,607 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/username]/ensure: created[0m
2018-12-04 06:06:52,609 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/password]/ensure: created[0m
2018-12-04 06:06:52,611 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-04 06:06:52,613 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-04 06:06:52,614 INFO: [mNotice: /Stage[main]/Mistral::Keystone::Authtoken/Keystone::Resource::Authtoken[mistral_config]/Mistral_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-04 06:06:52,619 INFO: [mNotice: /Stage[main]/Mistral/Oslo::Messaging::Default[mistral_config]/Mistral_config[DEFAULT/rpc_response_timeout]/ensure: created[0m
2018-12-04 06:06:52,644 INFO: [mNotice: /Stage[main]/Mistral/Oslo::Messaging::Rabbit[mistral_config]/Mistral_config[oslo_messaging_rabbit/rabbit_password]/ensure: created[0m
2018-12-04 06:06:52,645 INFO: [mNotice: /Stage[main]/Mistral/Oslo::Messaging::Rabbit[mistral_config]/Mistral_config[oslo_messaging_rabbit/rabbit_userid]/ensure: created[0m
2018-12-04 06:06:52,651 INFO: [mNotice: /Stage[main]/Mistral/Oslo::Messaging::Rabbit[mistral_config]/Mistral_config[oslo_messaging_rabbit/rabbit_host]/ensure: created[0m
2018-12-04 06:06:52,661 INFO: [mNotice: /Stage[main]/Mistral::Policy/Oslo::Policy[mistral_config]/Mistral_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-04 06:06:52,669 INFO: [mNotice: /Stage[main]/Mistral::Cors/Oslo::Cors[mistral_config]/Mistral_config[cors/allowed_origin]/ensure: created[0m
2018-12-04 06:06:52,675 INFO: [mNotice: /Stage[main]/Mistral::Cors/Oslo::Cors[mistral_config]/Mistral_config[cors/expose_headers]/ensure: created[0m
2018-12-04 06:06:52,677 INFO: [mNotice: /Stage[main]/Mistral::Cors/Oslo::Cors[mistral_config]/Mistral_config[cors/allow_headers]/ensure: created[0m
2018-12-04 06:06:52,679 INFO: [mNotice: /Stage[main]/Mistral::Deps/Anchor[mistral::config::end]: Triggered 'refresh' from 25 events[0m
2018-12-04 06:06:52,729 INFO: [mNotice: /Stage[main]/Zaqar::Logging/Oslo::Log[zaqar_config]/Zaqar_config[DEFAULT/log_dir]/ensure: created[0m
2018-12-04 06:06:52,875 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/auth_uri]/ensure: created[0m
2018-12-04 06:06:53,363 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/auth_type]/ensure: created[0m
2018-12-04 06:06:53,554 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/auth_url]/ensure: created[0m
2018-12-04 06:06:53,570 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/username]/ensure: created[0m
2018-12-04 06:06:53,588 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/password]/ensure: created[0m
2018-12-04 06:06:53,607 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/user_domain_name]/ensure: created[0m
2018-12-04 06:06:53,622 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/project_name]/ensure: created[0m
2018-12-04 06:06:53,650 INFO: [mNotice: /Stage[main]/Zaqar::Keystone::Authtoken/Keystone::Resource::Authtoken[zaqar_config]/Zaqar_config[keystone_authtoken/project_domain_name]/ensure: created[0m
2018-12-04 06:06:53,684 INFO: [mNotice: /Stage[main]/Zaqar::Policy/Oslo::Policy[zaqar_config]/Zaqar_config[oslo_policy/policy_file]/ensure: created[0m
2018-12-04 06:06:53,702 INFO: [mNotice: /Stage[main]/Zaqar::Deps/Anchor[zaqar::config::end]: Triggered 'refresh' from 25 events[0m
2018-12-04 06:06:53,718 INFO: [mNotice: /Stage[main]/Main/Zaqar::Server_instance[1]/File[/etc/zaqar/1.conf]/ensure: defined content as '{md5}7750571c15d53265e703cc60ea7938af'[0m
2018-12-04 06:06:53,745 INFO: [mNotice: /Stage[main]/Ssh::Server::Config/Concat[/etc/ssh/sshd_config]/File[/etc/ssh/sshd_config]/content: content changed '{md5}40d961cd3154f0439fcac1a50bd77b96' to '{md5}56dac259c7b06586e1726ee6685171ba'[0m
2018-12-04 06:06:53,909 INFO: [mNotice: /Stage[main]/Ssh::Server::Service/Service[sshd]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:06:53,935 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Certmonger_user/Tripleo::Certmonger::Haproxy[undercloud-haproxy-public]/Concat[/etc/pki/tls/certs/undercloud-172.16.0.10.pem]/File[/etc/pki/tls/certs/undercloud-172.16.0.10.pem]/seluser: seluser changed 'unconfined_u' to 'system_u'[0m
2018-12-04 06:06:54,039 INFO: [mNotice: /Stage[main]/Haproxy/Haproxy::Instance[haproxy]/Haproxy::Install[haproxy]/Package[haproxy]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:06:55,504 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[keystone_admin]/Tripleo::Firewall::Rule[100 keystone_admin_haproxy]/Firewall[100 keystone_admin_haproxy ipv4]/ensure: created[0m
2018-12-04 06:06:56,967 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[keystone_admin]/Tripleo::Firewall::Rule[100 keystone_admin_haproxy]/Firewall[100 keystone_admin_haproxy ipv6]/ensure: created[0m
2018-12-04 06:06:58,074 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[keystone_public]/Tripleo::Firewall::Rule[100 keystone_public_haproxy]/Firewall[100 keystone_public_haproxy ipv4]/ensure: created[0m
2018-12-04 06:06:59,612 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[keystone_public]/Tripleo::Firewall::Rule[100 keystone_public_haproxy]/Firewall[100 keystone_public_haproxy ipv6]/ensure: created[0m
2018-12-04 06:07:01,180 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[keystone_public]/Tripleo::Firewall::Rule[100 keystone_public_haproxy_ssl]/Firewall[100 keystone_public_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:07:02,196 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[keystone_public]/Tripleo::Firewall::Rule[100 keystone_public_haproxy_ssl]/Firewall[100 keystone_public_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:07:03,773 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[neutron]/Tripleo::Firewall::Rule[100 neutron_haproxy]/Firewall[100 neutron_haproxy ipv4]/ensure: created[0m
2018-12-04 06:07:04,774 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[neutron]/Tripleo::Firewall::Rule[100 neutron_haproxy]/Firewall[100 neutron_haproxy ipv6]/ensure: created[0m
2018-12-04 06:07:06,536 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[neutron]/Tripleo::Firewall::Rule[100 neutron_haproxy_ssl]/Firewall[100 neutron_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:07:08,050 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[neutron]/Tripleo::Firewall::Rule[100 neutron_haproxy_ssl]/Firewall[100 neutron_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:07:09,705 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[glance_api]/Tripleo::Firewall::Rule[100 glance_api_haproxy]/Firewall[100 glance_api_haproxy ipv4]/ensure: created[0m
2018-12-04 06:07:10,740 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[glance_api]/Tripleo::Firewall::Rule[100 glance_api_haproxy]/Firewall[100 glance_api_haproxy ipv6]/ensure: created[0m
2018-12-04 06:07:12,466 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[glance_api]/Tripleo::Firewall::Rule[100 glance_api_haproxy_ssl]/Firewall[100 glance_api_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:07:14,051 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[glance_api]/Tripleo::Firewall::Rule[100 glance_api_haproxy_ssl]/Firewall[100 glance_api_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:07:15,667 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_osapi]/Tripleo::Firewall::Rule[100 nova_osapi_haproxy]/Firewall[100 nova_osapi_haproxy ipv4]/ensure: created[0m
2018-12-04 06:07:16,849 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_osapi]/Tripleo::Firewall::Rule[100 nova_osapi_haproxy]/Firewall[100 nova_osapi_haproxy ipv6]/ensure: created[0m
2018-12-04 06:07:18,505 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_osapi]/Tripleo::Firewall::Rule[100 nova_osapi_haproxy_ssl]/Firewall[100 nova_osapi_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:07:20,032 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_osapi]/Tripleo::Firewall::Rule[100 nova_osapi_haproxy_ssl]/Firewall[100 nova_osapi_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:07:22,073 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_placement]/Tripleo::Firewall::Rule[100 nova_placement_haproxy]/Firewall[100 nova_placement_haproxy ipv4]/ensure: created[0m
2018-12-04 06:07:23,153 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_placement]/Tripleo::Firewall::Rule[100 nova_placement_haproxy]/Firewall[100 nova_placement_haproxy ipv6]/ensure: created[0m
2018-12-04 06:07:24,804 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_placement]/Tripleo::Firewall::Rule[100 nova_placement_haproxy_ssl]/Firewall[100 nova_placement_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:07:26,353 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_placement]/Tripleo::Firewall::Rule[100 nova_placement_haproxy_ssl]/Firewall[100 nova_placement_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:07:27,970 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_metadata]/Tripleo::Firewall::Rule[100 nova_metadata_haproxy]/Firewall[100 nova_metadata_haproxy ipv4]/ensure: created[0m
2018-12-04 06:07:29,525 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[nova_metadata]/Tripleo::Firewall::Rule[100 nova_metadata_haproxy]/Firewall[100 nova_metadata_haproxy ipv6]/ensure: created[0m
2018-12-04 06:07:31,155 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[aodh]/Tripleo::Firewall::Rule[100 aodh_haproxy]/Firewall[100 aodh_haproxy ipv4]/ensure: created[0m
2018-12-04 06:07:32,704 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[aodh]/Tripleo::Firewall::Rule[100 aodh_haproxy]/Firewall[100 aodh_haproxy ipv6]/ensure: created[0m
2018-12-04 06:07:34,380 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[aodh]/Tripleo::Firewall::Rule[100 aodh_haproxy_ssl]/Firewall[100 aodh_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:07:35,966 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[aodh]/Tripleo::Firewall::Rule[100 aodh_haproxy_ssl]/Firewall[100 aodh_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:07:37,598 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[panko]/Tripleo::Firewall::Rule[100 panko_haproxy]/Firewall[100 panko_haproxy ipv4]/ensure: created[0m
2018-12-04 06:07:39,157 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[panko]/Tripleo::Firewall::Rule[100 panko_haproxy]/Firewall[100 panko_haproxy ipv6]/ensure: created[0m
2018-12-04 06:07:40,828 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[panko]/Tripleo::Firewall::Rule[100 panko_haproxy_ssl]/Firewall[100 panko_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:07:42,406 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[panko]/Tripleo::Firewall::Rule[100 panko_haproxy_ssl]/Firewall[100 panko_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:07:44,095 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[gnocchi]/Tripleo::Firewall::Rule[100 gnocchi_haproxy]/Firewall[100 gnocchi_haproxy ipv4]/ensure: created[0m
2018-12-04 06:07:45,684 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[gnocchi]/Tripleo::Firewall::Rule[100 gnocchi_haproxy]/Firewall[100 gnocchi_haproxy ipv6]/ensure: created[0m
2018-12-04 06:07:47,360 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[gnocchi]/Tripleo::Firewall::Rule[100 gnocchi_haproxy_ssl]/Firewall[100 gnocchi_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:07:48,949 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[gnocchi]/Tripleo::Firewall::Rule[100 gnocchi_haproxy_ssl]/Firewall[100 gnocchi_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:07:50,674 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[mistral]/Tripleo::Firewall::Rule[100 mistral_haproxy]/Firewall[100 mistral_haproxy ipv4]/ensure: created[0m
2018-12-04 06:07:52,481 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[mistral]/Tripleo::Firewall::Rule[100 mistral_haproxy]/Firewall[100 mistral_haproxy ipv6]/ensure: created[0m
2018-12-04 06:07:54,176 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[mistral]/Tripleo::Firewall::Rule[100 mistral_haproxy_ssl]/Firewall[100 mistral_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:07:55,774 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[mistral]/Tripleo::Firewall::Rule[100 mistral_haproxy_ssl]/Firewall[100 mistral_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:07:57,957 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[swift_proxy_server]/Tripleo::Firewall::Rule[100 swift_proxy_server_haproxy]/Firewall[100 swift_proxy_server_haproxy ipv4]/ensure: created[0m
2018-12-04 06:07:59,565 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[swift_proxy_server]/Tripleo::Firewall::Rule[100 swift_proxy_server_haproxy]/Firewall[100 swift_proxy_server_haproxy ipv6]/ensure: created[0m
2018-12-04 06:08:01,291 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[swift_proxy_server]/Tripleo::Firewall::Rule[100 swift_proxy_server_haproxy_ssl]/Firewall[100 swift_proxy_server_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:08:02,916 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[swift_proxy_server]/Tripleo::Firewall::Rule[100 swift_proxy_server_haproxy_ssl]/Firewall[100 swift_proxy_server_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:08:04,616 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[heat_api]/Tripleo::Firewall::Rule[100 heat_api_haproxy]/Firewall[100 heat_api_haproxy ipv4]/ensure: created[0m
2018-12-04 06:08:06,230 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[heat_api]/Tripleo::Firewall::Rule[100 heat_api_haproxy]/Firewall[100 heat_api_haproxy ipv6]/ensure: created[0m
2018-12-04 06:08:08,423 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[heat_api]/Tripleo::Firewall::Rule[100 heat_api_haproxy_ssl]/Firewall[100 heat_api_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:08:10,051 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[heat_api]/Tripleo::Firewall::Rule[100 heat_api_haproxy_ssl]/Firewall[100 heat_api_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:08:11,833 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic]/Tripleo::Firewall::Rule[100 ironic_haproxy]/Firewall[100 ironic_haproxy ipv4]/ensure: created[0m
2018-12-04 06:08:13,940 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic]/Tripleo::Firewall::Rule[100 ironic_haproxy]/Firewall[100 ironic_haproxy ipv6]/ensure: created[0m
2018-12-04 06:08:15,663 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic]/Tripleo::Firewall::Rule[100 ironic_haproxy_ssl]/Firewall[100 ironic_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:08:17,309 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic]/Tripleo::Firewall::Rule[100 ironic_haproxy_ssl]/Firewall[100 ironic_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:08:19,563 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic-inspector]/Tripleo::Firewall::Rule[100 ironic-inspector_haproxy]/Firewall[100 ironic-inspector_haproxy ipv4]/ensure: created[0m
2018-12-04 06:08:21,201 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic-inspector]/Tripleo::Firewall::Rule[100 ironic-inspector_haproxy]/Firewall[100 ironic-inspector_haproxy ipv6]/ensure: created[0m
2018-12-04 06:08:22,941 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic-inspector]/Tripleo::Firewall::Rule[100 ironic-inspector_haproxy_ssl]/Firewall[100 ironic-inspector_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:08:25,067 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ironic-inspector]/Tripleo::Firewall::Rule[100 ironic-inspector_haproxy_ssl]/Firewall[100 ironic-inspector_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:08:26,958 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[docker-registry]/Tripleo::Firewall::Rule[100 docker-registry_haproxy]/Firewall[100 docker-registry_haproxy ipv4]/ensure: created[0m
2018-12-04 06:08:28,782 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[docker-registry]/Tripleo::Firewall::Rule[100 docker-registry_haproxy]/Firewall[100 docker-registry_haproxy ipv6]/ensure: created[0m
2018-12-04 06:08:31,033 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[docker-registry]/Tripleo::Firewall::Rule[100 docker-registry_haproxy_ssl]/Firewall[100 docker-registry_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:08:32,707 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[docker-registry]/Tripleo::Firewall::Rule[100 docker-registry_haproxy_ssl]/Firewall[100 docker-registry_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:08:34,954 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_api]/Tripleo::Firewall::Rule[100 zaqar_api_haproxy]/Firewall[100 zaqar_api_haproxy ipv4]/ensure: created[0m
2018-12-04 06:08:36,658 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_api]/Tripleo::Firewall::Rule[100 zaqar_api_haproxy]/Firewall[100 zaqar_api_haproxy ipv6]/ensure: created[0m
2018-12-04 06:08:38,928 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_api]/Tripleo::Firewall::Rule[100 zaqar_api_haproxy_ssl]/Firewall[100 zaqar_api_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:08:40,634 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_api]/Tripleo::Firewall::Rule[100 zaqar_api_haproxy_ssl]/Firewall[100 zaqar_api_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:08:42,982 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_ws]/Tripleo::Firewall::Rule[100 zaqar_ws_haproxy]/Firewall[100 zaqar_ws_haproxy ipv4]/ensure: created[0m
2018-12-04 06:08:44,702 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_ws]/Tripleo::Firewall::Rule[100 zaqar_ws_haproxy]/Firewall[100 zaqar_ws_haproxy ipv6]/ensure: created[0m
2018-12-04 06:08:46,962 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_ws]/Tripleo::Firewall::Rule[100 zaqar_ws_haproxy_ssl]/Firewall[100 zaqar_ws_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:08:48,678 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[zaqar_ws]/Tripleo::Firewall::Rule[100 zaqar_ws_haproxy_ssl]/Firewall[100 zaqar_ws_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:08:50,969 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ui]/Tripleo::Firewall::Rule[100 ui_haproxy]/Firewall[100 ui_haproxy ipv4]/ensure: created[0m
2018-12-04 06:08:53,241 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ui]/Tripleo::Firewall::Rule[100 ui_haproxy]/Firewall[100 ui_haproxy ipv6]/ensure: created[0m
2018-12-04 06:08:55,125 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ui]/Tripleo::Firewall::Rule[100 ui_haproxy_ssl]/Firewall[100 ui_haproxy_ssl ipv4]/ensure: created[0m
2018-12-04 06:08:57,369 INFO: [mNotice: /Stage[main]/Tripleo::Haproxy/Tripleo::Haproxy::Endpoint[ui]/Tripleo::Firewall::Rule[100 ui_haproxy_ssl]/Firewall[100 ui_haproxy_ssl ipv6]/ensure: created[0m
2018-12-04 06:08:58,145 INFO: [mNotice: /Stage[main]/Keepalived::Service/Service[keepalived]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:09:07,590 INFO: [mNotice: /Stage[main]/Rabbitmq::Install/Package[rabbitmq-server]/ensure: created[0m
2018-12-04 06:09:07,594 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[/etc/rabbitmq]/owner: owner changed 'rabbitmq' to 'root'[0m
2018-12-04 06:09:07,595 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[/etc/rabbitmq]/group: group changed 'rabbitmq' to 'root'[0m
2018-12-04 06:09:07,602 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[/etc/rabbitmq/ssl]/ensure: created[0m
2018-12-04 06:09:07,615 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[rabbitmq.config]/content: content changed '{md5}b346ec0a8320f85f795bf612f6b02da7' to '{md5}350e5a8df33b06d7ac6f1e8bb24adaaf'[0m
2018-12-04 06:09:07,616 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[rabbitmq.config]/owner: owner changed 'rabbitmq' to 'root'[0m
2018-12-04 06:09:07,616 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[rabbitmq.config]/mode: mode changed '0644' to '0640'[0m
2018-12-04 06:09:07,621 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[rabbitmq-env.config]/ensure: defined content as '{md5}390e4051ad2dc97e52b6efec7b1df680'[0m
2018-12-04 06:09:07,628 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[rabbitmq-inetrc]/ensure: defined content as '{md5}12f8d1a1f9f57f23c1be6c7bf2286e73'[0m
2018-12-04 06:09:07,633 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[rabbitmqadmin.conf]/ensure: defined content as '{md5}44d4ef5cb86ab30e6127e83939ef09c4'[0m
2018-12-04 06:09:07,636 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[/etc/systemd/system/rabbitmq-server.service.d]/ensure: created[0m
2018-12-04 06:09:07,642 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[/etc/systemd/system/rabbitmq-server.service.d/limits.conf]/ensure: defined content as '{md5}8eb9ff6c576b9869944215af3a568c2e'[0m
2018-12-04 06:09:07,777 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/Exec[rabbitmq-systemd-reload]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:09:07,785 INFO: [mNotice: /Stage[main]/Rabbitmq::Config/File[/etc/security/limits.d/rabbitmq-server.conf]/ensure: defined content as '{md5}5ddc6ba5fcaeddd5b1565e5adfda5236'[0m
2018-12-04 06:09:11,282 INFO: [mNotice: /Stage[main]/Rabbitmq/Rabbitmq_plugin[rabbitmq_management]/ensure: created[0m
2018-12-04 06:09:16,750 INFO: [mNotice: /Stage[main]/Rabbitmq::Service/Service[rabbitmq-server]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:09:16,799 INFO: [mNotice: /Stage[main]/Rabbitmq::Install::Rabbitmqadmin/Archive[rabbitmqadmin]/ensure: download archive from http://172.16.0.1:15672/cli/rabbitmqadmin to /var/lib/rabbitmq/rabbitmqadmin  without cleanup[0m
2018-12-04 06:09:16,812 INFO: [mNotice: /Stage[main]/Rabbitmq::Install::Rabbitmqadmin/File[/usr/local/bin/rabbitmqadmin]/ensure: defined content as '{md5}76394723569012aa8a197a08f1b53926'[0m
2018-12-04 06:09:23,682 INFO: [mNotice: /Stage[main]/Nova::Db::Sync_api/Exec[nova-db-sync-api]/returns: executed successfully[0m
2018-12-04 06:09:29,010 INFO: [mNotice: /Stage[main]/Nova::Db::Sync_api/Exec[nova-db-sync-api]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:09:29,016 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::dbsync_api::end]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:09:29,017 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::cell_v2::begin]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:09:29,017 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::db_online_data_migrations::begin]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:09:34,593 INFO: [mNotice: /Stage[main]/Nova::Cell_v2::Map_cell0/Exec[nova-cell_v2-map_cell0]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:09:34,595 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::cell_v2::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:09:41,501 INFO: [mNotice: /Stage[main]/Ironic::Inspector::Db::Sync/Exec[ironic-inspector-dbsync]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:09:41,505 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic-inspector::dbsync::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:09:41,506 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic-inspector::service::begin]: Triggered 'refresh' from 3 events[0m
2018-12-04 06:09:44,523 INFO: [mNotice: /Stage[main]/Swift::Storage::Account/Swift::Storage::Generic[account]/Package[swift-account]/ensure: created[0m
2018-12-04 06:09:47,204 INFO: [mNotice: /Stage[main]/Swift::Storage::Container/Swift::Storage::Generic[container]/Package[swift-container]/ensure: created[0m
2018-12-04 06:09:49,802 INFO: [mNotice: /Stage[main]/Swift::Storage::Object/Swift::Storage::Generic[object]/Package[swift-object]/ensure: created[0m
2018-12-04 06:09:49,806 INFO: [mNotice: /Stage[main]/Swift::Deps/Anchor[swift::install::end]: Triggered 'refresh' from 4 events[0m
2018-12-04 06:09:49,815 INFO: [mNotice: /Stage[main]/Swift/File[/var/lib/swift]/group: group changed 'root' to 'swift'[0m
2018-12-04 06:09:49,820 INFO: [mNotice: /Stage[main]/Swift/File[/etc/swift/swift.conf]/owner: owner changed 'root' to 'swift'[0m
2018-12-04 06:09:49,824 INFO: [mNotice: /Stage[main]/Swift/Swift_config[swift-hash/swift_hash_path_suffix]/value: value changed '%SWIFT_HASH_PATH_SUFFIX%' to '3b40319cbc5c5439e83c1ba3cceb5a69eb9a44ba'[0m
2018-12-04 06:09:49,827 INFO: [mNotice: /Stage[main]/Swift/Swift_config[swift-constraints/max_header_size]/ensure: created[0m
2018-12-04 06:09:49,832 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/bind_ip]/ensure: created[0m
2018-12-04 06:09:49,834 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/workers]/value: value changed '8' to '2'[0m
2018-12-04 06:09:49,839 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/log_name]/ensure: created[0m
2018-12-04 06:09:49,842 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/log_facility]/ensure: created[0m
2018-12-04 06:09:49,845 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/log_level]/ensure: created[0m
2018-12-04 06:09:49,847 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/log_headers]/ensure: created[0m
2018-12-04 06:09:49,850 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/log_address]/ensure: created[0m
2018-12-04 06:09:49,857 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[pipeline:main/pipeline]/value: value changed 'catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk tempurl ratelimit copy container-quotas account-quotas slo dlo versioned_writes proxy-logging proxy-server' to 'catch_errors healthcheck proxy-logging cache ratelimit bulk tempurl formpost authtoken keystone staticweb copy slo dlo versioned_writes proxy-logging proxy-server'[0m
2018-12-04 06:09:49,861 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/set log_name]/ensure: created[0m
2018-12-04 06:09:49,864 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/set log_facility]/ensure: created[0m
2018-12-04 06:09:49,867 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/set log_level]/ensure: created[0m
2018-12-04 06:09:49,870 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/set log_address]/ensure: created[0m
2018-12-04 06:09:49,873 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/log_handoffs]/ensure: created[0m
2018-12-04 06:09:49,878 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/allow_account_management]/value: value changed 'true' to 'True'[0m
2018-12-04 06:09:49,881 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/account_autocreate]/value: value changed 'true' to 'True'[0m
2018-12-04 06:09:49,888 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[app:proxy-server/node_timeout]/ensure: created[0m
2018-12-04 06:09:49,891 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/cors_allow_origin]/ensure: created[0m
2018-12-04 06:09:49,896 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift_proxy_config[DEFAULT/strict_cors_mode]/ensure: created[0m
2018-12-04 06:09:49,908 INFO: [mNotice: /Stage[main]/Swift::Proxy::Bulk/Swift_proxy_config[filter:bulk/max_containers_per_extraction]/ensure: created[0m
2018-12-04 06:09:49,912 INFO: [mNotice: /Stage[main]/Swift::Proxy::Bulk/Swift_proxy_config[filter:bulk/max_failed_extractions]/ensure: created[0m
2018-12-04 06:09:49,914 INFO: [mNotice: /Stage[main]/Swift::Proxy::Bulk/Swift_proxy_config[filter:bulk/max_deletes_per_request]/ensure: created[0m
2018-12-04 06:09:49,917 INFO: [mNotice: /Stage[main]/Swift::Proxy::Bulk/Swift_proxy_config[filter:bulk/yield_frequency]/ensure: created[0m
2018-12-04 06:09:49,927 INFO: [mNotice: /Stage[main]/Swift::Proxy::Keystone/Swift_proxy_config[filter:keystone/reseller_prefix]/ensure: created[0m
2018-12-04 06:09:49,933 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/File[/var/cache/swift]/mode: mode changed '0755' to '0700'[0m
2018-12-04 06:09:49,936 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/log_name]/ensure: created[0m
2018-12-04 06:09:49,939 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/signing_dir]/value: value changed '/tmp/keystone-signing-swift' to '/var/cache/swift'[0m
2018-12-04 06:09:49,943 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/www_authenticate_uri]/ensure: created[0m
2018-12-04 06:09:49,947 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/auth_url]/ensure: created[0m
2018-12-04 06:09:49,953 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/auth_plugin]/ensure: created[0m
2018-12-04 06:09:49,955 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/project_domain_id]/ensure: created[0m
2018-12-04 06:09:49,960 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/user_domain_id]/ensure: created[0m
2018-12-04 06:09:50,478 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/project_name]/ensure: created[0m
2018-12-04 06:09:50,481 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/username]/ensure: created[0m
2018-12-04 06:09:50,484 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/password]/ensure: created[0m
2018-12-04 06:09:50,489 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/delay_auth_decision]/ensure: created[0m
2018-12-04 06:09:50,491 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/cache]/ensure: created[0m
2018-12-04 06:09:50,494 INFO: [mNotice: /Stage[main]/Swift::Proxy::Authtoken/Swift_proxy_config[filter:authtoken/include_service_catalog]/ensure: created[0m
2018-12-04 06:09:50,497 INFO: [mNotice: /Stage[main]/Swift::Proxy::Staticweb/Swift_proxy_config[filter:staticweb/use]/ensure: created[0m
2018-12-04 06:09:50,506 INFO: [mNotice: /Stage[main]/Swift::Proxy::Copy/Swift_proxy_config[filter:copy/object_post_as_copy]/value: value changed 'false' to 'True'[0m
2018-12-04 06:09:50,510 INFO: [mNotice: /Stage[main]/Swift::Proxy::Slo/Swift_proxy_config[filter:slo/max_manifest_segments]/ensure: created[0m
2018-12-04 06:09:50,513 INFO: [mNotice: /Stage[main]/Swift::Proxy::Slo/Swift_proxy_config[filter:slo/max_manifest_size]/ensure: created[0m
2018-12-04 06:09:50,516 INFO: [mNotice: /Stage[main]/Swift::Proxy::Slo/Swift_proxy_config[filter:slo/min_segment_size]/ensure: created[0m
2018-12-04 06:09:50,519 INFO: [mNotice: /Stage[main]/Swift::Proxy::Slo/Swift_proxy_config[filter:slo/rate_limit_after_segment]/ensure: created[0m
2018-12-04 06:09:50,522 INFO: [mNotice: /Stage[main]/Swift::Proxy::Slo/Swift_proxy_config[filter:slo/rate_limit_segments_per_sec]/ensure: created[0m
2018-12-04 06:09:50,525 INFO: [mNotice: /Stage[main]/Swift::Proxy::Slo/Swift_proxy_config[filter:slo/max_get_time]/ensure: created[0m
2018-12-04 06:09:50,531 INFO: [mNotice: /Stage[main]/Swift::Proxy::Dlo/Swift_proxy_config[filter:dlo/rate_limit_after_segment]/ensure: created[0m
2018-12-04 06:09:50,534 INFO: [mNotice: /Stage[main]/Swift::Proxy::Dlo/Swift_proxy_config[filter:dlo/rate_limit_segments_per_sec]/ensure: created[0m
2018-12-04 06:09:50,537 INFO: [mNotice: /Stage[main]/Swift::Proxy::Dlo/Swift_proxy_config[filter:dlo/max_get_time]/ensure: created[0m
2018-12-04 06:09:50,543 INFO: [mNotice: /Stage[main]/Swift::Proxy::Versioned_writes/Swift_proxy_config[filter:versioned_writes/allow_versioned_writes]/ensure: created[0m
2018-12-04 06:09:50,551 INFO: [mNotice: /Stage[main]/Swift::Proxy::Ratelimit/Swift_proxy_config[filter:ratelimit/clock_accuracy]/ensure: created[0m
2018-12-04 06:09:50,552 INFO: [mNotice: /Stage[main]/Swift::Proxy::Ratelimit/Swift_proxy_config[filter:ratelimit/max_sleep_time_seconds]/ensure: created[0m
2018-12-04 06:09:50,557 INFO: [mNotice: /Stage[main]/Swift::Proxy::Ratelimit/Swift_proxy_config[filter:ratelimit/log_sleep_time_seconds]/ensure: created[0m
2018-12-04 06:09:50,560 INFO: [mNotice: /Stage[main]/Swift::Proxy::Ratelimit/Swift_proxy_config[filter:ratelimit/rate_buffer_seconds]/ensure: created[0m
2018-12-04 06:09:50,562 INFO: [mNotice: /Stage[main]/Swift::Proxy::Ratelimit/Swift_proxy_config[filter:ratelimit/account_ratelimit]/ensure: created[0m
2018-12-04 06:09:50,580 INFO: [mNotice: /Stage[main]/Swift::Proxy::Formpost/Swift_proxy_config[filter:formpost/use]/ensure: created[0m
2018-12-04 06:09:51,079 INFO: [mNotice: /Stage[main]/Swift::Ringbuilder/Swift::Ringbuilder::Create[object]/Exec[create_object]/returns: executed successfully[0m
2018-12-04 06:09:51,562 INFO: [1;33mWarning: Unexpected line: Ring file /etc/swift/object.ring.gz not found, probably it hasn't been written yet[0m
2018-12-04 06:09:51,562 INFO: [1;33mWarning: Unexpected line: Devices:   id region zone ip address:port replication ip:port  name weight partitions balance flags meta[0m
2018-12-04 06:09:51,563 INFO: [1;33mWarning: Unexpected line: There are no devices in this ring, or all devices have been deleted[0m
2018-12-04 06:09:52,038 INFO: [mNotice: /Stage[main]/Main/Ring_object_device[172.16.0.1:6000/1]/ensure: created[0m
2018-12-04 06:09:52,524 INFO: [mNotice: /Stage[main]/Swift::Ringbuilder/Swift::Ringbuilder::Create[account]/Exec[create_account]/returns: executed successfully[0m
2018-12-04 06:09:53,007 INFO: [1;33mWarning: Unexpected line: Ring file /etc/swift/account.ring.gz not found, probably it hasn't been written yet[0m
2018-12-04 06:09:53,008 INFO: [1;33mWarning: Unexpected line: Devices:   id region zone ip address:port replication ip:port  name weight partitions balance flags meta[0m
2018-12-04 06:09:53,008 INFO: [1;33mWarning: Unexpected line: There are no devices in this ring, or all devices have been deleted[0m
2018-12-04 06:09:53,697 INFO: [mNotice: /Stage[main]/Main/Ring_account_device[172.16.0.1:6002/1]/ensure: created[0m
2018-12-04 06:09:54,265 INFO: [mNotice: /Stage[main]/Swift::Ringbuilder/Swift::Ringbuilder::Create[container]/Exec[create_container]/returns: executed successfully[0m
2018-12-04 06:09:54,735 INFO: [1;33mWarning: Unexpected line: Ring file /etc/swift/container.ring.gz not found, probably it hasn't been written yet[0m
2018-12-04 06:09:54,735 INFO: [1;33mWarning: Unexpected line: Devices:   id region zone ip address:port replication ip:port  name weight partitions balance flags meta[0m
2018-12-04 06:09:54,736 INFO: [1;33mWarning: Unexpected line: There are no devices in this ring, or all devices have been deleted[0m
2018-12-04 06:09:55,206 INFO: [mNotice: /Stage[main]/Main/Ring_container_device[172.16.0.1:6001/1]/ensure: created[0m
2018-12-04 06:09:55,789 INFO: [mNotice: /Stage[main]/Swift::Ringbuilder/Swift::Ringbuilder::Rebalance[object]/Exec[rebalance_object]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:09:56,330 INFO: [mNotice: /Stage[main]/Swift::Ringbuilder/Swift::Ringbuilder::Rebalance[account]/Exec[rebalance_account]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:09:56,868 INFO: [mNotice: /Stage[main]/Swift::Ringbuilder/Swift::Ringbuilder::Rebalance[container]/Exec[rebalance_container]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:09:56,873 INFO: [mNotice: /Stage[main]/Swift::Storage::Account/Swift::Storage::Generic[account]/File[/etc/swift/account-server/]/owner: owner changed 'root' to 'swift'[0m
2018-12-04 06:09:56,874 INFO: [mNotice: /Stage[main]/Swift::Storage::Account/Swift::Storage::Generic[account]/File[/etc/swift/account-server/]/group: group changed 'root' to 'swift'[0m
2018-12-04 06:09:56,884 INFO: [mNotice: /Stage[main]/Swift::Storage::Container/Swift::Storage::Generic[container]/File[/etc/swift/container-server/]/owner: owner changed 'root' to 'swift'[0m
2018-12-04 06:09:56,884 INFO: [mNotice: /Stage[main]/Swift::Storage::Container/Swift::Storage::Generic[container]/File[/etc/swift/container-server/]/group: group changed 'root' to 'swift'[0m
2018-12-04 06:09:56,886 INFO: [mNotice: /Stage[main]/Swift::Storage::Object/Swift::Storage::Generic[object]/File[/etc/swift/object-server/]/owner: owner changed 'root' to 'swift'[0m
2018-12-04 06:09:56,888 INFO: [mNotice: /Stage[main]/Swift::Storage::Object/Swift::Storage::Generic[object]/File[/etc/swift/object-server/]/group: group changed 'root' to 'swift'[0m
2018-12-04 06:09:56,918 INFO: [mNotice: /Stage[main]/Swift::Storage::All/Swift::Storage::Server[6002]/Concat[/etc/swift/account-server.conf]/File[/etc/swift/account-server.conf]/content: content changed '{md5}07e5a1a1e5a0ab83d745e20680eb32c1' to '{md5}144199aa8b50ff6cf7fbac3fa24b667d'[0m
2018-12-04 06:09:56,919 INFO: [mNotice: /Stage[main]/Swift::Storage::All/Swift::Storage::Server[6002]/Concat[/etc/swift/account-server.conf]/File[/etc/swift/account-server.conf]/mode: mode changed '0640' to '0644'[0m
2018-12-04 06:09:56,946 INFO: [mNotice: /Stage[main]/Swift::Storage::All/Swift::Storage::Server[6001]/Concat[/etc/swift/container-server.conf]/File[/etc/swift/container-server.conf]/content: content changed '{md5}4998257eb89ff63e838b37686ebb1ee7' to '{md5}ff8453ec614df7da5c22e3fad444cb5c'[0m
2018-12-04 06:09:56,947 INFO: [mNotice: /Stage[main]/Swift::Storage::All/Swift::Storage::Server[6001]/Concat[/etc/swift/container-server.conf]/File[/etc/swift/container-server.conf]/mode: mode changed '0640' to '0644'[0m
2018-12-04 06:09:56,974 INFO: [mNotice: /Stage[main]/Swift::Storage::All/Swift::Storage::Server[6000]/Concat[/etc/swift/object-server.conf]/File[/etc/swift/object-server.conf]/content: content changed '{md5}8c3bfdea900f37c8b2cbd5d9fe5d664c' to '{md5}17629e1211783d4e936aa7b2779aeea3'[0m
2018-12-04 06:09:56,974 INFO: [mNotice: /Stage[main]/Swift::Storage::All/Swift::Storage::Server[6000]/Concat[/etc/swift/object-server.conf]/File[/etc/swift/object-server.conf]/mode: mode changed '0640' to '0644'[0m
2018-12-04 06:09:56,978 INFO: [mNotice: /Stage[main]/Swift::Deps/Anchor[swift::config::end]: Triggered 'refresh' from 62 events[0m
2018-12-04 06:09:56,980 INFO: [mNotice: /Stage[main]/Swift::Deps/Anchor[swift::service::begin]: Triggered 'refresh' from 3 events[0m
2018-12-04 06:09:57,021 INFO: [mNotice: /Stage[main]/Haproxy/Haproxy::Instance[haproxy]/Haproxy::Config[haproxy]/Concat[/etc/haproxy/haproxy.cfg]/File[/etc/haproxy/haproxy.cfg]/seluser: seluser changed 'unconfined_u' to 'system_u'[0m
2018-12-04 06:09:57,034 INFO: [mNotice: /Stage[main]/Haproxy/Haproxy::Instance[haproxy]/Haproxy::Service[haproxy]/File[/etc/sysconfig/haproxy]/content: content changed '{md5}2d8c2707741c581482214fefeed95a47' to '{md5}e2410b4501510c91882f106e6db20a3b'[0m
2018-12-04 06:09:57,646 INFO: [mNotice: /Stage[main]/Haproxy/Haproxy::Instance[haproxy]/Haproxy::Service[haproxy]/Service[haproxy]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:09:57,646 INFO: [mNotice: /Stage[main]/Haproxy/Haproxy::Instance[haproxy]/Anchor[haproxy::haproxy::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:10:03,536 INFO: [mNotice: /Stage[main]/Keystone/Package[keystone]/ensure: created[0m
2018-12-04 06:10:03,622 INFO: [mNotice: /Stage[main]/Keystone::Deps/Anchor[keystone::install::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:10:04,557 INFO: [mNotice: /Stage[main]/Keystone/File[/etc/keystone/fernet-keys]/seluser: seluser changed 'unconfined_u' to 'system_u'[0m
2018-12-04 06:10:04,561 INFO: [mNotice: /Stage[main]/Keystone/File[/etc/keystone/fernet-keys]/seltype: seltype changed 'var_t' to 'etc_t'[0m
2018-12-04 06:10:04,564 INFO: [mNotice: /Stage[main]/Keystone/File[/etc/keystone/credential-keys]/seluser: seluser changed 'unconfined_u' to 'system_u'[0m
2018-12-04 06:10:04,564 INFO: [mNotice: /Stage[main]/Keystone/File[/etc/keystone/credential-keys]/seltype: seltype changed 'var_t' to 'etc_t'[0m
2018-12-04 06:10:04,623 INFO: [mNotice: /Stage[main]/Apache::Mod::Mime/File[mime.conf]/ensure: defined content as '{md5}9da85e58f3bd6c780ce76db603b7f028'[0m
2018-12-04 06:10:04,630 INFO: [mNotice: /Stage[main]/Apache::Mod::Mime_magic/File[mime_magic.conf]/ensure: defined content as '{md5}b258529b332429e2ff8344f726a95457'[0m
2018-12-04 06:10:04,635 INFO: [mNotice: /Stage[main]/Apache::Mod::Alias/File[alias.conf]/ensure: defined content as '{md5}983e865be85f5e0daaed7433db82995e'[0m
2018-12-04 06:10:04,641 INFO: [mNotice: /Stage[main]/Apache::Mod::Autoindex/File[autoindex.conf]/ensure: defined content as '{md5}2421a3c6df32c7e38c2a7a22afdf5728'[0m
2018-12-04 06:10:04,647 INFO: [mNotice: /Stage[main]/Apache::Mod::Deflate/File[deflate.conf]/ensure: defined content as '{md5}a045d750d819b1e9dae3fbfb3f20edd5'[0m
2018-12-04 06:10:04,653 INFO: [mNotice: /Stage[main]/Apache::Mod::Dir/File[dir.conf]/ensure: defined content as '{md5}c741d8ea840e6eb999d739eed47c69d7'[0m
2018-12-04 06:10:04,659 INFO: [mNotice: /Stage[main]/Apache::Mod::Negotiation/File[negotiation.conf]/ensure: defined content as '{md5}47284b5580b986a6ba32580b6ffb9fd7'[0m
2018-12-04 06:10:04,665 INFO: [mNotice: /Stage[main]/Apache::Mod::Setenvif/File[setenvif.conf]/ensure: defined content as '{md5}c7ede4173da1915b7ec088201f030c28'[0m
2018-12-04 06:10:04,671 INFO: [mNotice: /Stage[main]/Apache::Mod::Prefork/File[/etc/httpd/conf.modules.d/prefork.conf]/ensure: defined content as '{md5}f58b0483b70b4e73b5f67ff37b8f24a0'[0m
2018-12-04 06:10:04,678 INFO: [mNotice: /Stage[main]/Keystone::Wsgi::Apache/File[/var/www/cgi-bin/keystone]/ensure: created[0m
2018-12-04 06:10:04,686 INFO: [mNotice: /Stage[main]/Keystone::Wsgi::Apache/File[keystone_wsgi_admin]/ensure: defined content as '{md5}d6dda52b0e14d80a652ecf42686d3962'[0m
2018-12-04 06:10:04,693 INFO: [mNotice: /Stage[main]/Keystone::Wsgi::Apache/File[keystone_wsgi_main]/ensure: defined content as '{md5}072422f0d75777ed1783e6910b3ddc58'[0m
2018-12-04 06:10:04,698 INFO: [mNotice: /Stage[main]/Keystone::Cron::Token_flush/Cron[keystone-manage token_flush]/ensure: created[0m
2018-12-04 06:10:04,748 INFO: [mNotice: /Stage[main]/Cinder::Deps/Anchor[cinder::service::end]: Triggered 'refresh' from 36 events[0m
2018-12-04 06:10:04,758 INFO: [mNotice: /Stage[main]/Apache::Mod::Proxy/File[proxy.conf]/ensure: defined content as '{md5}9eab682d8c4c89abd0ff20c1a60b908d'[0m
2018-12-04 06:10:05,804 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::dbsync::begin]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:10:07,622 INFO: [mNotice: /Stage[main]/Glance::Db::Sync/Exec[glance-manage db_sync]/returns: executed successfully[0m
2018-12-04 06:10:09,434 INFO: [mNotice: /Stage[main]/Glance::Db::Sync/Exec[glance-manage db_sync]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:10:09,437 INFO: [mNotice: /Stage[main]/Glance::Deps/Anchor[glance::dbsync::end]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:10:11,400 INFO: [mNotice: /Stage[main]/Glance::Db::Metadefs/Exec[glance-manage db_load_metadefs]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:10:17,305 INFO: [mNotice: /Stage[main]/Nova::Db::Sync/Exec[nova-db-sync]/returns: executed successfully[0m
2018-12-04 06:10:23,063 INFO: [mNotice: /Stage[main]/Nova::Db::Sync/Exec[nova-db-sync]: Triggered 'refresh' from 3 events[0m
2018-12-04 06:10:23,065 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::dbsync::end]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:10:23,072 INFO: [mNotice: /Stage[main]/Nova::Cron::Archive_deleted_rows/Cron[nova-manage db archive_deleted_rows]/ensure: created[0m
2018-12-04 06:10:25,410 INFO: [mNotice: /Stage[main]/Neutron::Db::Sync/Exec[neutron-db-sync]/returns: executed successfully[0m
2018-12-04 06:10:27,887 INFO: [mNotice: /Stage[main]/Neutron::Db::Sync/Exec[neutron-db-sync]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:10:27,887 INFO: [mNotice: /Stage[main]/Neutron::Deps/Anchor[neutron::dbsync::end]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:10:27,888 INFO: [mNotice: /Stage[main]/Neutron::Deps/Anchor[neutron::service::begin]: Triggered 'refresh' from 3 events[0m
2018-12-04 06:10:28,528 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Service[neutron-dhcp-service]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:10:29,164 INFO: [mNotice: /Stage[main]/Neutron::Agents::L3/Service[neutron-l3]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:10:30,732 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Service[neutron-ovs-agent-service]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:10:34,314 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Service[neutron-destroy-patch-ports-service]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:10:36,142 INFO: [mNotice: /Stage[main]/Heat::Db::Sync/Exec[heat-dbsync]/returns: executed successfully[0m
2018-12-04 06:10:37,878 INFO: [mNotice: /Stage[main]/Heat::Db::Sync/Exec[heat-dbsync]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:10:37,883 INFO: [mNotice: /Stage[main]/Heat::Deps/Anchor[heat::dbsync::end]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:10:39,749 INFO: [mNotice: /Stage[main]/Ironic::Db::Sync/Exec[ironic-dbsync]/returns: executed successfully[0m
2018-12-04 06:10:41,572 INFO: [mNotice: /Stage[main]/Ironic::Db::Sync/Exec[ironic-dbsync]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:10:41,576 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::dbsync::end]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:10:41,577 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::db_online_data_migrations::begin]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:10:45,939 INFO: [mNotice: /Stage[main]/Ironic::Db::Online_data_migrations/Exec[ironic-db-online-data-migrations]/returns: executed successfully[0m
2018-12-04 06:10:50,261 INFO: [mNotice: /Stage[main]/Ironic::Db::Online_data_migrations/Exec[ironic-db-online-data-migrations]: Triggered 'refresh' from 4 events[0m
2018-12-04 06:10:50,266 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::db_online_data_migrations::end]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:10:50,267 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::service::begin]: Triggered 'refresh' from 3 events[0m
2018-12-04 06:10:50,412 INFO: [mNotice: /Stage[main]/Ironic::Api/Service[ironic-api]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:10:51,162 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Service[ironic-conductor]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:10:51,163 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::service::end]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:10:54,021 INFO: [mNotice: /Stage[main]/Mistral::Db::Sync/Exec[mistral-db-sync]/returns: executed successfully[0m
2018-12-04 06:10:57,467 INFO: [mNotice: /Stage[main]/Mistral::Db::Sync/Exec[mistral-db-sync]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:10:58,703 INFO: [mNotice: /Stage[main]/Zaqar::Db::Sync/Exec[zaqar-db-sync]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:10:58,707 INFO: [mNotice: /Stage[main]/Zaqar::Deps/Anchor[zaqar::dbsync::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:10:58,708 INFO: [mNotice: /Stage[main]/Zaqar::Deps/Anchor[zaqar::service::begin]: Triggered 'refresh' from 3 events[0m
2018-12-04 06:10:58,824 INFO: [mNotice: /Stage[main]/Zaqar::Server/Service[openstack-zaqar]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:10:58,988 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::service::begin]: Triggered 'refresh' from 4 events[0m
2018-12-04 06:10:58,990 INFO: [mNotice: /Stage[main]/Heat::Deps/Anchor[heat::service::begin]: Triggered 'refresh' from 3 events[0m
2018-12-04 06:10:59,125 INFO: [mNotice: /Stage[main]/Heat::Api/Service[heat-api]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:10:59,253 INFO: [mNotice: /Stage[main]/Heat::Api_cfn/Service[heat-api-cfn]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:11:00,009 INFO: [mNotice: /Stage[main]/Heat::Engine/Service[heat-engine]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:11:01,018 INFO: [mNotice: /Stage[main]/Apache/Concat[/etc/httpd/conf/ports.conf]/File[/etc/httpd/conf/ports.conf]/ensure: defined content as '{md5}a7c131f3c51a0718ba2b06eca8a7eae6'[0m
2018-12-04 06:11:01,030 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf/httpd.conf]/content: content changed '{md5}f5e7449c0f17bc856e86011cb5d152ba' to '{md5}6f0fe8cf897c889c32ac3051ee26961b'[0m
2018-12-04 06:11:01,036 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[log_config]/File[log_config.load]/ensure: defined content as '{md5}785d35cb285e190d589163b45263ca89'[0m
2018-12-04 06:11:01,044 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[systemd]/File[systemd.load]/ensure: defined content as '{md5}26e5d44aae258b3e9d821cbbbd3e2826'[0m
2018-12-04 06:11:01,053 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[unixd]/File[unixd.load]/ensure: defined content as '{md5}0e8468ecc1265f8947b8725f4d1be9c0'[0m
2018-12-04 06:11:01,060 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[authz_host]/File[authz_host.load]/ensure: defined content as '{md5}d1045f54d2798499ca0f030ca0eef920'[0m
2018-12-04 06:11:01,067 INFO: [mNotice: /Stage[main]/Apache::Mod::Actions/Apache::Mod[actions]/File[actions.load]/ensure: defined content as '{md5}599866dfaf734f60f7e2d41ee8235515'[0m
2018-12-04 06:11:01,080 INFO: [mNotice: /Stage[main]/Apache::Mod::Authn_core/Apache::Mod[authn_core]/File[authn_core.load]/ensure: defined content as '{md5}704d6e8b02b0eca0eba4083960d16c52'[0m
2018-12-04 06:11:01,087 INFO: [mNotice: /Stage[main]/Apache::Mod::Cache/Apache::Mod[cache]/File[cache.load]/ensure: defined content as '{md5}01e4d392225b518a65b0f7d6c4e21d29'[0m
2018-12-04 06:11:01,095 INFO: [mNotice: /Stage[main]/Apache::Mod::Ext_filter/Apache::Mod[ext_filter]/File[ext_filter.load]/ensure: defined content as '{md5}76d5e0ac3411a4be57ac33ebe2e52ac8'[0m
2018-12-04 06:11:01,104 INFO: [mNotice: /Stage[main]/Apache::Mod::Mime/Apache::Mod[mime]/File[mime.load]/ensure: defined content as '{md5}e36257b9efab01459141d423cae57c7c'[0m
2018-12-04 06:11:01,647 INFO: [mNotice: /Stage[main]/Apache::Mod::Mime_magic/Apache::Mod[mime_magic]/File[mime_magic.load]/ensure: defined content as '{md5}cb8670bb2fb352aac7ebf3a85d52094c'[0m
2018-12-04 06:11:01,656 INFO: [mNotice: /Stage[main]/Apache::Mod::Rewrite/Apache::Mod[rewrite]/File[rewrite.load]/ensure: defined content as '{md5}26e2683352fc1599f29573ff0d934e79'[0m
2018-12-04 06:11:01,663 INFO: [mNotice: /Stage[main]/Apache::Mod::Speling/Apache::Mod[speling]/File[speling.load]/ensure: defined content as '{md5}f82e9e6b871a276c324c9eeffcec8a61'[0m
2018-12-04 06:11:01,670 INFO: [mNotice: /Stage[main]/Apache::Mod::Suexec/Apache::Mod[suexec]/File[suexec.load]/ensure: defined content as '{md5}c7d5c61c534ba423a79b0ae78ff9be35'[0m
2018-12-04 06:11:01,678 INFO: [mNotice: /Stage[main]/Apache::Mod::Version/Apache::Mod[version]/File[version.load]/ensure: defined content as '{md5}1c9243de22ace4dc8266442c48ae0c92'[0m
2018-12-04 06:11:01,684 INFO: [mNotice: /Stage[main]/Apache::Mod::Vhost_alias/Apache::Mod[vhost_alias]/File[vhost_alias.load]/ensure: defined content as '{md5}eca907865997d50d5130497665c3f82e'[0m
2018-12-04 06:11:01,693 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[auth_digest]/File[auth_digest.load]/ensure: defined content as '{md5}df9e85f8da0b239fe8e698ae7ead4f60'[0m
2018-12-04 06:11:01,699 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[authn_anon]/File[authn_anon.load]/ensure: defined content as '{md5}bf57b94b5aec35476fc2a2dc3861f132'[0m
2018-12-04 06:11:01,705 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[authn_dbm]/File[authn_dbm.load]/ensure: defined content as '{md5}90ee8f8ef1a017cacadfda4225e10651'[0m
2018-12-04 06:11:01,710 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[authz_dbm]/File[authz_dbm.load]/ensure: defined content as '{md5}c1363277984d22f99b70f7dce8753b60'[0m
2018-12-04 06:11:01,717 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[authz_owner]/File[authz_owner.load]/ensure: defined content as '{md5}f30a9be1016df87f195449d9e02d1857'[0m
2018-12-04 06:11:01,726 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[expires]/File[expires.load]/ensure: defined content as '{md5}f0825bad1e470de86ffabeb86dcc5d95'[0m
2018-12-04 06:11:01,733 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[include]/File[include.load]/ensure: defined content as '{md5}88095a914eedc3c2c184dd5d74c3954c'[0m
2018-12-04 06:11:01,740 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[logio]/File[logio.load]/ensure: defined content as '{md5}084533c7a44e9129d0e6df952e2472b6'[0m
2018-12-04 06:11:01,747 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[substitute]/File[substitute.load]/ensure: defined content as '{md5}8077c34a71afcf41c8fc644830935915'[0m
2018-12-04 06:11:01,756 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[usertrack]/File[usertrack.load]/ensure: defined content as '{md5}e95fbbf030fabec98b948f8dc217775c'[0m
2018-12-04 06:11:01,767 INFO: [mNotice: /Stage[main]/Apache::Mod::Alias/Apache::Mod[alias]/File[alias.load]/ensure: defined content as '{md5}3cf2fa309ccae4c29a4b875d0894cd79'[0m
2018-12-04 06:11:01,778 INFO: [mNotice: /Stage[main]/Apache::Mod::Authn_file/Apache::Mod[authn_file]/File[authn_file.load]/ensure: defined content as '{md5}d41656680003d7b890267bb73621c60b'[0m
2018-12-04 06:11:01,794 INFO: [mNotice: /Stage[main]/Apache::Mod::Autoindex/Apache::Mod[autoindex]/File[autoindex.load]/ensure: defined content as '{md5}515cdf5b573e961a60d2931d39248648'[0m
2018-12-04 06:11:01,806 INFO: [mNotice: /Stage[main]/Apache::Mod::Dav/Apache::Mod[dav]/File[dav.load]/ensure: defined content as '{md5}588e496251838c4840c14b28b5aa7881'[0m
2018-12-04 06:11:01,819 INFO: [mNotice: /Stage[main]/Apache::Mod::Dav_fs/File[dav_fs.conf]/ensure: defined content as '{md5}899a57534f3d84efa81887ec93c90c9b'[0m
2018-12-04 06:11:01,828 INFO: [mNotice: /Stage[main]/Apache::Mod::Dav_fs/Apache::Mod[dav_fs]/File[dav_fs.load]/ensure: defined content as '{md5}2996277c73b1cd684a9a3111c355e0d3'[0m
2018-12-04 06:11:01,840 INFO: [mNotice: /Stage[main]/Apache::Mod::Deflate/Apache::Mod[deflate]/File[deflate.load]/ensure: defined content as '{md5}2d1a1afcae0c70557251829a8586eeaf'[0m
2018-12-04 06:11:01,852 INFO: [mNotice: /Stage[main]/Apache::Mod::Dir/Apache::Mod[dir]/File[dir.load]/ensure: defined content as '{md5}1bfb1c2a46d7351fc9eb47c659dee068'[0m
2018-12-04 06:11:01,859 INFO: [mNotice: /Stage[main]/Apache::Mod::Negotiation/Apache::Mod[negotiation]/File[negotiation.load]/ensure: defined content as '{md5}d262ee6a5f20d9dd7f87770638dc2ccd'[0m
2018-12-04 06:11:01,872 INFO: [mNotice: /Stage[main]/Apache::Mod::Setenvif/Apache::Mod[setenvif]/File[setenvif.load]/ensure: defined content as '{md5}ec6c99f7cc8e35bdbcf8028f652c9f6d'[0m
2018-12-04 06:11:01,880 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[auth_basic]/File[auth_basic.load]/ensure: defined content as '{md5}494bcf4b843f7908675d663d8dc1bdc8'[0m
2018-12-04 06:11:01,886 INFO: [mNotice: /Stage[main]/Apache::Mod::Filter/Apache::Mod[filter]/File[filter.load]/ensure: defined content as '{md5}66a1e2064a140c3e7dca7ac33877700e'[0m
2018-12-04 06:11:01,897 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[authz_core]/File[authz_core.load]/ensure: defined content as '{md5}39942569bff2abdb259f9a347c7246bc'[0m
2018-12-04 06:11:01,905 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[access_compat]/File[access_compat.load]/ensure: defined content as '{md5}d5feb88bec4570e2dbc41cce7e0de003'[0m
2018-12-04 06:11:01,912 INFO: [mNotice: /Stage[main]/Apache::Mod::Authz_user/Apache::Mod[authz_user]/File[authz_user.load]/ensure: defined content as '{md5}63594303ee808423679b1ea13dd5a784'[0m
2018-12-04 06:11:01,921 INFO: [mNotice: /Stage[main]/Apache::Default_mods/Apache::Mod[authz_groupfile]/File[authz_groupfile.load]/ensure: defined content as '{md5}ae005a36b3ac8c20af36c434561c8a75'[0m
2018-12-04 06:11:01,931 INFO: [mNotice: /Stage[main]/Apache::Mod::Env/Apache::Mod[env]/File[env.load]/ensure: defined content as '{md5}d74184d40d0ee24ba02626a188ee7e1a'[0m
2018-12-04 06:11:01,938 INFO: [mNotice: /Stage[main]/Apache::Mod::Prefork/Apache::Mpm[prefork]/File[/etc/httpd/conf.modules.d/prefork.load]/ensure: defined content as '{md5}157529aafcf03fa491bc924103e4608e'[0m
2018-12-04 06:11:01,948 INFO: [mNotice: /Stage[main]/Apache::Mod::Cgi/Apache::Mod[cgi]/File[cgi.load]/ensure: defined content as '{md5}ac20c5c5779b37ab06b480d6485a0881'[0m
2018-12-04 06:11:02,711 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.d/README]/ensure: removed[0m
2018-12-04 06:11:02,723 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.d/autoindex.conf]/ensure: removed[0m
2018-12-04 06:11:02,723 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.d/userdir.conf]/ensure: removed[0m
2018-12-04 06:11:02,724 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.d/welcome.conf]/ensure: removed[0m
2018-12-04 06:11:02,785 INFO: [mNotice: /Stage[main]/Apache::Mod::Wsgi/File[wsgi.conf]/ensure: defined content as '{md5}8b3feb3fc2563de439920bb2c52cbd11'[0m
2018-12-04 06:11:02,804 INFO: [mNotice: /Stage[main]/Nova::Wsgi::Apache_placement/File[/etc/httpd/conf.d/00-nova-placement-api.conf]/content: content changed '{md5}611e31d39e1635bfabc0aafc51b43d0b' to '{md5}612d455490cfecc4b51db6656ea39240'[0m
2018-12-04 06:11:02,857 INFO: [mNotice: /Stage[main]/Tripleo::Ui/File[/etc/httpd/conf.d/openstack-tripleo-ui.conf]/content: content changed '{md5}0bb5ccf9a90544699ec07adf8028d99a' to '{md5}ec9dfa67b5507ef6f7a8bba6345bc07d'[0m
2018-12-04 06:11:02,890 INFO: [mNotice: /Stage[main]/Apache::Mod::Wsgi/Apache::Mod[wsgi]/File[wsgi.load]/ensure: defined content as '{md5}e1795e051e7aae1f865fde0d3b86a507'[0m
2018-12-04 06:11:03,051 INFO: [mNotice: /Stage[main]/Nova::Wsgi::Apache_api/Openstacklib::Wsgi::Apache[nova_api_wsgi]/File[/var/www/cgi-bin/nova]/ensure: created[0m
2018-12-04 06:11:03,062 INFO: [mNotice: /Stage[main]/Nova::Wsgi::Apache_api/Openstacklib::Wsgi::Apache[nova_api_wsgi]/File[nova_api_wsgi]/ensure: defined content as '{md5}8bcfb466d72544dd31a4f339243ed669'[0m
2018-12-04 06:11:03,088 INFO: [mNotice: /Stage[main]/Nova::Wsgi::Apache_placement/Openstacklib::Wsgi::Apache[placement_wsgi]/File[placement_wsgi]/ensure: defined content as '{md5}2c992c50344eb1765282cb9fb70126db'[0m
2018-12-04 06:11:09,252 INFO: [mNotice: /Stage[main]/Nova::Conductor/Nova::Generic_service[conductor]/Service[nova-conductor]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:11:15,495 INFO: [mNotice: /Stage[main]/Nova::Scheduler/Nova::Generic_service[scheduler]/Service[nova-scheduler]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:11:15,509 INFO: [mNotice: /Stage[main]/Apache::Mod::Headers/Apache::Mod[headers]/File[headers.load]/ensure: defined content as '{md5}96094c96352002c43ada5bdf8650ff38'[0m
2018-12-04 06:11:15,516 INFO: [mNotice: /Stage[main]/Ironic::Wsgi::Apache/Openstacklib::Wsgi::Apache[ironic_wsgi]/File[/var/www/cgi-bin/ironic]/ensure: created[0m
2018-12-04 06:11:15,525 INFO: [mNotice: /Stage[main]/Ironic::Wsgi::Apache/Openstacklib::Wsgi::Apache[ironic_wsgi]/File[ironic_wsgi]/ensure: defined content as '{md5}1d56c8d9da9a51b60ed54ef55cb43c99'[0m
2018-12-04 06:11:15,536 INFO: [mNotice: /Stage[main]/Apache::Mod::Proxy/Apache::Mod[proxy]/File[proxy.load]/ensure: defined content as '{md5}fe26a0a70f572eb256a3c6c183a62223'[0m
2018-12-04 06:11:15,545 INFO: [mNotice: /Stage[main]/Apache::Mod::Proxy_http/Apache::Mod[proxy_http]/File[proxy_http.load]/ensure: defined content as '{md5}0329b852b123a914fca8b072de61f913'[0m
2018-12-04 06:11:15,552 INFO: [mNotice: /Stage[main]/Apache::Mod::Proxy_wstunnel/Apache::Mod[proxy_wstunnel]/File[proxy_wstunnel.load]/ensure: defined content as '{md5}8036815f495618f4dde9d68796622e1c'[0m
2018-12-04 06:11:16,832 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/00-base.conf]/ensure: removed[0m
2018-12-04 06:11:16,835 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/00-dav.conf]/ensure: removed[0m
2018-12-04 06:11:16,839 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/00-lua.conf]/ensure: removed[0m
2018-12-04 06:11:16,840 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/00-mpm.conf]/ensure: removed[0m
2018-12-04 06:11:16,843 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/00-proxy.conf]/ensure: removed[0m
2018-12-04 06:11:16,845 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/00-systemd.conf]/ensure: removed[0m
2018-12-04 06:11:16,848 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/01-cgi.conf]/ensure: removed[0m
2018-12-04 06:11:16,851 INFO: [mNotice: /Stage[main]/Apache/File[/etc/httpd/conf.modules.d/10-wsgi.conf]/ensure: removed[0m
2018-12-04 06:11:16,863 INFO: [mNotice: /Stage[main]/Tripleo::Ui/File[/var/www/openstack-tripleo-ui/dist/tripleo_ui_config.js]/ensure: defined content as '{md5}417a6a8b2b286a7388e1c2d13eadd38d'[0m
2018-12-04 06:11:16,871 INFO: [mNotice: /Stage[main]/Zaqar::Wsgi::Apache/Openstacklib::Wsgi::Apache[zaqar_wsgi]/File[/var/www/cgi-bin/zaqar]/ensure: created[0m
2018-12-04 06:11:16,881 INFO: [mNotice: /Stage[main]/Zaqar::Wsgi::Apache/Openstacklib::Wsgi::Apache[zaqar_wsgi]/File[zaqar_wsgi]/ensure: defined content as '{md5}3a0f81ec944ad0c68f7db4b58b1f72d6'[0m
2018-12-04 06:11:17,598 INFO: [mNotice: /Stage[main]/Main/Zaqar::Server_instance[1]/Service[openstack-zaqar@1]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:11:17,605 INFO: [mNotice: /Stage[main]/Zaqar::Deps/Anchor[zaqar::service::end]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:11:17,631 INFO: [mNotice: /Stage[main]/Apache/Apache::Vhost[default]/Concat[15-default.conf]/File[/etc/httpd/conf.d/15-default.conf]/ensure: defined content as '{md5}a430bf4e003be964b419e7aea251c6c4'[0m
2018-12-04 06:11:17,700 INFO: [mNotice: /Stage[main]/Keystone::Wsgi::Apache/Apache::Vhost[keystone_wsgi_main]/Concat[10-keystone_wsgi_main.conf]/File[/etc/httpd/conf.d/10-keystone_wsgi_main.conf]/ensure: defined content as '{md5}4bc613c0fef7f70479d049e15d10b578'[0m
2018-12-04 06:11:17,735 INFO: [mNotice: /Stage[main]/Keystone::Wsgi::Apache/Apache::Vhost[keystone_wsgi_admin]/Concat[10-keystone_wsgi_admin.conf]/File[/etc/httpd/conf.d/10-keystone_wsgi_admin.conf]/ensure: defined content as '{md5}18b4101885aef38162db6e5ecc3e5d53'[0m
2018-12-04 06:11:17,773 INFO: [mNotice: /Stage[main]/Heat::Wsgi::Apache_api/Heat::Wsgi::Apache[api]/Openstacklib::Wsgi::Apache[heat_api_wsgi]/File[/var/www/cgi-bin/heat]/ensure: created[0m
2018-12-04 06:11:17,784 INFO: [mNotice: /Stage[main]/Heat::Wsgi::Apache_api/Heat::Wsgi::Apache[api]/Openstacklib::Wsgi::Apache[heat_api_wsgi]/File[heat_api_wsgi]/ensure: defined content as '{md5}640891728ce5d46ae40234228561597c'[0m
2018-12-04 06:11:17,798 INFO: [mNotice: /Stage[main]/Heat::Wsgi::Apache_api_cfn/Heat::Wsgi::Apache[api_cfn]/Openstacklib::Wsgi::Apache[heat_api_cfn_wsgi]/File[heat_api_cfn_wsgi]/ensure: defined content as '{md5}c3ae61ab87649c8cdfab8977da2b194b'[0m
2018-12-04 06:11:17,850 INFO: [mNotice: /Stage[main]/Ironic::Pxe/Apache::Vhost[ipxe_vhost]/Concat[10-ipxe_vhost.conf]/File[/etc/httpd/conf.d/10-ipxe_vhost.conf]/ensure: defined content as '{md5}0ffa81700d1dc962149c4ec89737928f'[0m
2018-12-04 06:11:17,907 INFO: [mNotice: /Stage[main]/Tripleo::Ui/Apache::Vhost[tripleo-ui]/Concat[25-tripleo-ui.conf]/File[/etc/httpd/conf.d/25-tripleo-ui.conf]/ensure: defined content as '{md5}6f445a10a54ce247c7a7ef08ec54a696'[0m
2018-12-04 06:11:17,987 INFO: [mNotice: /Stage[main]/Nova::Wsgi::Apache_api/Openstacklib::Wsgi::Apache[nova_api_wsgi]/Apache::Vhost[nova_api_wsgi]/Concat[10-nova_api_wsgi.conf]/File[/etc/httpd/conf.d/10-nova_api_wsgi.conf]/ensure: defined content as '{md5}33285bf3f08f2f3b021a2807652cc40d'[0m
2018-12-04 06:11:18,040 INFO: [mNotice: /Stage[main]/Nova::Wsgi::Apache_placement/Openstacklib::Wsgi::Apache[placement_wsgi]/Apache::Vhost[placement_wsgi]/Concat[10-placement_wsgi.conf]/File[/etc/httpd/conf.d/10-placement_wsgi.conf]/ensure: defined content as '{md5}d38465beadc83f41a57a959fa61c6a09'[0m
2018-12-04 06:11:18,719 INFO: [mNotice: /Stage[main]/Ironic::Wsgi::Apache/Openstacklib::Wsgi::Apache[ironic_wsgi]/Apache::Vhost[ironic_wsgi]/Concat[10-ironic_wsgi.conf]/File[/etc/httpd/conf.d/10-ironic_wsgi.conf]/ensure: defined content as '{md5}9e014c53a7f4cd62876abc6ccb52e004'[0m
2018-12-04 06:11:18,758 INFO: [mNotice: /Stage[main]/Zaqar::Wsgi::Apache/Openstacklib::Wsgi::Apache[zaqar_wsgi]/Apache::Vhost[zaqar_wsgi]/Concat[10-zaqar_wsgi.conf]/File[/etc/httpd/conf.d/10-zaqar_wsgi.conf]/ensure: defined content as '{md5}9071c8409a1ab79cac66ac4554844a86'[0m
2018-12-04 06:11:18,800 INFO: [mNotice: /Stage[main]/Heat::Wsgi::Apache_api/Heat::Wsgi::Apache[api]/Openstacklib::Wsgi::Apache[heat_api_wsgi]/Apache::Vhost[heat_api_wsgi]/Concat[10-heat_api_wsgi.conf]/File[/etc/httpd/conf.d/10-heat_api_wsgi.conf]/ensure: defined content as '{md5}0d440c48ef3f4939ca5d1bc8ac556afc'[0m
2018-12-04 06:11:18,838 INFO: [mNotice: /Stage[main]/Heat::Wsgi::Apache_api_cfn/Heat::Wsgi::Apache[api_cfn]/Openstacklib::Wsgi::Apache[heat_api_cfn_wsgi]/Apache::Vhost[heat_api_cfn_wsgi]/Concat[10-heat_api_cfn_wsgi.conf]/File[/etc/httpd/conf.d/10-heat_api_cfn_wsgi.conf]/ensure: defined content as '{md5}7dc9db1a66197cdd93db868ed0fe3204'[0m
2018-12-04 06:11:18,865 INFO: [mNotice: /Stage[main]/Keystone::Deps/Anchor[keystone::config::end]: Triggered 'refresh' from 11 events[0m
2018-12-04 06:11:18,875 INFO: [mNotice: /Stage[main]/Keystone/Exec[keystone-manage fernet_setup]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:11:18,875 INFO: [mNotice: /Stage[main]/Keystone/Exec[keystone-manage credential_setup]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:11:21,498 INFO: [mNotice: /Stage[main]/Keystone::Db::Sync/Exec[keystone-manage db_sync]/returns: executed successfully[0m
2018-12-04 06:11:24,092 INFO: [mNotice: /Stage[main]/Keystone::Db::Sync/Exec[keystone-manage db_sync]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:11:24,095 INFO: [mNotice: /Stage[main]/Keystone::Deps/Anchor[keystone::dbsync::end]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:11:26,904 INFO: [mNotice: /Stage[main]/Keystone/Exec[keystone-manage bootstrap]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:11:26,907 INFO: [mNotice: /Stage[main]/Keystone::Deps/Anchor[keystone::service::begin]: Triggered 'refresh' from 6 events[0m
2018-12-04 06:11:27,695 INFO: [mNotice: /Stage[main]/Apache::Service/Service[httpd]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:11:27,697 INFO: [mNotice: /Stage[main]/Keystone::Deps/Anchor[keystone::service::end]: Triggered 'refresh' from 36 events[0m
2018-12-04 06:14:21,677 INFO: [1;31mError: Failed to apply catalog: Execution of '/bin/openstack role list --quiet --format csv' returned 1: An unexpected error prevented the server from fulfilling your request. (HTTP 500) (Request-ID: req-b4f76bbd-e454-40d3-b6aa-754e7449468f) (tried 38, for a total of 170 seconds)[0m
2018-12-04 06:14:21,681 INFO: Changes:
2018-12-04 06:14:21,681 INFO:             Total: 978
2018-12-04 06:14:21,681 INFO: Events:
2018-12-04 06:14:21,681 INFO:           Success: 978
2018-12-04 06:14:21,681 INFO:             Total: 978
2018-12-04 06:14:21,682 INFO: Resources:
2018-12-04 06:14:21,682 INFO:             Total: 2817
2018-12-04 06:14:21,682 INFO:         Restarted: 82
2018-12-04 06:14:21,682 INFO:       Out of sync: 965
2018-12-04 06:14:21,682 INFO:           Changed: 965
2018-12-04 06:14:21,683 INFO: Time:
2018-12-04 06:14:21,683 INFO:        Policy rcd: 0.00
2018-12-04 06:14:21,683 INFO:     Mysql datadir: 0.00
2018-12-04 06:14:21,683 INFO:      Nova cell v2: 0.00
2018-12-04 06:14:21,683 INFO:    Neutron api config: 0.00
2018-12-04 06:14:21,684 INFO:         Resources: 0.00
2018-12-04 06:14:21,684 INFO:      Swift config: 0.00
2018-12-04 06:14:21,684 INFO:       Concat file: 0.01
2018-12-04 06:14:21,684 INFO:    Nova paste api ini: 0.01
2018-12-04 06:14:21,684 INFO:    Glance swift config: 0.01
2018-12-04 06:14:21,685 INFO:            Sysctl: 0.01
2018-12-04 06:14:21,685 INFO:    Ironic neutron agent config: 0.02
2018-12-04 06:14:21,685 INFO:    Swift object expirer config: 0.02
2018-12-04 06:14:21,685 INFO:            Anchor: 0.02
2018-12-04 06:14:21,685 INFO:    Mysql database: 0.02
2018-12-04 06:14:21,686 INFO:    Concat fragment: 0.03
2018-12-04 06:14:21,686 INFO:    Neutron l3 agent config: 0.03
2018-12-04 06:14:21,686 INFO:             Group: 0.03
2018-12-04 06:14:21,686 INFO:           Archive: 0.04
2018-12-04 06:14:21,686 INFO:    Neutron agent ovs: 0.04
2018-12-04 06:14:21,687 INFO:    Sysctl runtime: 0.05
2018-12-04 06:14:21,687 INFO:    Neutron dhcp agent config: 0.05
2018-12-04 06:14:21,687 INFO:         Vs bridge: 0.06
2018-12-04 06:14:21,687 INFO:              User: 0.07
2018-12-04 06:14:21,687 INFO:        Mysql user: 0.08
2018-12-04 06:14:21,688 INFO:              Cron: 0.09
2018-12-04 06:14:21,688 INFO:    Glance cache config: 0.25
2018-12-04 06:14:21,688 INFO:    Neutron plugin ml2: 0.27
2018-12-04 06:14:21,688 INFO:    Certmonger certificate: 0.32
2018-12-04 06:14:21,688 INFO:    Mistral config: 0.43
2018-12-04 06:14:21,689 INFO:       Mysql grant: 0.54
2018-12-04 06:14:21,689 INFO:    Glance registry config: 0.57
2018-12-04 06:14:21,689 INFO:    Swift proxy config: 0.71
2018-12-04 06:14:21,689 INFO:    Ring container device: 0.94
2018-12-04 06:14:21,690 INFO:    Ring object device: 0.96
2018-12-04 06:14:21,690 INFO:    Ring account device: 1.17
2018-12-04 06:14:21,690 INFO:    Ironic inspector config: 1.24
2018-12-04 06:14:21,690 INFO:      Zaqar config: 1.33
2018-12-04 06:14:21,690 INFO:            Augeas: 1.53
2018-12-04 06:14:21,691 INFO:     Ironic config: 10.17
2018-12-04 06:14:21,691 INFO:          Last run: 1543922061
2018-12-04 06:14:21,691 INFO:          Firewall: 170.17
2018-12-04 06:14:21,691 INFO:           Package: 208.61
2018-12-04 06:14:21,691 INFO:       Nova config: 22.05
2018-12-04 06:14:21,692 INFO:       Heat config: 3.03
2018-12-04 06:14:21,692 INFO:    Keystone config: 3.24
2018-12-04 06:14:21,692 INFO:    Neutron config: 3.40
2018-12-04 06:14:21,692 INFO:    Rabbitmq plugin: 3.50
2018-12-04 06:14:21,692 INFO:              File: 3.63
2018-12-04 06:14:21,693 INFO:           Service: 34.89
2018-12-04 06:14:21,693 INFO:    Config retrieval: 36.35
2018-12-04 06:14:21,693 INFO:              Exec: 36.96
2018-12-04 06:14:21,693 INFO:    Glance api config: 4.34
2018-12-04 06:14:21,693 INFO:             Total: 551.28
2018-12-04 06:14:21,694 INFO: Version:
2018-12-04 06:14:21,694 INFO:            Config: 1543921266
2018-12-04 06:14:21,694 INFO:            Puppet: 4.8.2
2018-12-04 06:14:40,556 INFO: + rc=1
2018-12-04 06:14:40,556 INFO: + set -e
2018-12-04 06:14:40,556 INFO: + echo 'puppet apply exited with exit code 1'
2018-12-04 06:14:40,557 INFO: puppet apply exited with exit code 1
2018-12-04 06:14:40,557 INFO: + '[' 1 '!=' 2 -a 1 '!=' 0 ']'
2018-12-04 06:14:40,557 INFO: + exit 1
2018-12-04 06:14:40,557 INFO: [2018-12-04 06:14:40,556] (os-refresh-config) [ERROR] during configure phase. [Command '['dib-run-parts', '/usr/libexec/os-refresh-config/configure.d']' returned non-zero exit status 1]
2018-12-04 06:14:40,558 INFO: 
2018-12-04 06:14:40,558 INFO: [2018-12-04 06:14:40,557] (os-refresh-config) [ERROR] Aborting...
2018-12-04 06:14:40,568 DEBUG: An exception occurred
Traceback (most recent call last):
  File "/usr/lib/python2.7/site-packages/instack_undercloud/undercloud.py", line 2334, in install
    _run_orc(instack_env)
  File "/usr/lib/python2.7/site-packages/instack_undercloud/undercloud.py", line 1601, in _run_orc
    _run_live_command(args, instack_env, 'os-refresh-config')
  File "/usr/lib/python2.7/site-packages/instack_undercloud/undercloud.py", line 668, in _run_live_command
    raise RuntimeError('%s failed. See log for details.' % name)
RuntimeError: os-refresh-config failed. See log for details.
2018-12-04 06:14:40,570 ERROR: 
#############################################################################
Undercloud install failed.

Reason: os-refresh-config failed. See log for details.

See the previous output for details about what went wrong.  The full install
log can be found at /home/stack/.instack/install-undercloud.log.

#############################################################################

2018-12-04 06:17:45,532 INFO: Logging to /home/stack/.instack/install-undercloud.log
2018-12-04 06:17:45,605 INFO: Checking for a FQDN hostname...
2018-12-04 06:17:45,679 INFO: Static hostname detected as undercloud.example.com
2018-12-04 06:17:45,715 INFO: Transient hostname detected as undercloud.example.com
2018-12-04 06:17:45,717 WARNING: Option "undercloud_public_vip" from group "DEFAULT" is deprecated. Use option "undercloud_public_host" from group "DEFAULT".
2018-12-04 06:17:45,717 WARNING: Option "undercloud_admin_vip" from group "DEFAULT" is deprecated. Use option "undercloud_admin_host" from group "DEFAULT".
2018-12-04 06:17:45,791 INFO: Running yum clean all
2018-12-04 06:17:46,079 INFO: Loaded plugins: search-disabled-repos
2018-12-04 06:17:46,105 INFO: Cleaning repos: rhel-7-server-extras-rpms
2018-12-04 06:17:46,105 INFO:               : rhel-7-server-openstack-13-devtools-rpms
2018-12-04 06:17:46,105 INFO:               : rhel-7-server-openstack-13-optools-rpms
2018-12-04 06:17:46,106 INFO:               : rhel-7-server-openstack-13-rpms rhel-7-server-rh-common-rpms
2018-12-04 06:17:46,106 INFO:               : rhel-7-server-rpms rhel-ha-for-rhel-7-server-rpms
2018-12-04 06:17:46,151 INFO: yum-clean-all completed successfully
2018-12-04 06:17:46,152 INFO: Running yum update
2018-12-04 06:17:46,442 INFO: Loaded plugins: search-disabled-repos
2018-12-04 06:17:48,609 INFO: No packages marked for update
2018-12-04 06:17:48,637 INFO: yum-update completed successfully
2018-12-04 06:17:48,699 INFO: Running instack
2018-12-04 06:17:48,988 INFO: INFO: 2018-12-04 06:17:48,987 -- Starting run of instack
2018-12-04 06:17:49,000 INFO: INFO: 2018-12-04 06:17:48,999 -- Using json file: /usr/share/instack-undercloud/json-files/rhel-7-undercloud-packages.json
2018-12-04 06:17:49,000 INFO: INFO: 2018-12-04 06:17:49,000 -- Running Installation
2018-12-04 06:17:49,009 INFO: INFO: 2018-12-04 06:17:49,001 -- Initialized with elements path: /usr/share/tripleo-puppet-elements /usr/share/instack-undercloud /usr/share/tripleo-image-elements /usr/share/diskimage-builder/elements
2018-12-04 06:17:49,044 INFO: WARNING: 2018-12-04 06:17:49,043 -- expand_dependencies() deprecated, use get_elements
2018-12-04 06:17:49,090 INFO: INFO: 2018-12-04 06:17:49,089 -- List of all elements and dependencies: epel undercloud-install dib-python source-repositories install-types puppet-modules install-bin pip-manifest puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url pkg-map enable-packages-install puppet os-apply-config hiera package-installs
2018-12-04 06:17:49,091 INFO: INFO: 2018-12-04 06:17:49,089 -- Excluding element pip-and-virtualenv
2018-12-04 06:17:49,091 INFO: INFO: 2018-12-04 06:17:49,089 -- Excluding element epel
2018-12-04 06:17:49,092 INFO: INFO: 2018-12-04 06:17:49,089 -- Excluding element pip-manifest
2018-12-04 06:17:49,092 INFO: INFO: 2018-12-04 06:17:49,089 -- Excluding element package-installs
2018-12-04 06:17:49,093 INFO: INFO: 2018-12-04 06:17:49,090 -- Excluding element pkg-map
2018-12-04 06:17:49,093 INFO: INFO: 2018-12-04 06:17:49,090 -- Excluding element puppet
2018-12-04 06:17:49,094 INFO: INFO: 2018-12-04 06:17:49,090 -- Excluding element cache-url
2018-12-04 06:17:49,094 INFO: INFO: 2018-12-04 06:17:49,090 -- Excluding element dib-python
2018-12-04 06:17:49,095 INFO: INFO: 2018-12-04 06:17:49,091 -- Excluding element install-bin
2018-12-04 06:17:49,096 INFO: INFO: 2018-12-04 06:17:49,091 -- List of all elements and dependencies after excludes: undercloud-install source-repositories install-types puppet-modules puppet-stack-config os-refresh-config element-manifest manifests enable-packages-install os-apply-config hiera
2018-12-04 06:17:49,335 INFO: INFO: 2018-12-04 06:17:49,335 --   Running hook extra-data
2018-12-04 06:17:49,335 INFO: INFO: 2018-12-04 06:17:49,335 -- ############### Begin stdout/stderr logging ###############
2018-12-04 06:17:49,354 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/../environment.d/00-dib-v2-env
2018-12-04 06:17:49,358 INFO: + source /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/../environment.d/00-dib-v2-env
2018-12-04 06:17:49,359 INFO: ++ export 'IMAGE_ELEMENT=epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs'
2018-12-04 06:17:49,359 INFO: ++ IMAGE_ELEMENT='epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs'
2018-12-04 06:17:49,360 INFO: ++ export 'IMAGE_ELEMENT_YAML={cache-url: /usr/share/diskimage-builder/elements/cache-url, dib-python: /usr/share/diskimage-builder/elements/dib-python,
2018-12-04 06:17:49,360 INFO:   element-manifest: /usr/share/diskimage-builder/elements/element-manifest, enable-packages-install: /usr/share/tripleo-image-elements/enable-packages-install,
2018-12-04 06:17:49,360 INFO:   epel: /usr/share/diskimage-builder/elements/epel, hiera: /usr/share/tripleo-puppet-elements/hiera,
2018-12-04 06:17:49,361 INFO:   install-bin: /usr/share/diskimage-builder/elements/install-bin, install-types: /usr/share/diskimage-builder/elements/install-types,
2018-12-04 06:17:49,361 INFO:   manifests: /usr/share/diskimage-builder/elements/manifests, os-apply-config: /usr/share/tripleo-image-elements/os-apply-config,
2018-12-04 06:17:49,362 INFO:   os-refresh-config: /usr/share/tripleo-image-elements/os-refresh-config, package-installs: /usr/share/diskimage-builder/elements/package-installs,
2018-12-04 06:17:49,362 INFO:   pip-and-virtualenv: /usr/share/diskimage-builder/elements/pip-and-virtualenv, pip-manifest: /usr/share/tripleo-image-elements/pip-manifest,
2018-12-04 06:17:49,362 INFO:   pkg-map: /usr/share/diskimage-builder/elements/pkg-map, puppet: /usr/share/tripleo-puppet-elements/puppet,
2018-12-04 06:17:49,363 INFO:   puppet-modules: /usr/share/tripleo-puppet-elements/puppet-modules, puppet-stack-config: /usr/share/instack-undercloud/puppet-stack-config,
2018-12-04 06:17:49,363 INFO:   source-repositories: /usr/share/diskimage-builder/elements/source-repositories,
2018-12-04 06:17:49,363 INFO:   undercloud-install: /usr/share/instack-undercloud/undercloud-install}
2018-12-04 06:17:49,363 INFO: '
2018-12-04 06:17:49,364 INFO: ++ IMAGE_ELEMENT_YAML='{cache-url: /usr/share/diskimage-builder/elements/cache-url, dib-python: /usr/share/diskimage-builder/elements/dib-python,
2018-12-04 06:17:49,364 INFO:   element-manifest: /usr/share/diskimage-builder/elements/element-manifest, enable-packages-install: /usr/share/tripleo-image-elements/enable-packages-install,
2018-12-04 06:17:49,364 INFO:   epel: /usr/share/diskimage-builder/elements/epel, hiera: /usr/share/tripleo-puppet-elements/hiera,
2018-12-04 06:17:49,365 INFO:   install-bin: /usr/share/diskimage-builder/elements/install-bin, install-types: /usr/share/diskimage-builder/elements/install-types,
2018-12-04 06:17:49,365 INFO:   manifests: /usr/share/diskimage-builder/elements/manifests, os-apply-config: /usr/share/tripleo-image-elements/os-apply-config,
2018-12-04 06:17:49,365 INFO:   os-refresh-config: /usr/share/tripleo-image-elements/os-refresh-config, package-installs: /usr/share/diskimage-builder/elements/package-installs,
2018-12-04 06:17:49,366 INFO:   pip-and-virtualenv: /usr/share/diskimage-builder/elements/pip-and-virtualenv, pip-manifest: /usr/share/tripleo-image-elements/pip-manifest,
2018-12-04 06:17:49,366 INFO:   pkg-map: /usr/share/diskimage-builder/elements/pkg-map, puppet: /usr/share/tripleo-puppet-elements/puppet,
2018-12-04 06:17:49,367 INFO:   puppet-modules: /usr/share/tripleo-puppet-elements/puppet-modules, puppet-stack-config: /usr/share/instack-undercloud/puppet-stack-config,
2018-12-04 06:17:49,367 INFO:   source-repositories: /usr/share/diskimage-builder/elements/source-repositories,
2018-12-04 06:17:49,367 INFO:   undercloud-install: /usr/share/instack-undercloud/undercloud-install}
2018-12-04 06:17:49,367 INFO: '
2018-12-04 06:17:49,367 INFO: ++ export -f get_image_element_array
2018-12-04 06:17:49,368 INFO: + set +o xtrace
2018-12-04 06:17:49,368 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/../environment.d/01-export-install-types.bash
2018-12-04 06:17:49,369 INFO: + source /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/../environment.d/01-export-install-types.bash
2018-12-04 06:17:49,369 INFO: ++ export DIB_DEFAULT_INSTALLTYPE=package
2018-12-04 06:17:49,369 INFO: ++ DIB_DEFAULT_INSTALLTYPE=package
2018-12-04 06:17:49,369 INFO: + set +o xtrace
2018-12-04 06:17:49,370 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/../environment.d/01-puppet-module-pins.sh
2018-12-04 06:17:49,370 INFO: + source /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/../environment.d/01-puppet-module-pins.sh
2018-12-04 06:17:49,370 INFO: ++ export DIB_REPOREF_puppetlabs_ntp=4.2.x
2018-12-04 06:17:49,370 INFO: ++ DIB_REPOREF_puppetlabs_ntp=4.2.x
2018-12-04 06:17:49,371 INFO: + set +o xtrace
2018-12-04 06:17:49,371 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/../environment.d/02-puppet-modules-install-types.sh
2018-12-04 06:17:49,371 INFO: + source /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/../environment.d/02-puppet-modules-install-types.sh
2018-12-04 06:17:49,372 INFO: ++ DIB_DEFAULT_INSTALLTYPE=package
2018-12-04 06:17:49,372 INFO: ++ DIB_INSTALLTYPE_puppet_modules=package
2018-12-04 06:17:49,372 INFO: ++ '[' package = source ']'
2018-12-04 06:17:49,372 INFO: + set +o xtrace
2018-12-04 06:17:49,373 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/../environment.d/10-os-apply-config-venv-dir.bash
2018-12-04 06:17:49,373 INFO: + source /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/../environment.d/10-os-apply-config-venv-dir.bash
2018-12-04 06:17:49,373 INFO: ++ '[' -z '' ']'
2018-12-04 06:17:49,373 INFO: ++ export OS_APPLY_CONFIG_VENV_DIR=/opt/stack/venvs/os-apply-config
2018-12-04 06:17:49,374 INFO: ++ OS_APPLY_CONFIG_VENV_DIR=/opt/stack/venvs/os-apply-config
2018-12-04 06:17:49,374 INFO: + set +o xtrace
2018-12-04 06:17:49,374 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/../environment.d/14-manifests
2018-12-04 06:17:49,375 INFO: + source /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/../environment.d/14-manifests
2018-12-04 06:17:49,376 INFO: ++ export DIB_MANIFEST_IMAGE_DIR=/etc/dib-manifests
2018-12-04 06:17:49,376 INFO: ++ DIB_MANIFEST_IMAGE_DIR=/etc/dib-manifests
2018-12-04 06:17:49,376 INFO: ++ export DIB_MANIFEST_SAVE_DIR=instack.d/
2018-12-04 06:17:49,376 INFO: ++ DIB_MANIFEST_SAVE_DIR=instack.d/
2018-12-04 06:17:49,376 INFO: + set +o xtrace
2018-12-04 06:17:49,377 INFO: dib-run-parts Running /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/10-install-git
2018-12-04 06:17:49,381 INFO: + yum -y install git
2018-12-04 06:17:49,649 INFO: Loaded plugins: search-disabled-repos
2018-12-04 06:17:49,821 INFO: Package git-1.8.3.1-20.el7.x86_64 already installed and latest version
2018-12-04 06:17:49,822 INFO: Nothing to do
2018-12-04 06:17:49,846 INFO: dib-run-parts 10-install-git completed
2018-12-04 06:17:49,847 INFO: dib-run-parts Running /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/20-manifest-dir
2018-12-04 06:17:49,851 INFO: + set -eu
2018-12-04 06:17:49,852 INFO: + set -o pipefail
2018-12-04 06:17:49,852 INFO: + sudo mkdir -p /root/.instack/tmp/instack.fdwJF6/mnt//etc/dib-manifests
2018-12-04 06:17:49,880 INFO: dib-run-parts 20-manifest-dir completed
2018-12-04 06:17:49,880 INFO: dib-run-parts Running /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/75-inject-element-manifest
2018-12-04 06:17:49,886 INFO: + set -eu
2018-12-04 06:17:49,886 INFO: + set -o pipefail
2018-12-04 06:17:49,887 INFO: + DIB_ELEMENT_MANIFEST_PATH=/etc/dib-manifests/dib-element-manifest
2018-12-04 06:17:49,887 INFO: ++ dirname /etc/dib-manifests/dib-element-manifest
2018-12-04 06:17:49,889 INFO: + sudo mkdir -p /root/.instack/tmp/instack.fdwJF6/mnt//etc/dib-manifests
2018-12-04 06:17:49,916 INFO: + sudo /bin/bash -c 'echo epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs | tr '\'' '\'' '\''\n'\'' > /root/.instack/tmp/instack.fdwJF6/mnt//etc/dib-manifests/dib-element-manifest'
2018-12-04 06:17:49,948 INFO: dib-run-parts 75-inject-element-manifest completed
2018-12-04 06:17:49,949 INFO: dib-run-parts Running /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/98-source-repositories
2018-12-04 06:17:49,973 INFO: Getting /root/.cache/image-create/source-repositories/repositories_flock: Tue Dec  4 06:17:49 EST 2018 for /root/.instack/tmp/instack.fdwJF6/hook38_zpY/source-repository-puppet-modules
2018-12-04 06:17:49,979 INFO: (0001 / 0081)
2018-12-04 06:17:49,986 INFO: puppetlabs-apache install type not set to source
2018-12-04 06:17:49,987 INFO: (0002 / 0081)
2018-12-04 06:17:49,994 INFO: puppet-aodh install type not set to source
2018-12-04 06:17:49,995 INFO: (0003 / 0081)
2018-12-04 06:17:50,002 INFO: puppet-auditd install type not set to source
2018-12-04 06:17:50,003 INFO: (0004 / 0081)
2018-12-04 06:17:50,009 INFO: puppet-barbican install type not set to source
2018-12-04 06:17:50,010 INFO: (0005 / 0081)
2018-12-04 06:17:50,017 INFO: puppet-cassandra install type not set to source
2018-12-04 06:17:50,018 INFO: (0006 / 0081)
2018-12-04 06:17:50,024 INFO: puppet-ceph install type not set to source
2018-12-04 06:17:50,025 INFO: (0007 / 0081)
2018-12-04 06:17:50,032 INFO: puppet-ceilometer install type not set to source
2018-12-04 06:17:50,033 INFO: (0008 / 0081)
2018-12-04 06:17:50,039 INFO: puppet-congress install type not set to source
2018-12-04 06:17:50,040 INFO: (0009 / 0081)
2018-12-04 06:17:50,047 INFO: puppet-gnocchi install type not set to source
2018-12-04 06:17:50,048 INFO: (0010 / 0081)
2018-12-04 06:17:50,054 INFO: puppet-certmonger install type not set to source
2018-12-04 06:17:50,055 INFO: (0011 / 0081)
2018-12-04 06:17:50,061 INFO: puppet-cinder install type not set to source
2018-12-04 06:17:50,062 INFO: (0012 / 0081)
2018-12-04 06:17:50,068 INFO: puppet-common install type not set to source
2018-12-04 06:17:50,069 INFO: (0013 / 0081)
2018-12-04 06:17:50,076 INFO: puppet-contrail install type not set to source
2018-12-04 06:17:50,077 INFO: (0014 / 0081)
2018-12-04 06:17:50,083 INFO: puppetlabs-concat install type not set to source
2018-12-04 06:17:50,084 INFO: (0015 / 0081)
2018-12-04 06:17:50,091 INFO: puppetlabs-firewall install type not set to source
2018-12-04 06:17:50,092 INFO: (0016 / 0081)
2018-12-04 06:17:50,098 INFO: puppet-glance install type not set to source
2018-12-04 06:17:50,099 INFO: (0017 / 0081)
2018-12-04 06:17:50,106 INFO: puppet-gluster install type not set to source
2018-12-04 06:17:50,107 INFO: (0018 / 0081)
2018-12-04 06:17:50,115 INFO: puppetlabs-haproxy install type not set to source
2018-12-04 06:17:50,116 INFO: (0019 / 0081)
2018-12-04 06:17:50,124 INFO: puppet-heat install type not set to source
2018-12-04 06:17:50,125 INFO: (0020 / 0081)
2018-12-04 06:17:50,132 INFO: puppet-healthcheck install type not set to source
2018-12-04 06:17:50,133 INFO: (0021 / 0081)
2018-12-04 06:17:50,139 INFO: puppet-horizon install type not set to source
2018-12-04 06:17:50,140 INFO: (0022 / 0081)
2018-12-04 06:17:50,147 INFO: puppetlabs-inifile install type not set to source
2018-12-04 06:17:50,148 INFO: (0023 / 0081)
2018-12-04 06:17:50,155 INFO: puppet-kafka install type not set to source
2018-12-04 06:17:50,156 INFO: (0024 / 0081)
2018-12-04 06:17:50,163 INFO: puppet-keystone install type not set to source
2018-12-04 06:17:50,164 INFO: (0025 / 0081)
2018-12-04 06:17:50,171 INFO: puppet-manila install type not set to source
2018-12-04 06:17:50,172 INFO: (0026 / 0081)
2018-12-04 06:17:50,178 INFO: puppet-memcached install type not set to source
2018-12-04 06:17:50,179 INFO: (0027 / 0081)
2018-12-04 06:17:50,186 INFO: puppet-mistral install type not set to source
2018-12-04 06:17:50,187 INFO: (0028 / 0081)
2018-12-04 06:17:50,193 INFO: puppetlabs-mongodb install type not set to source
2018-12-04 06:17:50,194 INFO: (0029 / 0081)
2018-12-04 06:17:50,201 INFO: puppetlabs-mysql install type not set to source
2018-12-04 06:17:50,202 INFO: (0030 / 0081)
2018-12-04 06:17:50,209 INFO: puppet-neutron install type not set to source
2018-12-04 06:17:50,210 INFO: (0031 / 0081)
2018-12-04 06:17:50,216 INFO: puppet-nova install type not set to source
2018-12-04 06:17:50,217 INFO: (0032 / 0081)
2018-12-04 06:17:50,224 INFO: puppet-octavia install type not set to source
2018-12-04 06:17:50,225 INFO: (0033 / 0081)
2018-12-04 06:17:50,231 INFO: puppet-oslo install type not set to source
2018-12-04 06:17:50,232 INFO: (0034 / 0081)
2018-12-04 06:17:50,239 INFO: puppet-nssdb install type not set to source
2018-12-04 06:17:50,240 INFO: (0035 / 0081)
2018-12-04 06:17:50,246 INFO: puppet-opendaylight install type not set to source
2018-12-04 06:17:50,247 INFO: (0036 / 0081)
2018-12-04 06:17:50,254 INFO: puppet-ovn install type not set to source
2018-12-04 06:17:50,255 INFO: (0037 / 0081)
2018-12-04 06:17:50,261 INFO: puppet-panko install type not set to source
2018-12-04 06:17:50,262 INFO: (0038 / 0081)
2018-12-04 06:17:50,268 INFO: puppet-puppet install type not set to source
2018-12-04 06:17:50,269 INFO: (0039 / 0081)
2018-12-04 06:17:50,276 INFO: puppetlabs-rabbitmq install type not set to source
2018-12-04 06:17:50,277 INFO: (0040 / 0081)
2018-12-04 06:17:50,283 INFO: puppet-redis install type not set to source
2018-12-04 06:17:50,284 INFO: (0041 / 0081)
2018-12-04 06:17:50,291 INFO: puppetlabs-rsync install type not set to source
2018-12-04 06:17:50,292 INFO: (0042 / 0081)
2018-12-04 06:17:50,298 INFO: puppet-sahara install type not set to source
2018-12-04 06:17:50,299 INFO: (0043 / 0081)
2018-12-04 06:17:50,306 INFO: sensu-puppet install type not set to source
2018-12-04 06:17:50,307 INFO: (0044 / 0081)
2018-12-04 06:17:50,313 INFO: puppet-tacker install type not set to source
2018-12-04 06:17:50,314 INFO: (0045 / 0081)
2018-12-04 06:17:50,321 INFO: puppet-trove install type not set to source
2018-12-04 06:17:50,322 INFO: (0046 / 0081)
2018-12-04 06:17:50,328 INFO: puppet-ssh install type not set to source
2018-12-04 06:17:50,329 INFO: (0047 / 0081)
2018-12-04 06:17:50,336 INFO: puppet-staging install type not set to source
2018-12-04 06:17:50,337 INFO: (0048 / 0081)
2018-12-04 06:17:50,344 INFO: puppetlabs-stdlib install type not set to source
2018-12-04 06:17:50,345 INFO: (0049 / 0081)
2018-12-04 06:17:50,351 INFO: puppet-swift install type not set to source
2018-12-04 06:17:50,352 INFO: (0050 / 0081)
2018-12-04 06:17:50,359 INFO: puppetlabs-sysctl install type not set to source
2018-12-04 06:17:50,360 INFO: (0051 / 0081)
2018-12-04 06:17:50,367 INFO: puppet-timezone install type not set to source
2018-12-04 06:17:50,368 INFO: (0052 / 0081)
2018-12-04 06:17:50,375 INFO: puppet-uchiwa install type not set to source
2018-12-04 06:17:50,376 INFO: (0053 / 0081)
2018-12-04 06:17:50,382 INFO: puppetlabs-vcsrepo install type not set to source
2018-12-04 06:17:50,384 INFO: (0054 / 0081)
2018-12-04 06:17:50,390 INFO: puppet-vlan install type not set to source
2018-12-04 06:17:50,391 INFO: (0055 / 0081)
2018-12-04 06:17:50,397 INFO: puppet-vswitch install type not set to source
2018-12-04 06:17:50,398 INFO: (0056 / 0081)
2018-12-04 06:17:50,405 INFO: puppetlabs-xinetd install type not set to source
2018-12-04 06:17:50,406 INFO: (0057 / 0081)
2018-12-04 06:17:50,412 INFO: puppet-zookeeper install type not set to source
2018-12-04 06:17:50,413 INFO: (0058 / 0081)
2018-12-04 06:17:50,419 INFO: puppet-openstacklib install type not set to source
2018-12-04 06:17:50,420 INFO: (0059 / 0081)
2018-12-04 06:17:50,427 INFO: puppet-module-keepalived install type not set to source
2018-12-04 06:17:50,428 INFO: (0060 / 0081)
2018-12-04 06:17:50,435 INFO: puppetlabs-ntp install type not set to source
2018-12-04 06:17:50,436 INFO: (0061 / 0081)
2018-12-04 06:17:50,442 INFO: puppet-snmp install type not set to source
2018-12-04 06:17:50,443 INFO: (0062 / 0081)
2018-12-04 06:17:50,449 INFO: puppet-tripleo install type not set to source
2018-12-04 06:17:50,450 INFO: (0063 / 0081)
2018-12-04 06:17:50,457 INFO: puppet-ironic install type not set to source
2018-12-04 06:17:50,458 INFO: (0064 / 0081)
2018-12-04 06:17:50,464 INFO: puppet-ipaclient install type not set to source
2018-12-04 06:17:50,465 INFO: (0065 / 0081)
2018-12-04 06:17:50,473 INFO: puppetlabs-corosync install type not set to source
2018-12-04 06:17:50,473 INFO: (0066 / 0081)
2018-12-04 06:17:50,480 INFO: puppet-pacemaker install type not set to source
2018-12-04 06:17:50,481 INFO: (0067 / 0081)
2018-12-04 06:17:50,488 INFO: puppet_aviator install type not set to source
2018-12-04 06:17:50,489 INFO: (0068 / 0081)
2018-12-04 06:17:50,496 INFO: puppet-openstack_extras install type not set to source
2018-12-04 06:17:50,497 INFO: (0069 / 0081)
2018-12-04 06:17:50,504 INFO: konstantin-fluentd install type not set to source
2018-12-04 06:17:50,505 INFO: (0070 / 0081)
2018-12-04 06:17:50,511 INFO: puppet-elasticsearch install type not set to source
2018-12-04 06:17:50,512 INFO: (0071 / 0081)
2018-12-04 06:17:50,519 INFO: puppet-kibana3 install type not set to source
2018-12-04 06:17:50,520 INFO: (0072 / 0081)
2018-12-04 06:17:50,527 INFO: puppetlabs-git install type not set to source
2018-12-04 06:17:50,528 INFO: (0073 / 0081)
2018-12-04 06:17:50,535 INFO: puppet-datacat install type not set to source
2018-12-04 06:17:50,536 INFO: (0074 / 0081)
2018-12-04 06:17:50,543 INFO: puppet-kmod install type not set to source
2018-12-04 06:17:50,544 INFO: (0075 / 0081)
2018-12-04 06:17:50,551 INFO: puppet-zaqar install type not set to source
2018-12-04 06:17:50,552 INFO: (0076 / 0081)
2018-12-04 06:17:50,558 INFO: puppet-ec2api install type not set to source
2018-12-04 06:17:50,559 INFO: (0077 / 0081)
2018-12-04 06:17:50,565 INFO: puppet-qdr install type not set to source
2018-12-04 06:17:50,566 INFO: (0078 / 0081)
2018-12-04 06:17:50,573 INFO: puppet-systemd install type not set to source
2018-12-04 06:17:50,574 INFO: (0079 / 0081)
2018-12-04 06:17:50,580 INFO: puppet-etcd install type not set to source
2018-12-04 06:17:50,581 INFO: (0080 / 0081)
2018-12-04 06:17:50,588 INFO: puppet-veritas_hyperscale install type not set to source
2018-12-04 06:17:50,589 INFO: (0081 / 0081)
2018-12-04 06:17:50,596 INFO: puppet-ptp install type not set to source
2018-12-04 06:17:50,598 INFO: dib-run-parts 98-source-repositories completed
2018-12-04 06:17:50,599 INFO: dib-run-parts Running /root/.instack/tmp/instack.fdwJF6/hook38_zpY/extra-data.d/99-enable-install-types
2018-12-04 06:17:50,604 INFO: + set -eu
2018-12-04 06:17:50,604 INFO: + set -o pipefail
2018-12-04 06:17:50,605 INFO: + declare -a SPECIFIED_ELEMS
2018-12-04 06:17:50,605 INFO: + SPECIFIED_ELEMS[0]=
2018-12-04 06:17:50,606 INFO: + PREFIX=DIB_INSTALLTYPE_
2018-12-04 06:17:50,606 INFO: ++ env
2018-12-04 06:17:50,606 INFO: ++ grep '^DIB_INSTALLTYPE_'
2018-12-04 06:17:50,607 INFO: ++ cut -d= -f1
2018-12-04 06:17:50,607 INFO: ++ echo ''
2018-12-04 06:17:50,608 INFO: + INSTALL_TYPE_VARS=
2018-12-04 06:17:50,609 INFO: ++ find /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d -maxdepth 1 -name '*-package-install' -type d
2018-12-04 06:17:50,611 INFO: + default_install_type_dirs=/root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/puppet-modules-package-install
2018-12-04 06:17:50,611 INFO: + for _install_dir in '$default_install_type_dirs'
2018-12-04 06:17:50,612 INFO: + SUFFIX=-package-install
2018-12-04 06:17:50,612 INFO: ++ basename /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/puppet-modules-package-install
2018-12-04 06:17:50,613 INFO: + _install_dir=puppet-modules-package-install
2018-12-04 06:17:50,613 INFO: + INSTALLDIRPREFIX=puppet-modules
2018-12-04 06:17:50,613 INFO: + found=0
2018-12-04 06:17:50,614 INFO: + '[' 0 = 0 ']'
2018-12-04 06:17:50,614 INFO: + pushd /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d
2018-12-04 06:17:50,614 INFO: ~/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d /home/stack
2018-12-04 06:17:50,615 INFO: + ln -sf puppet-modules-package-install/75-puppet-modules-package .
2018-12-04 06:17:50,615 INFO: + popd
2018-12-04 06:17:50,615 INFO: /home/stack
2018-12-04 06:17:50,617 INFO: dib-run-parts 99-enable-install-types completed
2018-12-04 06:17:50,618 INFO: dib-run-parts ----------------------- PROFILING -----------------------
2018-12-04 06:17:50,618 INFO: dib-run-parts
2018-12-04 06:17:50,620 INFO: dib-run-parts Target: extra-data.d
2018-12-04 06:17:50,620 INFO: dib-run-parts
2018-12-04 06:17:50,620 INFO: dib-run-parts Script                                     Seconds
2018-12-04 06:17:50,620 INFO: dib-run-parts ---------------------------------------  ----------
2018-12-04 06:17:50,621 INFO: dib-run-parts
2018-12-04 06:17:50,632 INFO: dib-run-parts 10-install-git                                0.468
2018-12-04 06:17:50,640 INFO: dib-run-parts 20-manifest-dir                               0.031
2018-12-04 06:17:50,648 INFO: dib-run-parts 75-inject-element-manifest                    0.066
2018-12-04 06:17:50,655 INFO: dib-run-parts 98-source-repositories                        0.648
2018-12-04 06:17:50,662 INFO: dib-run-parts 99-enable-install-types                       0.017
2018-12-04 06:17:50,665 INFO: dib-run-parts
2018-12-04 06:17:50,665 INFO: dib-run-parts --------------------- END PROFILING ---------------------
2018-12-04 06:17:50,666 INFO: INFO: 2018-12-04 06:17:50,666 -- ############### End stdout/stderr logging ###############
2018-12-04 06:17:50,666 INFO: INFO: 2018-12-04 06:17:50,666 --   Running hook pre-install
2018-12-04 06:17:50,667 INFO: INFO: 2018-12-04 06:17:50,667 --     Skipping hook pre-install, the hook directory doesn't exist at /root/.instack/tmp/instack.fdwJF6/hook38_zpY/pre-install.d
2018-12-04 06:17:50,667 INFO: INFO: 2018-12-04 06:17:50,667 --   Running hook install
2018-12-04 06:17:50,668 INFO: INFO: 2018-12-04 06:17:50,667 -- ############### Begin stdout/stderr logging ###############
2018-12-04 06:17:50,688 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../environment.d/00-dib-v2-env
2018-12-04 06:17:50,691 INFO: + source /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../environment.d/00-dib-v2-env
2018-12-04 06:17:50,692 INFO: ++ export 'IMAGE_ELEMENT=epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs'
2018-12-04 06:17:50,693 INFO: ++ IMAGE_ELEMENT='epel undercloud-install dib-python source-repositories install-types install-bin pip-manifest pkg-map puppet-stack-config os-refresh-config element-manifest manifests pip-and-virtualenv cache-url puppet enable-packages-install puppet-modules os-apply-config hiera package-installs'
2018-12-04 06:17:50,693 INFO: ++ export 'IMAGE_ELEMENT_YAML={cache-url: /usr/share/diskimage-builder/elements/cache-url, dib-python: /usr/share/diskimage-builder/elements/dib-python,
2018-12-04 06:17:50,694 INFO:   element-manifest: /usr/share/diskimage-builder/elements/element-manifest, enable-packages-install: /usr/share/tripleo-image-elements/enable-packages-install,
2018-12-04 06:17:50,695 INFO:   epel: /usr/share/diskimage-builder/elements/epel, hiera: /usr/share/tripleo-puppet-elements/hiera,
2018-12-04 06:17:50,695 INFO:   install-bin: /usr/share/diskimage-builder/elements/install-bin, install-types: /usr/share/diskimage-builder/elements/install-types,
2018-12-04 06:17:50,696 INFO:   manifests: /usr/share/diskimage-builder/elements/manifests, os-apply-config: /usr/share/tripleo-image-elements/os-apply-config,
2018-12-04 06:17:50,696 INFO:   os-refresh-config: /usr/share/tripleo-image-elements/os-refresh-config, package-installs: /usr/share/diskimage-builder/elements/package-installs,
2018-12-04 06:17:50,697 INFO:   pip-and-virtualenv: /usr/share/diskimage-builder/elements/pip-and-virtualenv, pip-manifest: /usr/share/tripleo-image-elements/pip-manifest,
2018-12-04 06:17:50,697 INFO:   pkg-map: /usr/share/diskimage-builder/elements/pkg-map, puppet: /usr/share/tripleo-puppet-elements/puppet,
2018-12-04 06:17:50,697 INFO:   puppet-modules: /usr/share/tripleo-puppet-elements/puppet-modules, puppet-stack-config: /usr/share/instack-undercloud/puppet-stack-config,
2018-12-04 06:17:50,698 INFO:   source-repositories: /usr/share/diskimage-builder/elements/source-repositories,
2018-12-04 06:17:50,698 INFO:   undercloud-install: /usr/share/instack-undercloud/undercloud-install}
2018-12-04 06:17:50,698 INFO: '
2018-12-04 06:17:50,699 INFO: ++ IMAGE_ELEMENT_YAML='{cache-url: /usr/share/diskimage-builder/elements/cache-url, dib-python: /usr/share/diskimage-builder/elements/dib-python,
2018-12-04 06:17:50,699 INFO:   element-manifest: /usr/share/diskimage-builder/elements/element-manifest, enable-packages-install: /usr/share/tripleo-image-elements/enable-packages-install,
2018-12-04 06:17:50,700 INFO:   epel: /usr/share/diskimage-builder/elements/epel, hiera: /usr/share/tripleo-puppet-elements/hiera,
2018-12-04 06:17:50,700 INFO:   install-bin: /usr/share/diskimage-builder/elements/install-bin, install-types: /usr/share/diskimage-builder/elements/install-types,
2018-12-04 06:17:50,700 INFO:   manifests: /usr/share/diskimage-builder/elements/manifests, os-apply-config: /usr/share/tripleo-image-elements/os-apply-config,
2018-12-04 06:17:50,701 INFO:   os-refresh-config: /usr/share/tripleo-image-elements/os-refresh-config, package-installs: /usr/share/diskimage-builder/elements/package-installs,
2018-12-04 06:17:50,701 INFO:   pip-and-virtualenv: /usr/share/diskimage-builder/elements/pip-and-virtualenv, pip-manifest: /usr/share/tripleo-image-elements/pip-manifest,
2018-12-04 06:17:50,702 INFO:   pkg-map: /usr/share/diskimage-builder/elements/pkg-map, puppet: /usr/share/tripleo-puppet-elements/puppet,
2018-12-04 06:17:50,702 INFO:   puppet-modules: /usr/share/tripleo-puppet-elements/puppet-modules, puppet-stack-config: /usr/share/instack-undercloud/puppet-stack-config,
2018-12-04 06:17:50,702 INFO:   source-repositories: /usr/share/diskimage-builder/elements/source-repositories,
2018-12-04 06:17:50,703 INFO:   undercloud-install: /usr/share/instack-undercloud/undercloud-install}
2018-12-04 06:17:50,703 INFO: '
2018-12-04 06:17:50,703 INFO: ++ export -f get_image_element_array
2018-12-04 06:17:50,703 INFO: + set +o xtrace
2018-12-04 06:17:50,704 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../environment.d/01-export-install-types.bash
2018-12-04 06:17:50,704 INFO: + source /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../environment.d/01-export-install-types.bash
2018-12-04 06:17:50,704 INFO: ++ export DIB_DEFAULT_INSTALLTYPE=package
2018-12-04 06:17:50,704 INFO: ++ DIB_DEFAULT_INSTALLTYPE=package
2018-12-04 06:17:50,705 INFO: + set +o xtrace
2018-12-04 06:17:50,705 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../environment.d/01-puppet-module-pins.sh
2018-12-04 06:17:50,706 INFO: + source /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../environment.d/01-puppet-module-pins.sh
2018-12-04 06:17:50,706 INFO: ++ export DIB_REPOREF_puppetlabs_ntp=4.2.x
2018-12-04 06:17:50,706 INFO: ++ DIB_REPOREF_puppetlabs_ntp=4.2.x
2018-12-04 06:17:50,707 INFO: + set +o xtrace
2018-12-04 06:17:50,707 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../environment.d/02-puppet-modules-install-types.sh
2018-12-04 06:17:50,707 INFO: + source /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../environment.d/02-puppet-modules-install-types.sh
2018-12-04 06:17:50,708 INFO: ++ DIB_DEFAULT_INSTALLTYPE=package
2018-12-04 06:17:50,708 INFO: ++ DIB_INSTALLTYPE_puppet_modules=package
2018-12-04 06:17:50,708 INFO: ++ '[' package = source ']'
2018-12-04 06:17:50,708 INFO: + set +o xtrace
2018-12-04 06:17:50,709 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../environment.d/10-os-apply-config-venv-dir.bash
2018-12-04 06:17:50,709 INFO: + source /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../environment.d/10-os-apply-config-venv-dir.bash
2018-12-04 06:17:50,709 INFO: ++ '[' -z '' ']'
2018-12-04 06:17:50,709 INFO: ++ export OS_APPLY_CONFIG_VENV_DIR=/opt/stack/venvs/os-apply-config
2018-12-04 06:17:50,710 INFO: ++ OS_APPLY_CONFIG_VENV_DIR=/opt/stack/venvs/os-apply-config
2018-12-04 06:17:50,710 INFO: + set +o xtrace
2018-12-04 06:17:50,710 INFO: dib-run-parts Sourcing environment file /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../environment.d/14-manifests
2018-12-04 06:17:50,711 INFO: + source /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../environment.d/14-manifests
2018-12-04 06:17:50,711 INFO: ++ export DIB_MANIFEST_IMAGE_DIR=/etc/dib-manifests
2018-12-04 06:17:50,711 INFO: ++ DIB_MANIFEST_IMAGE_DIR=/etc/dib-manifests
2018-12-04 06:17:50,711 INFO: ++ export DIB_MANIFEST_SAVE_DIR=instack.d/
2018-12-04 06:17:50,712 INFO: ++ DIB_MANIFEST_SAVE_DIR=instack.d/
2018-12-04 06:17:50,712 INFO: + set +o xtrace
2018-12-04 06:17:50,712 INFO: dib-run-parts Running /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/02-puppet-stack-config
2018-12-04 06:17:51,899 INFO: dib-run-parts 02-puppet-stack-config completed
2018-12-04 06:17:51,900 INFO: dib-run-parts Running /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/10-hiera-yaml-symlink
2018-12-04 06:17:51,904 INFO: + set -o pipefail
2018-12-04 06:17:51,904 INFO: + ln -f -s /etc/puppet/hiera.yaml /etc/hiera.yaml
2018-12-04 06:17:51,908 INFO: dib-run-parts 10-hiera-yaml-symlink completed
2018-12-04 06:17:51,909 INFO: dib-run-parts Running /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/10-puppet-stack-config-puppet-module
2018-12-04 06:17:51,913 INFO: + set -o pipefail
2018-12-04 06:17:51,913 INFO: + mkdir -p /etc/puppet/manifests
2018-12-04 06:17:51,916 INFO: ++ dirname /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/10-puppet-stack-config-puppet-module
2018-12-04 06:17:51,917 INFO: + cp /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../puppet-stack-config.pp /etc/puppet/manifests/puppet-stack-config.pp
2018-12-04 06:17:51,921 INFO: dib-run-parts 10-puppet-stack-config-puppet-module completed
2018-12-04 06:17:51,922 INFO: dib-run-parts Running /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/11-create-template-root
2018-12-04 06:17:51,927 INFO: ++ os-apply-config --print-templates
2018-12-04 06:17:52,219 INFO: + TEMPLATE_ROOT=/usr/libexec/os-apply-config/templates
2018-12-04 06:17:52,219 INFO: + mkdir -p /usr/libexec/os-apply-config/templates
2018-12-04 06:17:52,223 INFO: dib-run-parts 11-create-template-root completed
2018-12-04 06:17:52,223 INFO: dib-run-parts Running /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/11-hiera-orc-install
2018-12-04 06:17:52,228 INFO: + set -o pipefail
2018-12-04 06:17:52,228 INFO: + mkdir -p /usr/libexec/os-refresh-config/configure.d/
2018-12-04 06:17:52,231 INFO: ++ dirname /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/11-hiera-orc-install
2018-12-04 06:17:52,232 INFO: + install -m 0755 -o root -g root /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../10-hiera-disable /usr/libexec/os-refresh-config/configure.d/10-hiera-disable
2018-12-04 06:17:52,240 INFO: ++ dirname /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/11-hiera-orc-install
2018-12-04 06:17:52,242 INFO: + install -m 0755 -o root -g root /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../40-hiera-datafiles /usr/libexec/os-refresh-config/configure.d/40-hiera-datafiles
2018-12-04 06:17:52,251 INFO: dib-run-parts 11-hiera-orc-install completed
2018-12-04 06:17:52,252 INFO: dib-run-parts Running /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/75-puppet-modules-package
2018-12-04 06:17:52,257 INFO: + find /opt/stack/puppet-modules/ -mindepth 1
2018-12-04 06:17:52,257 INFO: + read
2018-12-04 06:17:52,265 INFO: + ln -f -s /usr/share/openstack-puppet/modules/aodh /usr/share/openstack-puppet/modules/apache /usr/share/openstack-puppet/modules/archive /usr/share/openstack-puppet/modules/auditd /usr/share/openstack-puppet/modules/barbican /usr/share/openstack-puppet/modules/cassandra /usr/share/openstack-puppet/modules/ceilometer /usr/share/openstack-puppet/modules/ceph /usr/share/openstack-puppet/modules/certmonger /usr/share/openstack-puppet/modules/cinder /usr/share/openstack-puppet/modules/collectd /usr/share/openstack-puppet/modules/concat /usr/share/openstack-puppet/modules/contrail /usr/share/openstack-puppet/modules/corosync /usr/share/openstack-puppet/modules/datacat /usr/share/openstack-puppet/modules/designate /usr/share/openstack-puppet/modules/dns /usr/share/openstack-puppet/modules/ec2api /usr/share/openstack-puppet/modules/elasticsearch /usr/share/openstack-puppet/modules/fdio /usr/share/openstack-puppet/modules/firewall /usr/share/openstack-puppet/modules/fluentd /usr/share/openstack-puppet/modules/git /usr/share/openstack-puppet/modules/glance /usr/share/openstack-puppet/modules/gnocchi /usr/share/openstack-puppet/modules/haproxy /usr/share/openstack-puppet/modules/heat /usr/share/openstack-puppet/modules/horizon /usr/share/openstack-puppet/modules/inifile /usr/share/openstack-puppet/modules/ipaclient /usr/share/openstack-puppet/modules/ironic /usr/share/openstack-puppet/modules/java /usr/share/openstack-puppet/modules/kafka /usr/share/openstack-puppet/modules/keepalived /usr/share/openstack-puppet/modules/keystone /usr/share/openstack-puppet/modules/kibana3 /usr/share/openstack-puppet/modules/kmod /usr/share/openstack-puppet/modules/manila /usr/share/openstack-puppet/modules/memcached /usr/share/openstack-puppet/modules/midonet /usr/share/openstack-puppet/modules/mistral /usr/share/openstack-puppet/modules/module-data /usr/share/openstack-puppet/modules/mysql /usr/share/openstack-puppet/modules/n1k_vsm /usr/share/openstack-puppet/modules/neutron /usr/share/openstack-puppet/modules/nova /usr/share/openstack-puppet/modules/nssdb /usr/share/openstack-puppet/modules/ntp /usr/share/openstack-puppet/modules/octavia /usr/share/openstack-puppet/modules/opendaylight /usr/share/openstack-puppet/modules/openstack_extras /usr/share/openstack-puppet/modules/openstacklib /usr/share/openstack-puppet/modules/oslo /usr/share/openstack-puppet/modules/ovn /usr/share/openstack-puppet/modules/pacemaker /usr/share/openstack-puppet/modules/panko /usr/share/openstack-puppet/modules/rabbitmq /usr/share/openstack-puppet/modules/redis /usr/share/openstack-puppet/modules/remote /usr/share/openstack-puppet/modules/rsync /usr/share/openstack-puppet/modules/sahara /usr/share/openstack-puppet/modules/sensu /usr/share/openstack-puppet/modules/snmp /usr/share/openstack-puppet/modules/ssh /usr/share/openstack-puppet/modules/staging /usr/share/openstack-puppet/modules/stdlib /usr/share/openstack-puppet/modules/swift /usr/share/openstack-puppet/modules/sysctl /usr/share/openstack-puppet/modules/systemd /usr/share/openstack-puppet/modules/timezone /usr/share/openstack-puppet/modules/tomcat /usr/share/openstack-puppet/modules/tripleo /usr/share/openstack-puppet/modules/trove /usr/share/openstack-puppet/modules/uchiwa /usr/share/openstack-puppet/modules/vcsrepo /usr/share/openstack-puppet/modules/veritas_hyperscale /usr/share/openstack-puppet/modules/vswitch /usr/share/openstack-puppet/modules/xinetd /usr/share/openstack-puppet/modules/zaqar /usr/share/openstack-puppet/modules/zookeeper /etc/puppet/modules/
2018-12-04 06:17:52,269 INFO: dib-run-parts 75-puppet-modules-package completed
2018-12-04 06:17:52,270 INFO: dib-run-parts Running /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/99-install-config-templates
2018-12-04 06:17:52,275 INFO: ++ os-apply-config --print-templates
2018-12-04 06:17:52,554 INFO: + TEMPLATE_ROOT=/usr/libexec/os-apply-config/templates
2018-12-04 06:17:52,554 INFO: ++ dirname /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/99-install-config-templates
2018-12-04 06:17:52,556 INFO: + TEMPLATE_SOURCE=/root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../os-apply-config
2018-12-04 06:17:52,557 INFO: + mkdir -p /usr/libexec/os-apply-config/templates
2018-12-04 06:17:52,559 INFO: + '[' -d /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../os-apply-config ']'
2018-12-04 06:17:52,559 INFO: + rsync '--exclude=.*.swp' -Cr /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../os-apply-config/ /usr/libexec/os-apply-config/templates/
2018-12-04 06:17:52,568 INFO: dib-run-parts 99-install-config-templates completed
2018-12-04 06:17:52,568 INFO: dib-run-parts Running /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/99-os-refresh-config-install-scripts
2018-12-04 06:17:52,573 INFO: ++ os-refresh-config --print-base
2018-12-04 06:17:52,653 INFO: + SCRIPT_BASE=/usr/libexec/os-refresh-config
2018-12-04 06:17:52,654 INFO: ++ dirname /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/99-os-refresh-config-install-scripts
2018-12-04 06:17:52,655 INFO: + SCRIPT_SOURCE=/root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../os-refresh-config
2018-12-04 06:17:52,656 INFO: + rsync -r /root/.instack/tmp/instack.fdwJF6/hook38_zpY/install.d/../os-refresh-config/ /usr/libexec/os-refresh-config/
2018-12-04 06:17:52,663 INFO: dib-run-parts 99-os-refresh-config-install-scripts completed
2018-12-04 06:17:52,663 INFO: dib-run-parts ----------------------- PROFILING -----------------------
2018-12-04 06:17:52,663 INFO: dib-run-parts
2018-12-04 06:17:52,665 INFO: dib-run-parts Target: install.d
2018-12-04 06:17:52,665 INFO: dib-run-parts
2018-12-04 06:17:52,666 INFO: dib-run-parts Script                                     Seconds
2018-12-04 06:17:52,666 INFO: dib-run-parts ---------------------------------------  ----------
2018-12-04 06:17:52,666 INFO: dib-run-parts
2018-12-04 06:17:52,677 INFO: dib-run-parts 02-puppet-stack-config                        1.187
2018-12-04 06:17:52,684 INFO: dib-run-parts 10-hiera-yaml-symlink                         0.007
2018-12-04 06:17:52,691 INFO: dib-run-parts 10-puppet-stack-config-puppet-module          0.011
2018-12-04 06:17:52,698 INFO: dib-run-parts 11-create-template-root                       0.300
2018-12-04 06:17:52,705 INFO: dib-run-parts 11-hiera-orc-install                          0.026
2018-12-04 06:17:52,712 INFO: dib-run-parts 75-puppet-modules-package                     0.016
2018-12-04 06:17:52,719 INFO: dib-run-parts 99-install-config-templates                   0.297
2018-12-04 06:17:52,726 INFO: dib-run-parts 99-os-refresh-config-install-scripts          0.093
2018-12-04 06:17:52,729 INFO: dib-run-parts
2018-12-04 06:17:52,729 INFO: dib-run-parts --------------------- END PROFILING ---------------------
2018-12-04 06:17:52,737 INFO: INFO: 2018-12-04 06:17:52,729 -- ############### End stdout/stderr logging ###############
2018-12-04 06:17:52,738 INFO: INFO: 2018-12-04 06:17:52,730 --   Running hook post-install
2018-12-04 06:17:52,738 INFO: INFO: 2018-12-04 06:17:52,730 --     Skipping hook post-install, the hook directory doesn't exist at /root/.instack/tmp/instack.fdwJF6/hook38_zpY/post-install.d
2018-12-04 06:17:52,739 INFO: INFO: 2018-12-04 06:17:52,737 -- Ending run of instack.
2018-12-04 06:17:52,753 INFO: Instack completed successfully
2018-12-04 06:17:52,753 INFO: Running os-refresh-config
2018-12-04 06:17:52,856 INFO: [2018-12-04 06:17:52,856] (os-refresh-config) [INFO] Starting phase configure
2018-12-04 06:17:52,871 INFO: dib-run-parts Tue Dec  4 06:17:52 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/10-hiera-disable
2018-12-04 06:17:52,874 INFO: + '[' -f /etc/puppet/hiera.yaml ']'
2018-12-04 06:17:52,875 INFO: + grep yaml /etc/puppet/hiera.yaml
2018-12-04 06:17:52,880 INFO: dib-run-parts Tue Dec  4 06:17:52 EST 2018 10-hiera-disable completed
2018-12-04 06:17:52,881 INFO: dib-run-parts Tue Dec  4 06:17:52 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/20-os-apply-config
2018-12-04 06:17:53,166 INFO: [2018/12/04 06:17:53 AM] [WARNING] DEPRECATED: falling back to /var/run/os-collect-config/os_config_files.json
2018-12-04 06:17:53,176 INFO: [2018/12/04 06:17:53 AM] [INFO] writing /etc/os-net-config/config.json
2018-12-04 06:17:53,177 INFO: [2018/12/04 06:17:53 AM] [INFO] writing /root/stackrc
2018-12-04 06:17:53,178 INFO: [2018/12/04 06:17:53 AM] [INFO] writing /root/tripleo-undercloud-passwords
2018-12-04 06:17:53,178 INFO: [2018/12/04 06:17:53 AM] [INFO] writing /etc/puppet/hiera.yaml
2018-12-04 06:17:53,179 INFO: [2018/12/04 06:17:53 AM] [INFO] writing /var/opt/undercloud-stack/masquerade
2018-12-04 06:17:53,180 INFO: [2018/12/04 06:17:53 AM] [INFO] writing /etc/puppet/hieradata/RedHat.yaml
2018-12-04 06:17:53,180 INFO: [2018/12/04 06:17:53 AM] [INFO] writing /etc/puppet/hieradata/CentOS.yaml
2018-12-04 06:17:53,181 INFO: [2018/12/04 06:17:53 AM] [INFO] writing /var/run/heat-config/heat-config
2018-12-04 06:17:53,181 INFO: [2018/12/04 06:17:53 AM] [INFO] writing /etc/os-collect-config.conf
2018-12-04 06:17:53,182 INFO: [2018/12/04 06:17:53 AM] [INFO] success
2018-12-04 06:17:53,194 INFO: dib-run-parts Tue Dec  4 06:17:53 EST 2018 20-os-apply-config completed
2018-12-04 06:17:53,196 INFO: dib-run-parts Tue Dec  4 06:17:53 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/30-reload-keepalived
2018-12-04 06:17:53,199 INFO: + systemctl is-enabled keepalived
2018-12-04 06:17:53,211 INFO: enabled
2018-12-04 06:17:53,211 INFO: + systemctl reload keepalived
2018-12-04 06:17:53,256 INFO: dib-run-parts Tue Dec  4 06:17:53 EST 2018 30-reload-keepalived completed
2018-12-04 06:17:53,258 INFO: dib-run-parts Tue Dec  4 06:17:53 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/40-hiera-datafiles
2018-12-04 06:17:53,537 INFO: [2018/12/04 06:17:53 AM] [WARNING] DEPRECATED: falling back to /var/run/os-collect-config/os_config_files.json
2018-12-04 06:17:53,558 INFO: dib-run-parts Tue Dec  4 06:17:53 EST 2018 40-hiera-datafiles completed
2018-12-04 06:17:53,560 INFO: dib-run-parts Tue Dec  4 06:17:53 EST 2018 Running /usr/libexec/os-refresh-config/configure.d/50-puppet-stack-config
2018-12-04 06:17:53,564 INFO: + set -o pipefail
2018-12-04 06:17:53,564 INFO: + puppet_apply puppet apply --summarize --detailed-exitcodes /etc/puppet/manifests/puppet-stack-config.pp
2018-12-04 06:17:53,564 INFO: + set +e
2018-12-04 06:17:53,565 INFO: + puppet apply --summarize --detailed-exitcodes /etc/puppet/manifests/puppet-stack-config.pp
2018-12-04 06:18:02,998 INFO: [mNotice: hiera(): Cannot load backend module_data: cannot load such file -- hiera/backend/module_data_backend[0m
2018-12-04 06:18:03,192 INFO: [1;33mWarning: ModuleLoader: module 'openstacklib' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:18:03,193 INFO:    (file & line not available)[0m
2018-12-04 06:18:03,710 INFO: [mNotice: hiera(): Cannot load backend module_data: cannot load such file -- hiera/backend/module_data_backend[0m
2018-12-04 06:18:03,834 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:18:03,834 INFO:                     with Stdlib::Compat::Bool. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 54]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-04 06:18:03,835 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:18:03,840 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:18:03,841 INFO:                     with Stdlib::Compat::Absolute_Path. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 55]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-04 06:18:03,841 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:18:03,957 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:18:03,958 INFO:                     with Stdlib::Compat::String. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 56]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-04 06:18:03,958 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:18:03,981 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:18:03,982 INFO:                     with Stdlib::Compat::Array. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 66]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-04 06:18:03,982 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:18:03,987 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:18:03,987 INFO:                     with Pattern[]. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 68]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-04 06:18:03,988 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:18:03,998 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:18:03,999 INFO:                     with Stdlib::Compat::Numeric. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/ntp/manifests/init.pp", 76]:["/etc/puppet/modules/tripleo/manifests/profile/base/time/ntp.pp", 34]
2018-12-04 06:18:03,999 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:18:04,562 INFO: [1;33mWarning: This method is deprecated, please use match expressions with Stdlib::Compat::Ipv6 instead. They are described at https://docs.puppet.com/puppet/latest/reference/lang_data_type.html#match-expressions. at ["/etc/puppet/modules/rabbitmq/manifests/install/rabbitmqadmin.pp", 37]:["/etc/puppet/modules/rabbitmq/manifests/init.pp", 316]
2018-12-04 06:18:04,562 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:18:04,719 INFO: [1;33mWarning: tag is a metaparam; this value will inherit to all contained resources in the tripleo::firewall::rule definition[0m
2018-12-04 06:18:04,844 INFO: [mNotice: Scope(Class[Tripleo::Firewall::Post]): At this stage, all network traffic is blocked.[0m
2018-12-04 06:18:06,278 INFO: [1;33mWarning: notify is a metaparam; this value will inherit to all contained resources in the keepalived::instance definition[0m
2018-12-04 06:18:06,377 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:18:06,378 INFO:                     with Stdlib::Compat::Hash. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/tripleo/manifests/profile/base/database/mysql.pp", 103]:["/etc/puppet/manifests/puppet-stack-config.pp", 97]
2018-12-04 06:18:06,378 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:18:06,468 INFO: [1;33mWarning: ModuleLoader: module 'mysql' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:18:06,469 INFO:    (file & line not available)[0m
2018-12-04 06:18:06,861 INFO: [1;33mWarning: ModuleLoader: module 'keystone' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:18:06,862 INFO:    (file & line not available)[0m
2018-12-04 06:18:07,845 INFO: [1;33mWarning: ModuleLoader: module 'glance' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:18:07,845 INFO:    (file & line not available)[0m
2018-12-04 06:18:08,104 INFO: [1;33mWarning: ModuleLoader: module 'nova' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:18:08,105 INFO:    (file & line not available)[0m
2018-12-04 06:18:08,309 INFO: [1;33mWarning: Unknown variable: '::nova::db::mysql_api::setup_cell0'. at /etc/puppet/modules/nova/manifests/db/mysql.pp:53:28[0m
2018-12-04 06:18:08,375 INFO: [1;33mWarning: ModuleLoader: module 'neutron' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:18:08,375 INFO:    (file & line not available)[0m
2018-12-04 06:18:09,336 INFO: [1;33mWarning: ModuleLoader: module 'heat' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:18:09,337 INFO:    (file & line not available)[0m
2018-12-04 06:18:09,501 INFO: [1;33mWarning: ModuleLoader: module 'ironic' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:18:09,502 INFO:    (file & line not available)[0m
2018-12-04 06:18:09,665 INFO: [1;33mWarning: ModuleLoader: module 'swift' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:18:09,665 INFO:    (file & line not available)[0m
2018-12-04 06:18:10,155 INFO: [1;33mWarning: Scope(Class[Keystone]): keystone::rabbit_host, keystone::rabbit_hosts, keystone::rabbit_password, keystone::rabbit_port, keystone::rabbit_userid and keystone::rabbit_virtual_host are deprecated. Please use keystone::default_transport_url instead.[0m
2018-12-04 06:18:12,329 INFO: [1;33mWarning: Scope(Class[Glance::Notify::Rabbitmq]): glance::notify::rabbitmq::rabbit_host, glance::notify::rabbitmq::rabbit_hosts, glance::notify::rabbitmq::rabbit_password, glance::notify::rabbitmq::rabbit_port, glance::notify::rabbitmq::rabbit_userid and glance::notify::rabbitmq::rabbit_virtual_host are deprecated. Please use glance::notify::rabbitmq::default_transport_url instead.[0m
2018-12-04 06:18:12,490 INFO: [1;33mWarning: Scope(Class[Nova::Db]): placement_database_connection has no effect as of pike, and may be removed in a future release[0m
2018-12-04 06:18:12,490 INFO: [1;33mWarning: Scope(Class[Nova::Db]): placement_slave_connection has no effect as of pike, and may be removed in a future release[0m
2018-12-04 06:18:12,885 INFO: [1;33mWarning: ModuleLoader: module 'cinder' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:18:12,885 INFO:    (file & line not available)[0m
2018-12-04 06:18:13,474 INFO: [1;33mWarning: Unknown variable: 'until_complete_real'. at /etc/puppet/modules/nova/manifests/cron/archive_deleted_rows.pp:91:90[0m
2018-12-04 06:18:13,568 INFO: [1;33mWarning: This method is deprecated, please use match expressions with Stdlib::Compat::Array instead. They are described at https://docs.puppet.com/puppet/latest/reference/lang_data_type.html#match-expressions. at ["/etc/puppet/modules/nova/manifests/scheduler/filter.pp", 147]:["/etc/puppet/manifests/puppet-stack-config.pp", 400]
2018-12-04 06:18:13,569 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:18:13,849 INFO: [1;33mWarning: Scope(Class[Neutron]): neutron::rabbit_host, neutron::rabbit_hosts, neutron::rabbit_password, neutron::rabbit_port, neutron::rabbit_user, neutron::rabbit_virtual_host and neutron::rpc_backend are deprecated. Please use neutron::default_transport_url instead.[0m
2018-12-04 06:18:15,535 INFO: [1;33mWarning: Unknown variable: 'methods_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:100:56[0m
2018-12-04 06:18:15,535 INFO: [1;33mWarning: Unknown variable: 'incoming_remove_headers_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:101:56[0m
2018-12-04 06:18:15,536 INFO: [1;33mWarning: Unknown variable: 'incoming_allow_headers_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:102:56[0m
2018-12-04 06:18:15,536 INFO: [1;33mWarning: Unknown variable: 'outgoing_remove_headers_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:103:56[0m
2018-12-04 06:18:15,537 INFO: [1;33mWarning: Unknown variable: 'outgoing_allow_headers_real'. at /etc/puppet/modules/swift/manifests/proxy/tempurl.pp:104:56[0m
2018-12-04 06:18:15,778 INFO: [1;33mWarning: Scope(Class[Swift::Storage::All]): The default port for the object storage server has changed from 6000 to 6200 and will be changed in a later release[0m
2018-12-04 06:18:15,778 INFO: [1;33mWarning: Scope(Class[Swift::Storage::All]): The default port for the container storage server has changed from 6001 to 6201 and will be changed in a later release[0m
2018-12-04 06:18:15,779 INFO: [1;33mWarning: Scope(Class[Swift::Storage::All]): The default port for the account storage server has changed from 6002 to 6202 and will be changed in a later release[0m
2018-12-04 06:18:16,435 INFO: [1;33mWarning: This method is deprecated, please use the stdlib validate_legacy function,
2018-12-04 06:18:16,436 INFO:                     with Stdlib::Compat::Integer. There is further documentation for validate_legacy function in the README. at ["/etc/puppet/modules/heat/manifests/wsgi/apache_api_cfn.pp", 125]:["/etc/puppet/manifests/puppet-stack-config.pp", 528]
2018-12-04 06:18:16,436 INFO:    (at /etc/puppet/modules/stdlib/lib/puppet/functions/deprecation.rb:28:in `deprecation')[0m
2018-12-04 06:18:17,014 INFO: [1;33mWarning: Unknown variable: '::ironic::conductor::swift_account'. at /etc/puppet/modules/ironic/manifests/glance.pp:117:30[0m
2018-12-04 06:18:17,015 INFO: [1;33mWarning: Unknown variable: '::ironic::conductor::swift_temp_url_key'. at /etc/puppet/modules/ironic/manifests/glance.pp:118:35[0m
2018-12-04 06:18:17,016 INFO: [1;33mWarning: Unknown variable: '::ironic::conductor::swift_temp_url_duration'. at /etc/puppet/modules/ironic/manifests/glance.pp:119:40[0m
2018-12-04 06:18:17,055 INFO: [1;33mWarning: Unknown variable: '::ironic::api::neutron_url'. at /etc/puppet/modules/ironic/manifests/neutron.pp:58:29[0m
2018-12-04 06:18:18,384 INFO: [1;33mWarning: ModuleLoader: module 'mistral' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:18:18,384 INFO:    (file & line not available)[0m
2018-12-04 06:18:18,472 INFO: [1;33mWarning: Unknown variable: '::mistral::database_idle_timeout'. at /etc/puppet/modules/mistral/manifests/db.pp:57:40[0m
2018-12-04 06:18:18,473 INFO: [1;33mWarning: Unknown variable: '::mistral::database_min_pool_size'. at /etc/puppet/modules/mistral/manifests/db.pp:58:40[0m
2018-12-04 06:18:18,474 INFO: [1;33mWarning: Unknown variable: '::mistral::database_max_pool_size'. at /etc/puppet/modules/mistral/manifests/db.pp:59:40[0m
2018-12-04 06:18:18,474 INFO: [1;33mWarning: Unknown variable: '::mistral::database_max_retries'. at /etc/puppet/modules/mistral/manifests/db.pp:60:40[0m
2018-12-04 06:18:18,475 INFO: [1;33mWarning: Unknown variable: '::mistral::database_retry_interval'. at /etc/puppet/modules/mistral/manifests/db.pp:61:40[0m
2018-12-04 06:18:18,476 INFO: [1;33mWarning: Unknown variable: '::mistral::database_max_overflow'. at /etc/puppet/modules/mistral/manifests/db.pp:62:40[0m
2018-12-04 06:18:18,682 INFO: [1;33mWarning: Scope(Class[Mistral]): mistral::rabbit_host, mistral::rabbit_hosts, mistral::rabbit_password, mistral::rabbit_port, mistral::rabbit_userid, mistral::rabbit_virtual_host and mistral::rpc_backend are deprecated. Please use mistral::default_transport_url instead.[0m
2018-12-04 06:18:18,966 INFO: [1;33mWarning: ModuleLoader: module 'zaqar' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:18:18,966 INFO:    (file & line not available)[0m
2018-12-04 06:18:20,399 INFO: [1;33mWarning: Unknown variable: 'ca_pem'. at /etc/puppet/modules/tripleo/manifests/certmonger/haproxy.pp:95:22[0m
2018-12-04 06:18:21,119 INFO: [1;33mWarning: ModuleLoader: module 'oslo' has unresolved dependencies - it will only see those that are resolved. Use 'puppet module list --tree' to see information about modules
2018-12-04 06:18:21,119 INFO:    (file & line not available)[0m
2018-12-04 06:18:21,263 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[keystone_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-04 06:18:22,633 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[glance_api_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-04 06:18:22,655 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[glance_registry_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-04 06:18:23,026 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[neutron_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-04 06:18:23,126 INFO: [1;33mWarning: Scope(Neutron::Plugins::Ml2::Type_driver[local]): local type_driver is useful only for single-box, because it provides no connectivity between hosts[0m
2018-12-04 06:18:24,076 INFO: [1;33mWarning: Scope(Oslo::Messaging::Rabbit[mistral_config]): The oslo_messaging rabbit_host, rabbit_hosts, rabbit_port, rabbit_userid, rabbit_password, rabbit_virtual_host parameters have been deprecated by the [DEFAULT]\transport_url. Please use oslo::messaging::default::transport_url instead.[0m
2018-12-04 06:18:24,937 INFO: [1;33mWarning: Scope(Haproxy::Config[haproxy]): haproxy: The $merge_options parameter will default to true in the next major release. Please review the documentation regarding the implications.[0m
2018-12-04 06:18:31,342 INFO: [mNotice: Compiled catalog for undercloud.example.com in environment production in 28.75 seconds[0m
2018-12-04 06:18:46,489 INFO: [mNotice: /Stage[setup]/Tripleo::Network::Os_net_config/Exec[os-net-config]/returns: executed successfully[0m
2018-12-04 06:18:46,514 INFO: [mNotice: /Stage[setup]/Tripleo::Network::Os_net_config/Exec[trigger-keepalived-restart]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:18:52,195 INFO: [mNotice: /Stage[main]/Main/Exec[restart rsyslog]/returns: executed successfully[0m
2018-12-04 06:19:00,479 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Certmonger_user/Tripleo::Certmonger::Haproxy[undercloud-haproxy-public]/Certmonger_certificate[undercloud-haproxy-public-cert]/dnsname: defined 'dnsname' as '172.16.0.10'[0m
2018-12-04 06:19:00,481 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Certmonger_user/Tripleo::Certmonger::Haproxy[undercloud-haproxy-public]/Certmonger_certificate[undercloud-haproxy-public-cert]/postsave_cmd: postsave_cmd changed '"cat /etc/pki/tls/certs/undercloud-front.crt  /etc/pki/tls/private/undercloud-front.key > /etc/pki/tls/certs/undercloud-172.16.0.10.pem && /usr/bin/instack-haproxy-cert-update '/etc/pki/tls/certs/undercloud-front.crt' '/etc/pki/tls/private/undercloud-front.key' /etc/pki/tls/certs/undercloud-172.16.0.10.pem undercloud-haproxy-public-cert"' to 'cat /etc/pki/tls/certs/undercloud-front.crt  /etc/pki/tls/private/undercloud-front.key > /etc/pki/tls/certs/undercloud-172.16.0.10.pem && /usr/bin/instack-haproxy-cert-update '/etc/pki/tls/certs/undercloud-front.crt' '/etc/pki/tls/private/undercloud-front.key' /etc/pki/tls/certs/undercloud-172.16.0.10.pem undercloud-haproxy-public-cert'[0m
2018-12-04 06:19:00,967 INFO: [mNotice: /Stage[main]/Keepalived::Config/Concat[/etc/keepalived/keepalived.conf]/File[/etc/keepalived/keepalived.conf]/content: content changed '{md5}8964035d05d5fc17153cf0603d016c22' to '{md5}675c103ce74ddf3a143bd45eb172b77f'[0m
2018-12-04 06:19:45,569 INFO: [mNotice: /Stage[main]/Tripleo::Profile::Base::Certmonger_user/Tripleo::Certmonger::Haproxy[undercloud-haproxy-public]/Concat[/etc/pki/tls/certs/undercloud-172.16.0.10.pem]/File[/etc/pki/tls/certs/undercloud-172.16.0.10.pem]/content: content changed '{md5}29e6e69ee19153a247134528492c86a9' to '{md5}c2d1a68a4c76a27a1d3ed662110d9aa4'[0m
2018-12-04 06:19:46,890 INFO: [mNotice: /Stage[main]/Keepalived::Service/Service[keepalived]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:19:55,672 INFO: [mNotice: /Stage[main]/Nova::Db::Sync_api/Exec[nova-db-sync-api]/returns: executed successfully[0m
2018-12-04 06:19:55,673 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::dbsync_api::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:19:55,675 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::cell_v2::begin]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:19:55,676 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::db_online_data_migrations::begin]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:01,214 INFO: [mNotice: /Stage[main]/Nova::Cell_v2::Map_cell0/Exec[nova-cell_v2-map_cell0]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:01,216 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::cell_v2::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:12,126 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::dbsync::begin]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:20:13,999 INFO: [mNotice: /Stage[main]/Glance::Db::Sync/Exec[glance-manage db_sync]/returns: executed successfully[0m
2018-12-04 06:20:14,001 INFO: [mNotice: /Stage[main]/Glance::Deps/Anchor[glance::dbsync::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:15,969 INFO: [mNotice: /Stage[main]/Glance::Db::Metadefs/Exec[glance-manage db_load_metadefs]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:21,998 INFO: [mNotice: /Stage[main]/Nova::Db::Sync/Exec[nova-db-sync]/returns: executed successfully[0m
2018-12-04 06:20:27,957 INFO: [mNotice: /Stage[main]/Nova::Db::Sync/Exec[nova-db-sync]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:27,958 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::dbsync::end]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:20:30,327 INFO: [mNotice: /Stage[main]/Neutron::Db::Sync/Exec[neutron-db-sync]/returns: executed successfully[0m
2018-12-04 06:20:30,328 INFO: [mNotice: /Stage[main]/Neutron::Deps/Anchor[neutron::dbsync::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:30,330 INFO: [mNotice: /Stage[main]/Neutron::Deps/Anchor[neutron::service::begin]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:37,681 INFO: [mNotice: /Stage[main]/Neutron::Agents::Dhcp/Service[neutron-dhcp-service]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:37,863 INFO: [mNotice: /Stage[main]/Neutron::Agents::L3/Service[neutron-l3]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:40,052 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Service[neutron-ovs-agent-service]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:42,573 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Ovs/Service[neutron-destroy-patch-ports-service]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:44,342 INFO: [mNotice: /Stage[main]/Heat::Db::Sync/Exec[heat-dbsync]/returns: executed successfully[0m
2018-12-04 06:20:44,343 INFO: [mNotice: /Stage[main]/Heat::Deps/Anchor[heat::dbsync::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:46,266 INFO: [mNotice: /Stage[main]/Ironic::Db::Sync/Exec[ironic-dbsync]/returns: executed successfully[0m
2018-12-04 06:20:46,268 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::dbsync::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:46,269 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::db_online_data_migrations::begin]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:50,654 INFO: [mNotice: /Stage[main]/Ironic::Db::Online_data_migrations/Exec[ironic-db-online-data-migrations]/returns: executed successfully[0m
2018-12-04 06:20:55,037 INFO: [mNotice: /Stage[main]/Ironic::Db::Online_data_migrations/Exec[ironic-db-online-data-migrations]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:20:55,038 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::db_online_data_migrations::end]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:20:55,039 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::service::begin]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:20:55,152 INFO: [mNotice: /Stage[main]/Ironic::Api/Service[ironic-api]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:21:00,904 INFO: [mNotice: /Stage[main]/Ironic::Conductor/Service[ironic-conductor]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:21:00,905 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic::service::end]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:21:04,287 INFO: [mNotice: /Stage[main]/Mistral::Db::Sync/Exec[mistral-db-sync]/returns: executed successfully[0m
2018-12-04 06:21:04,518 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::service::begin]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:21:04,520 INFO: [mNotice: /Stage[main]/Heat::Deps/Anchor[heat::service::begin]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:21:04,641 INFO: [mNotice: /Stage[main]/Heat::Api/Service[heat-api]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:21:04,765 INFO: [mNotice: /Stage[main]/Heat::Api_cfn/Service[heat-api-cfn]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:21:05,105 INFO: [mNotice: /Stage[main]/Heat::Engine/Service[heat-engine]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:21:13,414 INFO: [mNotice: /Stage[main]/Nova::Conductor/Nova::Generic_service[conductor]/Service[nova-conductor]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:21:21,393 INFO: [mNotice: /Stage[main]/Nova::Scheduler/Nova::Generic_service[scheduler]/Service[nova-scheduler]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:21:26,045 INFO: [mNotice: /Stage[main]/Keystone::Db::Sync/Exec[keystone-manage db_sync]/returns: executed successfully[0m
2018-12-04 06:21:26,047 INFO: [mNotice: /Stage[main]/Keystone::Deps/Anchor[keystone::dbsync::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:21:29,130 INFO: [mNotice: /Stage[main]/Keystone/Exec[keystone-manage bootstrap]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:21:29,132 INFO: [mNotice: /Stage[main]/Keystone::Deps/Anchor[keystone::service::begin]: Triggered 'refresh' from 2 events[0m
2018-12-04 06:22:16,252 INFO: [mNotice: /Stage[main]/Heat::Deps/Anchor[heat::service::end]: Triggered 'refresh' from 3 events[0m
2018-12-04 06:22:29,062 INFO: [mNotice: /Stage[main]/Neutron::Server/Service[neutron-server]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:22:37,207 INFO: [mNotice: /Stage[main]/Glance::Deps/Anchor[glance::service::begin]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:23:24,289 INFO: [mNotice: /Stage[main]/Nova::Api/Nova::Generic_service[api]/Service[nova-api]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:23:24,954 INFO: [mNotice: /Stage[main]/Swift::Proxy/Swift::Service[swift-proxy-server]/Service[swift-proxy-server]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:23:25,665 INFO: [mNotice: /Stage[main]/Swift::Objectexpirer/Swift::Service[swift-object-expirer]/Service[swift-object-expirer]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:23:50,763 INFO: [mNotice: /Stage[main]/Neutron::Agents::Ml2::Networking_baremetal/Service[ironic-neutron-agent-service]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:23:50,765 INFO: [mNotice: /Stage[main]/Neutron::Deps/Anchor[neutron::service::end]: Triggered 'refresh' from 6 events[0m
2018-12-04 06:23:51,477 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Service[ironic-inspector]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:23:52,273 INFO: [mNotice: /Stage[main]/Ironic::Inspector/Service[ironic-inspector-dnsmasq]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:23:52,274 INFO: [mNotice: /Stage[main]/Ironic::Deps/Anchor[ironic-inspector::service::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:24:07,565 INFO: [mNotice: /Stage[main]/Mistral::Db::Sync/Exec[mistral-db-populate]/returns: executed successfully[0m
2018-12-04 06:24:23,389 INFO: [mNotice: /Stage[main]/Mistral::Db::Sync/Exec[mistral-db-populate]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:24:23,392 INFO: [mNotice: /Stage[main]/Mistral::Deps/Anchor[mistral::dbsync::end]: Triggered 'refresh' from 3 events[0m
2018-12-04 06:24:23,393 INFO: [mNotice: /Stage[main]/Mistral::Deps/Anchor[mistral::service::begin]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:24:24,364 INFO: [mNotice: /Stage[main]/Mistral::Api/Service[mistral-api]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:24:25,192 INFO: [mNotice: /Stage[main]/Mistral::Engine/Service[mistral-engine]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:24:26,345 INFO: [mNotice: /Stage[main]/Mistral::Executor/Service[mistral-executor]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:24:26,350 INFO: [mNotice: /Stage[main]/Mistral::Deps/Anchor[mistral::service::end]: Triggered 'refresh' from 3 events[0m
2018-12-04 06:24:32,811 INFO: [mNotice: /Stage[main]/Nova::Compute/Nova::Generic_service[compute]/Service[nova-compute]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:24:32,821 INFO: [mNotice: /Stage[main]/Nova::Deps/Anchor[nova::service::end]: Triggered 'refresh' from 4 events[0m
2018-12-04 06:24:32,821 INFO: [mNotice: /Stage[main]/Nova::Logging/File[/var/log/nova/nova-manage.log]/seluser: seluser changed 'unconfined_u' to 'system_u'[0m
2018-12-04 06:24:38,956 INFO: [mNotice: /Stage[main]/Nova::Cell_v2::Discover_hosts/Exec[nova-cell_v2-discover_hosts]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:24:39,917 INFO: [mNotice: /Stage[main]/Swift::Storage::Account/Swift::Service[swift-account-reaper]/Service[swift-account-reaper]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:24:40,940 INFO: [mNotice: /Stage[main]/Swift::Storage::Container/Swift::Service[swift-container-updater]/Service[swift-container-updater]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:24:41,866 INFO: [mNotice: /Stage[main]/Swift::Storage::Container/Swift::Service[swift-container-sync]/Service[swift-container-sync]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:24:42,819 INFO: [mNotice: /Stage[main]/Swift::Storage::Object/Swift::Service[swift-object-updater]/Service[swift-object-updater]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:24:43,713 INFO: [mNotice: /Stage[main]/Swift::Storage::Object/Swift::Service[swift-object-reconstructor]/Service[swift-object-reconstructor]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:24:45,430 INFO: [mNotice: /Stage[main]/Swift::Storage::Account/Swift::Storage::Generic[account]/Swift::Service[swift-account-server]/Service[swift-account-server]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:24:46,485 INFO: [mNotice: /Stage[main]/Swift::Storage::Container/Swift::Storage::Generic[container]/Swift::Service[swift-container-server]/Service[swift-container-server]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:24:47,704 INFO: [mNotice: /Stage[main]/Swift::Storage::Object/Swift::Storage::Generic[object]/Swift::Service[swift-object-server]/Service[swift-object-server]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:24:48,024 INFO: [mNotice: /Stage[main]/Swift::Deps/Anchor[swift::service::end]: Triggered 'refresh' from 10 events[0m
2018-12-04 06:24:49,043 INFO: [mNotice: /Stage[main]/Glance::Api/Service[glance-api]/ensure: ensure changed 'stopped' to 'running'[0m
2018-12-04 06:24:49,050 INFO: [mNotice: /Stage[main]/Glance::Deps/Anchor[glance::service::end]: Triggered 'refresh' from 1 events[0m
2018-12-04 06:24:51,943 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Post/Tripleo::Firewall::Rule[998 log all]/Firewall[998 log all ipv4]/ensure: created[0m
2018-12-04 06:24:54,191 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Post/Tripleo::Firewall::Rule[998 log all]/Firewall[998 log all ipv6]/ensure: created[0m
2018-12-04 06:24:57,691 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Post/Tripleo::Firewall::Rule[999 drop all]/Firewall[999 drop all ipv4]/ensure: created[0m
2018-12-04 06:24:59,965 INFO: [mNotice: /Stage[main]/Tripleo::Firewall::Post/Tripleo::Firewall::Rule[999 drop all]/Firewall[999 drop all ipv6]/ensure: created[0m
2018-12-04 06:25:00,113 INFO: [mNotice: /Stage[main]/Firewall::Linux::Redhat/File[/etc/sysconfig/iptables]/seluser: seluser changed 'unconfined_u' to 'system_u'[0m
2018-12-04 06:25:00,118 INFO: [mNotice: /Stage[main]/Firewall::Linux::Redhat/File[/etc/sysconfig/ip6tables]/seluser: seluser changed 'unconfined_u' to 'system_u'[0m
2018-12-04 06:25:00,203 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Exec[nonpersistent_v4_rules_cleanup]/returns: executed successfully[0m
2018-12-04 06:25:00,282 INFO: [mNotice: /Stage[main]/Tripleo::Firewall/Exec[nonpersistent_v6_rules_cleanup]/returns: executed successfully[0m
2018-12-04 06:25:04,175 INFO: [mNotice: Applied catalog in 385.35 seconds[0m
2018-12-04 06:25:04,301 INFO: Changes:
2018-12-04 06:25:04,301 INFO:             Total: 45
2018-12-04 06:25:04,301 INFO: Events:
2018-12-04 06:25:04,302 INFO:           Success: 45
2018-12-04 06:25:04,302 INFO:             Total: 45
2018-12-04 06:25:04,302 INFO: Resources:
2018-12-04 06:25:04,302 INFO:             Total: 2924
2018-12-04 06:25:04,302 INFO:       Out of sync: 44
2018-12-04 06:25:04,303 INFO:           Changed: 44
2018-12-04 06:25:04,303 INFO:         Restarted: 49
2018-12-04 06:25:04,303 INFO: Time:
2018-12-04 06:25:04,303 INFO:        Filebucket: 0.00
2018-12-04 06:25:04,303 INFO:           Archive: 0.00
2018-12-04 06:25:04,304 INFO:          Schedule: 0.00
2018-12-04 06:25:04,304 INFO:        Policy rcd: 0.00
2018-12-04 06:25:04,304 INFO:      Nova cell v2: 0.00
2018-12-04 06:25:04,304 INFO:     Mysql datadir: 0.00
2018-12-04 06:25:04,304 INFO:    Keystone domain: 0.00
2018-12-04 06:25:04,305 INFO:            Sysctl: 0.00
2018-12-04 06:25:04,305 INFO:     Keystone role: 0.00
2018-12-04 06:25:04,305 INFO:             Group: 0.00
2018-12-04 06:25:04,305 INFO:    Sysctl runtime: 0.00
2018-12-04 06:25:04,305 INFO:    Keystone tenant: 0.00
2018-12-04 06:25:04,306 INFO:    Neutron api config: 0.00
2018-12-04 06:25:04,306 INFO:         Resources: 0.00
2018-12-04 06:25:04,306 INFO:              Cron: 0.00
2018-12-04 06:25:04,306 INFO:      Swift config: 0.00
2018-12-04 06:25:04,306 INFO:    Glance swift config: 0.00
2018-12-04 06:25:04,307 INFO:              User: 0.00
2018-12-04 06:25:04,307 INFO:    Mysql database: 0.00
2018-12-04 06:25:04,307 INFO:       Concat file: 0.01
2018-12-04 06:25:04,307 INFO:    Keystone service: 0.01
2018-12-04 06:25:04,308 INFO:    Nova paste api ini: 0.01
2018-12-04 06:25:04,308 INFO:       Mysql grant: 0.01
2018-12-04 06:25:04,308 INFO:    Keystone endpoint: 0.01
2018-12-04 06:25:04,308 INFO:    Swift object expirer config: 0.01
2018-12-04 06:25:04,309 INFO:    Ironic neutron agent config: 0.01
2018-12-04 06:25:04,309 INFO:        Mysql user: 0.01
2018-12-04 06:25:04,309 INFO:    Neutron l3 agent config: 0.02
2018-12-04 06:25:04,309 INFO:            Anchor: 0.02
2018-12-04 06:25:04,309 INFO:    Concat fragment: 0.02
2018-12-04 06:25:04,310 INFO:    Neutron plugin ml2: 0.02
2018-12-04 06:25:04,310 INFO:    Neutron dhcp agent config: 0.03
2018-12-04 06:25:04,310 INFO:    Neutron agent ovs: 0.03
2018-12-04 06:25:04,310 INFO:         Vs bridge: 0.06
2018-12-04 06:25:04,310 INFO:    Certmonger certificate: 0.39
2018-12-04 06:25:04,311 INFO:    Mistral config: 0.43
2018-12-04 06:25:04,311 INFO:    Glance cache config: 0.45
2018-12-04 06:25:04,311 INFO:    Ring object device: 0.48
2018-12-04 06:25:04,311 INFO:    Ironic inspector config: 0.49
2018-12-04 06:25:04,312 INFO:    Ring container device: 0.50
2018-12-04 06:25:04,312 INFO:    Ring account device: 0.50
2018-12-04 06:25:04,312 INFO:    Glance registry config: 0.57
2018-12-04 06:25:04,312 INFO:    Swift proxy config: 0.75
2018-12-04 06:25:04,312 INFO:            Augeas: 0.98
2018-12-04 06:25:04,313 INFO:      Zaqar config: 1.21
2018-12-04 06:25:04,313 INFO:    Rabbitmq plugin: 1.89
2018-12-04 06:25:04,313 INFO:          Firewall: 11.11
2018-12-04 06:25:04,313 INFO:          Last run: 1543922704
2018-12-04 06:25:04,313 INFO:           Package: 2.20
2018-12-04 06:25:04,314 INFO:              File: 2.71
2018-12-04 06:25:04,314 INFO:       Nova config: 24.68
2018-12-04 06:25:04,314 INFO:    Keystone config: 3.41
2018-12-04 06:25:04,314 INFO:    Glance api config: 3.69
2018-12-04 06:25:04,314 INFO:    Neutron config: 3.71
2018-12-04 06:25:04,315 INFO:       Heat config: 3.91
2018-12-04 06:25:04,315 INFO:             Total: 311.06
2018-12-04 06:25:04,315 INFO:           Service: 35.37
2018-12-04 06:25:04,315 INFO:    Config retrieval: 35.66
2018-12-04 06:25:04,315 INFO:    Keystone user role: 46.19
2018-12-04 06:25:04,316 INFO:              Exec: 47.11
2018-12-04 06:25:04,316 INFO:     Keystone user: 74.00
2018-12-04 06:25:04,316 INFO:     Ironic config: 8.34
2018-12-04 06:25:04,316 INFO: Version:
2018-12-04 06:25:04,316 INFO:            Config: 1543922282
2018-12-04 06:25:04,317 INFO:            Puppet: 4.8.2
2018-12-04 06:25:22,175 INFO: + rc=2
2018-12-04 06:25:22,176 INFO: + set -e
2018-12-04 06:25:22,176 INFO: + echo 'puppet apply exited with exit code 2'
2018-12-04 06:25:22,176 INFO: puppet apply exited with exit code 2
2018-12-04 06:25:22,176 INFO: + '[' 2 '!=' 2 -a 2 '!=' 0 ']'
2018-12-04 06:25:22,180 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 50-puppet-stack-config completed
2018-12-04 06:25:22,182 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 ----------------------- PROFILING -----------------------
2018-12-04 06:25:22,184 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018
2018-12-04 06:25:22,187 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 Target: configure.d
2018-12-04 06:25:22,189 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018
2018-12-04 06:25:22,191 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 Script                                     Seconds
2018-12-04 06:25:22,193 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 ---------------------------------------  ----------
2018-12-04 06:25:22,194 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018
2018-12-04 06:25:22,206 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 10-hiera-disable                              0.006
2018-12-04 06:25:22,214 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 20-os-apply-config                            0.309
2018-12-04 06:25:22,222 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 30-reload-keepalived                          0.057
2018-12-04 06:25:22,230 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 40-hiera-datafiles                            0.297
2018-12-04 06:25:22,238 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 50-puppet-stack-config                      448.617
2018-12-04 06:25:22,242 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018
2018-12-04 06:25:22,244 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 --------------------- END PROFILING ---------------------
2018-12-04 06:25:22,244 INFO: [2018-12-04 06:25:22,244] (os-refresh-config) [INFO] Completed phase configure
2018-12-04 06:25:22,245 INFO: [2018-12-04 06:25:22,245] (os-refresh-config) [INFO] Starting phase post-configure
2018-12-04 06:25:22,261 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 Running /usr/libexec/os-refresh-config/post-configure.d/10-iptables
2018-12-04 06:25:22,264 INFO: + set -o pipefail
2018-12-04 06:25:22,265 INFO: + EXTERNAL_BRIDGE=br-ctlplane
2018-12-04 06:25:22,265 INFO: + iptables -w -t nat -C PREROUTING -d 169.254.169.254/32 -i br-ctlplane -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 8775
2018-12-04 06:25:22,268 INFO: iptables: No chain/target/match by that name.
2018-12-04 06:25:22,269 INFO: + iptables -w -t nat -I PREROUTING -d 169.254.169.254/32 -i br-ctlplane -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 8775
2018-12-04 06:25:22,284 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 10-iptables completed
2018-12-04 06:25:22,286 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 Running /usr/libexec/os-refresh-config/post-configure.d/80-seedstack-masquerade
2018-12-04 06:25:22,290 INFO: + RULES_SCRIPT=/var/opt/undercloud-stack/masquerade
2018-12-04 06:25:22,290 INFO: + . /var/opt/undercloud-stack/masquerade
2018-12-04 06:25:22,290 INFO: ++ IPTCOMMAND=iptables
2018-12-04 06:25:22,290 INFO: ++ [[ 172.16.0.1 =~ : ]]
2018-12-04 06:25:22,291 INFO: ++ iptables -w -t nat -F BOOTSTACK_MASQ_NEW
2018-12-04 06:25:22,292 INFO: iptables: No chain/target/match by that name.
2018-12-04 06:25:22,292 INFO: ++ true
2018-12-04 06:25:22,292 INFO: ++ iptables -w -t nat -D POSTROUTING -j BOOTSTACK_MASQ_NEW
2018-12-04 06:25:22,293 INFO: iptables v1.4.21: Couldn't load target `BOOTSTACK_MASQ_NEW':No such file or directory
2018-12-04 06:25:22,294 INFO: 
2018-12-04 06:25:22,294 INFO: Try `iptables -h' or 'iptables --help' for more information.
2018-12-04 06:25:22,294 INFO: ++ true
2018-12-04 06:25:22,294 INFO: ++ iptables -w -t nat -X BOOTSTACK_MASQ_NEW
2018-12-04 06:25:22,295 INFO: iptables: No chain/target/match by that name.
2018-12-04 06:25:22,295 INFO: ++ true
2018-12-04 06:25:22,295 INFO: ++ iptables -w -t nat -N BOOTSTACK_MASQ_NEW
2018-12-04 06:25:22,297 INFO: ++ NETWORK=172.16.0.0/24
2018-12-04 06:25:22,298 INFO: ++ NETWORKS=172.16.0.0/24,192.168.24.0/24,
2018-12-04 06:25:22,298 INFO: ++ NETWORKS=172.16.0.0/24,192.168.24.0/24
2018-12-04 06:25:22,298 INFO: ++ iptables -w -t nat -A BOOTSTACK_MASQ_NEW -s 172.16.0.0/24 -d 172.16.0.0/24,192.168.24.0/24 -j RETURN
2018-12-04 06:25:22,300 INFO: ++ iptables -w -t nat -A BOOTSTACK_MASQ_NEW -s 172.16.0.0/24 -j MASQUERADE
2018-12-04 06:25:22,302 INFO: ++ NETWORK=192.168.24.0/24
2018-12-04 06:25:22,302 INFO: ++ NETWORKS=172.16.0.0/24,192.168.24.0/24,
2018-12-04 06:25:22,302 INFO: ++ NETWORKS=172.16.0.0/24,192.168.24.0/24
2018-12-04 06:25:22,302 INFO: ++ iptables -w -t nat -A BOOTSTACK_MASQ_NEW -s 192.168.24.0/24 -d 172.16.0.0/24,192.168.24.0/24 -j RETURN
2018-12-04 06:25:22,304 INFO: ++ iptables -w -t nat -A BOOTSTACK_MASQ_NEW -s 192.168.24.0/24 -j MASQUERADE
2018-12-04 06:25:22,305 INFO: ++ iptables -w -t nat -I POSTROUTING -j BOOTSTACK_MASQ_NEW
2018-12-04 06:25:22,307 INFO: ++ iptables -w -t nat -F BOOTSTACK_MASQ
2018-12-04 06:25:22,309 INFO: iptables: No chain/target/match by that name.
2018-12-04 06:25:22,309 INFO: ++ true
2018-12-04 06:25:22,309 INFO: ++ iptables -w -t nat -D POSTROUTING -j BOOTSTACK_MASQ
2018-12-04 06:25:22,310 INFO: iptables v1.4.21: Couldn't load target `BOOTSTACK_MASQ':No such file or directory
2018-12-04 06:25:22,311 INFO: 
2018-12-04 06:25:22,311 INFO: Try `iptables -h' or 'iptables --help' for more information.
2018-12-04 06:25:22,311 INFO: ++ true
2018-12-04 06:25:22,311 INFO: ++ iptables -w -t nat -X BOOTSTACK_MASQ
2018-12-04 06:25:22,312 INFO: iptables: No chain/target/match by that name.
2018-12-04 06:25:22,312 INFO: ++ true
2018-12-04 06:25:22,312 INFO: ++ iptables -w -t nat -E BOOTSTACK_MASQ_NEW BOOTSTACK_MASQ
2018-12-04 06:25:22,314 INFO: ++ iptables -w -D FORWARD -j REJECT --reject-with icmp-host-prohibited
2018-12-04 06:25:22,316 INFO: + iptables-save
2018-12-04 06:25:22,323 INFO: + /bin/test -f /etc/sysconfig/iptables
2018-12-04 06:25:22,324 INFO: + /bin/grep -q neutron- /etc/sysconfig/iptables
2018-12-04 06:25:22,325 INFO: + /bin/sed -i /neutron-/d /etc/sysconfig/iptables
2018-12-04 06:25:22,329 INFO: + /bin/test -f /etc/sysconfig/ip6tables
2018-12-04 06:25:22,330 INFO: + /bin/grep -q neutron- /etc/sysconfig/ip6tables
2018-12-04 06:25:22,332 INFO: + /bin/test -f /etc/sysconfig/iptables
2018-12-04 06:25:22,333 INFO: + /bin/grep -v '\-m comment \--comment' /etc/sysconfig/iptables
2018-12-04 06:25:22,334 INFO: + /bin/grep -q ironic-inspector
2018-12-04 06:25:22,335 INFO: + /bin/test -f /etc/sysconfig/ip6tables
2018-12-04 06:25:22,337 INFO: + /bin/grep -v '\-m comment \--comment' /etc/sysconfig/ip6tables
2018-12-04 06:25:22,337 INFO: + /bin/grep -q ironic-inspector
2018-12-04 06:25:22,343 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 80-seedstack-masquerade completed
2018-12-04 06:25:22,345 INFO: dib-run-parts Tue Dec  4 06:25:22 EST 2018 Running /usr/libexec/os-refresh-config/post-configure.d/98-undercloud-setup
2018-12-04 06:25:22,349 INFO: + source /root/tripleo-undercloud-passwords
2018-12-04 06:25:22,350 INFO: +++ sudo hiera admin_password
2018-12-04 06:25:22,526 INFO: ++ UNDERCLOUD_ADMIN_PASSWORD=527462cc3bc8c7744b741446431c6cfcd6f0d8ee
2018-12-04 06:25:22,526 INFO: +++ sudo hiera keystone::admin_token
2018-12-04 06:25:22,689 INFO: ++ UNDERCLOUD_ADMIN_TOKEN=92a1390dfc94341d01fe9da7ef4e13a8c3e69d4e
2018-12-04 06:25:22,690 INFO: +++ sudo hiera ceilometer::metering_secret
2018-12-04 06:25:22,849 INFO: ++ UNDERCLOUD_CEILOMETER_METERING_SECRET=1ae67fad7ea2873fe97db7ba808f6d672604c937
2018-12-04 06:25:22,850 INFO: +++ sudo hiera ceilometer::keystone::authtoken::password
2018-12-04 06:25:23,003 INFO: ++ UNDERCLOUD_CEILOMETER_PASSWORD=dea401a21e09ba8d8cd4f80683de939aac57ec8f
2018-12-04 06:25:23,004 INFO: +++ sudo hiera snmpd_readonly_user_password
2018-12-04 06:25:23,160 INFO: ++ UNDERCLOUD_CEILOMETER_SNMPD_PASSWORD=nil
2018-12-04 06:25:23,161 INFO: +++ sudo hiera snmpd_readonly_user_name
2018-12-04 06:25:23,301 INFO: ++ UNDERCLOUD_CEILOMETER_SNMPD_USER=nil
2018-12-04 06:25:23,302 INFO: +++ sudo hiera admin_password
2018-12-04 06:25:23,441 INFO: ++ UNDERCLOUD_DB_PASSWORD=527462cc3bc8c7744b741446431c6cfcd6f0d8ee
2018-12-04 06:25:23,442 INFO: +++ sudo hiera glance::api::keystone_password
2018-12-04 06:25:23,581 INFO: ++ UNDERCLOUD_GLANCE_PASSWORD=nil
2018-12-04 06:25:23,582 INFO: +++ sudo hiera tripleo::haproxy::haproxy_stats_password
2018-12-04 06:25:23,764 INFO: ++ UNDERCLOUD_HAPROXY_STATS_PASSWORD=64f9d8d7663243db239b772f79b57b85b0646a44
2018-12-04 06:25:23,765 INFO: +++ sudo hiera heat::engine::auth_encryption_key
2018-12-04 06:25:23,905 INFO: ++ UNDERCLOUD_HEAT_ENCRYPTION_KEY=6045d94c318c54abe94ba2a5594b0ab7
2018-12-04 06:25:23,905 INFO: +++ sudo hiera heat::keystone_password
2018-12-04 06:25:24,041 INFO: ++ UNDERCLOUD_HEAT_PASSWORD=nil
2018-12-04 06:25:24,041 INFO: +++ sudo hiera heat_stack_domain_admin_password
2018-12-04 06:25:24,177 INFO: ++ UNDERCLOUD_HEAT_STACK_DOMAIN_ADMIN_PASSWORD=dfd9981cb4df1327df752ca065e2430a49284b28
2018-12-04 06:25:24,177 INFO: +++ sudo hiera horizon_secret_key
2018-12-04 06:25:24,324 INFO: ++ UNDERCLOUD_HORIZON_SECRET_KEY=9b7ba66180a76f8a66673a8d03fc986dc7b9fbfe
2018-12-04 06:25:24,325 INFO: +++ sudo hiera ironic::api::authtoken::password
2018-12-04 06:25:24,465 INFO: ++ UNDERCLOUD_IRONIC_PASSWORD=1b0ab87e4a1b3414321b1de70e6ba1028626e93a
2018-12-04 06:25:24,465 INFO: +++ sudo hiera neutron::server::auth_password
2018-12-04 06:25:24,606 INFO: ++ UNDERCLOUD_NEUTRON_PASSWORD=nil
2018-12-04 06:25:24,607 INFO: +++ sudo hiera nova::keystone::authtoken::password
2018-12-04 06:25:24,747 INFO: ++ UNDERCLOUD_NOVA_PASSWORD=edf03cfa07e73fb909396b35750947cff6106a03
2018-12-04 06:25:24,747 INFO: +++ sudo hiera rabbit_cookie
2018-12-04 06:25:24,885 INFO: ++ UNDERCLOUD_RABBIT_COOKIE=54456ead6169862b0e117436443859fdd6e3e9dc
2018-12-04 06:25:24,885 INFO: +++ sudo hiera rabbit_password
2018-12-04 06:25:25,023 INFO: ++ UNDERCLOUD_RABBIT_PASSWORD=nil
2018-12-04 06:25:25,023 INFO: +++ sudo hiera rabbit_username
2018-12-04 06:25:25,170 INFO: ++ UNDERCLOUD_RABBIT_USERNAME=nil
2018-12-04 06:25:25,171 INFO: +++ sudo hiera swift::swift_hash_suffix
2018-12-04 06:25:25,309 INFO: ++ UNDERCLOUD_SWIFT_HASH_SUFFIX=nil
2018-12-04 06:25:25,309 INFO: +++ sudo hiera swift::proxy::authtoken::admin_password
2018-12-04 06:25:25,449 INFO: ++ UNDERCLOUD_SWIFT_PASSWORD=nil
2018-12-04 06:25:25,450 INFO: +++ sudo hiera mistral::admin_password
2018-12-04 06:25:25,591 INFO: ++ UNDERCLOUD_MISTRAL_PASSWORD=nil
2018-12-04 06:25:25,592 INFO: +++ sudo hiera zaqar::keystone::authtoken::password
2018-12-04 06:25:25,740 INFO: ++ UNDERCLOUD_ZAQAR_PASSWORD=6115144bed3a148eae14d7e08d98beaa4838ac8b
2018-12-04 06:25:25,740 INFO: +++ sudo hiera cinder::keystone::authtoken::password
2018-12-04 06:25:25,884 INFO: ++ UNDERCLOUD_CINDER_PASSWORD=9b18e8acf1f75069d9ccb7374948b8986486d5c8
2018-12-04 06:25:25,884 INFO: + source /root/stackrc
2018-12-04 06:25:25,885 INFO: +++ set
2018-12-04 06:25:25,886 INFO: +++ awk '{FS="="}  /^OS_/ {print $1}'
2018-12-04 06:25:25,888 INFO: ++ NOVA_VERSION=1.1
2018-12-04 06:25:25,888 INFO: ++ export NOVA_VERSION
2018-12-04 06:25:25,888 INFO: ++ OS_PASSWORD=527462cc3bc8c7744b741446431c6cfcd6f0d8ee
2018-12-04 06:25:25,888 INFO: ++ export OS_PASSWORD
2018-12-04 06:25:25,889 INFO: ++ OS_AUTH_TYPE=password
2018-12-04 06:25:25,889 INFO: ++ export OS_AUTH_TYPE
2018-12-04 06:25:25,889 INFO: ++ OS_AUTH_URL=https://172.16.0.10:13000/
2018-12-04 06:25:25,889 INFO: ++ PYTHONWARNINGS='ignore:Certificate has no, ignore:A true SSLContext object is not available'
2018-12-04 06:25:25,890 INFO: ++ export OS_AUTH_URL
2018-12-04 06:25:25,890 INFO: ++ export PYTHONWARNINGS
2018-12-04 06:25:25,890 INFO: ++ OS_USERNAME=admin
2018-12-04 06:25:25,891 INFO: ++ OS_PROJECT_NAME=admin
2018-12-04 06:25:25,891 INFO: ++ COMPUTE_API_VERSION=1.1
2018-12-04 06:25:25,891 INFO: ++ IRONIC_API_VERSION=1.34
2018-12-04 06:25:25,891 INFO: ++ OS_BAREMETAL_API_VERSION=1.34
2018-12-04 06:25:25,892 INFO: ++ OS_NO_CACHE=True
2018-12-04 06:25:25,892 INFO: ++ OS_CLOUDNAME=undercloud
2018-12-04 06:25:25,892 INFO: ++ export OS_USERNAME
2018-12-04 06:25:25,892 INFO: ++ export OS_PROJECT_NAME
2018-12-04 06:25:25,893 INFO: ++ export COMPUTE_API_VERSION
2018-12-04 06:25:25,893 INFO: ++ export IRONIC_API_VERSION
2018-12-04 06:25:25,893 INFO: ++ export OS_BAREMETAL_API_VERSION
2018-12-04 06:25:25,893 INFO: ++ export OS_NO_CACHE
2018-12-04 06:25:25,894 INFO: ++ export OS_CLOUDNAME
2018-12-04 06:25:25,894 INFO: ++ OS_IDENTITY_API_VERSION=3
2018-12-04 06:25:25,894 INFO: ++ export OS_IDENTITY_API_VERSION
2018-12-04 06:25:25,895 INFO: ++ OS_PROJECT_DOMAIN_NAME=Default
2018-12-04 06:25:25,895 INFO: ++ export OS_PROJECT_DOMAIN_NAME
2018-12-04 06:25:25,895 INFO: ++ OS_USER_DOMAIN_NAME=Default
2018-12-04 06:25:25,895 INFO: ++ export OS_USER_DOMAIN_NAME
2018-12-04 06:25:25,896 INFO: ++ '[' -z '' ']'
2018-12-04 06:25:25,896 INFO: ++ export PS1=
2018-12-04 06:25:25,896 INFO: ++ PS1=
2018-12-04 06:25:25,897 INFO: ++ export 'PS1=${OS_CLOUDNAME:+($OS_CLOUDNAME)} '
2018-12-04 06:25:25,897 INFO: ++ PS1='${OS_CLOUDNAME:+($OS_CLOUDNAME)} '
2018-12-04 06:25:25,897 INFO: ++ export CLOUDPROMPT_ENABLED=1
2018-12-04 06:25:25,898 INFO: ++ CLOUDPROMPT_ENABLED=1
2018-12-04 06:25:25,898 INFO: + INSTACK_ROOT=
2018-12-04 06:25:25,898 INFO: + export INSTACK_ROOT
2018-12-04 06:25:25,899 INFO: + '[' -n '' ']'
2018-12-04 06:25:25,899 INFO: + '[' '!' -f /root/.ssh/authorized_keys ']'
2018-12-04 06:25:25,899 INFO: + '[' '!' -f /root/.ssh/id_rsa ']'
2018-12-04 06:25:25,900 INFO: + ssh-keygen -b 1024 -N '' -f /root/.ssh/id_rsa
2018-12-04 06:25:25,922 INFO: Generating public/private rsa key pair.
2018-12-04 06:25:25,922 INFO: Your identification has been saved in /root/.ssh/id_rsa.
2018-12-04 06:25:25,923 INFO: Your public key has been saved in /root/.ssh/id_rsa.pub.
2018-12-04 06:25:25,923 INFO: The key fingerprint is:
2018-12-04 06:25:25,923 INFO: SHA256:9EQdcOZRqoIfcBBbc6ELzV2+7W5zrNXzlldyQWIkeMg root@undercloud.example.com
2018-12-04 06:25:25,924 INFO: The key's randomart image is:
2018-12-04 06:25:25,924 INFO: +---[RSA 1024]----+
2018-12-04 06:25:25,924 INFO: |      o.+ B=B+.  |
2018-12-04 06:25:25,925 INFO: |       * E Bo= . |
2018-12-04 06:25:25,925 INFO: |      + * + = o  |
2018-12-04 06:25:25,925 INFO: |       * + . o . |
2018-12-04 06:25:25,925 INFO: |      . S o . . .|
2018-12-04 06:25:25,925 INFO: |       . o   .. +|
2018-12-04 06:25:25,926 INFO: |        .     .==|
2018-12-04 06:25:25,926 INFO: |             .ooB|
2018-12-04 06:25:25,926 INFO: |             .o=o|
2018-12-04 06:25:25,926 INFO: +----[SHA256]-----+
2018-12-04 06:25:25,927 INFO: + cat /root/.ssh/id_rsa.pub
2018-12-04 06:25:25,927 INFO: + '[' -e /usr/sbin/getenforce ']'
2018-12-04 06:25:25,927 INFO: ++ getenforce
2018-12-04 06:25:25,928 INFO: + '[' Enforcing == Enforcing ']'
2018-12-04 06:25:25,928 INFO: + set +e
2018-12-04 06:25:25,929 INFO: ++ find /root/.ssh/ -exec ls -lZ '{}' ';'
2018-12-04 06:25:25,929 INFO: ++ grep -v ssh_home_t
2018-12-04 06:25:25,942 INFO: + selinux_wrong_permission=
2018-12-04 06:25:25,942 INFO: + set -e
2018-12-04 06:25:25,942 INFO: + '[' -n '' ']'
2018-12-04 06:25:25,943 INFO: ++ openstack project show admin
2018-12-04 06:25:25,943 INFO: ++ awk '$2=="id" {print $4}'
2018-12-04 06:25:29,553 INFO: + openstack quota set --cores -1 --instances -1 --ram -1 f41e7348daff4254892e5cd759aa8806
2018-12-04 06:25:34,586 INFO: + rm -rf /root/.novaclient
2018-12-04 06:25:34,592 INFO: dib-run-parts Tue Dec  4 06:25:34 EST 2018 98-undercloud-setup completed
2018-12-04 06:25:34,593 INFO: dib-run-parts Tue Dec  4 06:25:34 EST 2018 Running /usr/libexec/os-refresh-config/post-configure.d/99-refresh-completed
2018-12-04 06:25:34,598 INFO: ++ os-apply-config --key completion-handle --type raw --key-default ''
2018-12-04 06:25:34,882 INFO: [2018/12/04 06:25:34 AM] [WARNING] DEPRECATED: falling back to /var/run/os-collect-config/os_config_files.json
2018-12-04 06:25:34,893 INFO: + HANDLE=
2018-12-04 06:25:34,894 INFO: ++ os-apply-config --key completion-signal --type raw --key-default ''
2018-12-04 06:25:35,182 INFO: [2018/12/04 06:25:35 AM] [WARNING] DEPRECATED: falling back to /var/run/os-collect-config/os_config_files.json
2018-12-04 06:25:35,193 INFO: + SIGNAL=
2018-12-04 06:25:35,193 INFO: ++ os-apply-config --key instance-id --type raw --key-default ''
2018-12-04 06:25:35,481 INFO: [2018/12/04 06:25:35 AM] [WARNING] DEPRECATED: falling back to /var/run/os-collect-config/os_config_files.json
2018-12-04 06:25:35,491 INFO: + ID=
2018-12-04 06:25:35,492 INFO: + '[' -n '' ']'
2018-12-04 06:25:35,492 INFO: + exit 0
2018-12-04 06:25:35,495 INFO: dib-run-parts Tue Dec  4 06:25:35 EST 2018 99-refresh-completed completed
2018-12-04 06:25:35,497 INFO: dib-run-parts Tue Dec  4 06:25:35 EST 2018 ----------------------- PROFILING -----------------------
2018-12-04 06:25:35,499 INFO: dib-run-parts Tue Dec  4 06:25:35 EST 2018
2018-12-04 06:25:35,502 INFO: dib-run-parts Tue Dec  4 06:25:35 EST 2018 Target: post-configure.d
2018-12-04 06:25:35,504 INFO: dib-run-parts Tue Dec  4 06:25:35 EST 2018
2018-12-04 06:25:35,506 INFO: dib-run-parts Tue Dec  4 06:25:35 EST 2018 Script                                     Seconds
2018-12-04 06:25:35,508 INFO: dib-run-parts Tue Dec  4 06:25:35 EST 2018 ---------------------------------------  ----------
2018-12-04 06:25:35,510 INFO: dib-run-parts Tue Dec  4 06:25:35 EST 2018
2018-12-04 06:25:35,521 INFO: dib-run-parts Tue Dec  4 06:25:35 EST 2018 10-iptables                                   0.020
2018-12-04 06:25:35,530 INFO: dib-run-parts Tue Dec  4 06:25:35 EST 2018 80-seedstack-masquerade                       0.053
2018-12-04 06:25:35,538 INFO: dib-run-parts Tue Dec  4 06:25:35 EST 2018 98-undercloud-setup                          12.243
2018-12-04 06:25:35,547 INFO: dib-run-parts Tue Dec  4 06:25:35 EST 2018 99-refresh-completed                          0.898
2018-12-04 06:25:35,550 INFO: dib-run-parts Tue Dec  4 06:25:35 EST 2018
2018-12-04 06:25:35,552 INFO: dib-run-parts Tue Dec  4 06:25:35 EST 2018 --------------------- END PROFILING ---------------------
2018-12-04 06:25:35,560 INFO: [2018-12-04 06:25:35,552] (os-refresh-config) [INFO] Completed phase post-configure
2018-12-04 06:25:35,563 INFO: os-refresh-config completed successfully
2018-12-04 06:25:35,953 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13000/ -H "Accept: application/json" -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5"
2018-12-04 06:25:35,957 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-04 06:25:35,981 DEBUG: https://172.16.0.10:13000 "GET / HTTP/1.1" 300 601
2018-12-04 06:25:35,992 DEBUG: RESP: [300] Date: Tue, 04 Dec 2018 11:25:35 GMT Server: Apache Vary: X-Auth-Token Content-Length: 601 Content-Type: application/json 
RESP BODY: {"versions": {"values": [{"status": "stable", "updated": "2018-02-28T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v3+json"}], "id": "v3.10", "links": [{"href": "https://172.16.0.10:13000/v3/", "rel": "self"}]}, {"status": "deprecated", "updated": "2016-08-04T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v2.0+json"}], "id": "v2.0", "links": [{"href": "https://172.16.0.10:13000/v2.0/", "rel": "self"}, {"href": "https://docs.openstack.org/", "type": "text/html", "rel": "describedby"}]}]}}

2018-12-04 06:25:35,993 DEBUG: Making authentication request to https://172.16.0.10:13000/v3/auth/tokens
2018-12-04 06:25:36,666 DEBUG: https://172.16.0.10:13000 "POST /v3/auth/tokens HTTP/1.1" 201 8096
2018-12-04 06:25:36,668 DEBUG: {"token": {"is_domain": false, "methods": ["password"], "roles": [{"id": "faa7ae54793f4277a142cde9530e003f", "name": "admin"}], "expires_at": "2018-12-04T15:25:36.000000Z", "project": {"domain": {"id": "default", "name": "Default"}, "id": "f41e7348daff4254892e5cd759aa8806", "name": "admin"}, "catalog": [{"endpoints": [{"url": "http://172.16.0.11:6385", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "0790be2fdc3b49d08f54f8afdd4a13f5"}, {"url": "https://172.16.0.10:13385", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "4d1975c16140453f8778e1fb9ac95da8"}, {"url": "http://172.16.0.11:6385", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "6d2c8e36ee28476ba57a9055e744d319"}], "type": "baremetal", "id": "2631f10c56d84d278c6347ffd617c17e", "name": "ironic"}, {"endpoints": [{"url": "http://172.16.0.11:8778/placement", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "17ffbfdf60bb4b1fa47d54afdfd65841"}, {"url": "http://172.16.0.11:8778/placement", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "4f70d07307cc47f7bef0a5e3b8fe3a67"}, {"url": "https://172.16.0.10:13778/placement", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "89249f96d951451f922f786665cd8ac0"}], "type": "placement", "id": "33d36ef2b525461882431a456c280ca4", "name": "placement"}, {"endpoints": [{"url": "http://172.16.0.11:35357", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "339fd082d7a348e0883ed89ccaf17e64"}, {"url": "http://172.16.0.11:5000", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "38e31d810fa946de8ccfbf1f7c3046b8"}, {"url": "https://172.16.0.10:13000", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "760586442ad141a8bb0f1ebc452fbf23"}], "type": "identity", "id": "3ecdc56d07224eac9452c019cb2faf2c", "name": "keystone"}, {"endpoints": [{"url": "http://172.16.0.11:8004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "19b04fec944847e3ae21b20f16b49cd1"}, {"url": "https://172.16.0.10:13004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "1bd9e950b8f64c758f56e16c1e416eb9"}, {"url": "http://172.16.0.11:8004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "f207a1d68f524c00be7cd409f67eb602"}], "type": "orchestration", "id": "6b802215e15547b4933f6ad11ff28780", "name": "heat"}, {"endpoints": [{"url": "http://172.16.0.11:8774/v2.1", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "04bf5d0432f24b62b7db4d32eb6e9185"}, {"url": "https://172.16.0.10:13774/v2.1", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "8e337cbbb3504e5e9f93d010f2a920f4"}, {"url": "http://172.16.0.11:8774/v2.1", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "97a0c76942474f85b0fc3128fe3d349c"}], "type": "compute", "id": "6c68f42f71d5448eb11cc7cd8a8b2b30", "name": "nova"}, {"endpoints": [{"url": "http://172.16.0.11:9696", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "8e04bd214b8f46b993b9f8b13f8ec47f"}, {"url": "http://172.16.0.11:9696", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "a133c18dcc214604a3bb74d5dc2e5c5f"}, {"url": "https://172.16.0.10:13696", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "df06914815264173860e975b90e1569c"}], "type": "network", "id": "acf2ad1a66ec4ae09be00688d95b112c", "name": "neutron"}, {"endpoints": [{"url": "http://172.16.0.11:5050", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "9d61189438cb45e3ac270c69205221ca"}, {"url": "http://172.16.0.11:5050", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "c2d37cd497924ce59d5bac0690d9e64b"}, {"url": "https://172.16.0.10:13050", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "d0b3f48230e349edafeac286fdeb8252"}], "type": "baremetal-introspection", "id": "b7b8e51747704c76888dcd514b7addf9", "name": "ironic-inspector"}, {"endpoints": [{"url": "http://172.16.0.11:8000/v1/f41e7348daff4254892e5cd759aa8806", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "848bf35e6ba8481fb799cc941e4c33de"}, {"url": "http://172.16.0.11:8000/v1/f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "a160fbb2b9a046f6902860dbf9e7900c"}, {"url": "https://172.16.0.10:13800/v1/f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "a314c5a74b3b4ca0ad7294b66e726139"}], "type": "cloudformation", "id": "cb16845f1f7c483e9775a0080437bfe9", "name": "heat-cfn"}, {"endpoints": [{"url": "http://172.16.0.11:9292", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "3561bb34989645fea114a7e5b4e443c7"}, {"url": "http://172.16.0.11:9292", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "8d54c0c6f22d43658500ee4b14d0d925"}, {"url": "https://172.16.0.10:13292", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "ee991dbd122a46aa90bb402e88d2cc12"}], "type": "image", "id": "cbe7192c36d147ceb3a550dce9ff3586", "name": "glance"}, {"endpoints": [{"url": "http://172.16.0.11:8888", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "068c25c0200a4a8aba4b6f6f6947a0f5"}, {"url": "http://172.16.0.11:8888", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "98d3307f2ce44059ac228d580f163ca1"}, {"url": "https://172.16.0.10:13888", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "a3978de44cb84351874ab38de3b38c15"}], "type": "messaging", "id": "d90c70e049d14593b7b666788ee43b6e", "name": "zaqar"}, {"endpoints": [{"url": "wss://172.16.0.10:9000", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "2eabe4530b0f41a9a78684eaffc82974"}, {"url": "ws://172.16.0.11:9000", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "300aa4234cf04b44b3de4d8604e6a75d"}, {"url": "ws://172.16.0.11:9000", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "bdaf3488dbab44a1ba43364b5d974e83"}], "type": "messaging-websocket", "id": "dc2d191d5b0e4491a88b1e372b18dbfe", "name": "zaqar-websocket"}, {"endpoints": [{"url": "http://172.16.0.11:8080", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "1e4eab20558248289100f101cfcf4e1e"}, {"url": "http://172.16.0.11:8080/v1/AUTH_f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "6ee57e5f8aec4f3fa81e3ffef1b6f261"}, {"url": "https://172.16.0.10:13808/v1/AUTH_f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "e4c21afceb3f4a8da0f2d75ba050dcdd"}], "type": "object-store", "id": "ea9a151f0e6342dbb1aeba5c885e208f", "name": "swift"}, {"endpoints": [{"url": "https://172.16.0.10:13989/v2", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "15a1660af0f2476cbda3fe163c56f368"}, {"url": "http://172.16.0.11:8989/v2", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "63fadaa6590b47a3bb91a7c789dce370"}, {"url": "http://172.16.0.11:8989/v2", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "b3b5817a37ef4565b92c2fc34b95f250"}], "type": "workflowv2", "id": "f7c8197dd44f4085baddd516aa6e7d7e", "name": "mistral"}], "user": {"domain": {"id": "default", "name": "Default"}, "password_expires_at": null, "name": "admin", "id": "b0c11cd6e04b4e08ac31e050977f22cf"}, "audit_ids": ["E9_l1m-zTxuTLQeJRPtsuw"], "issued_at": "2018-12-04T11:25:36.000000Z"}}
2018-12-04 06:25:36,711 DEBUG: found extension EntryPoint.parse('noauth = cinderclient.contrib.noauth:CinderNoAuthLoader')
2018-12-04 06:25:36,711 DEBUG: found extension EntryPoint.parse('v2token = keystoneauth1.loading._plugins.identity.v2:Token')
2018-12-04 06:25:36,712 DEBUG: found extension EntryPoint.parse('none = keystoneauth1.loading._plugins.noauth:NoAuth')
2018-12-04 06:25:36,712 DEBUG: found extension EntryPoint.parse('v3oauth1 = keystoneauth1.extras.oauth1._loading:V3OAuth1')
2018-12-04 06:25:36,712 DEBUG: found extension EntryPoint.parse('admin_token = keystoneauth1.loading._plugins.admin_token:AdminToken')
2018-12-04 06:25:36,712 DEBUG: found extension EntryPoint.parse('v3oidcauthcode = keystoneauth1.loading._plugins.identity.v3:OpenIDConnectAuthorizationCode')
2018-12-04 06:25:36,712 DEBUG: found extension EntryPoint.parse('v2password = keystoneauth1.loading._plugins.identity.v2:Password')
2018-12-04 06:25:36,712 DEBUG: found extension EntryPoint.parse('v3samlpassword = keystoneauth1.extras._saml2._loading:Saml2Password')
2018-12-04 06:25:36,712 DEBUG: found extension EntryPoint.parse('v3password = keystoneauth1.loading._plugins.identity.v3:Password')
2018-12-04 06:25:36,712 DEBUG: found extension EntryPoint.parse('v3adfspassword = keystoneauth1.extras._saml2._loading:ADFSPassword')
2018-12-04 06:25:36,712 DEBUG: found extension EntryPoint.parse('v3oidcaccesstoken = keystoneauth1.loading._plugins.identity.v3:OpenIDConnectAccessToken')
2018-12-04 06:25:36,713 DEBUG: found extension EntryPoint.parse('v3oidcpassword = keystoneauth1.loading._plugins.identity.v3:OpenIDConnectPassword')
2018-12-04 06:25:36,713 DEBUG: found extension EntryPoint.parse('v3kerberos = keystoneauth1.extras.kerberos._loading:Kerberos')
2018-12-04 06:25:36,713 DEBUG: found extension EntryPoint.parse('token = keystoneauth1.loading._plugins.identity.generic:Token')
2018-12-04 06:25:36,713 DEBUG: found extension EntryPoint.parse('v3oidcclientcredentials = keystoneauth1.loading._plugins.identity.v3:OpenIDConnectClientCredentials')
2018-12-04 06:25:36,713 DEBUG: found extension EntryPoint.parse('v3tokenlessauth = keystoneauth1.loading._plugins.identity.v3:TokenlessAuth')
2018-12-04 06:25:36,713 DEBUG: found extension EntryPoint.parse('v3token = keystoneauth1.loading._plugins.identity.v3:Token')
2018-12-04 06:25:36,713 DEBUG: found extension EntryPoint.parse('v3totp = keystoneauth1.loading._plugins.identity.v3:TOTP')
2018-12-04 06:25:36,713 DEBUG: found extension EntryPoint.parse('v3applicationcredential = keystoneauth1.loading._plugins.identity.v3:ApplicationCredential')
2018-12-04 06:25:36,713 DEBUG: found extension EntryPoint.parse('password = keystoneauth1.loading._plugins.identity.generic:Password')
2018-12-04 06:25:36,714 DEBUG: found extension EntryPoint.parse('v3fedkerb = keystoneauth1.extras.kerberos._loading:MappedKerberos')
2018-12-04 06:25:36,714 DEBUG: found extension EntryPoint.parse('v1password = swiftclient.authv1:PasswordLoader')
2018-12-04 06:25:36,714 DEBUG: found extension EntryPoint.parse('token_endpoint = openstackclient.api.auth_plugin:TokenEndpoint')
2018-12-04 06:25:36,715 DEBUG: found extension EntryPoint.parse('aodh-noauth = aodhclient.noauth:AodhNoAuthLoader')
2018-12-04 06:25:36,715 DEBUG: found extension EntryPoint.parse('gnocchi-basic = gnocchiclient.auth:GnocchiBasicLoader')
2018-12-04 06:25:36,715 DEBUG: found extension EntryPoint.parse('gnocchi-noauth = gnocchiclient.auth:GnocchiNoAuthLoader')
2018-12-04 06:25:36,728 DEBUG: Manager envvars:unknown running task network.GET.networks
2018-12-04 06:25:36,729 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13000/ -H "Accept: application/json" -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5"
2018-12-04 06:25:36,731 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-04 06:25:36,752 DEBUG: https://172.16.0.10:13000 "GET / HTTP/1.1" 300 601
2018-12-04 06:25:36,753 DEBUG: RESP: [300] Date: Tue, 04 Dec 2018 11:25:36 GMT Server: Apache Vary: X-Auth-Token Content-Length: 601 Content-Type: application/json 
RESP BODY: {"versions": {"values": [{"status": "stable", "updated": "2018-02-28T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v3+json"}], "id": "v3.10", "links": [{"href": "https://172.16.0.10:13000/v3/", "rel": "self"}]}, {"status": "deprecated", "updated": "2016-08-04T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v2.0+json"}], "id": "v2.0", "links": [{"href": "https://172.16.0.10:13000/v2.0/", "rel": "self"}, {"href": "https://docs.openstack.org/", "type": "text/html", "rel": "describedby"}]}]}}

2018-12-04 06:25:36,754 DEBUG: Making authentication request to https://172.16.0.10:13000/v3/auth/tokens
2018-12-04 06:25:37,471 DEBUG: https://172.16.0.10:13000 "POST /v3/auth/tokens HTTP/1.1" 201 8096
2018-12-04 06:25:37,472 DEBUG: {"token": {"is_domain": false, "methods": ["password"], "roles": [{"id": "faa7ae54793f4277a142cde9530e003f", "name": "admin"}], "expires_at": "2018-12-04T15:25:37.000000Z", "project": {"domain": {"id": "default", "name": "Default"}, "id": "f41e7348daff4254892e5cd759aa8806", "name": "admin"}, "catalog": [{"endpoints": [{"url": "http://172.16.0.11:6385", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "0790be2fdc3b49d08f54f8afdd4a13f5"}, {"url": "https://172.16.0.10:13385", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "4d1975c16140453f8778e1fb9ac95da8"}, {"url": "http://172.16.0.11:6385", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "6d2c8e36ee28476ba57a9055e744d319"}], "type": "baremetal", "id": "2631f10c56d84d278c6347ffd617c17e", "name": "ironic"}, {"endpoints": [{"url": "http://172.16.0.11:8778/placement", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "17ffbfdf60bb4b1fa47d54afdfd65841"}, {"url": "http://172.16.0.11:8778/placement", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "4f70d07307cc47f7bef0a5e3b8fe3a67"}, {"url": "https://172.16.0.10:13778/placement", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "89249f96d951451f922f786665cd8ac0"}], "type": "placement", "id": "33d36ef2b525461882431a456c280ca4", "name": "placement"}, {"endpoints": [{"url": "http://172.16.0.11:35357", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "339fd082d7a348e0883ed89ccaf17e64"}, {"url": "http://172.16.0.11:5000", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "38e31d810fa946de8ccfbf1f7c3046b8"}, {"url": "https://172.16.0.10:13000", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "760586442ad141a8bb0f1ebc452fbf23"}], "type": "identity", "id": "3ecdc56d07224eac9452c019cb2faf2c", "name": "keystone"}, {"endpoints": [{"url": "http://172.16.0.11:8004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "19b04fec944847e3ae21b20f16b49cd1"}, {"url": "https://172.16.0.10:13004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "1bd9e950b8f64c758f56e16c1e416eb9"}, {"url": "http://172.16.0.11:8004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "f207a1d68f524c00be7cd409f67eb602"}], "type": "orchestration", "id": "6b802215e15547b4933f6ad11ff28780", "name": "heat"}, {"endpoints": [{"url": "http://172.16.0.11:8774/v2.1", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "04bf5d0432f24b62b7db4d32eb6e9185"}, {"url": "https://172.16.0.10:13774/v2.1", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "8e337cbbb3504e5e9f93d010f2a920f4"}, {"url": "http://172.16.0.11:8774/v2.1", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "97a0c76942474f85b0fc3128fe3d349c"}], "type": "compute", "id": "6c68f42f71d5448eb11cc7cd8a8b2b30", "name": "nova"}, {"endpoints": [{"url": "http://172.16.0.11:9696", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "8e04bd214b8f46b993b9f8b13f8ec47f"}, {"url": "http://172.16.0.11:9696", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "a133c18dcc214604a3bb74d5dc2e5c5f"}, {"url": "https://172.16.0.10:13696", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "df06914815264173860e975b90e1569c"}], "type": "network", "id": "acf2ad1a66ec4ae09be00688d95b112c", "name": "neutron"}, {"endpoints": [{"url": "http://172.16.0.11:5050", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "9d61189438cb45e3ac270c69205221ca"}, {"url": "http://172.16.0.11:5050", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "c2d37cd497924ce59d5bac0690d9e64b"}, {"url": "https://172.16.0.10:13050", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "d0b3f48230e349edafeac286fdeb8252"}], "type": "baremetal-introspection", "id": "b7b8e51747704c76888dcd514b7addf9", "name": "ironic-inspector"}, {"endpoints": [{"url": "http://172.16.0.11:8000/v1/f41e7348daff4254892e5cd759aa8806", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "848bf35e6ba8481fb799cc941e4c33de"}, {"url": "http://172.16.0.11:8000/v1/f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "a160fbb2b9a046f6902860dbf9e7900c"}, {"url": "https://172.16.0.10:13800/v1/f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "a314c5a74b3b4ca0ad7294b66e726139"}], "type": "cloudformation", "id": "cb16845f1f7c483e9775a0080437bfe9", "name": "heat-cfn"}, {"endpoints": [{"url": "http://172.16.0.11:9292", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "3561bb34989645fea114a7e5b4e443c7"}, {"url": "http://172.16.0.11:9292", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "8d54c0c6f22d43658500ee4b14d0d925"}, {"url": "https://172.16.0.10:13292", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "ee991dbd122a46aa90bb402e88d2cc12"}], "type": "image", "id": "cbe7192c36d147ceb3a550dce9ff3586", "name": "glance"}, {"endpoints": [{"url": "http://172.16.0.11:8888", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "068c25c0200a4a8aba4b6f6f6947a0f5"}, {"url": "http://172.16.0.11:8888", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "98d3307f2ce44059ac228d580f163ca1"}, {"url": "https://172.16.0.10:13888", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "a3978de44cb84351874ab38de3b38c15"}], "type": "messaging", "id": "d90c70e049d14593b7b666788ee43b6e", "name": "zaqar"}, {"endpoints": [{"url": "wss://172.16.0.10:9000", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "2eabe4530b0f41a9a78684eaffc82974"}, {"url": "ws://172.16.0.11:9000", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "300aa4234cf04b44b3de4d8604e6a75d"}, {"url": "ws://172.16.0.11:9000", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "bdaf3488dbab44a1ba43364b5d974e83"}], "type": "messaging-websocket", "id": "dc2d191d5b0e4491a88b1e372b18dbfe", "name": "zaqar-websocket"}, {"endpoints": [{"url": "http://172.16.0.11:8080", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "1e4eab20558248289100f101cfcf4e1e"}, {"url": "http://172.16.0.11:8080/v1/AUTH_f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "6ee57e5f8aec4f3fa81e3ffef1b6f261"}, {"url": "https://172.16.0.10:13808/v1/AUTH_f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "e4c21afceb3f4a8da0f2d75ba050dcdd"}], "type": "object-store", "id": "ea9a151f0e6342dbb1aeba5c885e208f", "name": "swift"}, {"endpoints": [{"url": "https://172.16.0.10:13989/v2", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "15a1660af0f2476cbda3fe163c56f368"}, {"url": "http://172.16.0.11:8989/v2", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "63fadaa6590b47a3bb91a7c789dce370"}, {"url": "http://172.16.0.11:8989/v2", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "b3b5817a37ef4565b92c2fc34b95f250"}], "type": "workflowv2", "id": "f7c8197dd44f4085baddd516aa6e7d7e", "name": "mistral"}], "user": {"domain": {"id": "default", "name": "Default"}, "password_expires_at": null, "name": "admin", "id": "b0c11cd6e04b4e08ac31e050977f22cf"}, "audit_ids": ["LDn_LHshTDeD23ZBTPDeBQ"], "issued_at": "2018-12-04T11:25:37.000000Z"}}
2018-12-04 06:25:37,475 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13696 -H "Accept: application/json" -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5"
2018-12-04 06:25:37,477 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-04 06:25:37,498 DEBUG: https://172.16.0.10:13696 "GET / HTTP/1.1" 200 121
2018-12-04 06:25:37,499 DEBUG: RESP: [200] Content-Length: 121 Content-Type: application/json Date: Tue, 04 Dec 2018 11:25:37 GMT 
RESP BODY: {"versions": [{"status": "CURRENT", "id": "v2.0", "links": [{"href": "http://172.16.0.10:13696/v2.0/", "rel": "self"}]}]}

2018-12-04 06:25:37,500 DEBUG: REQ: curl -g -i -X GET "https://172.16.0.10:13696/v2.0/networks?name=ctlplane" -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}bf90ae84ecb66a1910c45844de8bccc6767144a7"
2018-12-04 06:25:39,342 DEBUG: https://172.16.0.10:13696 "GET /v2.0/networks?name=ctlplane HTTP/1.1" 200 695
2018-12-04 06:25:39,343 DEBUG: RESP: [200] Content-Type: application/json Content-Length: 695 X-Openstack-Request-Id: req-8e33dc95-06a3-401a-946c-2658857fc30c Date: Tue, 04 Dec 2018 11:25:39 GMT 
RESP BODY: {"networks":[{"provider:physical_network":"ctlplane","ipv6_address_scope":null,"revision_number":8,"port_security_enabled":true,"mtu":1500,"id":"b05efc83-67c8-42f2-a461-2961b6b2d56a","router:external":false,"availability_zone_hints":[],"availability_zones":["nova"],"ipv4_address_scope":null,"shared":false,"project_id":"f41e7348daff4254892e5cd759aa8806","l2_adjacency":true,"status":"ACTIVE","subnets":["d1642875-0c08-4152-8d6a-dc5d7c4797dd"],"description":"","tags":[],"updated_at":"2018-12-03T13:47:37Z","provider:segmentation_id":null,"name":"ctlplane","admin_state_up":true,"tenant_id":"f41e7348daff4254892e5cd759aa8806","created_at":"2018-12-03T11:18:42Z","provider:network_type":"flat"}]}

2018-12-04 06:25:39,343 DEBUG: GET call to network for https://172.16.0.10:13696/v2.0/networks?name=ctlplane used request id req-8e33dc95-06a3-401a-946c-2658857fc30c
2018-12-04 06:25:39,344 DEBUG: Manager envvars:unknown ran task network.GET.networks in 2.61480283737s
2018-12-04 06:25:39,345 INFO: Not creating ctlplane network, because it already exists.
2018-12-04 06:25:39,346 DEBUG: Manager envvars:unknown running task network.GET.subnets
2018-12-04 06:25:39,349 DEBUG: REQ: curl -g -i -X GET "https://172.16.0.10:13696/v2.0/subnets?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a&cidr=172.16.0.0%2F24" -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}bf90ae84ecb66a1910c45844de8bccc6767144a7"
2018-12-04 06:25:39,449 DEBUG: https://172.16.0.10:13696 "GET /v2.0/subnets?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a&cidr=172.16.0.0%2F24 HTTP/1.1" 200 708
2018-12-04 06:25:39,451 DEBUG: RESP: [200] Content-Type: application/json Content-Length: 708 X-Openstack-Request-Id: req-a2934a69-b0b1-45fe-8d0a-2989dc80160c Date: Tue, 04 Dec 2018 11:25:39 GMT 
RESP BODY: {"subnets":[{"updated_at":"2018-12-03T13:47:37Z","ipv6_ra_mode":null,"allocation_pools":[{"start":"172.16.0.20","end":"172.16.0.120"}],"host_routes":[{"nexthop":"172.16.0.1","destination":"169.254.169.254/32"}],"revision_number":1,"ipv6_address_mode":null,"id":"d1642875-0c08-4152-8d6a-dc5d7c4797dd","dns_nameservers":["8.8.8.8"],"gateway_ip":"172.16.0.1","project_id":"f41e7348daff4254892e5cd759aa8806","description":"","tags":[],"cidr":"172.16.0.0/24","subnetpool_id":null,"service_types":[],"name":"ctlplane-subnet","enable_dhcp":true,"segment_id":null,"network_id":"b05efc83-67c8-42f2-a461-2961b6b2d56a","tenant_id":"f41e7348daff4254892e5cd759aa8806","created_at":"2018-12-03T11:18:44Z","ip_version":4}]}

2018-12-04 06:25:39,452 DEBUG: GET call to network for https://172.16.0.10:13696/v2.0/subnets?network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a&cidr=172.16.0.0%2F24 used request id req-a2934a69-b0b1-45fe-8d0a-2989dc80160c
2018-12-04 06:25:39,452 DEBUG: Manager envvars:unknown ran task network.GET.subnets in 0.105527877808s
2018-12-04 06:25:39,455 WARNING: Local subnet ctlplane-subnet already exists and is not associated with a network segment. Any additional subnets will be ignored.
2018-12-04 06:25:39,456 DEBUG: Manager envvars:unknown running task network.PUT.subnets
2018-12-04 06:25:39,465 DEBUG: REQ: curl -g -i -X PUT https://172.16.0.10:13696/v2.0/subnets/d1642875-0c08-4152-8d6a-dc5d7c4797dd -H "User-Agent: os-client-config/1.29.0 keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "Content-Type: application/json" -H "X-Auth-Token: {SHA1}bf90ae84ecb66a1910c45844de8bccc6767144a7" -d '{"subnet": {"gateway_ip": "172.16.0.1", "allocation_pools": [{"start": "172.16.0.20", "end": "172.16.0.120"}], "host_routes": [{"nexthop": "172.16.0.1", "destination": "169.254.169.254/32"}], "name": "ctlplane-subnet"}}'
2018-12-04 06:25:40,278 DEBUG: https://172.16.0.10:13696 "PUT /v2.0/subnets/d1642875-0c08-4152-8d6a-dc5d7c4797dd HTTP/1.1" 200 705
2018-12-04 06:25:40,279 DEBUG: RESP: [200] Content-Type: application/json Content-Length: 705 X-Openstack-Request-Id: req-442e4467-ad47-4c12-8481-c305d311d20e Date: Tue, 04 Dec 2018 11:25:40 GMT 
RESP BODY: {"subnet":{"updated_at":"2018-12-04T11:25:39Z","ipv6_ra_mode":null,"allocation_pools":[{"start":"172.16.0.20","end":"172.16.0.120"}],"host_routes":[{"destination":"169.254.169.254/32","nexthop":"172.16.0.1"}],"revision_number":2,"ipv6_address_mode":null,"id":"d1642875-0c08-4152-8d6a-dc5d7c4797dd","dns_nameservers":["8.8.8.8"],"gateway_ip":"172.16.0.1","project_id":"f41e7348daff4254892e5cd759aa8806","description":"","tags":[],"cidr":"172.16.0.0/24","subnetpool_id":null,"service_types":[],"name":"ctlplane-subnet","enable_dhcp":true,"segment_id":null,"network_id":"b05efc83-67c8-42f2-a461-2961b6b2d56a","tenant_id":"f41e7348daff4254892e5cd759aa8806","created_at":"2018-12-03T11:18:44Z","ip_version":4}}

2018-12-04 06:25:40,280 DEBUG: PUT call to network for https://172.16.0.10:13696/v2.0/subnets/d1642875-0c08-4152-8d6a-dc5d7c4797dd used request id req-442e4467-ad47-4c12-8481-c305d311d20e
2018-12-04 06:25:40,280 DEBUG: Manager envvars:unknown ran task network.PUT.subnets in 0.823252916336s
2018-12-04 06:25:40,282 INFO: Subnet updated openstack.network.v2.subnet.Subnet(service_types=[], description=, enable_dhcp=True, tags=[], network_id=b05efc83-67c8-42f2-a461-2961b6b2d56a, tenant_id=f41e7348daff4254892e5cd759aa8806, created_at=2018-12-03T11:18:44Z, segment_id=None, dns_nameservers=[u'8.8.8.8'], updated_at=2018-12-04T11:25:39Z, gateway_ip=172.16.0.1, ipv6_ra_mode=None, allocation_pools=[{u'start': u'172.16.0.20', u'end': u'172.16.0.120'}], host_routes=[{u'nexthop': u'172.16.0.1', u'destination': u'169.254.169.254/32'}], revision_number=2, ip_version=4, ipv6_address_mode=None, cidr=172.16.0.0/24, id=d1642875-0c08-4152-8d6a-dc5d7c4797dd, subnetpool_id=None, name=ctlplane-subnet)
2018-12-04 06:25:40,285 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13774/v2.1/os-keypairs/default -H "User-Agent: python-novaclient" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:40,286 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-04 06:25:40,791 DEBUG: https://172.16.0.10:13774 "GET /v2.1/os-keypairs/default HTTP/1.1" 200 542
2018-12-04 06:25:40,792 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:40 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-96cc4d4b-0c1b-4392-b393-0a229103f8b3 x-compute-request-id: req-96cc4d4b-0c1b-4392-b393-0a229103f8b3 Content-Encoding: gzip Content-Length: 542 Content-Type: application/json 
RESP BODY: {"keypair": {"public_key": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsGB6ru/SfZvWgoivXrfUKZekW4lL7MCBsO221ycpqDy2GrqoJmzEkxn/Tor/6djTsjaQGWgmlXNGG9VHGGRFxAsDByK1x529bFoRwCCwwxMG/wmm5AFgvCUZISIblL2xVkN2+g5wUbnzMakaUkx+vYLY31aVmMkgh1Q+NtX48ZJ/92PcQZLMnEZOiPj7wYqPJhC/OisewdWZ8vDcD5JuEai++ET3BDxulEqitU9crtVNdURXusQ91RounhWaHTXIro0T2tKKprBbUb7R2CQQbb8TBpyH1O7+iK3j2LFDQzlS7YAunXAAQ2kr1RuiH5lSALw/5i5Gai+kD3e3kxz8h stack@undercloud.example.com", "user_id": "b0c11cd6e04b4e08ac31e050977f22cf", "name": "default", "deleted": false, "created_at": "2018-12-03T11:18:46.000000", "updated_at": null, "fingerprint": "d5:30:1f:ed:f6:c3:ab:88:14:dd:8b:a7:00:86:96:fd", "deleted_at": null, "id": 1}}

2018-12-04 06:25:40,792 DEBUG: GET call to compute for https://172.16.0.10:13774/v2.1/os-keypairs/default used request id req-96cc4d4b-0c1b-4392-b393-0a229103f8b3
2018-12-04 06:25:44,858 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13774/v2.1/flavors/detail -H "User-Agent: python-novaclient" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:46,248 DEBUG: https://172.16.0.10:13774 "GET /v2.1/flavors/detail HTTP/1.1" 200 508
2018-12-04 06:25:46,251 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:44 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-46a88106-9035-4316-a396-4dc0d6cb1055 x-compute-request-id: req-46a88106-9035-4316-a396-4dc0d6cb1055 Content-Encoding: gzip Content-Length: 508 Content-Type: application/json 
RESP BODY: {"flavors": [{"name": "baremetal", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "43e6e126-f6ef-4446-8f90-790fd4848d87"}, {"name": "swift-storage", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "4f67e9c1-a6ca-4bee-a904-94fb5a41dd73"}, {"name": "ceph-storage", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/59e72551-d212-4658-bab1-9281d25f3f50", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/59e72551-d212-4658-bab1-9281d25f3f50", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "59e72551-d212-4658-bab1-9281d25f3f50"}, {"name": "compute", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "78342552-7d94-4dd7-9b5b-ee087fdfeead"}, {"name": "control", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de"}, {"name": "block-storage", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/fd9db8be-824f-424a-9795-be74dc2cede4", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/fd9db8be-824f-424a-9795-be74dc2cede4", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "fd9db8be-824f-424a-9795-be74dc2cede4"}]}

2018-12-04 06:25:46,252 DEBUG: GET call to compute for https://172.16.0.10:13774/v2.1/flavors/detail used request id req-46a88106-9035-4316-a396-4dc0d6cb1055
2018-12-04 06:25:46,254 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13385/v1/nodes/?fields=uuid,resource_class -H "X-OpenStack-Ironic-API-Version: 1.21" -H "User-Agent: python-ironicclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:46,257 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-04 06:25:46,708 DEBUG: https://172.16.0.10:13385 "GET /v1/nodes/?fields=uuid,resource_class HTTP/1.1" 200 370
2018-12-04 06:25:46,710 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:46 GMT Server: Apache X-OpenStack-Ironic-API-Minimum-Version: 1.1 X-OpenStack-Ironic-API-Maximum-Version: 1.38 X-OpenStack-Ironic-API-Version: 1.21 Openstack-Request-Id: req-7be2d2ae-f8b8-4857-bd0c-b796cba317f6 Vary: Accept-Encoding Content-Encoding: gzip Content-Length: 370 Keep-Alive: timeout=15, max=100 Connection: Keep-Alive Content-Type: application/json 
RESP BODY: {"nodes": [{"uuid": "5950216b-2d11-4f9c-8cad-bdcd9528692a", "links": [{"href": "http://172.16.0.10:13385/v1/nodes/5950216b-2d11-4f9c-8cad-bdcd9528692a", "rel": "self"}, {"href": "http://172.16.0.10:13385/nodes/5950216b-2d11-4f9c-8cad-bdcd9528692a", "rel": "bookmark"}], "resource_class": "baremetal"}, {"uuid": "260c6ec9-cef8-4ce7-857e-6e5f36d4a7ea", "links": [{"href": "http://172.16.0.10:13385/v1/nodes/260c6ec9-cef8-4ce7-857e-6e5f36d4a7ea", "rel": "self"}, {"href": "http://172.16.0.10:13385/nodes/260c6ec9-cef8-4ce7-857e-6e5f36d4a7ea", "rel": "bookmark"}], "resource_class": "baremetal"}, {"uuid": "8d11db44-1fa7-4dc4-a91b-c81475a9ce20", "links": [{"href": "http://172.16.0.10:13385/v1/nodes/8d11db44-1fa7-4dc4-a91b-c81475a9ce20", "rel": "self"}, {"href": "http://172.16.0.10:13385/nodes/8d11db44-1fa7-4dc4-a91b-c81475a9ce20", "rel": "bookmark"}], "resource_class": "baremetal"}, {"uuid": "36f91fb2-825e-4092-abb1-8dbce56aa73f", "links": [{"href": "http://172.16.0.10:13385/v1/nodes/36f91fb2-825e-4092-abb1-8dbce56aa73f", "rel": "self"}, {"href": "http://172.16.0.10:13385/nodes/36f91fb2-825e-4092-abb1-8dbce56aa73f", "rel": "bookmark"}], "resource_class": "baremetal"}, {"uuid": "929fdd71-8bac-4895-9dbe-7046a2c6a713", "links": [{"href": "http://172.16.0.10:13385/v1/nodes/929fdd71-8bac-4895-9dbe-7046a2c6a713", "rel": "self"}, {"href": "http://172.16.0.10:13385/nodes/929fdd71-8bac-4895-9dbe-7046a2c6a713", "rel": "bookmark"}], "resource_class": "baremetal"}, {"uuid": "740231b7-e29e-479a-b1de-8345d3505db5", "links": [{"href": "http://172.16.0.10:13385/v1/nodes/740231b7-e29e-479a-b1de-8345d3505db5", "rel": "self"}, {"href": "http://172.16.0.10:13385/nodes/740231b7-e29e-479a-b1de-8345d3505db5", "rel": "bookmark"}], "resource_class": "baremetal"}, {"uuid": "d618f3f9-b3de-45f7-b458-69af658a81c5", "links": [{"href": "http://172.16.0.10:13385/v1/nodes/d618f3f9-b3de-45f7-b458-69af658a81c5", "rel": "self"}, {"href": "http://172.16.0.10:13385/nodes/d618f3f9-b3de-45f7-b458-69af658a81c5", "rel": "bookmark"}], "resource_class": "baremetal"}]}

2018-12-04 06:25:46,712 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13774/v2.1/flavors/detail -H "User-Agent: python-novaclient" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:46,769 DEBUG: https://172.16.0.10:13774 "GET /v2.1/flavors/detail HTTP/1.1" 200 508
2018-12-04 06:25:46,771 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:46 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-c02d1328-cd7a-4dc5-8bbd-498a4985940b x-compute-request-id: req-c02d1328-cd7a-4dc5-8bbd-498a4985940b Content-Encoding: gzip Content-Length: 508 Content-Type: application/json 
RESP BODY: {"flavors": [{"name": "baremetal", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "43e6e126-f6ef-4446-8f90-790fd4848d87"}, {"name": "swift-storage", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "4f67e9c1-a6ca-4bee-a904-94fb5a41dd73"}, {"name": "ceph-storage", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/59e72551-d212-4658-bab1-9281d25f3f50", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/59e72551-d212-4658-bab1-9281d25f3f50", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "59e72551-d212-4658-bab1-9281d25f3f50"}, {"name": "compute", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "78342552-7d94-4dd7-9b5b-ee087fdfeead"}, {"name": "control", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de"}, {"name": "block-storage", "links": [{"href": "https://172.16.0.10:13774/v2.1/flavors/fd9db8be-824f-424a-9795-be74dc2cede4", "rel": "self"}, {"href": "https://172.16.0.10:13774/flavors/fd9db8be-824f-424a-9795-be74dc2cede4", "rel": "bookmark"}], "ram": 4096, "OS-FLV-DISABLED:disabled": false, "vcpus": 1, "swap": "", "os-flavor-access:is_public": true, "rxtx_factor": 1.0, "OS-FLV-EXT-DATA:ephemeral": 0, "disk": 40, "id": "fd9db8be-824f-424a-9795-be74dc2cede4"}]}

2018-12-04 06:25:46,771 DEBUG: GET call to compute for https://172.16.0.10:13774/v2.1/flavors/detail used request id req-c02d1328-cd7a-4dc5-8bbd-498a4985940b
2018-12-04 06:25:46,772 INFO: Not creating flavor "baremetal" because it already exists.
2018-12-04 06:25:46,774 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13774/v2.1/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87/os-extra_specs -H "User-Agent: python-novaclient" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:46,804 DEBUG: https://172.16.0.10:13774 "GET /v2.1/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87/os-extra_specs HTTP/1.1" 200 134
2018-12-04 06:25:46,805 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:46 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-95fbfa04-838d-4497-bcb6-f4cef92913f6 x-compute-request-id: req-95fbfa04-838d-4497-bcb6-f4cef92913f6 Content-Encoding: gzip Content-Length: 134 Content-Type: application/json 
RESP BODY: {"extra_specs": {"capabilities:boot_option": "local", "resources:CUSTOM_BAREMETAL": "1", "resources:MEMORY_MB": "0", "resources:VCPU": "0", "resources:DISK_GB": "0"}}

2018-12-04 06:25:46,805 DEBUG: GET call to compute for https://172.16.0.10:13774/v2.1/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87/os-extra_specs used request id req-95fbfa04-838d-4497-bcb6-f4cef92913f6
2018-12-04 06:25:46,807 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87/os-extra_specs -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '{"extra_specs": {"capabilities:boot_option": "local", "resources:MEMORY_MB": "0", "resources:VCPU": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:DISK_GB": "0"}}'
2018-12-04 06:25:46,880 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87/os-extra_specs HTTP/1.1" 200 134
2018-12-04 06:25:46,881 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:46 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-a53546f6-5e16-4d94-b1b0-d77873c67a62 x-compute-request-id: req-a53546f6-5e16-4d94-b1b0-d77873c67a62 Content-Encoding: gzip Content-Length: 134 Content-Type: application/json 
RESP BODY: {"extra_specs": {"capabilities:boot_option": "local", "resources:VCPU": "0", "resources:MEMORY_MB": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:DISK_GB": "0"}}

2018-12-04 06:25:46,882 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors/43e6e126-f6ef-4446-8f90-790fd4848d87/os-extra_specs used request id req-a53546f6-5e16-4d94-b1b0-d77873c67a62
2018-12-04 06:25:46,882 INFO: Flavor baremetal updated to use custom resource class baremetal
2018-12-04 06:25:46,882 INFO: Not creating flavor "control" because it already exists.
2018-12-04 06:25:46,884 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13774/v2.1/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de/os-extra_specs -H "User-Agent: python-novaclient" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:46,912 DEBUG: https://172.16.0.10:13774 "GET /v2.1/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de/os-extra_specs HTTP/1.1" 200 149
2018-12-04 06:25:46,914 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:46 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-8a6a1314-b910-49c5-ae3d-12682740404b x-compute-request-id: req-8a6a1314-b910-49c5-ae3d-12682740404b Content-Encoding: gzip Content-Length: 149 Content-Type: application/json 
RESP BODY: {"extra_specs": {"resources:CUSTOM_BAREMETAL": "1", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "capabilities:boot_option": "local", "resources:VCPU": "0", "capabilities:profile": "control"}}

2018-12-04 06:25:46,914 DEBUG: GET call to compute for https://172.16.0.10:13774/v2.1/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de/os-extra_specs used request id req-8a6a1314-b910-49c5-ae3d-12682740404b
2018-12-04 06:25:46,916 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de/os-extra_specs -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '{"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "control", "resources:MEMORY_MB": "0", "resources:VCPU": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:DISK_GB": "0"}}'
2018-12-04 06:25:46,978 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de/os-extra_specs HTTP/1.1" 200 149
2018-12-04 06:25:46,979 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:46 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-818245e4-3152-4a0c-9115-40add7a9da0e x-compute-request-id: req-818245e4-3152-4a0c-9115-40add7a9da0e Content-Encoding: gzip Content-Length: 149 Content-Type: application/json 
RESP BODY: {"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "control", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:VCPU": "0"}}

2018-12-04 06:25:46,979 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors/b0bdc7ad-6eb5-4ebd-ba4d-fb9d993fb7de/os-extra_specs used request id req-818245e4-3152-4a0c-9115-40add7a9da0e
2018-12-04 06:25:46,980 INFO: Flavor control updated to use custom resource class baremetal
2018-12-04 06:25:46,980 INFO: Not creating flavor "compute" because it already exists.
2018-12-04 06:25:46,982 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13774/v2.1/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead/os-extra_specs -H "User-Agent: python-novaclient" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:47,012 DEBUG: https://172.16.0.10:13774 "GET /v2.1/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead/os-extra_specs HTTP/1.1" 200 149
2018-12-04 06:25:47,013 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:46 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-d7186e00-3a41-42e5-b480-566078a30941 x-compute-request-id: req-d7186e00-3a41-42e5-b480-566078a30941 Content-Encoding: gzip Content-Length: 149 Content-Type: application/json 
RESP BODY: {"extra_specs": {"resources:CUSTOM_BAREMETAL": "1", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "capabilities:boot_option": "local", "resources:VCPU": "0", "capabilities:profile": "compute"}}

2018-12-04 06:25:47,013 DEBUG: GET call to compute for https://172.16.0.10:13774/v2.1/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead/os-extra_specs used request id req-d7186e00-3a41-42e5-b480-566078a30941
2018-12-04 06:25:47,015 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead/os-extra_specs -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '{"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "compute", "resources:MEMORY_MB": "0", "resources:VCPU": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:DISK_GB": "0"}}'
2018-12-04 06:25:47,087 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead/os-extra_specs HTTP/1.1" 200 150
2018-12-04 06:25:47,089 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-f1687830-222e-4a65-a124-d6dc79e51d9d x-compute-request-id: req-f1687830-222e-4a65-a124-d6dc79e51d9d Content-Encoding: gzip Content-Length: 150 Content-Type: application/json 
RESP BODY: {"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "compute", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:VCPU": "0"}}

2018-12-04 06:25:47,089 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors/78342552-7d94-4dd7-9b5b-ee087fdfeead/os-extra_specs used request id req-f1687830-222e-4a65-a124-d6dc79e51d9d
2018-12-04 06:25:47,090 INFO: Flavor compute updated to use custom resource class baremetal
2018-12-04 06:25:47,090 INFO: Not creating flavor "ceph-storage" because it already exists.
2018-12-04 06:25:47,093 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13774/v2.1/flavors/59e72551-d212-4658-bab1-9281d25f3f50/os-extra_specs -H "User-Agent: python-novaclient" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:47,126 DEBUG: https://172.16.0.10:13774 "GET /v2.1/flavors/59e72551-d212-4658-bab1-9281d25f3f50/os-extra_specs HTTP/1.1" 200 153
2018-12-04 06:25:47,127 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-5b5a31a7-b6ea-4859-b3fe-dec95900a64c x-compute-request-id: req-5b5a31a7-b6ea-4859-b3fe-dec95900a64c Content-Encoding: gzip Content-Length: 153 Content-Type: application/json 
RESP BODY: {"extra_specs": {"resources:CUSTOM_BAREMETAL": "1", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "capabilities:boot_option": "local", "resources:VCPU": "0", "capabilities:profile": "ceph-storage"}}

2018-12-04 06:25:47,128 DEBUG: GET call to compute for https://172.16.0.10:13774/v2.1/flavors/59e72551-d212-4658-bab1-9281d25f3f50/os-extra_specs used request id req-5b5a31a7-b6ea-4859-b3fe-dec95900a64c
2018-12-04 06:25:47,130 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors/59e72551-d212-4658-bab1-9281d25f3f50/os-extra_specs -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '{"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "ceph-storage", "resources:MEMORY_MB": "0", "resources:VCPU": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:DISK_GB": "0"}}'
2018-12-04 06:25:47,194 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors/59e72551-d212-4658-bab1-9281d25f3f50/os-extra_specs HTTP/1.1" 200 153
2018-12-04 06:25:47,196 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-ba39c7aa-077d-401f-ac7f-b02773a5c204 x-compute-request-id: req-ba39c7aa-077d-401f-ac7f-b02773a5c204 Content-Encoding: gzip Content-Length: 153 Content-Type: application/json 
RESP BODY: {"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "ceph-storage", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:VCPU": "0"}}

2018-12-04 06:25:47,196 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors/59e72551-d212-4658-bab1-9281d25f3f50/os-extra_specs used request id req-ba39c7aa-077d-401f-ac7f-b02773a5c204
2018-12-04 06:25:47,196 INFO: Flavor ceph-storage updated to use custom resource class baremetal
2018-12-04 06:25:47,196 INFO: Not creating flavor "block-storage" because it already exists.
2018-12-04 06:25:47,199 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13774/v2.1/flavors/fd9db8be-824f-424a-9795-be74dc2cede4/os-extra_specs -H "User-Agent: python-novaclient" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:47,236 DEBUG: https://172.16.0.10:13774 "GET /v2.1/flavors/fd9db8be-824f-424a-9795-be74dc2cede4/os-extra_specs HTTP/1.1" 200 153
2018-12-04 06:25:47,237 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-450c807a-2660-495f-96d8-a610c6cb4aff x-compute-request-id: req-450c807a-2660-495f-96d8-a610c6cb4aff Content-Encoding: gzip Content-Length: 153 Content-Type: application/json 
RESP BODY: {"extra_specs": {"resources:CUSTOM_BAREMETAL": "1", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "capabilities:boot_option": "local", "resources:VCPU": "0", "capabilities:profile": "block-storage"}}

2018-12-04 06:25:47,237 DEBUG: GET call to compute for https://172.16.0.10:13774/v2.1/flavors/fd9db8be-824f-424a-9795-be74dc2cede4/os-extra_specs used request id req-450c807a-2660-495f-96d8-a610c6cb4aff
2018-12-04 06:25:47,239 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors/fd9db8be-824f-424a-9795-be74dc2cede4/os-extra_specs -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '{"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "block-storage", "resources:MEMORY_MB": "0", "resources:VCPU": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:DISK_GB": "0"}}'
2018-12-04 06:25:47,302 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors/fd9db8be-824f-424a-9795-be74dc2cede4/os-extra_specs HTTP/1.1" 200 154
2018-12-04 06:25:47,303 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-03e8dc60-6b4f-4782-bea3-b1ee60b59581 x-compute-request-id: req-03e8dc60-6b4f-4782-bea3-b1ee60b59581 Content-Encoding: gzip Content-Length: 154 Content-Type: application/json 
RESP BODY: {"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "block-storage", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:VCPU": "0"}}

2018-12-04 06:25:47,304 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors/fd9db8be-824f-424a-9795-be74dc2cede4/os-extra_specs used request id req-03e8dc60-6b4f-4782-bea3-b1ee60b59581
2018-12-04 06:25:47,304 INFO: Flavor block-storage updated to use custom resource class baremetal
2018-12-04 06:25:47,304 INFO: Not creating flavor "swift-storage" because it already exists.
2018-12-04 06:25:47,306 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13774/v2.1/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73/os-extra_specs -H "User-Agent: python-novaclient" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:47,337 DEBUG: https://172.16.0.10:13774 "GET /v2.1/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73/os-extra_specs HTTP/1.1" 200 154
2018-12-04 06:25:47,338 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-1f10d029-6793-4d29-a49b-4cfafd17882a x-compute-request-id: req-1f10d029-6793-4d29-a49b-4cfafd17882a Content-Encoding: gzip Content-Length: 154 Content-Type: application/json 
RESP BODY: {"extra_specs": {"resources:CUSTOM_BAREMETAL": "1", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "capabilities:boot_option": "local", "resources:VCPU": "0", "capabilities:profile": "swift-storage"}}

2018-12-04 06:25:47,339 DEBUG: GET call to compute for https://172.16.0.10:13774/v2.1/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73/os-extra_specs used request id req-1f10d029-6793-4d29-a49b-4cfafd17882a
2018-12-04 06:25:47,341 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13774/v2.1/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73/os-extra_specs -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '{"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "swift-storage", "resources:MEMORY_MB": "0", "resources:VCPU": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:DISK_GB": "0"}}'
2018-12-04 06:25:47,402 DEBUG: https://172.16.0.10:13774 "POST /v2.1/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73/os-extra_specs HTTP/1.1" 200 154
2018-12-04 06:25:47,403 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:25:47 GMT Server: Apache OpenStack-API-Version: compute 2.1 X-OpenStack-Nova-API-Version: 2.1 Vary: OpenStack-API-Version,X-OpenStack-Nova-API-Version,Accept-Encoding x-openstack-request-id: req-03070a84-6407-41fe-a864-e9a2f2be2200 x-compute-request-id: req-03070a84-6407-41fe-a864-e9a2f2be2200 Content-Encoding: gzip Content-Length: 154 Content-Type: application/json 
RESP BODY: {"extra_specs": {"capabilities:boot_option": "local", "capabilities:profile": "swift-storage", "resources:MEMORY_MB": "0", "resources:DISK_GB": "0", "resources:CUSTOM_BAREMETAL": "1", "resources:VCPU": "0"}}

2018-12-04 06:25:47,404 DEBUG: POST call to compute for https://172.16.0.10:13774/v2.1/flavors/4f67e9c1-a6ca-4bee-a904-94fb5a41dd73/os-extra_specs used request id req-03070a84-6407-41fe-a864-e9a2f2be2200
2018-12-04 06:25:47,404 INFO: Flavor swift-storage updated to use custom resource class baremetal
2018-12-04 06:25:47,404 INFO: Configuring Mistral workbooks
2018-12-04 06:25:47,405 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:47,406 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-04 06:25:48,702 DEBUG: https://172.16.0.10:13989 "GET /v2/workbooks HTTP/1.1" 200 255426
2018-12-04 06:25:48,720 DEBUG: RESP: [200] Content-Length: 255426 Content-Type: application/json Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: {"workbooks": [{"definition": "---\nversion: '2.0'\nname: tripleo.access.v1\ndescription: TripleO administration access workflows\n\nworkflows:\n\n  enable_ssh_admin:\n    description: >-\n      This workflow creates an admin user on the overcloud nodes,\n      which can then be used for connecting for automated\n      administrative or deployment tasks, e.g. via Ansible. The\n      workflow can be used both for Nova-managed and split-stack\n      deployments, assuming the correct input values are passed\n      in. The workflow defaults to Nova-managed approach, for which no\n      additional parameters need to be supplied. In case of\n      split-stack, temporary ssh connection details (user, key, list\n      of servers) need to be provided -- these are only used\n      temporarily to create the actual ssh admin user for use by\n      Mistral.\n    tags:\n      - tripleo-common-managed\n    input:\n      - ssh_private_key: null\n      - ssh_user: null\n      - ssh_servers: []\n      - overcloud_admin: tripleo-admin\n      - queue_name: tripleo\n    tasks:\n      get_pubkey:\n        action: tripleo.validations.get_pubkey\n        on-success: generate_playbook\n        publish:\n          pubkey: <% task().result %>\n\n      generate_playbook:\n        on-success:\n          - create_admin_via_nova: <% $.ssh_private_key = null %>\n          - create_admin_via_ssh: <% $.ssh_private_key != null %>\n        publish:\n          create_admin_tasks:\n            - name: create user <% $.overcloud_admin %>\n              user:\n                name: '<% $.overcloud_admin %>'\n            - name: grant admin rights to user <% $.overcloud_admin %>\n              copy:\n                dest: /etc/sudoers.d/<% $.overcloud_admin %>\n                content: |\n                  <% $.overcloud_admin %> ALL=(ALL) NOPASSWD:ALL\n                mode: 0440\n            - name: ensure .ssh dir exists for user <% $.overcloud_admin %>\n              file:\n                path: /home/<% $.overcloud_admin %>/.ssh\n                state: directory\n                owner: <% $.overcloud_admin %>\n                group: <% $.overcloud_admin %>\n                mode: 0700\n            - name: ensure authorized_keys file exists for user <% $.overcloud_admin %>\n              file:\n                path: /home/<% $.overcloud_admin %>/.ssh/authorized_keys\n                state: touch\n                owner: <% $.overcloud_admin %>\n                group: <% $.overcloud_admin %>\n                mode: 0700\n            - name: authorize TripleO Mistral key for user <% $.overcloud_admin %>\n              lineinfile:\n                path: /home/<% $.overcloud_admin %>/.ssh/authorized_keys\n                line: <% $.pubkey %>\n                regexp: \"Generated by TripleO\"\n\n      # Nova variant\n      create_admin_via_nova:\n        workflow: tripleo.access.v1.create_admin_via_nova\n        input:\n          queue_name: <% $.queue_name %>\n          ssh_servers: <% $.ssh_servers %>\n          tasks: <% $.create_admin_tasks %>\n          overcloud_admin: <% $.overcloud_admin %>\n\n      # SSH variant\n      create_admin_via_ssh:\n        workflow: tripleo.access.v1.create_admin_via_ssh\n        input:\n          ssh_private_key: <% $.ssh_private_key %>\n          ssh_user: <% $.ssh_user %>\n          ssh_servers: <% $.ssh_servers %>\n          tasks: <% $.create_admin_tasks %>\n\n  create_admin_via_nova:\n    input:\n      - tasks\n      - queue_name: tripleo\n      - ssh_servers: []\n      - overcloud_admin: tripleo-admin\n      - ansible_extra_env_variables:\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n    tags:\n      - tripleo-common-managed\n    tasks:\n      get_servers:\n        action: nova.servers_list\n        on-success: create_admin\n        publish:\n          servers: <% let(root => $) -> task().result._info.where($.addresses.ctlplane.addr.any($ in $root.ssh_servers)) %>\n\n      create_admin:\n        workflow: tripleo.deployment.v1.deploy_on_server\n        on-success: get_privkey\n        with-items: server in <% $.servers %>\n        input:\n          server_name: <% $.server.name %>\n          server_uuid: <% $.server.id %>\n          queue_name: <% $.queue_name %>\n          config_name: create_admin\n          group: ansible\n          config: |\n            - hosts: localhost\n              connection: local\n              tasks: <% json_pp($.tasks) %>\n\n      get_privkey:\n        action: tripleo.validations.get_privkey\n        on-success: wait_for_occ\n        publish:\n          privkey: <% task().result %>\n\n      wait_for_occ:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            overcloud:\n              hosts: <% $.ssh_servers.toDict($, {}) %>\n          remote_user: <% $.overcloud_admin %>\n          ssh_private_key: <% $.privkey %>\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          playbook:\n            - hosts: overcloud\n              gather_facts: no\n              tasks:\n                - name: wait for connection\n                  wait_for_connection:\n                    sleep: 5\n                    timeout: 300\n\n  create_admin_via_ssh:\n    input:\n      - tasks\n      - ssh_private_key\n      - ssh_user\n      - ssh_servers\n      - ansible_extra_env_variables:\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n\n    tags:\n      - tripleo-common-managed\n    tasks:\n      write_tmp_playbook:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            overcloud:\n              hosts: <% $.ssh_servers.toDict($, {}) %>\n          remote_user: <% $.ssh_user %>\n          ssh_private_key: <% $.ssh_private_key %>\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          become: true\n          become_user: root\n          playbook:\n            - hosts: overcloud\n              tasks: <% $.tasks %>\n", "name": "tripleo.access.v1", "tags": [], "created_at": "2018-12-03 11:18:49", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "a834aa44-b2b5-4f7b-82c7-8a4b8ed1819c"}, {"definition": "---\nversion: '2.0'\nname: tripleo.baremetal.v1\ndescription: TripleO Baremetal Workflows\n\nworkflows:\n\n  set_node_state:\n    input:\n      - node_uuid\n      - state_action\n      - target_state\n      - error_states:\n          # The default includes all failure states, even unused by TripleO.\n          - 'error'\n          - 'adopt failed'\n          - 'clean failed'\n          - 'deploy failed'\n          - 'inspect failed'\n          - 'rescue failed'\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_provision_state:\n        on-success: wait_for_provision_state\n        on-error: set_provision_state_failed\n        action: ironic.node_set_provision_state node_uuid=<% $.node_uuid %> state=<% $.state_action %>\n\n      set_provision_state_failed:\n        publish:\n          message: <% task(set_provision_state).result %>\n        on-complete: fail\n\n      wait_for_provision_state:\n        action: ironic.node_get\n        input:\n          node_id: <% $.node_uuid %>\n          fields: ['provision_state', 'last_error']\n        timeout: 1200 #20 minutes\n        retry:\n          delay: 3\n          count: 400\n          continue-on: <% not task().result.provision_state in [$.target_state] + $.error_states %>\n        on-complete:\n          - state_not_reached: <% task().result.provision_state != $.target_state %>\n\n      state_not_reached:\n        publish:\n          message: >-\n            Node <% $.node_uuid %> did not reach state \"<% $.target_state %>\",\n            the state is \"<% task(wait_for_provision_state).result.provision_state %>\",\n            error: <% task(wait_for_provision_state).result.last_error %>\n        on-complete: fail\n\n    output-on-error:\n      result: <% $.message %>\n\n  set_power_state:\n    input:\n      - node_uuid\n      - state_action\n      - target_state\n      - error_state: 'error'\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_power_state:\n        on-success: wait_for_power_state\n        on-error: set_power_state_failed\n        action: ironic.node_set_power_state node_id=<% $.node_uuid %> state=<% $.state_action %>\n\n      set_power_state_failed:\n        publish:\n          message: <% task(set_power_state).result %>\n        on-complete: fail\n\n      wait_for_power_state:\n        action: ironic.node_get\n        input:\n          node_id: <% $.node_uuid %>\n          fields: ['power_state', 'last_error']\n        timeout: 120 #2 minutes\n        retry:\n          delay: 6\n          count: 20\n          continue-on: <% not task().result.power_state in [$.target_state, $.error_state] %>\n        on-complete:\n          - state_not_reached: <% task().result.power_state != $.target_state %>\n\n      state_not_reached:\n        publish:\n          message: >-\n            Node <% $.node_uuid %> did not reach power state \"<% $.target_state %>\",\n            the state is \"<% task(wait_for_power_state).result.power_state %>\",\n            error: <% task(wait_for_power_state).result.last_error %>\n        on-complete: fail\n\n    output-on-error:\n      result: <% $.message %>\n\n  manual_cleaning:\n    input:\n      - node_uuid\n      - clean_steps\n      - timeout: 7200  # 2 hours (cleaning can take really long)\n      - retry_delay: 10\n      - retry_count: 720\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_provision_state:\n        on-success: wait_for_provision_state\n        on-error: set_provision_state_failed\n        action: ironic.node_set_provision_state node_uuid=<% $.node_uuid %> state='clean' cleansteps=<% $.clean_steps %>\n\n      set_provision_state_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_provision_state).result %>\n\n      wait_for_provision_state:\n        on-success: send_message\n        action: ironic.node_get node_id=<% $.node_uuid %>\n        timeout: <% $.timeout %>\n        retry:\n          delay: <% $.retry_delay %>\n          count: <% $.retry_count %>\n          continue-on: <% task().result.provision_state != 'manageable' %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.manual_cleaning\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_nodes:\n    description: Validate nodes JSON\n\n    input:\n     - nodes_json\n     - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      validate_nodes:\n        action: tripleo.baremetal.validate_nodes\n        on-success: send_message\n        on-error: validation_failed\n        input:\n           nodes_json: <% $.nodes_json %>\n\n      validation_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(validate_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.validate_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  register_or_update:\n    description: Take nodes JSON and create nodes in a \"manageable\" state\n\n    input:\n     - nodes_json\n     - remove: False\n     - queue_name: tripleo\n     - kernel_name: null\n     - ramdisk_name: null\n     - instance_boot_option: local\n     - initial_state: manageable\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      validate_input:\n        workflow: tripleo.baremetal.v1.validate_nodes\n        on-success: register_or_update_nodes\n        on-error: validation_failed\n        input:\n          nodes_json: <% $.nodes_json %>\n          queue_name: <% $.queue_name %>\n\n      validation_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(validate_input).result %>\n          registered_nodes: []\n\n      register_or_update_nodes:\n        action: tripleo.baremetal.register_or_update_nodes\n        on-success:\n          - set_nodes_managed: <% $.initial_state != \"enroll\" %>\n          - send_message: <% $.initial_state = \"enroll\" %>\n        on-error: set_status_failed_register_or_update_nodes\n        input:\n           nodes_json: <% $.nodes_json %>\n           remove: <% $.remove %>\n           kernel_name: <% $.kernel_name %>\n           ramdisk_name: <% $.ramdisk_name %>\n           instance_boot_option: <% $.instance_boot_option %>\n        publish:\n          registered_nodes: <% task().result %>\n          new_nodes: <% task().result.where($.provision_state = 'enroll') %>\n\n      set_status_failed_register_or_update_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(register_or_update_nodes).result %>\n          registered_nodes: []\n\n      set_nodes_managed:\n        on-success:\n          - set_nodes_available: <% $.initial_state = \"available\" %>\n          - send_message: <% $.initial_state != \"available\" %>\n        on-error: set_status_failed_nodes_managed\n        workflow: tripleo.baremetal.v1.manage\n        input:\n          node_uuids: <% $.new_nodes.uuid %>\n          queue_name: <% $.queue_name %>\n        publish:\n          status: SUCCESS\n          message: <% $.new_nodes.len() %> node(s) successfully moved to the \"manageable\" state.\n\n      set_status_failed_nodes_managed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_nodes_managed).result %>\n\n      set_nodes_available:\n        on-success: send_message\n        on-error: set_status_failed_nodes_available\n        workflow: tripleo.baremetal.v1.provide node_uuids=<% $.new_nodes.uuid %> queue_name=<% $.queue_name %>\n        publish:\n          status: SUCCESS\n          message: <% $.new_nodes.len() %> node(s) successfully moved to the \"available\" state.\n\n      set_status_failed_nodes_available:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_nodes_available).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.register_or_update\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                registered_nodes: <% $.registered_nodes or [] %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  provide:\n    description: Take a list of nodes and move them to \"available\"\n\n    input:\n      - node_uuids\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_nodes_available:\n        on-success: cell_v2_discover_hosts\n        on-error: set_status_failed_nodes_available\n        with-items: uuid in <% $.node_uuids %>\n        workflow: tripleo.baremetal.v1.set_node_state\n        input:\n          node_uuid: <% $.uuid %>\n          queue_name: <% $.queue_name %>\n          state_action: 'provide'\n          target_state: 'available'\n\n      set_status_failed_nodes_available:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_nodes_available).result %>\n\n      cell_v2_discover_hosts:\n        on-success: try_power_off\n        on-error: cell_v2_discover_hosts_failed\n        workflow: tripleo.baremetal.v1.cellv2_discovery\n        input:\n          node_uuids: <% $.node_uuids %>\n          queue_name: <% $.queue_name %>\n        timeout: 900 #15 minutes\n        retry:\n          delay: 30\n          count: 30\n\n      cell_v2_discover_hosts_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(cell_v2_discover_hosts).result %>\n\n      try_power_off:\n        on-success: send_message\n        on-error: power_off_failed\n        with-items: uuid in <% $.node_uuids %>\n        workflow: tripleo.baremetal.v1.set_power_state\n        input:\n          node_uuid: <% $.uuid %>\n          queue_name: <% $.queue_name %>\n          state_action: 'off'\n          target_state: 'power off'\n        publish:\n          status: SUCCESS\n          message: <% $.node_uuids.len() %> node(s) successfully moved to the \"available\" state.\n\n      power_off_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(try_power_off).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.provide\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  provide_manageable_nodes:\n    description: Provide all nodes in a 'manageable' state.\n\n    input:\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_manageable_nodes:\n        action: ironic.node_list maintenance=False associated=False\n        on-success: provide_manageable\n        on-error: set_status_failed_get_manageable_nodes\n        publish:\n          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>\n\n      set_status_failed_get_manageable_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_manageable_nodes).result %>\n\n      provide_manageable:\n        on-success: send_message\n        workflow: tripleo.baremetal.v1.provide\n        input:\n          node_uuids: <% $.managed_nodes %>\n          queue_name: <% $.queue_name %>\n        publish:\n          status: SUCCESS\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.provide_manageable_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  manage:\n    description: Set a list of nodes to 'manageable' state\n\n    input:\n      - node_uuids\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_nodes_manageable:\n        on-success: send_message\n        on-error: set_status_failed_nodes_manageable\n        with-items: uuid in <% $.node_uuids %>\n        workflow: tripleo.baremetal.v1.set_node_state\n        input:\n          node_uuid: <% $.uuid %>\n          state_action: 'manage'\n          target_state: 'manageable'\n          error_states:\n            # node going back to enroll designates power credentials failure\n            - 'enroll'\n            - 'error'\n\n      set_status_failed_nodes_manageable:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_nodes_manageable).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.manage\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  _introspect:\n    description: >\n        An internal workflow. The tripleo.baremetal.v1.introspect workflow\n        should be used for introspection.\n\n    input:\n      - node_uuid\n      - timeout\n      - queue_name\n\n    output:\n      result: <% task(start_introspection).result %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      start_introspection:\n        action: baremetal_introspection.introspect uuid=<% $.node_uuid %>\n        on-success: wait_for_introspection_to_finish\n        on-error: set_status_failed_start_introspection\n\n      set_status_failed_start_introspection:\n        publish:\n          status: FAILED\n          message: <% task(start_introspection).result %>\n          introspected_nodes: []\n        on-success: send_message\n\n      wait_for_introspection_to_finish:\n        action: baremetal_introspection.wait_for_finish\n        input:\n          uuids: <% [$.node_uuid] %>\n          # The interval is 10 seconds, so divide to make the overall timeout\n          # in seconds correct.\n          max_retries: <% $.timeout / 10 %>\n          retry_interval: 10\n        publish:\n          introspected_node: <% task().result.values().first() %>\n          status: <% bool(task().result.values().first().error) and \"FAILED\" or \"SUCCESS\" %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-success: wait_for_introspection_to_finish_success\n        on-error: wait_for_introspection_to_finish_error\n\n      wait_for_introspection_to_finish_success:\n        publish:\n          message: <% \"Introspection of node {0} completed. Status:{1}. Errors:{2}\".format($.introspected_node.uuid, $.status, $.introspected_node.error) %>\n        on-success: send_message\n\n      wait_for_introspection_to_finish_error:\n        publish:\n          message: <% \"Introspection of node {0} timed out.\".format($.node_uuid) %>\n        on-success: send_message\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1._introspect\n              payload:\n                status: <% $.status %>\n                message: <% $.message %>\n                introspected_node: <% $.get('introspected_node') %>\n                node_uuid: <% $.node_uuid %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  introspect:\n    description: >\n        Take a list of nodes and move them through introspection.\n\n        By default each node will attempt introspection up to 3 times (two\n        retries plus the initial attemp) if it fails. This behaviour can be\n        modified by changing the max_retry_attempts input.\n\n        The workflow will assume the node has timed out after 20 minutes (1200\n        seconds). This can be changed by passing the node_timeout input in\n        seconds.\n\n    input:\n      - node_uuids\n      - run_validations: False\n      - queue_name: tripleo\n      - concurrency: 20\n      - max_retry_attempts: 2\n      - node_timeout: 1200\n\n    tags:\n      - tripleo-common-managed\n\n    task-defaults:\n      on-error: unhandled_error\n\n    tasks:\n      initialize:\n        publish:\n          introspection_attempt: 1\n        on-complete:\n          - run_validations: <% $.run_validations %>\n          - introspect_nodes: <% not $.run_validations %>\n\n      run_validations:\n        workflow: tripleo.validations.v1.run_groups\n        input:\n          group_names:\n            - 'pre-introspection'\n          queue_name: <% $.queue_name %>\n        on-success: introspect_nodes\n        on-error: set_validations_failed\n\n      set_validations_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(run_validations).result %>\n\n      introspect_nodes:\n        with-items: uuid in <% $.node_uuids %>\n        concurrency: <% $.concurrency %>\n        workflow: _introspect\n        input:\n          node_uuid: <% $.uuid %>\n          queue_name: <% $.queue_name %>\n          timeout: <% $.node_timeout %>\n        # on-error is triggered if one or more nodes failed introspection. We\n        # still go to get_introspection_status as it will collect the result\n        # for each node. Unless we hit the retry limit.\n        on-error:\n         - get_introspection_status: <% $.introspection_attempt <= $.max_retry_attempts %>\n         - max_retry_attempts_reached: <% $.introspection_attempt > $.max_retry_attempts %>\n        on-success: get_introspection_status\n\n      get_introspection_status:\n        with-items: uuid in <% $.node_uuids %>\n        action: baremetal_introspection.get_status\n        input:\n          uuid: <% $.uuid %>\n        publish:\n          introspected_nodes: <% task().result.toDict($.uuid, $) %>\n          # Currently there is no way for us to ignore user introspection\n          # aborts. This means we will retry aborted nodes until the Ironic API\n          # gives us more details (error code or a boolean to show aborts etc.)\n          # If a node hasn't finished, we consider it to be failed.\n          # TODO(d0ugal): When possible, don't retry introspection of nodes\n          #               that a user manually aborted.\n          failed_introspection: <% task().result.where($.finished = true and $.error != null).select($.uuid) + task().result.where($.finished = false).select($.uuid) %>\n        publish-on-error:\n          # If a node fails to start introspection, getting the status can fail.\n          # When that happens, the result is a string and the nodes need to be\n          # filtered out.\n          introspected_nodes: <% task().result.where(isDict($)).toDict($.uuid, $) %>\n          # If there was an error, the exception string we get doesn't give us\n          # the UUID. So we use a set difference to find the UUIDs missing in\n          # the results. These are then added to the failed nodes.\n          failed_introspection: <% ($.node_uuids.toSet() - task().result.where(isDict($)).select($.uuid).toSet()) + task().result.where(isDict($)).where($.finished = true and $.error != null).toSet() + task().result.where(isDict($)).where($.finished = false).toSet() %>\n        on-error: increase_attempt_counter\n        on-success:\n          - successful_introspection: <% $.failed_introspection.len() = 0 %>\n          - increase_attempt_counter: <% $.failed_introspection.len() > 0 %>\n\n      increase_attempt_counter:\n        publish:\n          introspection_attempt: <% $.introspection_attempt + 1 %>\n        on-complete:\n          retry_failed_nodes\n\n      retry_failed_nodes:\n        publish:\n          status: RUNNING\n          message: <% 'Retrying {0} nodes that failed introspection. Attempt {1} of {2} '.format($.failed_introspection.len(), $.introspection_attempt, $.max_retry_attempts + 1) %>\n          # We are about to retry, update the tracking stats.\n          node_uuids: <% $.failed_introspection %>\n        on-success:\n          - send_message\n          - introspect_nodes\n\n      max_retry_attempts_reached:\n        publish:\n          status: FAILED\n          message: <% 'Retry limit reached with {0} nodes still failing introspection'.format($.failed_introspection.len()) %>\n        on-complete: send_message\n\n      successful_introspection:\n        publish:\n          status: SUCCESS\n          message: Successfully introspected <% $.introspected_nodes.len() %> node(s).\n        on-complete: send_message\n\n      unhandled_error:\n        publish:\n          status: FAILED\n          message: \"Unhandled workflow error\"\n        on-complete: send_message\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.introspect\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                introspected_nodes: <% $.get('introspected_nodes', []) %>\n                failed_introspection: <% $.get('failed_introspection', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  introspect_manageable_nodes:\n    description: Introspect all nodes in a 'manageable' state.\n\n    input:\n      - run_validations: False\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_manageable_nodes:\n        action: ironic.node_list maintenance=False associated=False\n        on-success: validate_nodes\n        on-error: set_status_failed_get_manageable_nodes\n        publish:\n          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>\n\n      set_status_failed_get_manageable_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_manageable_nodes).result %>\n\n      validate_nodes:\n        on-success:\n          - introspect_manageable: <% $.managed_nodes.len() > 0 %>\n          - set_status_failed_no_nodes: <% $.managed_nodes.len() = 0 %>\n\n      set_status_failed_no_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: No manageable nodes to introspect. Check node states and maintenance.\n\n      introspect_manageable:\n        on-success: send_message\n        on-error: set_status_introspect_manageable\n        workflow: tripleo.baremetal.v1.introspect\n        input:\n          node_uuids: <% $.managed_nodes %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          introspected_nodes: <% task().result.introspected_nodes %>\n\n      set_status_introspect_manageable:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(introspect_manageable).result %>\n          introspected_nodes: []\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.introspect_manageable_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                introspected_nodes: <% $.get('introspected_nodes', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  configure:\n    description: Take a list of manageable nodes and update their boot configuration.\n\n    input:\n      - node_uuids\n      - queue_name: tripleo\n      - kernel_name: bm-deploy-kernel\n      - ramdisk_name: bm-deploy-ramdisk\n      - instance_boot_option: null\n      - root_device: null\n      - root_device_minimum_size: 4\n      - overwrite_root_device_hints: False\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      configure_boot:\n        on-success: configure_root_device\n        on-error: set_status_failed_configure_boot\n        with-items: node_uuid in <% $.node_uuids %>\n        action: tripleo.baremetal.configure_boot node_uuid=<% $.node_uuid %> kernel_name=<% $.kernel_name %> ramdisk_name=<% $.ramdisk_name %> instance_boot_option=<% $.instance_boot_option %>\n\n      configure_root_device:\n        on-success: send_message\n        on-error: set_status_failed_configure_root_device\n        with-items: node_uuid in <% $.node_uuids %>\n        action: tripleo.baremetal.configure_root_device node_uuid=<% $.node_uuid %> root_device=<% $.root_device %> minimum_size=<% $.root_device_minimum_size %> overwrite=<% $.overwrite_root_device_hints %>\n        publish:\n          status: SUCCESS\n          message: 'Successfully configured the nodes.'\n\n      set_status_failed_configure_boot:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(configure_boot).result %>\n\n      set_status_failed_configure_root_device:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(configure_root_device).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.configure\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  configure_manageable_nodes:\n    description: Update the boot configuration of all nodes in 'manageable' state.\n\n    input:\n      - queue_name: tripleo\n      - kernel_name: 'bm-deploy-kernel'\n      - ramdisk_name: 'bm-deploy-ramdisk'\n      - instance_boot_option: null\n      - root_device: null\n      - root_device_minimum_size: 4\n      - overwrite_root_device_hints: False\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_manageable_nodes:\n        action: ironic.node_list maintenance=False associated=False\n        on-success: configure_manageable\n        on-error: set_status_failed_get_manageable_nodes\n        publish:\n          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>\n\n      configure_manageable:\n        on-success: send_message\n        on-error: set_status_failed_configure_manageable\n        workflow: tripleo.baremetal.v1.configure\n        input:\n          node_uuids: <% $.managed_nodes %>\n          queue_name: <% $.queue_name %>\n          kernel_name: <% $.kernel_name %>\n          ramdisk_name: <% $.ramdisk_name %>\n          instance_boot_option: <% $.instance_boot_option %>\n          root_device: <% $.root_device %>\n          root_device_minimum_size: <% $.root_device_minimum_size %>\n          overwrite_root_device_hints: <% $.overwrite_root_device_hints %>\n        publish:\n          message: 'Manageable nodes configured successfully.'\n\n      set_status_failed_configure_manageable:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(configure_manageable).result %>\n\n      set_status_failed_get_manageable_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_manageable_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.configure_manageable_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  tag_node:\n    description: Tag a node with a role\n    input:\n      - node_uuid\n      - role: null\n      - queue_name: tripleo\n\n    task-defaults:\n      on-error: send_message\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      update_node:\n        on-success: send_message\n        action: tripleo.baremetal.update_node_capability node_uuid=<% $.node_uuid %> capability='profile' value=<% $.role %>\n        publish:\n          message: <% task().result %>\n          status: SUCCESS\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.tag_node\n              payload:\n                status: <% $.get('status', 'FAILED') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  tag_nodes:\n    description: Runs the tag_node workflow in a loop\n    input:\n      - tag_node_uuids\n      - untag_node_uuids\n      - role\n      - plan: overcloud\n      - queue_name: tripleo\n\n    task-defaults:\n      on-error: send_message\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      tag_nodes:\n        with-items: node_uuid in <% $.tag_node_uuids %>\n        workflow: tripleo.baremetal.v1.tag_node\n        input:\n          node_uuid: <% $.node_uuid %>\n          queue_name: <% $.queue_name %>\n          role: <% $.role %>\n        concurrency: 1\n        on-success: untag_nodes\n\n      untag_nodes:\n        with-items: node_uuid in <% $.untag_node_uuids %>\n        workflow: tripleo.baremetal.v1.tag_node\n        input:\n          node_uuid: <% $.node_uuid %>\n          queue_name: <% $.queue_name %>\n        concurrency: 1\n        on-success: update_role_parameters\n\n      update_role_parameters:\n        on-success: send_message\n        action: tripleo.parameters.update_role role=<% $.role %> container=<% $.plan %>\n        publish:\n          message: <% task().result %>\n          status: SUCCESS\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.tag_nodes\n              payload:\n                status: <% $.get('status', 'FAILED') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  nodes_with_hint:\n    description: Find nodes matching a hint regex\n    input:\n      - hint_regex\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_nodes:\n        with-items: provision_state in <% ['available', 'active'] %>\n        action: ironic.node_list maintenance=false provision_state=<% $.provision_state %> detail=true\n        on-success: get_matching_nodes\n        on-error: set_status_failed_get_nodes\n\n      get_matching_nodes:\n        with-items: node in <% task(get_nodes).result.flatten() %>\n        action: tripleo.baremetal.get_node_hint node=<% $.node %>\n        on-success: send_message\n        on-error: set_status_failed_get_matching_nodes\n        publish:\n          matching_nodes: <% let(hint_regex => $.hint_regex) -> task().result.where($.hint and $.hint.matches($hint_regex)).uuid %>\n\n      set_status_failed_get_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_nodes).result %>\n\n      set_status_failed_get_matching_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_matching_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.nodes_with_hint\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                matching_nodes: <% $.matching_nodes or [] %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  nodes_with_profile:\n    description: Find nodes with a specific profile\n    input:\n      - profile\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_active_nodes:\n        action: ironic.node_list maintenance=false provision_state='active' detail=true\n        on-success: get_available_nodes\n        on-error: set_status_failed_get_active_nodes\n\n      get_available_nodes:\n        action: ironic.node_list maintenance=false provision_state='available' detail=true\n        on-success: get_matching_nodes\n        on-error: set_status_failed_get_available_nodes\n\n      get_matching_nodes:\n        with-items: node in <% task(get_available_nodes).result + task(get_active_nodes).result %>\n        action: tripleo.baremetal.get_profile node=<% $.node %>\n        on-success: send_message\n        on-error: set_status_failed_get_matching_nodes\n        publish:\n          matching_nodes: <% let(input_profile_name => $.profile) -> task().result.where($.profile = $input_profile_name).uuid %>\n\n      set_status_failed_get_active_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_active_nodes).result %>\n\n      set_status_failed_get_available_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_available_nodes).result %>\n\n      set_status_failed_get_matching_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_matching_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.nodes_with_profile\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                matching_nodes: <% $.matching_nodes or [] %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  create_raid_configuration:\n    description: Create and apply RAID configuration for given nodes\n    input:\n      - node_uuids\n      - configuration\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_configuration:\n        with-items: node_uuid in <% $.node_uuids %>\n        action: ironic.node_set_target_raid_config node_ident=<% $.node_uuid %> target_raid_config=<% $.configuration %>\n        on-success: apply_configuration\n        on-error: set_configuration_failed\n\n      set_configuration_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_configuration).result %>\n\n      apply_configuration:\n        with-items: node_uuid in <% $.node_uuids %>\n        workflow: tripleo.baremetal.v1.manual_cleaning\n        input:\n          node_uuid: <% $.node_uuid %>\n          clean_steps:\n            - interface: raid\n              step: delete_configuration\n            - interface: raid\n              step: create_configuration\n          timeout: 1800  # building RAID should be fast than general cleaning\n          retry_count: 180\n          retry_delay: 10\n        on-success: send_message\n        on-error: apply_configuration_failed\n        publish:\n          message: <% task().result %>\n          status: SUCCESS\n\n      apply_configuration_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(apply_configuration).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.create_raid_configuration\n              payload:\n                status: <% $.get('status', 'FAILED') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n\n  cellv2_discovery:\n    description: Run cell_v2 host discovery\n\n    input:\n      - node_uuids\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      cell_v2_discover_hosts:\n        on-success: wait_for_nova_resources\n        on-error: cell_v2_discover_hosts_failed\n        action: tripleo.baremetal.cell_v2_discover_hosts\n\n      cell_v2_discover_hosts_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(cell_v2_discover_hosts).result %>\n\n      wait_for_nova_resources:\n        on-success: send_message\n        on-error: wait_for_nova_resources_failed\n        with-items: node_uuid in <% $.node_uuids %>\n        action: nova.hypervisors_find hypervisor_hostname=<% $.node_uuid %>\n\n      wait_for_nova_resources_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(wait_for_nova_resources).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.cellv2_discovery\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n\n  discover_nodes:\n    description: Run nodes discovery over the given IP range\n\n    input:\n      - ip_addresses\n      - credentials\n      - ports: [623]\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_all_nodes:\n        action: ironic.node_list\n        input:\n          fields: [\"uuid\", \"driver\", \"driver_info\"]\n          limit: 0\n        on-success: get_candidate_nodes\n        on-error: get_all_nodes_failed\n        publish:\n          existing_nodes: <% task().result %>\n\n      get_all_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_all_nodes).result %>\n\n      get_candidate_nodes:\n        action: tripleo.baremetal.get_candidate_nodes\n        input:\n          ip_addresses: <% $.ip_addresses %>\n          credentials: <% $.credentials %>\n          ports: <% $.ports %>\n          existing_nodes: <% $.existing_nodes %>\n        on-success: probe_nodes\n        on-error: get_candidate_nodes_failed\n        publish:\n          candidates: <% task().result %>\n\n      get_candidate_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_candidate_nodes).result %>\n\n      probe_nodes:\n        action: tripleo.baremetal.probe_node\n        on-success: send_message\n        on-error: probe_nodes_failed\n        input:\n          ip: <% $.node.ip %>\n          port: <% $.node.port %>\n          username: <% $.node.username %>\n          password: <% $.node.password %>\n        with-items:\n          - node in <% $.candidates %>\n        publish:\n          nodes_json: <% task().result.where($ != null) %>\n\n      probe_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(probe_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.discover_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                nodes_json: <% $.get('nodes_json', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  discover_and_enroll_nodes:\n    description: Run nodes discovery over the given IP range and enroll nodes\n\n    input:\n      - ip_addresses\n      - credentials\n      - ports: [623]\n      - kernel_name: null\n      - ramdisk_name: null\n      - instance_boot_option: local\n      - initial_state: manageable\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      discover_nodes:\n        workflow: tripleo.baremetal.v1.discover_nodes\n        input:\n          ip_addresses: <% $.ip_addresses %>\n          ports: <% $.ports %>\n          credentials: <% $.credentials %>\n          queue_name: <% $.queue_name %>\n        on-success: enroll_nodes\n        on-error: discover_nodes_failed\n        publish:\n          nodes_json: <% task().result.nodes_json %>\n\n      discover_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(discover_nodes).result %>\n\n      enroll_nodes:\n        workflow: tripleo.baremetal.v1.register_or_update\n        input:\n          nodes_json: <% $.nodes_json %>\n          kernel_name: <% $.kernel_name %>\n          ramdisk_name: <% $.ramdisk_name %>\n          instance_boot_option: <% $.instance_boot_option %>\n          initial_state: <% $.initial_state %>\n        on-success: send_message\n        on-error: enroll_nodes_failed\n        publish:\n          registered_nodes: <% task().result.registered_nodes %>\n\n      enroll_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(enroll_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.discover_and_enroll_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                registered_nodes: <% $.get('registered_nodes', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1", "tags": [], "created_at": "2018-12-03 11:18:54", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "3f64b5b3-ae06-4dcd-a3cd-20e97786c148"}, {"definition": "---\nversion: '2.0'\nname: tripleo.storage.v1\ndescription: TripleO manages Ceph with ceph-ansible\n\nworkflows:\n  ceph-install:\n    # allows for additional extra_vars via workflow input\n    input:\n      - ansible_playbook_verbosity: 0\n      - ansible_skip_tags: 'package-install,with_pkg'\n      - ansible_env_variables: {}\n      - ansible_extra_env_variables:\n          ANSIBLE_CONFIG: /usr/share/ceph-ansible/ansible.cfg\n          ANSIBLE_ACTION_PLUGINS: /usr/share/ceph-ansible/plugins/actions/\n          ANSIBLE_ROLES_PATH: /usr/share/ceph-ansible/roles/\n          ANSIBLE_RETRY_FILES_ENABLED: 'False'\n          ANSIBLE_LOG_PATH: /var/log/mistral/ceph-install-workflow.log\n          ANSIBLE_LIBRARY: /usr/share/ceph-ansible/library/\n          ANSIBLE_SSH_RETRIES: '3'\n          ANSIBLE_HOST_KEY_CHECKING: 'False'\n          DEFAULT_FORKS: '25'\n      - ceph_ansible_extra_vars: {}\n      - ceph_ansible_playbook: /usr/share/ceph-ansible/site-docker.yml.sample\n      - node_data_lookup: '{}'\n      - swift_container: 'ceph_ansible_fetch_dir'\n    tags:\n      - tripleo-common-managed\n    tasks:\n      set_swift_container:\n        publish:\n          swift_container: <% concat(env().get('heat_stack_name', ''), '_', $.swift_container) %>\n        on-complete: collect_puppet_hieradata\n      collect_puppet_hieradata:\n        on-success: check_hieradata\n        publish:\n          hieradata: <% env().get('role_merged_configs', {}).values().select($.keys()).flatten().select(regex('^ceph::profile::params::osds$').search($)).where($ != null).toSet() %>\n      check_hieradata:\n        on-success:\n        - set_blacklisted_ips: <% not bool($.hieradata) %>\n        - fail(msg=<% 'Ceph deployment stopped, puppet-ceph hieradata found. Convert it into ceph-ansible variables. {0}'.format($.hieradata) %>): <% bool($.hieradata) %>\n      set_blacklisted_ips:\n        publish:\n          blacklisted_ips: <% env().get('blacklisted_ip_addresses', []) %>\n        on-success: set_ip_lists\n      set_ip_lists:\n        publish:\n          mgr_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mgr_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          mon_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mon_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          osd_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_osd_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          mds_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mds_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          rgw_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_rgw_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          nfs_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_nfs_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          rbdmirror_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_rbdmirror_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          client_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_client_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n        on-success: merge_ip_lists\n      merge_ip_lists:\n        publish:\n          ips_list: <% ($.mgr_ips + $.mon_ips + $.osd_ips + $.mds_ips + $.rgw_ips + $.nfs_ips + $.rbdmirror_ips + $.client_ips).toSet() %>\n        on-success: enable_ssh_admin\n      enable_ssh_admin:\n        workflow: tripleo.access.v1.enable_ssh_admin\n        input:\n          ssh_servers: <% $.ips_list %>\n        on-success: get_private_key\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: make_fetch_directory\n      make_fetch_directory:\n        action: tripleo.files.make_temp_dir\n        publish:\n          fetch_directory: <% task().result.path %>\n        on-success: verify_container_exists\n      verify_container_exists:\n        action: swift.head_container container=<% $.swift_container %>\n        on-success: restore_fetch_directory\n        on-error: create_container\n      restore_fetch_directory:\n        action: tripleo.files.restore_temp_dir_from_swift\n        input:\n          container: <% $.swift_container %>\n          path: <% $.fetch_directory %>\n        on-success: collect_nodes_uuid\n      create_container:\n        action: swift.put_container container=<% $.swift_container %>\n        on-success: collect_nodes_uuid\n      collect_nodes_uuid:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            overcloud:\n              hosts: <% $.ips_list.toDict($, {}) %>\n          remote_user: tripleo-admin\n          become: true\n          become_user: root\n          verbosity: 0\n          ssh_private_key: <% $.private_key %>\n          #NOTE(gfidente): set ANSIBLE_CALLBACK_WHITELIST to empty string to avoid spurious output\n          #in the json output. The publish: directive will in fact parse the output.\n          extra_env_variables:\n            ANSIBLE_CALLBACK_WHITELIST: ''\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n            ANSIBLE_STDOUT_CALLBACK: 'json'\n          playbook:\n            - hosts: overcloud\n              gather_facts: no\n              tasks:\n                - name: collect machine id\n                  command: dmidecode -s system-uuid\n                  register: dmidecode\n                  # NOTE(tonyb): 0 == no error, 1 == -EPERM or bad data and 2 == Command not found\n                  # 1 and 2 aren't great but shouldn't cause the deploy to fail.  If we're using\n                  # the node specific data we'll fail then.  If we aren't then lets keep moving\n                  failed_when: dmidecode.rc not in [0, 1, 2]\n        publish:\n          ansible_output: <% json_parse(task().result.stderr) %>\n        on-success: set_ip_uuids\n      set_ip_uuids:\n        publish:\n          ip_uuids: <% let(root => $.ansible_output.get('plays')[0].get('tasks')[0].get('hosts')) -> $.ips_list.toDict($, $root.get($).get('stdout')) %>\n        on-success: parse_node_data_lookup\n      parse_node_data_lookup:\n        publish:\n          json_node_data_lookup: <% json_parse($.node_data_lookup) %>\n        on-error:\n          - fail(msg=<% 'Ceph deployment stopped, NodeDataLookup (node_data_lookup) is not valid JSON. {0}'.format($.node_data_lookup) %>)\n        on-success: map_node_data_lookup\n      map_node_data_lookup:\n        publish:\n          ips_data: <% let(uuids => $.ip_uuids, root => $) -> $.ips_list.toDict($, $root.json_node_data_lookup.get($uuids.get($, \"NO-UUID-FOUND\"), {})) %>\n        on-success: set_role_vars\n      set_role_vars:\n        publish:\n          # NOTE(gfidente): collect role settings from all tht roles\n          mgr_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mgr_ansible_vars', {})).aggregate($1 + $2) %>\n          mon_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mon_ansible_vars', {})).aggregate($1 + $2) %>\n          osd_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_osd_ansible_vars', {})).aggregate($1 + $2) %>\n          mds_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mds_ansible_vars', {})).aggregate($1 + $2) %>\n          rgw_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_rgw_ansible_vars', {})).aggregate($1 + $2) %>\n          nfs_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_nfs_ansible_vars', {})).aggregate($1 + $2) %>\n          rbdmirror_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_rbdmirror_ansible_vars', {})).aggregate($1 + $2) %>\n          client_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_client_ansible_vars', {})).aggregate($1 + $2) %>\n        on-success: build_extra_vars\n      build_extra_vars:\n        publish:\n          # NOTE(gfidente): merge vars from all ansible roles\n          extra_vars: <% {'fetch_directory'=> $.fetch_directory} + $.mgr_vars + $.mon_vars + $.osd_vars + $.mds_vars + $.rgw_vars + $.nfs_vars + $.client_vars + $.rbdmirror_vars + $.ceph_ansible_extra_vars %>\n        on-success: ceph_install\n      ceph_install:\n        with-items: playbook in <% list($.ceph_ansible_playbook).flatten() %>\n        concurrency: 1\n        action: tripleo.ansible-playbook\n        input:\n          trash_output: true\n          inventory:\n            mgrs:\n              hosts: <% let(root => $) -> $.mgr_ips.toDict($, $root.ips_data.get($, {})) %>\n            mons:\n              hosts: <% let(root => $) -> $.mon_ips.toDict($, $root.ips_data.get($, {})) %>\n            osds:\n              hosts: <% let(root => $) -> $.osd_ips.toDict($, $root.ips_data.get($, {})) %>\n            mdss:\n              hosts: <% let(root => $) -> $.mds_ips.toDict($, $root.ips_data.get($, {})) %>\n            rgws:\n              hosts: <% let(root => $) -> $.rgw_ips.toDict($, $root.ips_data.get($, {})) %>\n            nfss:\n              hosts: <% let(root => $) -> $.nfs_ips.toDict($, $root.ips_data.get($, {})) %>\n            rbdmirrors:\n              hosts: <% let(root => $) -> $.rbdmirror_ips.toDict($, $root.ips_data.get($, {})) %>\n            clients:\n              hosts: <% let(root => $) -> $.client_ips.toDict($, $root.ips_data.get($, {})) %>\n            all:\n              vars: <% $.extra_vars %>\n          playbook: <% $.playbook %>\n          remote_user: tripleo-admin\n          become: true\n          become_user: root\n          verbosity: <% $.ansible_playbook_verbosity %>\n          ssh_private_key: <% $.private_key %>\n          skip_tags: <% $.ansible_skip_tags %>\n          extra_env_variables: <% $.ansible_extra_env_variables.mergeWith($.ansible_env_variables) %>\n          extra_vars:\n            ireallymeanit: 'yes'\n        publish:\n          output: <% task().result %>\n        on-complete: save_fetch_directory\n      save_fetch_directory:\n        action: tripleo.files.save_temp_dir_to_swift\n        input:\n          container: <% $.swift_container %>\n          path: <% $.fetch_directory %>\n        on-success: purge_fetch_directory\n      purge_fetch_directory:\n        action: tripleo.files.remove_temp_dir path=<% $.fetch_directory %>\n", "name": "tripleo.storage.v1", "tags": [], "created_at": "2018-12-03 11:18:55", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "ccee3c73-2ffd-4da6-8d56-7ece3d0b6cc1"}, {"definition": "---\nversion: '2.0'\nname: tripleo.deployment.v1\ndescription: TripleO deployment workflows\n\nworkflows:\n\n  deploy_on_server:\n\n    input:\n      - server_uuid\n      - server_name\n      - config\n      - config_name\n      - group\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      deploy_config:\n        action: tripleo.deployment.config\n        on-complete: send_message\n        input:\n          server_id: <% $.server_uuid %>\n          name: <% $.config_name %>\n          config: <% $.config %>\n          group: <% $.group %>\n        publish:\n          stdout: <% task().result.deploy_stdout %>\n          stderr: <% task().result.deploy_stderr %>\n          status_code: <% task().result.deploy_status_code %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.deploy_on_server\n              payload:\n                status: <% $.get(\"status\", \"SUCCESS\") %>\n                message: <% $.get(\"message\", \"\") %>\n                server_uuid: <% $.server_uuid %>\n                server_name: <% $.server_name %>\n                config_name: <% $.config_name %>\n                status_code: <% $.get(\"status_code\", \"\") %>\n                stdout: <% $.get(\"stdout\", \"\") %>\n                stderr: <% $.get(\"stderr\", \"\") %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  deploy_on_servers:\n\n    input:\n      - server_name\n      - config_name\n      - config\n      - group: script\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      check_if_all_servers:\n        on-success:\n          - get_servers_matching: <% $.server_name != \"all\" %>\n          - get_all_servers: <% $.server_name = \"all\" %>\n\n      get_servers_matching:\n        action: nova.servers_list\n        on-success: deploy_on_servers\n        publish:\n          servers_with_name: <% task().result._info.where($.name.indexOf(execution().input.server_name) > -1) %>\n\n      get_all_servers:\n        action: nova.servers_list\n        on-success: deploy_on_servers\n        publish:\n          servers_with_name: <% task().result._info %>\n\n      deploy_on_servers:\n        on-success: send_success_message\n        on-error: send_failed_message\n        with-items: server in <% $.servers_with_name %>\n        workflow: tripleo.deployment.v1.deploy_on_server\n        input:\n          server_name: <% $.server.name %>\n          server_uuid: <% $.server.id %>\n          config: <% $.config %>\n          config_name: <% $.config_name %>\n          group: <% $.group %>\n          queue_name: <% $.queue_name %>\n\n      send_success_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.deploy_on_servers\n              payload:\n                status: SUCCESS\n                execution: <% execution() %>\n\n      send_failed_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.deploy_on_servers\n              payload:\n                status: FAILED\n                message: <% task(deploy_on_servers).result %>\n                execution: <% execution() %>\n        on-success: fail\n\n  deploy_plan:\n\n    description: >\n      Deploy the overcloud for a plan.\n\n    input:\n      - container\n      - run_validations: False\n      - timeout: 240\n      - skip_deploy_identifier: False\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      add_validation_ssh_key:\n        workflow: tripleo.validations.v1.add_validation_ssh_key_parameter\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n        on-complete:\n          - run_validations: <% $.run_validations %>\n          - create_swift_rings_backup_plan: <% not $.run_validations %>\n\n      run_validations:\n        workflow: tripleo.validations.v1.run_groups\n        input:\n          group_names:\n            - 'pre-deployment'\n          plan: <% $.container %>\n          queue_name: <% $.queue_name %>\n        on-success: create_swift_rings_backup_plan\n        on-error: set_validations_failed\n\n      set_validations_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(run_validations).result %>\n\n      create_swift_rings_backup_plan:\n        workflow: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan\n        on-success: cell_v2_discover_hosts\n        on-error: create_swift_rings_backup_plan_set_status_failed\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n          use_default_templates: true\n\n      cell_v2_discover_hosts:\n        on-success: deploy\n        on-error: cell_v2_discover_hosts_failed\n        action: tripleo.baremetal.cell_v2_discover_hosts\n\n      cell_v2_discover_hosts_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(cell_v2_discover_hosts).result %>\n\n      deploy:\n        action: tripleo.deployment.deploy\n        input:\n          timeout: <% $.timeout %>\n          container: <% $.container %>\n          skip_deploy_identifier: <% $.skip_deploy_identifier %>\n        on-success: send_message\n        on-error: set_deployment_failed\n\n      create_swift_rings_backup_plan_set_status_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(create_swift_rings_backup_plan).result %>\n\n      set_deployment_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(deploy).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.deploy_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  get_horizon_url:\n\n    description: >\n      Retrieve the Horizon URL from the Overcloud stack.\n\n    input:\n      - stack: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    output:\n      horizon_url: <% $.horizon_url %>\n\n    tasks:\n      get_horizon_url:\n        action: heat.stacks_get\n        input:\n          stack_id: <% $.stack %>\n        publish:\n          horizon_url: <% task().result.outputs.where($.output_key = \"EndpointMap\").output_value.HorizonPublic.uri.single() %>\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.get_horizon_url\n              payload:\n                horizon_url: <% $.get('horizon_url', '') %>\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  config_download_deploy:\n\n    description: >\n      Configure the overcloud with config-download.\n\n    input:\n      - timeout: 240\n      - queue_name: tripleo\n      - plan_name: overcloud\n      - work_dir: /var/lib/mistral\n      - verbosity: 1\n      - blacklist: []\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_blacklisted_hostnames:\n        action: heat.stacks_output_show\n        input:\n          stack_id: <% $.plan_name %>\n          output_key: BlacklistedHostnames\n        publish:\n          blacklisted_hostnames: <% task().result.output.output_value.where($ != \"\") %>\n        on-success: get_config\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_config:\n        action: tripleo.config.get_overcloud_config\n        input:\n          container: <% $.get('plan_name') %>\n        on-success: download_config\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      download_config:\n        action: tripleo.config.download_config\n        input:\n          work_dir: <% $.get('work_dir') %>/<% execution().id %>\n        on-success: send_msg_config_download\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      send_msg_config_download:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.config_download\n              payload:\n                status: <% $.get('status', 'RUNNING') %>\n                message: Config downloaded at <% $.get('work_dir') %>/<% execution().id %>\n                execution: <% execution() %>\n        on-success: get_private_key\n\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: generate_inventory\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      generate_inventory:\n        action: tripleo.ansible-generate-inventory\n        input:\n          ansible_ssh_user: tripleo-admin\n          work_dir: <% $.get('work_dir') %>/<% execution().id %>\n          plan_name: <% $.get('plan_name') %>\n        publish:\n          inventory: <% task().result %>\n        on-success: send_msg_generate_inventory\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      send_msg_generate_inventory:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.config_download\n              payload:\n                status: <% $.get('status', 'RUNNING') %>\n                message: Inventory generated at <% $.get('inventory') %>\n                execution: <% execution() %>\n        on-success: send_msg_run_ansible\n\n      send_msg_run_ansible:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.config_download\n              payload:\n                status: <% $.get('status', 'RUNNING') %>\n                message: >\n                  Running ansible playbook at <% $.get('work_dir') %>/<% execution().id %>/deploy_steps_playbook.yaml.\n                  See log file at <% $.get('work_dir') %>/<% execution().id %>/ansible.log for progress.\n                  ...\n                execution: <% execution() %>\n        on-success: run_ansible\n\n      run_ansible:\n        action: tripleo.ansible-playbook\n        input:\n          inventory: <% $.inventory %>\n          playbook: <% $.get('work_dir') %>/<% execution().id %>/deploy_steps_playbook.yaml\n          remote_user: tripleo-admin\n          ssh_extra_args: '-o StrictHostKeyChecking=no'\n          ssh_private_key: <% $.private_key %>\n          use_openstack_credentials: true\n          verbosity: <% $.get('verbosity') %>\n          become: true\n          timeout: <% $.timeout %>\n          work_dir: <% $.get('work_dir') %>/<% execution().id %>\n          queue_name: <% $.queue_name %>\n          reproduce_command: true\n          trash_output: true\n          blacklisted_hostnames: <% $.blacklisted_hostnames %>\n        publish:\n          log_path: <% task(run_ansible).result.get('log_path') %>\n        on-success:\n          - ansible_passed: <% task().result.returncode = 0 %>\n          - ansible_failed: <% task().result.returncode != 0 %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/ansible.log.\n\n      ansible_passed:\n        on-success: send_message\n        publish:\n          status: SUCCESS\n          message: Ansible passed.\n\n      ansible_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/ansible.log.\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.config_download\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.deployment.v1", "tags": [], "created_at": "2018-12-03 11:18:56", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "4eb617a8-75b7-4f5e-93f2-16ec8184be11"}, {"definition": "---\nversion: '2.0'\nname: tripleo.derive_params.v1\ndescription: TripleO Workflows to derive deployment parameters from the introspected data\n\nworkflows:\n\n  derive_parameters:\n    description: The main workflow for deriving parameters from the introspected data\n\n    input:\n      - plan: overcloud\n      - queue_name: tripleo\n      - user_inputs: {}\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_flattened_parameters:\n        action: tripleo.parameters.get_flatten container=<% $.plan %>\n        publish:\n          environment_parameters: <% task().result.environment_parameters %>\n          heat_resource_tree: <% task().result.heat_resource_tree %>\n        on-success:\n          - get_roles: <% $.environment_parameters and $.heat_resource_tree %>\n          - set_status_failed_get_flattened_parameters: <% (not $.environment_parameters) or (not $.heat_resource_tree) %>\n        on-error: set_status_failed_get_flattened_parameters\n\n      get_roles:\n        action: tripleo.role.list container=<% $.plan %>\n        publish:\n          role_name_list: <% task().result %>\n        on-success:\n          - get_valid_roles: <% $.role_name_list %>\n          - set_status_failed_get_roles: <% not $.role_name_list %>\n        on-error: set_status_failed_on_error_get_roles\n\n      # Obtain only the roles which has count > 0, by checking <RoleName>Count parameter, like ComputeCount\n      get_valid_roles:\n        publish:\n          valid_role_name_list: <% let(hr => $.heat_resource_tree.parameters) -> $.role_name_list.where(int($hr.get(concat($, 'Count'), {}).get('default', 0)) > 0) %>\n        on-success:\n          - for_each_role: <% $.valid_role_name_list %>\n          - set_status_failed_get_valid_roles: <% not $.valid_role_name_list %>\n\n      # Execute the basic preparation workflow for each role to get introspection data\n      for_each_role:\n        with-items: role_name in <% $.valid_role_name_list %>\n        concurrency: 1\n        workflow: _derive_parameters_per_role\n        input:\n          plan: <% $.plan %>\n          role_name: <% $.role_name %>\n          environment_parameters: <% $.environment_parameters %>\n          heat_resource_tree: <% $.heat_resource_tree %>\n          user_inputs: <% $.user_inputs %>\n        publish:\n          # Gets all the roles derived parameters as dictionary\n          result: <% task().result.select($.get('derived_parameters', {})).sum() %>\n        on-success: reset_derive_parameters_in_plan\n        on-error: set_status_failed_for_each_role\n\n      reset_derive_parameters_in_plan:\n        action: tripleo.parameters.reset\n        input:\n          container: <% $.plan %>\n          key: 'derived_parameters'\n        on-success:\n          # Add the derived parameters to the deployment plan only when $.result\n          # (the derived parameters) is non-empty. Otherwise, we're done.\n          - update_derive_parameters_in_plan: <% $.result %>\n          - send_message: <% not $.result %>\n        on-error: set_status_failed_reset_derive_parameters_in_plan\n\n      update_derive_parameters_in_plan:\n        action: tripleo.parameters.update\n        input:\n          container: <% $.plan %>\n          key: 'derived_parameters'\n          parameters: <% $.get('result', {}) %>\n        on-success: send_message\n        on-error: set_status_failed_update_derive_parameters_in_plan\n\n      set_status_failed_get_flattened_parameters:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_flattened_parameters).result %>\n\n      set_status_failed_get_roles:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: \"Unable to determine the list of roles in the deployment plan\"\n\n      set_status_failed_on_error_get_roles:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_roles).result %>\n\n      set_status_failed_get_valid_roles:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: 'Unable to determine the list of valid roles in the deployment plan.'\n\n      set_status_failed_for_each_role:\n        on-success: update_message_format\n        publish:\n          status: FAILED\n          # gets the status and message for all roles from task result.\n          message: <% task(for_each_role).result.select(dict('role_name' => $.role_name, 'status' => $.get('status', 'SUCCESS'), 'message' => $.get('message', ''))) %>\n\n      update_message_format:\n        on-success: send_message\n        publish:\n          # updates the message format(Role 'role name': message) for each roles which are failed and joins the message list as string with ', ' separator.\n          message: <% $.message.where($.get('status', 'SUCCESS') != 'SUCCESS').select(concat(\"Role '{}':\".format($.role_name), \" \", $.get('message', '(error unknown)'))).join(', ') %>\n\n      set_status_failed_reset_derive_parameters_in_plan:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(reset_derive_parameters_in_plan).result %>\n\n      set_status_failed_update_derive_parameters_in_plan:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(update_derive_parameters_in_plan).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.derive_params.v1.derive_parameters\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                result: <% $.get('result', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n\n  _derive_parameters_per_role:\n    description: >\n      Workflow which runs per role to get the introspection data on the first matching node assigned to role.\n      Once introspection data is fetched, this worklow will trigger the actual derive parameters workflow\n    input:\n      - plan\n      - role_name\n      - environment_parameters\n      - heat_resource_tree\n      - user_inputs\n\n    output:\n      derived_parameters: <% $.get('derived_parameters', {}) %>\n      # Need role_name in output parameter to display the status for all roles in main workflow when any role fails here.\n      role_name: <% $.role_name %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_role_info:\n        workflow: _get_role_info\n        input:\n          role_name: <% $.role_name %>\n          heat_resource_tree: <% $.heat_resource_tree %>\n        publish:\n          role_features: <% task().result.get('role_features', []) %>\n          role_services: <% task().result.get('role_services', []) %>\n        on-success:\n          # Continue only if there are features associated with this role. Otherwise, we're done.\n          - get_scheduler_hints: <% $.role_features %>\n        on-error: set_status_failed_get_role_info\n\n      # Find a node associated with this role. Look for nodes matching any scheduler hints\n      # associated with the role, and if there are no scheduler hints then locate nodes\n      # with a profile matching the role's flavor.\n      get_scheduler_hints:\n        publish:\n          scheduler_hints: <% let(param_name => concat($.role_name, 'SchedulerHints')) -> $.heat_resource_tree.parameters.get($param_name, {}).get('default', {}) %>\n        on-success:\n          - get_hint_regex: <% $.scheduler_hints %>\n          # If there are no scheduler hints then move on to use the flavor\n          - get_flavor_name: <% not $.scheduler_hints %>\n\n      get_hint_regex:\n        publish:\n          hint_regex: <% $.scheduler_hints.get('capabilities:node', '').replace('%index%', '(\\d+)') %>\n        on-success:\n          - get_node_with_hint: <% $.hint_regex %>\n          # If there is no 'capabilities:node' hint then move on to use the flavor\n          - get_flavor_name: <% not $.hint_regex %>\n\n      get_node_with_hint:\n        workflow: tripleo.baremetal.v1.nodes_with_hint\n        input:\n          hint_regex: <% concat('^', $.hint_regex, '$') %>\n        publish:\n          role_node_uuid: <% task().result.matching_nodes.first('') %>\n        on-success:\n          - get_introspection_data: <% $.role_node_uuid %>\n          # If no nodes match the scheduler hint then move on to use the flavor\n          - get_flavor_name: <% not $.role_node_uuid %>\n        on-error: set_status_failed_on_error_get_node_with_hint\n\n      get_flavor_name:\n        publish:\n          flavor_name: <% let(param_name => concat('Overcloud', $.role_name, 'Flavor').replace('OvercloudControllerFlavor', 'OvercloudControlFlavor')) -> $.heat_resource_tree.parameters.get($param_name, {}).get('default', '') %>\n        on-success:\n          - get_profile_name: <% $.flavor_name %>\n          - set_status_failed_get_flavor_name: <% not $.flavor_name %>\n\n      get_profile_name:\n        action: tripleo.parameters.get_profile_of_flavor flavor_name=<% $.flavor_name %>\n        publish:\n          profile_name: <% task().result %>\n        on-success: get_profile_node\n        on-error: set_status_failed_get_profile_name\n\n      get_profile_node:\n        workflow: tripleo.baremetal.v1.nodes_with_profile\n        input:\n          profile: <% $.profile_name %>\n        publish:\n          role_node_uuid: <% task().result.matching_nodes.first('') %>\n        on-success:\n          - get_introspection_data: <% $.role_node_uuid %>\n          - set_status_failed_no_matching_node_get_profile_node: <% not $.role_node_uuid %>\n        on-error: set_status_failed_on_error_get_profile_node\n\n      get_introspection_data:\n        action: baremetal_introspection.get_data uuid=<% $.role_node_uuid %>\n        publish:\n          hw_data: <% task().result %>\n          # Establish an empty dictionary of derived_parameters prior to\n          # invoking the individual \"feature\" algorithms\n          derived_parameters: <% dict() %>\n        on-success: handle_dpdk_feature\n        on-error: set_status_failed_get_introspection_data\n\n      handle_dpdk_feature:\n        on-success:\n          - get_dpdk_derive_params: <% $.role_features.contains('DPDK') %>\n          - handle_sriov_feature: <% not $.role_features.contains('DPDK') %>\n\n      get_dpdk_derive_params:\n        workflow: tripleo.derive_params_formulas.v1.dpdk_derive_params\n        input:\n          plan: <% $.plan %>\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n          user_inputs: <% $.user_inputs %>\n        publish:\n          derived_parameters: <% task().result.get('derived_parameters', {}) %>\n        on-success: handle_sriov_feature\n        on-error: set_status_failed_get_dpdk_derive_params\n\n      handle_sriov_feature:\n        on-success:\n          - get_sriov_derive_params: <% $.role_features.contains('SRIOV') %>\n          - handle_host_feature: <% not $.role_features.contains('SRIOV') %>\n\n      get_sriov_derive_params:\n        workflow: tripleo.derive_params_formulas.v1.sriov_derive_params\n        input:\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n          derived_parameters: <% $.derived_parameters %>\n        publish:\n          derived_parameters: <% task().result.get('derived_parameters', {}) %>\n        on-success: handle_host_feature\n        on-error: set_status_failed_get_sriov_derive_params\n\n      handle_host_feature:\n        on-success:\n          - get_host_derive_params: <% $.role_features.contains('HOST') %>\n          - handle_hci_feature: <% not $.role_features.contains('HOST') %>\n\n      get_host_derive_params:\n        workflow: tripleo.derive_params_formulas.v1.host_derive_params\n        input:\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n          user_inputs: <% $.user_inputs %>\n          derived_parameters: <% $.derived_parameters %>\n        publish:\n          derived_parameters: <% task().result.get('derived_parameters', {}) %>\n        on-success: handle_hci_feature\n        on-error: set_status_failed_get_host_derive_params\n\n      handle_hci_feature:\n        on-success:\n          - get_hci_derive_params: <% $.role_features.contains('HCI') %>\n\n      get_hci_derive_params:\n        workflow: tripleo.derive_params_formulas.v1.hci_derive_params\n        input:\n          role_name: <% $.role_name %>\n          environment_parameters: <% $.environment_parameters %>\n          heat_resource_tree: <% $.heat_resource_tree %>\n          introspection_data: <% $.hw_data %>\n          user_inputs: <% $.user_inputs %>\n          derived_parameters: <% $.derived_parameters %>\n        publish:\n          derived_parameters: <% task().result.get('derived_parameters', {}) %>\n        on-error: set_status_failed_get_hci_derive_params\n        # Done (no more derived parameter features)\n\n      set_status_failed_get_role_info:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_role_info).result.get('message', '') %>\n        on-success: fail\n\n      set_status_failed_get_flavor_name:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% \"Unable to determine flavor for role '{0}'\".format($.role_name) %>\n        on-success: fail\n\n      set_status_failed_get_profile_name:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_profile_name).result %>\n        on-success: fail\n\n      set_status_failed_no_matching_node_get_profile_node:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% \"Unable to determine matching node for profile '{0}'\".format($.profile_name) %>\n        on-success: fail\n\n      set_status_failed_on_error_get_profile_node:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_profile_node).result %>\n        on-success: fail\n\n      set_status_failed_on_error_get_node_with_hint:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_node_with_hint).result %>\n        on-success: fail\n\n      set_status_failed_get_introspection_data:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_introspection_data).result %>\n        on-success: fail\n\n      set_status_failed_get_dpdk_derive_params:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_dpdk_derive_params).result.message %>\n        on-success: fail\n\n      set_status_failed_get_sriov_derive_params:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_sriov_derive_params).result.message %>\n        on-success: fail\n\n      set_status_failed_get_host_derive_params:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_host_derive_params).result.message %>\n        on-success: fail\n\n      set_status_failed_get_hci_derive_params:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_hci_derive_params).result.message %>\n        on-success: fail\n\n\n  _get_role_info:\n    description: >\n      Workflow that determines the list of derived parameter features (DPDK,\n      HCI, etc.) for a role based on the services assigned to the role.\n\n    input:\n      - role_name\n      - heat_resource_tree\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_resource_chains:\n        publish:\n          resource_chains: <% $.heat_resource_tree.resources.values().where($.get('type', '') = 'OS::Heat::ResourceChain') %>\n        on-success:\n          - get_role_chain: <% $.resource_chains %>\n          - set_status_failed_get_resource_chains: <% not $.resource_chains %>\n\n      get_role_chain:\n        publish:\n          role_chain: <% let(chain_name => concat($.role_name, 'ServiceChain'))-> $.heat_resource_tree.resources.values().where($.name = $chain_name).first({}) %>\n        on-success:\n          - get_service_chain: <% $.role_chain %>\n          - set_status_failed_get_role_chain: <% not $.role_chain %>\n\n      get_service_chain:\n        publish:\n          service_chain: <% let(resources => $.role_chain.resources)-> $.resource_chains.where($resources.contains($.id)).first('') %>\n        on-success:\n          - get_role_services: <% $.service_chain %>\n          - set_status_failed_get_service_chain: <% not $.service_chain %>\n\n      get_role_services:\n        publish:\n          role_services: <% let(resources => $.heat_resource_tree.resources)-> $.service_chain.resources.select($resources.get($)) %>\n        on-success:\n          - check_features: <% $.role_services %>\n          - set_status_failed_get_role_services: <% not $.role_services %>\n\n      check_features:\n        on-success: build_feature_dict\n        publish:\n          # The role supports the DPDK feature if the NeutronDatapathType parameter is present\n          dpdk: <% let(resources => $.heat_resource_tree.resources) -> $.role_services.any($.get('parameters', []).contains('NeutronDatapathType') or $.get('resources', []).select($resources.get($)).any($.get('parameters', []).contains('NeutronDatapathType'))) %>\n\n          # The role supports the DPDK feature in ODL if the OvsEnableDpdk parameter value is true in role parameters.\n          odl_dpdk: <% let(role => $.role_name) -> $.heat_resource_tree.parameters.get(concat($role, 'Parameters'), {}).get('default', {}).get('OvsEnableDpdk', false) %>\n\n          # The role supports the SRIOV feature if it includes NeutronSriovAgent services.\n          sriov: <% $.role_services.any($.get('type', '').endsWith('::NeutronSriovAgent')) %>\n\n          # The role supports the HCI feature if it includes both NovaCompute and CephOSD services.\n          hci: <% $.role_services.any($.get('type', '').endsWith('::NovaCompute')) and $.role_services.any($.get('type', '').endsWith('::CephOSD')) %>\n\n      build_feature_dict:\n        on-success: filter_features\n        publish:\n          feature_dict: <% dict(DPDK => ($.dpdk or $.odl_dpdk), SRIOV => $.sriov, HOST => ($.dpdk or $.odl_dpdk or $.sriov), HCI => $.hci) %>\n\n      filter_features:\n        publish:\n          # The list of features that are enabled (i.e. are true in the feature_dict).\n          role_features: <% let(feature_dict => $.feature_dict)-> $feature_dict.keys().where($feature_dict[$]) %>\n\n      set_status_failed_get_resource_chains:\n        publish:\n          message: <% 'Unable to locate any resource chains in the heat resource tree' %>\n        on-success: fail\n\n      set_status_failed_get_role_chain:\n        publish:\n          message: <% \"Unable to determine the service chain resource for role '{0}'\".format($.role_name) %>\n        on-success: fail\n\n      set_status_failed_get_service_chain:\n        publish:\n          message: <% \"Unable to determine the service chain for role '{0}'\".format($.role_name) %>\n        on-success: fail\n\n      set_status_failed_get_role_services:\n        publish:\n          message: <% \"Unable to determine list of services for role '{0}'\".format($.role_name) %>\n        on-success: fail\n", "name": "tripleo.derive_params.v1", "tags": [], "created_at": "2018-12-03 11:18:58", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "fa1df666-182f-4cfd-b9c5-a4c23966aacb"}, {"definition": "---\nversion: '2.0'\nname: tripleo.fernet_keys.v1\ndescription: TripleO fernet key rotation workflows\n\nworkflows:\n\n  rotate_fernet_keys:\n\n    input:\n      - container\n      - queue_name: tripleo\n      - ansible_extra_env_variables:\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      rotate_keys:\n        action: tripleo.parameters.rotate_fernet_keys container=<% $.container %>\n        on-success: deploy_ssh_key\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      deploy_ssh_key:\n        workflow: tripleo.validations.v1.copy_ssh_key\n        on-success: get_privkey\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_privkey:\n        action: tripleo.validations.get_privkey\n        on-success: deploy_keys\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      deploy_keys:\n        action: tripleo.ansible-playbook\n        input:\n          hosts: keystone\n          inventory: /usr/bin/tripleo-ansible-inventory\n          ssh_private_key: <% task(get_privkey).result %>\n          extra_env_variables: <% $.ansible_extra_env_variables + dict(TRIPLEO_PLAN_NAME=>$.container) %>\n          verbosity: 0\n          remote_user: heat-admin\n          become: true\n          extra_vars:\n            fernet_keys: <% task(rotate_keys).result %>\n          use_openstack_credentials: true\n          playbook: /usr/share/tripleo-common/playbooks/rotate-keys.yaml\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.fernet_keys.v1.rotate_fernet_keys\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.fernet_keys.v1", "tags": [], "created_at": "2018-12-03 11:19:02", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "973e7fcc-ad26-4580-95a3-f70097396243"}, {"definition": "---\nversion: '2.0'\nname: tripleo.derive_params_formulas.v1\ndescription: TripleO Workflows to derive deployment parameters from the introspected data\n\nworkflows:\n\n\n  dpdk_derive_params:\n    description: >\n      Workflow to derive parameters for DPDK service.\n    input:\n      - plan\n      - role_name\n      - hw_data # introspection data\n      - user_inputs\n      - derived_parameters: {}\n\n    output:\n      derived_parameters: <% $.derived_parameters.mergeWith($.get('dpdk_parameters', {})) %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_network_config:\n        action: tripleo.parameters.get_network_config\n        input:\n          container: <% $.plan %>\n          role_name: <% $.role_name %>\n        publish:\n          network_configs: <% task().result.get('network_config', []) %>\n        on-success: get_dpdk_nics_numa_info\n        on-error: set_status_failed_get_network_config\n\n      get_dpdk_nics_numa_info:\n        action: tripleo.derive_params.get_dpdk_nics_numa_info\n        input:\n          network_configs: <% $.network_configs %>\n          inspect_data: <% $.hw_data %>\n        publish:\n          dpdk_nics_numa_info: <% task().result %>\n        on-success:\n          # TODO: Need to remove condtions here\n          # adding condition and throw error in action for empty check\n          - get_dpdk_nics_numa_nodes: <% $.dpdk_nics_numa_info %>\n          - set_status_failed_get_dpdk_nics_numa_info: <% not $.dpdk_nics_numa_info %>\n        on-error: set_status_failed_on_error_get_dpdk_nics_numa_info\n\n      get_dpdk_nics_numa_nodes:\n        publish:\n          dpdk_nics_numa_nodes: <% $.dpdk_nics_numa_info.groupBy($.numa_node).select($[0]).orderBy($) %>\n        on-success:\n          - get_numa_nodes: <% $.dpdk_nics_numa_nodes %>\n          - set_status_failed_get_dpdk_nics_numa_nodes: <% not $.dpdk_nics_numa_nodes %>\n\n      get_numa_nodes:\n        publish:\n          numa_nodes: <% $.hw_data.numa_topology.ram.select($.numa_node).orderBy($) %>\n        on-success:\n          - get_num_phy_cores_per_numa_for_pmd: <% $.numa_nodes %>\n          - set_status_failed_get_numa_nodes: <% not $.numa_nodes %>\n\n      get_num_phy_cores_per_numa_for_pmd:\n        publish:\n          num_phy_cores_per_numa_node_for_pmd: <% $.user_inputs.get('num_phy_cores_per_numa_node_for_pmd', 0) %>\n        on-success:\n          - get_num_cores_per_numa_nodes: <% isInteger($.num_phy_cores_per_numa_node_for_pmd) and $.num_phy_cores_per_numa_node_for_pmd > 0 %>\n          - set_status_failed_get_num_phy_cores_per_numa_for_pmd_invalid: <% not isInteger($.num_phy_cores_per_numa_node_for_pmd) %>\n          - set_status_failed_get_num_phy_cores_per_numa_for_pmd_not_provided: <% $.num_phy_cores_per_numa_node_for_pmd = 0 %>\n\n      # For NUMA node with DPDK nic, number of cores should be used from user input\n      # For NUMA node without DPDK nic, number of cores should be 1\n      get_num_cores_per_numa_nodes:\n        publish:\n          num_cores_per_numa_nodes: <% let(dpdk_nics_nodes => $.dpdk_nics_numa_nodes, cores => $.num_phy_cores_per_numa_node_for_pmd) -> $.numa_nodes.select(switch($ in $dpdk_nics_nodes => $cores, not $ in $dpdk_nics_nodes => 1)) %>\n        on-success: get_pmd_cpus\n\n      get_pmd_cpus:\n        action: tripleo.derive_params.get_dpdk_core_list\n        input:\n          inspect_data: <% $.hw_data %>\n          numa_nodes_cores_count: <% $.num_cores_per_numa_nodes %>\n        publish:\n          pmd_cpus: <% task().result %>\n        on-success:\n          - get_pmd_cpus_range_list: <% $.pmd_cpus %>\n          - set_status_failed_get_pmd_cpus: <% not $.pmd_cpus %>\n        on-error: set_status_failed_on_error_get_pmd_cpus\n\n      get_pmd_cpus_range_list:\n        action: tripleo.derive_params.convert_number_to_range_list\n        input:\n          num_list: <% $.pmd_cpus %>\n        publish:\n          pmd_cpus: <% task().result %>\n        on-success: get_host_cpus\n        on-error: set_status_failed_get_pmd_cpus_range_list\n\n      get_host_cpus:\n        workflow: tripleo.derive_params_formulas.v1.get_host_cpus\n        input:\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n        publish:\n          host_cpus: <% task().result.get('host_cpus', '') %>\n        on-success: get_sock_mem\n        on-error: set_status_failed_get_host_cpus\n\n      get_sock_mem:\n        action: tripleo.derive_params.get_dpdk_socket_memory\n        input:\n          dpdk_nics_numa_info: <% $.dpdk_nics_numa_info %>\n          numa_nodes: <% $.numa_nodes %>\n          overhead: <% $.user_inputs.get('overhead', 800) %>\n          packet_size_in_buffer: <% 4096*64 %>\n        publish:\n          sock_mem: <% task().result %>\n        on-success:\n          - get_dpdk_parameters: <% $.sock_mem %>\n          - set_status_failed_get_sock_mem: <% not $.sock_mem %>\n        on-error: set_status_failed_on_error_get_sock_mem\n\n      get_dpdk_parameters:\n        publish:\n          dpdk_parameters: <% dict(concat($.role_name, 'Parameters') => dict('OvsPmdCoreList' => $.get('pmd_cpus', ''), 'OvsDpdkCoreList' => $.get('host_cpus', ''), 'OvsDpdkSocketMemory' => $.get('sock_mem', ''))) %>\n\n      set_status_failed_get_network_config:\n        publish:\n          status: FAILED\n          message: <% task(get_network_config).result %>\n        on-success: fail\n\n      set_status_failed_get_dpdk_nics_numa_info:\n        publish:\n          status: FAILED\n          message: \"Unable to determine DPDK NIC's NUMA information\"\n        on-success: fail\n\n      set_status_failed_on_error_get_dpdk_nics_numa_info:\n        publish:\n          status: FAILED\n          message: <% task(get_dpdk_nics_numa_info).result %>\n        on-success: fail\n\n      set_status_failed_get_dpdk_nics_numa_nodes:\n        publish:\n          status: FAILED\n          message: \"Unable to determine DPDK NIC's numa nodes\"\n        on-success: fail\n\n      set_status_failed_get_numa_nodes:\n        publish:\n          status: FAILED\n          message: 'Unable to determine available NUMA nodes'\n        on-success: fail\n\n      set_status_failed_get_num_phy_cores_per_numa_for_pmd_invalid:\n        publish:\n          status: FAILED\n          message: <% \"num_phy_cores_per_numa_node_for_pmd user input '{0}' is invalid\".format($.num_phy_cores_per_numa_node_for_pmd) %>\n        on-success: fail\n\n      set_status_failed_get_num_phy_cores_per_numa_for_pmd_not_provided:\n        publish:\n          status: FAILED\n          message: 'num_phy_cores_per_numa_node_for_pmd user input is not provided'\n        on-success: fail\n\n      set_status_failed_get_pmd_cpus:\n        publish:\n          status: FAILED\n          message: 'Unable to determine OvsPmdCoreList parameter'\n        on-success: fail\n\n      set_status_failed_on_error_get_pmd_cpus:\n        publish:\n          status: FAILED\n          message: <% task(get_pmd_cpus).result %>\n        on-success: fail\n\n      set_status_failed_get_pmd_cpus_range_list:\n        publish:\n          status: FAILED\n          message: <% task(get_pmd_cpus_range_list).result %>\n        on-success: fail\n\n      set_status_failed_get_host_cpus:\n        publish:\n          status: FAILED\n          message: <% task(get_host_cpus).result.get('message', '') %>\n        on-success: fail\n\n      set_status_failed_get_sock_mem:\n        publish:\n          status: FAILED\n          message: 'Unable to determine OvsDpdkSocketMemory parameter'\n        on-success: fail\n\n      set_status_failed_on_error_get_sock_mem:\n        publish:\n          status: FAILED\n          message: <% task(get_sock_mem).result %>\n        on-success: fail\n\n\n  sriov_derive_params:\n    description: >\n      This workflow derives parameters for the SRIOV feature.\n\n    input:\n      - role_name\n      - hw_data # introspection data\n      - derived_parameters: {}\n\n    output:\n      derived_parameters: <% $.derived_parameters.mergeWith($.get('sriov_parameters', {})) %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_host_cpus:\n        workflow: tripleo.derive_params_formulas.v1.get_host_cpus\n        input:\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n        publish:\n          host_cpus: <% task().result.get('host_cpus', '') %>\n        on-success: get_sriov_parameters\n        on-error: set_status_failed_get_host_cpus\n\n      get_sriov_parameters:\n        publish:\n          # SriovHostCpusList parameter is added temporarily and it's removed later from derived parameters result.\n          sriov_parameters: <% dict(concat($.role_name, 'Parameters') => dict('SriovHostCpusList' => $.get('host_cpus', ''))) %>\n\n      set_status_failed_get_host_cpus:\n        publish:\n          status: FAILED\n          message: <% task(get_host_cpus).result.get('message', '') %>\n        on-success: fail\n\n\n  get_host_cpus:\n    description: >\n      Fetching the host CPU list from the introspection data, and then converting the raw list into a range list.\n\n    input:\n      - hw_data # introspection data\n\n    output:\n      host_cpus: <% $.get('host_cpus', '') %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_host_cpus:\n        action: tripleo.derive_params.get_host_cpus_list inspect_data=<% $.hw_data %>\n        publish:\n          host_cpus: <% task().result  %>\n        on-success:\n          - get_host_cpus_range_list: <% $.host_cpus %>\n          - set_status_failed_get_host_cpus: <% not $.host_cpus %>\n        on-error: set_status_failed_on_error_get_host_cpus\n\n      get_host_cpus_range_list:\n        action: tripleo.derive_params.convert_number_to_range_list\n        input:\n          num_list: <% $.host_cpus %>\n        publish:\n          host_cpus: <% task().result %>\n        on-error: set_status_failed_get_host_cpus_range_list\n\n      set_status_failed_get_host_cpus:\n        publish:\n          status: FAILED\n          message: 'Unable to determine host cpus'\n        on-success: fail\n\n      set_status_failed_on_error_get_host_cpus:\n        publish:\n          status: FAILED\n          message: <% task(get_host_cpus).result %>\n        on-success: fail\n\n      set_status_failed_get_host_cpus_range_list:\n        publish:\n          status: FAILED\n          message: <% task(get_host_cpus_range_list).result %>\n        on-success: fail\n\n\n  host_derive_params:\n    description: >\n      This workflow derives parameters for the Host process, and is mainly associated with CPU pinning and huge memory pages.\n      This workflow can be dependent on any feature or also can be invoked individually as well.\n\n    input:\n      - role_name\n      - hw_data # introspection data\n      - user_inputs\n      - derived_parameters: {}\n\n    output:\n      derived_parameters: <% $.derived_parameters.mergeWith($.get('host_parameters', {})) %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_cpus:\n        publish:\n          cpus: <% $.hw_data.numa_topology.cpus %>\n        on-success:\n          - get_role_derive_params: <% $.cpus %>\n          - set_status_failed_get_cpus: <% not $.cpus %>\n\n      get_role_derive_params:\n        publish:\n          role_derive_params: <% $.derived_parameters.get(concat($.role_name, 'Parameters'), {}) %>\n          # removing the role parameters (eg. ComputeParameters) in derived_parameters dictionary since already copied in role_derive_params.\n          derived_parameters: <% $.derived_parameters.delete(concat($.role_name, 'Parameters')) %>\n        on-success: get_host_cpus\n\n      get_host_cpus:\n        publish:\n          host_cpus: <% $.role_derive_params.get('OvsDpdkCoreList', '') or $.role_derive_params.get('SriovHostCpusList', '') %>\n          # SriovHostCpusList parameter is added temporarily for host_cpus and not needed in derived_parameters result.\n          # SriovHostCpusList parameter is deleted in derived_parameters list and adding the updated role parameters\n          # back in the derived_parameters.\n          derived_parameters: <% $.derived_parameters + dict(concat($.role_name, 'Parameters') => $.role_derive_params.delete('SriovHostCpusList')) %>\n        on-success: get_host_dpdk_combined_cpus\n\n      get_host_dpdk_combined_cpus:\n        publish:\n          host_dpdk_combined_cpus: <% let(pmd_cpus => $.role_derive_params.get('OvsPmdCoreList', '')) -> switch($pmd_cpus => concat($pmd_cpus, ',', $.host_cpus), not $pmd_cpus => $.host_cpus) %>\n          reserved_cpus: []\n        on-success:\n          - get_host_dpdk_combined_cpus_num_list: <% $.host_dpdk_combined_cpus %>\n          - set_status_failed_get_host_dpdk_combined_cpus: <% not $.host_dpdk_combined_cpus %>\n\n      get_host_dpdk_combined_cpus_num_list:\n        action: tripleo.derive_params.convert_range_to_number_list\n        input:\n          range_list: <% $.host_dpdk_combined_cpus %>\n        publish:\n          host_dpdk_combined_cpus: <% task().result %>\n          reserved_cpus: <% task().result.split(',') %>\n        on-success: get_nova_cpus\n        on-error: set_status_failed_get_host_dpdk_combined_cpus_num_list\n\n      get_nova_cpus:\n        publish:\n          nova_cpus: <% let(reserved_cpus => $.reserved_cpus) -> $.cpus.select($.thread_siblings).flatten().where(not (str($) in $reserved_cpus)).join(',') %>\n        on-success:\n          - get_isol_cpus: <% $.nova_cpus %>\n          - set_status_failed_get_nova_cpus: <% not $.nova_cpus %>\n\n      # concatinates OvsPmdCoreList range format and NovaVcpuPinSet in range format. it may not be in perfect range format.\n      # example: concatinates '12-15,19' and 16-18' ranges '12-15,19,16-18'\n      get_isol_cpus:\n        publish:\n          isol_cpus: <% let(pmd_cpus => $.role_derive_params.get('OvsPmdCoreList','')) -> switch($pmd_cpus => concat($pmd_cpus, ',', $.nova_cpus), not $pmd_cpus => $.nova_cpus) %>\n        on-success: get_isol_cpus_num_list\n\n      # Gets the isol_cpus in the number list\n      # example: '12-15,19,16-18' into '12,13,14,15,16,17,18,19'\n      get_isol_cpus_num_list:\n        action: tripleo.derive_params.convert_range_to_number_list\n        input:\n          range_list: <% $.isol_cpus %>\n        publish:\n          isol_cpus: <% task().result %>\n        on-success: get_nova_cpus_range_list\n        on-error: set_status_failed_get_isol_cpus_num_list\n\n      get_nova_cpus_range_list:\n        action: tripleo.derive_params.convert_number_to_range_list\n        input:\n          num_list: <% $.nova_cpus %>\n        publish:\n          nova_cpus: <% task().result %>\n        on-success: get_isol_cpus_range_list\n        on-error: set_status_failed_get_nova_cpus_range_list\n\n      # converts number format isol_cpus into range format\n      # example: '12,13,14,15,16,17,18,19' into '12-19'\n      get_isol_cpus_range_list:\n        action: tripleo.derive_params.convert_number_to_range_list\n        input:\n          num_list: <% $.isol_cpus %>\n        publish:\n          isol_cpus: <% task().result %>\n        on-success: get_host_mem\n        on-error: set_status_failed_get_isol_cpus_range_list\n\n      get_host_mem:\n        publish:\n          host_mem: <% $.user_inputs.get('host_mem_default', 4096) %>\n        on-success: check_default_hugepage_supported\n\n      check_default_hugepage_supported:\n        publish:\n          default_hugepage_supported: <% $.hw_data.get('inventory', {}).get('cpu', {}).get('flags', []).contains('pdpe1gb') %>\n        on-success:\n          - get_total_memory: <% $.default_hugepage_supported %>\n          - set_status_failed_check_default_hugepage_supported: <% not $.default_hugepage_supported %>\n\n      get_total_memory:\n        publish:\n          total_memory: <% $.hw_data.get('inventory', {}).get('memory', {}).get('physical_mb', 0) %>\n        on-success:\n          - get_hugepage_allocation_percentage: <% $.total_memory %>\n          - set_status_failed_get_total_memory: <% not $.total_memory %>\n\n      get_hugepage_allocation_percentage:\n        publish:\n          huge_page_allocation_percentage: <% $.user_inputs.get('huge_page_allocation_percentage', 0) %>\n        on-success:\n          - get_hugepages: <% isInteger($.huge_page_allocation_percentage) and $.huge_page_allocation_percentage > 0 %>\n          - set_status_failed_get_hugepage_allocation_percentage_invalid: <% not isInteger($.huge_page_allocation_percentage) %>\n          - set_status_failed_get_hugepage_allocation_percentage_not_provided: <% $.huge_page_allocation_percentage = 0 %>\n\n      get_hugepages:\n        publish:\n          hugepages: <% let(huge_page_perc => float($.huge_page_allocation_percentage)/100)-> int((($.total_memory/1024)-4) * $huge_page_perc) %>\n        on-success:\n          - get_cpu_model: <% $.hugepages %>\n          - set_status_failed_get_hugepages: <% not $.hugepages %>\n\n      get_cpu_model:\n        publish:\n          intel_cpu_model: <% $.hw_data.get('inventory', {}).get('cpu', {}).get('model_name', '').startsWith('Intel') %>\n        on-success: get_iommu_info\n\n      get_iommu_info:\n        publish:\n          iommu_info: <% switch($.intel_cpu_model => 'intel_iommu=on iommu=pt', not $.intel_cpu_model => '') %>\n        on-success: get_kernel_args\n\n      get_kernel_args:\n        publish:\n          kernel_args: <% concat('default_hugepagesz=1GB hugepagesz=1G ', 'hugepages=', str($.hugepages), ' ',  $.iommu_info, ' isolcpus=', $.isol_cpus) %>\n        on-success: get_host_parameters\n\n      get_host_parameters:\n        publish:\n          host_parameters: <% dict(concat($.role_name, 'Parameters') => dict('NovaVcpuPinSet' => $.get('nova_cpus', ''), 'NovaReservedHostMemory' => $.get('host_mem', ''), 'KernelArgs' => $.get('kernel_args', ''), 'IsolCpusList' => $.get('isol_cpus', ''))) %>\n\n      set_status_failed_get_cpus:\n        publish:\n          status: FAILED\n          message: \"Unable to determine CPU's on NUMA nodes\"\n        on-success: fail\n\n      set_status_failed_get_host_dpdk_combined_cpus:\n        publish:\n          status: FAILED\n          message: 'Unable to combine host and dpdk cpus list'\n        on-success: fail\n\n      set_status_failed_get_host_dpdk_combined_cpus_num_list:\n        publish:\n          status: FAILED\n          message: <% task(get_host_dpdk_combined_cpus_num_list).result %>\n        on-success: fail\n\n      set_status_failed_get_nova_cpus:\n        publish:\n          status: FAILED\n          message: 'Unable to determine nova vcpu pin set'\n        on-success: fail\n\n      set_status_failed_get_nova_cpus_range_list:\n        publish:\n          status: FAILED\n          message: <% task(get_nova_cpus_range_list).result %>\n        on-success: fail\n\n      set_status_failed_get_isol_cpus_num_list:\n        publish:\n          status: FAILED\n          message: <% task(get_isol_cpus_num_list).result %>\n        on-success: fail\n\n      set_status_failed_get_isol_cpus_range_list:\n        publish:\n          status: FAILED\n          message: <% task(get_isol_cpus_range_list).result %>\n        on-success: fail\n\n      set_status_failed_check_default_hugepage_supported:\n        publish:\n          status: FAILED\n          message: 'default huge page size 1GB is not supported'\n        on-success: fail\n\n      set_status_failed_get_total_memory:\n        publish:\n          status: FAILED\n          message: 'Unable to determine total memory'\n        on-success: fail\n\n      set_status_failed_get_hugepage_allocation_percentage_invalid:\n        publish:\n          status: FAILED\n          message: <% \"huge_page_allocation_percentage user input '{0}' is invalid\".format($.huge_page_allocation_percentage) %>\n        on-success: fail\n\n      set_status_failed_get_hugepage_allocation_percentage_not_provided:\n        publish:\n          status: FAILED\n          message: 'huge_page_allocation_percentage user input is not provided'\n        on-success: fail\n\n      set_status_failed_get_hugepages:\n        publish:\n          status: FAILED\n          message: 'Unable to determine huge pages'\n        on-success: fail\n\n\n  hci_derive_params:\n    description: Derive the deployment parameters for HCI\n    input:\n      - role_name\n      - environment_parameters\n      - heat_resource_tree\n      - introspection_data\n      - user_inputs\n      - derived_parameters: {}\n\n    output:\n      derived_parameters: <% $.derived_parameters.mergeWith($.get('hci_parameters', {})) %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_hci_inputs:\n        publish:\n          hci_profile: <% $.user_inputs.get('hci_profile', '') %>\n          hci_profile_config: <% $.user_inputs.get('hci_profile_config', {}) %>\n          MB_PER_GB: 1024\n        on-success:\n          - get_average_guest_memory_size_in_mb: <% $.hci_profile and $.hci_profile_config.get($.hci_profile, {}) %>\n          - set_failed_invalid_hci_profile: <% $.hci_profile and not $.hci_profile_config.get($.hci_profile, {}) %>\n          # When no hci_profile is specified, the workflow terminates without deriving any HCI parameters.\n\n      get_average_guest_memory_size_in_mb:\n        publish:\n          average_guest_memory_size_in_mb: <% $.hci_profile_config.get($.hci_profile, {}).get('average_guest_memory_size_in_mb', 0) %>\n        on-success:\n          - get_average_guest_cpu_utilization_percentage: <% isInteger($.average_guest_memory_size_in_mb) %>\n          - set_failed_invalid_average_guest_memory_size_in_mb: <% not isInteger($.average_guest_memory_size_in_mb) %>\n\n      get_average_guest_cpu_utilization_percentage:\n        publish:\n          average_guest_cpu_utilization_percentage: <% $.hci_profile_config.get($.hci_profile, {}).get('average_guest_cpu_utilization_percentage', 0) %>\n        on-success:\n          - get_gb_overhead_per_guest: <% isInteger($.average_guest_cpu_utilization_percentage) %>\n          - set_failed_invalid_average_guest_cpu_utilization_percentage: <% not isInteger($.average_guest_cpu_utilization_percentage) %>\n\n      get_gb_overhead_per_guest:\n        publish:\n          gb_overhead_per_guest: <% $.user_inputs.get('gb_overhead_per_guest', 0.5) %>\n        on-success:\n          - get_gb_per_osd: <% isNumber($.gb_overhead_per_guest) %>\n          - set_failed_invalid_gb_overhead_per_guest: <% not isNumber($.gb_overhead_per_guest) %>\n\n      get_gb_per_osd:\n        publish:\n          gb_per_osd: <% $.user_inputs.get('gb_per_osd', 5) %>\n        on-success:\n          - get_cores_per_osd: <% isNumber($.gb_per_osd) %>\n          - set_failed_invalid_gb_per_osd: <% not isNumber($.gb_per_osd) %>\n\n      get_cores_per_osd:\n        publish:\n          cores_per_osd: <% $.user_inputs.get('cores_per_osd', 1.0) %>\n        on-success:\n          - get_extra_configs: <% isNumber($.cores_per_osd) %>\n          - set_failed_invalid_cores_per_osd: <% not isNumber($.cores_per_osd) %>\n\n      get_extra_configs:\n        publish:\n          extra_config: <% $.environment_parameters.get('ExtraConfig', {}) %>\n          role_extra_config: <% $.environment_parameters.get(concat($.role_name, 'ExtraConfig'), {}) %>\n          role_env_params: <% $.environment_parameters.get(concat($.role_name, 'Parameters'), {}) %>\n          role_derive_params: <% $.derived_parameters.get(concat($.role_name, 'Parameters'), {}) %>\n        on-success: get_num_osds\n\n      get_num_osds:\n        publish:\n          num_osds: <% $.heat_resource_tree.parameters.get('CephAnsibleDisksConfig', {}).get('default', {}).get('devices', []).count() %>\n        on-success:\n          - get_memory_mb: <% $.num_osds %>\n          # If there's no CephAnsibleDisksConfig then look for OSD configuration in hiera data\n          - get_num_osds_from_hiera: <% not $.num_osds %>\n\n      get_num_osds_from_hiera:\n        publish:\n          num_osds: <% $.role_extra_config.get('ceph::profile::params::osds', $.extra_config.get('ceph::profile::params::osds', {})).keys().count() %>\n        on-success:\n          - get_memory_mb: <% $.num_osds %>\n          - set_failed_no_osds: <% not $.num_osds %>\n\n      get_memory_mb:\n        publish:\n          memory_mb: <% $.introspection_data.get('memory_mb', 0) %>\n        on-success:\n          - get_nova_vcpu_pin_set: <% $.memory_mb %>\n          - set_failed_get_memory_mb: <% not $.memory_mb %>\n\n      # Determine the number of CPU cores available to Nova and Ceph. If\n      # NovaVcpuPinSet is defined then use the number of vCPUs in the set,\n      # otherwise use all of the cores identified in the introspection data.\n\n      get_nova_vcpu_pin_set:\n        publish:\n          # NovaVcpuPinSet can be defined in multiple locations, and it's\n          # important to select the value in order of precedence:\n          #\n          # 1) User specified value for this role\n          # 2) User specified default value for all roles\n          # 3) Value derived by another derived parameters workflow\n          nova_vcpu_pin_set: <% $.role_env_params.get('NovaVcpuPinSet', $.environment_parameters.get('NovaVcpuPinSet', $.role_derive_params.get('NovaVcpuPinSet', ''))) %>\n        on-success:\n          - get_nova_vcpu_count: <% $.nova_vcpu_pin_set %>\n          - get_num_cores: <% not $.nova_vcpu_pin_set %>\n\n      get_nova_vcpu_count:\n        action: tripleo.derive_params.convert_range_to_number_list\n        input:\n          range_list: <% $.nova_vcpu_pin_set %>\n        publish:\n          num_cores: <% task().result.split(',').count() %>\n        on-success: calculate_nova_parameters\n        on-error: set_failed_get_nova_vcpu_count\n\n      get_num_cores:\n        publish:\n          num_cores: <% $.introspection_data.get('cpus', 0) %>\n        on-success:\n          - calculate_nova_parameters: <% $.num_cores %>\n          - set_failed_get_num_cores: <% not $.num_cores %>\n\n      # HCI calculations are broken into multiple steps. This is necessary\n      # because variables published by a Mistral task are not available\n      # for use by that same task. Variables computed and published in a task\n      # are only available in subsequent tasks.\n      #\n      # The HCI calculations compute two Nova parameters:\n      # - reserved_host_memory\n      # - cpu_allocation_ratio\n      #\n      # The reserved_host_memory calculation computes the amount of memory\n      # that needs to be reserved for Ceph and the total amount of \"guest\n      # overhead\" memory that is based on the anticipated number of guests.\n      # Psuedo-code for the calculation (disregarding MB and GB units) is\n      # as follows:\n      #\n      #   ceph_memory = mem_per_osd * num_osds\n      #   nova_memory = total_memory - ceph_memory\n      #   num_guests = nova_memory /\n      #                (average_guest_memory_size + overhead_per_guest)\n      #   reserved_memory = ceph_memory + (num_guests * overhead_per_guest)\n      #\n      # The cpu_allocation_ratio calculation is similar in that it takes into\n      # account the number of cores that must be reserved for Ceph.\n      #\n      #   ceph_cores = cores_per_osd * num_osds\n      #   guest_cores = num_cores - ceph_cores\n      #   guest_vcpus = guest_cores / average_guest_utilization\n      #   cpu_allocation_ratio = guest_vcpus / num_cores\n\n      calculate_nova_parameters:\n        publish:\n          avg_guest_util: <% $.average_guest_cpu_utilization_percentage / 100.0 %>\n          avg_guest_size_gb: <% $.average_guest_memory_size_in_mb / float($.MB_PER_GB) %>\n          memory_gb: <% $.memory_mb / float($.MB_PER_GB) %>\n          ceph_mem_gb: <% $.gb_per_osd * $.num_osds %>\n          nonceph_cores: <% $.num_cores - int($.cores_per_osd * $.num_osds) %>\n        on-success: calc_step_2\n\n      calc_step_2:\n        publish:\n          num_guests: <% int(($.memory_gb - $.ceph_mem_gb) / ($.avg_guest_size_gb + $.gb_overhead_per_guest)) %>\n          guest_vcpus: <% $.nonceph_cores / $.avg_guest_util %>\n        on-success: calc_step_3\n\n      calc_step_3:\n        publish:\n          reserved_host_memory: <% $.MB_PER_GB * int($.ceph_mem_gb + ($.num_guests * $.gb_overhead_per_guest)) %>\n          cpu_allocation_ratio: <% $.guest_vcpus / $.num_cores %>\n        on-success: validate_results\n\n      validate_results:\n        publish:\n          # Verify whether HCI is viable:\n          # - At least 80% of the memory is reserved for Ceph and guest overhead\n          # - At least half of the CPU cores must be available to Nova\n          mem_ok: <% $.reserved_host_memory <= ($.memory_mb * 0.8) %>\n          cpu_ok: <% $.cpu_allocation_ratio >= 0.5 %>\n        on-success:\n          - set_failed_insufficient_mem: <% not $.mem_ok %>\n          - set_failed_insufficient_cpu: <% not $.cpu_ok %>\n          - publish_hci_parameters: <% $.mem_ok and $.cpu_ok %>\n\n      publish_hci_parameters:\n        publish:\n          # TODO(abishop): Update this when the cpu_allocation_ratio can be set\n          # via a THT parameter (no such parameter currently exists). Until a\n          # THT parameter exists, use hiera data to set the cpu_allocation_ratio.\n          hci_parameters: <% dict(concat($.role_name, 'Parameters') => dict('NovaReservedHostMemory' => $.reserved_host_memory)) + dict(concat($.role_name, 'ExtraConfig') => dict('nova::cpu_allocation_ratio' => $.cpu_allocation_ratio)) %>\n\n      set_failed_invalid_hci_profile:\n        publish:\n          message: \"'<% $.hci_profile %>' is not a valid HCI profile.\"\n        on-success: fail\n\n      set_failed_invalid_average_guest_memory_size_in_mb:\n        publish:\n          message: \"'<% $.average_guest_memory_size_in_mb %>' is not a valid average_guest_memory_size_in_mb value.\"\n        on-success: fail\n\n      set_failed_invalid_gb_overhead_per_guest:\n        publish:\n          message: \"'<% $.gb_overhead_per_guest %>' is not a valid gb_overhead_per_guest value.\"\n        on-success: fail\n\n      set_failed_invalid_gb_per_osd:\n        publish:\n          message: \"'<% $.gb_per_osd %>' is not a valid gb_per_osd value.\"\n        on-success: fail\n\n      set_failed_invalid_cores_per_osd:\n        publish:\n          message: \"'<% $.cores_per_osd %>' is not a valid cores_per_osd value.\"\n        on-success: fail\n\n      set_failed_invalid_average_guest_cpu_utilization_percentage:\n        publish:\n          message: \"'<% $.average_guest_cpu_utilization_percentage %>' is not a valid average_guest_cpu_utilization_percentage value.\"\n        on-success: fail\n\n      set_failed_no_osds:\n        publish:\n          message: \"No Ceph OSDs found in the overcloud definition ('ceph::profile::params::osds').\"\n        on-success: fail\n\n      set_failed_get_memory_mb:\n        publish:\n          message: \"Unable to determine the amount of physical memory (no 'memory_mb' found in introspection_data).\"\n        on-success: fail\n\n      set_failed_get_nova_vcpu_count:\n        publish:\n          message: <% task(get_nova_vcpu_count).result %>\n        on-success: fail\n\n      set_failed_get_num_cores:\n        publish:\n          message: \"Unable to determine the number of CPU cores (no 'cpus' found in introspection_data).\"\n        on-success: fail\n\n      set_failed_insufficient_mem:\n        publish:\n          message: \"<% $.memory_mb %> MB is not enough memory to run hyperconverged.\"\n        on-success: fail\n\n      set_failed_insufficient_cpu:\n        publish:\n          message: \"<% $.num_cores %> CPU cores are not enough to run hyperconverged.\"\n        on-success: fail\n", "name": "tripleo.derive_params_formulas.v1", "tags": [], "created_at": "2018-12-03 11:19:02", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "bc5dc13a-902d-4396-bd53-ccc503597905"}, {"definition": "---\nversion: '2.0'\nname: tripleo.octavia_post.v1\ndescription: TripleO Octavia post deployment Workflows\n\nworkflows:\n\n  octavia_post_deploy:\n    description: Octavia post deployment\n    input:\n      - amp_image_name\n      - amp_image_filename\n      - amp_image_tag\n      - amp_ssh_key_name\n      - amp_ssh_key_path\n      - amp_ssh_key_data\n      - auth_username\n      - auth_password\n      - auth_project_name\n      - lb_mgmt_net_name\n      - lb_mgmt_subnet_name\n      - lb_sec_group_name\n      - lb_mgmt_subnet_cidr\n      - lb_mgmt_subnet_gateway\n      - lb_mgmt_subnet_pool_start\n      - lb_mgmt_subnet_pool_end\n      - generate_certs\n      - octavia_ansible_playbook\n      - overcloud_admin\n      - ca_cert_path\n      - ca_private_key_path\n      - ca_passphrase\n      - client_cert_path\n      - mgmt_port_dev\n      - overcloud_password\n      - overcloud_project\n      - overcloud_pub_auth_uri\n      - ansible_extra_env_variables:\n          ANSIBLE_HOST_KEY_CHECKING: 'False'\n          ANSIBLE_SSH_RETRIES: '3'\n    tags:\n      - tripleo-common-managed\n    tasks:\n      get_overcloud_stack_details:\n        publish:\n          # TODO(beagles), we are making an assumption about the octavia heatlh manager and\n          # controller worker needing\n          #\n          octavia_controller_ips: <% env().get('service_ips', {}).get('octavia_worker_ctlplane_node_ips', []) %>\n        on-success: enable_ssh_admin\n\n      enable_ssh_admin:\n        workflow: tripleo.access.v1.enable_ssh_admin\n        input:\n          ssh_servers: <% $.octavia_controller_ips %>\n        on-success: get_private_key\n\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: make_local_temp_directory\n\n      make_local_temp_directory:\n        action: tripleo.files.make_temp_dir\n        publish:\n          undercloud_local_dir: <% task().result.path %>\n        on-success: make_remote_temp_directory\n\n      make_remote_temp_directory:\n        action: tripleo.files.make_temp_dir\n        publish:\n          undercloud_remote_dir: <% task().result.path %>\n        on-success: build_local_connection_environment_vars\n\n      build_local_connection_environment_vars:\n        publish:\n          ansible_local_connection_variables: <% dict('ANSIBLE_REMOTE_TEMP' => $.undercloud_remote_dir, 'ANSIBLE_LOCAL_TEMP' => $.undercloud_local_dir) + $.ansible_extra_env_variables %>\n        on-success: upload_amphora\n\n      upload_amphora:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            undercloud:\n              hosts:\n                localhost:\n                  ansible_connection: local\n\n          playbook: <% $.octavia_ansible_playbook %>\n          remote_user: stack\n          extra_env_variables: <% $.ansible_local_connection_variables %>\n          extra_vars:\n            os_password: <% $.overcloud_password %>\n            os_username: <% $.overcloud_admin %>\n            os_project_name: <% $.overcloud_project %>\n            os_auth_url: <% $.overcloud_pub_auth_uri %>\n            os_auth_type: \"password\"\n            os_identity_api_version: \"3\"\n            amp_image_name: <% $.amp_image_name %>\n            amp_image_filename: <% $.amp_image_filename %>\n            amp_image_tag: <% $.amp_image_tag %>\n            amp_ssh_key_name: <% $.amp_ssh_key_name %>\n            amp_ssh_key_path: <% $.amp_ssh_key_path %>\n            amp_ssh_key_data: <% $.amp_ssh_key_data %>\n            auth_username: <% $.auth_username %>\n            auth_password: <% $.auth_password %>\n            auth_project_name: <% $.auth_project_name %>\n        on-success: config_octavia\n\n      config_octavia:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            octavia_nodes:\n              hosts: <% $.octavia_controller_ips.toDict($, {}) %>\n          verbosity: 0\n          playbook: <% $.octavia_ansible_playbook %>\n          remote_user: tripleo-admin\n          become: true\n          become_user: root\n          ssh_private_key: <% $.private_key %>\n          ssh_common_args: '-o StrictHostKeyChecking=no'\n          ssh_extra_args: '-o UserKnownHostsFile=/dev/null'\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          extra_vars:\n            os_password: <% $.overcloud_password %>\n            os_username: <% $.overcloud_admin %>\n            os_project_name: <% $.overcloud_project %>\n            os_auth_url: <% $.overcloud_pub_auth_uri %>\n            os_auth_type: \"password\"\n            os_identity_api_version: \"3\"\n            amp_image_tag: <% $.amp_image_tag %>\n            lb_mgmt_net_name: <% $.lb_mgmt_net_name %>\n            lb_mgmt_subnet_name: <% $.lb_mgmt_subnet_name %>\n            lb_sec_group_name: <% $.lb_sec_group_name %>\n            lb_mgmt_subnet_cidr: <% $.lb_mgmt_subnet_cidr %>\n            lb_mgmt_subnet_gateway: <% $.lb_mgmt_subnet_gateway %>\n            lb_mgmt_subnet_pool_start: <% $.lb_mgmt_subnet_pool_start %>\n            lb_mgmt_subnet_pool_end: <% $.lb_mgmt_subnet_pool_end %>\n            ca_cert_path: <% $.ca_cert_path %>\n            ca_private_key_path: <% $.ca_private_key_path %>\n            ca_passphrase: <% $.ca_passphrase %>\n            client_cert_path: <% $.client_cert_path %>\n            generate_certs: <% $.generate_certs %>\n            mgmt_port_dev: <% $.mgmt_port_dev %>\n            auth_project_name: <% $.auth_project_name %>\n        on-complete: purge_local_temp_dir\n      purge_local_temp_dir:\n        action: tripleo.files.remove_temp_dir path=<% $.undercloud_local_dir %>\n        on-complete: purge_remote_temp_dir\n      purge_remote_temp_dir:\n        action: tripleo.files.remove_temp_dir path=<% $.undercloud_remote_dir %>\n\n", "name": "tripleo.octavia_post.v1", "tags": [], "created_at": "2018-12-03 11:19:03", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "69f3c5a9-bc16-4c51-b834-a466dfc1100a"}, {"definition": "---\nversion: '2.0'\nname: tripleo.networks.v1\ndescription: TripleO Overcloud Networks Workflows v1\n\nworkflows:\n\n  validate_networks_input:\n    description: >\n      Validate that required fields are present.\n\n    input:\n      - networks\n      - queue_name: tripleo\n\n    output:\n      result: <% task(validate_network_names).result %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      validate_network_names:\n        publish:\n          network_name_present: <% $.networks.all($.containsKey('name')) %>\n        on-success:\n          - set_status_success: <% $.network_name_present = true %>\n          - set_status_error: <% $.network_name_present = false %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(validate_network_names).result %>\n\n      set_status_error:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: \"One or more entries did not contain the required field 'name'\"\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.networks.v1.validate_networks_input\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_networks:\n    description: >\n      Takes data in networks parameter in json format, validates its contents,\n      and persists them in network_data.yaml. After successful update,\n      templates are regenerated.\n\n    input:\n      - container: overcloud\n      - networks\n      - network_data_file: 'network_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      validate_input:\n        description: >\n          validate the format of input (input includes required fields for\n          each network)\n        workflow: validate_networks_input\n        input:\n          networks: <% $.networks %>\n        on-success: validate_network_files\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      validate_network_files:\n        description: >\n          validate that Network names exist in Swift container\n        workflow: tripleo.plan_management.v1.validate_network_files\n        input:\n          container: <% $.container %>\n          network_data: <% $.networks %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().network_data %>\n        on-success: get_available_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      get_available_networks:\n        workflow: tripleo.plan_management.v1.list_available_networks\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n        publish:\n          available_networks: <% task().result.available_networks %>\n        on-success: get_current_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      get_current_networks:\n        workflow: tripleo.plan_management.v1.get_network_data\n        input:\n          container: <% $.container %>\n          network_data_file: <% $.network_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          current_networks: <% task().result.network_data %>\n        on-success: update_network_data\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      update_network_data:\n        description: >\n          Combine (or replace) the network data\n        action: tripleo.plan.update_networks\n        input:\n          networks: <% $.available_networks %>\n          current_networks: <% $.current_networks %>\n          remove_all: false\n        publish:\n          new_network_data: <% task().result.network_data %>\n        on-success: update_network_data_in_swift\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      update_network_data_in_swift:\n        description: >\n          update network_data.yaml object in Swift with data from workflow input\n        action: swift.put_object\n        input:\n          container: <% $.container %>\n          obj: <% $.network_data_file %>\n          contents: <% yaml_dump($.new_network_data) %>\n        on-success: regenerate_templates\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      regenerate_templates:\n        action: tripleo.templates.process container=<% $.container %>\n        on-success: get_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      get_networks:\n        description: >\n          run GetNetworksAction to get updated contents of network_data.yaml and\n          provide it as output\n        workflow: tripleo.plan_management.v1.get_network_data\n        input:\n          container: <% $.container %>\n          network_data_file: <% $.network_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().network_data %>\n        on-success: set_status_success\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(get_networks).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.networks.v1.update_networks\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.networks.v1", "tags": [], "created_at": "2018-12-03 11:19:03", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "8d5dca8c-ada5-43df-be70-43e6cdc1cef4"}, {"definition": "---\nversion: '2.0'\nname: tripleo.package_update.v1\ndescription: TripleO update workflows\n\nworkflows:\n\n  # Updates a workload cloud stack\n  package_update_plan:\n    description: Take a container and perform a package update with possible breakpoints\n\n    input:\n      - container\n      - ceph_ansible_playbook\n      - timeout: 240\n      - queue_name: tripleo\n      - skip_deploy_identifier: False\n      - config_dir: '/tmp/'\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      update:\n        action: tripleo.package_update.update_stack\n        input:\n          timeout: <% $.timeout %>\n          container: <% $.container %>\n          ceph_ansible_playbook: <% $.ceph_ansible_playbook %>\n        on-success: clean_plan\n        on-error: set_update_failed\n\n      clean_plan:\n        action: tripleo.plan.update_plan_environment\n        input:\n          container: <% $.container %>\n          parameter: CephAnsiblePlaybook\n          env_key: parameter_defaults\n          delete: true\n        on-success: send_message\n        on-error: set_update_failed\n\n\n      set_update_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(update).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.package_update_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  get_config:\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_config:\n        action: tripleo.config.get_overcloud_config container=<% $.container %>\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n        publish-on-error:\n          status: FAILED\n          message: Init Minor update failed\n        on-complete: send_message\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.package_update_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_nodes:\n    description: Take a container and perform an update nodes by nodes\n\n    input:\n      - node_user: heat-admin\n      - nodes\n      - playbook\n      - inventory_file\n      - ansible_queue_name: tripleo\n      - module_path: /usr/share/ansible-modules\n      - ansible_extra_env_variables:\n            ANSIBLE_LOG_PATH: /var/log/mistral/package_update.log\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n      - verbosity: 1\n      - work_dir: /var/lib/mistral\n      - skip_tags: ''\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      download_config:\n        action: tripleo.config.download_config\n        input:\n          work_dir: <% $.work_dir %>/<% execution().id %>\n        on-success: get_private_key\n        on-error: node_update_failed\n\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: node_update\n\n      node_update:\n        action: tripleo.ansible-playbook\n        input:\n          inventory: <% $.inventory_file %>\n          playbook: <% $.work_dir %>/<% execution().id %>/<% $.playbook %>\n          remote_user: <% $.node_user %>\n          become: true\n          become_user: root\n          verbosity: <% $.verbosity %>\n          ssh_private_key: <% $.private_key %>\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          limit_hosts: <% $.nodes %>\n          module_path: <% $.module_path %>\n          queue_name: <% $.ansible_queue_name %>\n          execution_id: <% execution().id %>\n          skip_tags: <% $.skip_tags %>\n          trash_output: true\n        on-success:\n          - node_update_passed: <% task().result.returncode = 0 %>\n          - node_update_failed: <% task().result.returncode != 0 %>\n        on-error: node_update_failed\n        publish:\n          output: <% task().result %>\n\n      node_update_passed:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: Updated nodes - <% $.nodes %>\n\n      node_update_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: Failed to update nodes - <% $.nodes %>, please see the logs.\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.ansible_queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.update_nodes\n              payload:\n                status: <% $.status %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_converge_plan:\n    description: Take a container and perform the converge for minor update\n\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      remove_noop:\n        action: tripleo.plan.remove_noop_deploystep\n        input:\n          container: <% $.container %>\n        on-success: send_message\n        on-error: set_update_failed\n\n      set_update_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(remove_noop).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.update_converge_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  converge_upgrade_plan:\n    description: Take a container and perform the converge step of a major upgrade\n\n    input:\n      - container\n      - timeout: 240\n      - queue_name: tripleo\n      - skip_deploy_identifier: False\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      remove_noop:\n        action: tripleo.plan.remove_noop_deploystep\n        input:\n          container: <% $.container %>\n        on-success: send_message\n        on-error: set_update_failed\n\n      set_update_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(upgrade_converge).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.major_upgrade.v1.converge_upgrade_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  ffwd_upgrade_converge_plan:\n    description: ffwd-upgrade converge removes DeploymentSteps no-op from plan\n\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      remove_noop:\n        action: tripleo.plan.remove_noop_deploystep\n        input:\n          container: <% $.container %>\n        on-success: send_message\n        on-error: set_update_failed\n\n      set_update_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(remove_noop).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.ffwd_upgrade_converge_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.package_update.v1", "tags": [], "created_at": "2018-12-03 11:19:04", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "30274b50-4833-48c3-972a-7520aad2bebf"}, {"definition": "---\nversion: '2.0'\nname: tripleo.plan_management.v1\ndescription: TripleO Overcloud Deployment Workflows v1\n\nworkflows:\n\n  create_default_deployment_plan:\n    description: >\n      This workflow exists to maintain backwards compatibility in pike.  This\n      workflow will likely be removed in queens in favor of create_deployment_plan.\n    input:\n      - container\n      - queue_name: tripleo\n      - generate_passwords: true\n    tags:\n      - tripleo-common-managed\n    tasks:\n      call_create_deployment_plan:\n        workflow: tripleo.plan_management.v1.create_deployment_plan\n        on-success: set_status_success\n        on-error: call_create_deployment_plan_set_status_failed\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n          generate_passwords: <% $.generate_passwords %>\n          use_default_templates: true\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(call_create_deployment_plan).result %>\n\n      call_create_deployment_plan_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(call_create_deployment_plan).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.create_default_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  create_deployment_plan:\n    description: >\n      This workflow provides the capability to create a deployment plan using\n      the default heat templates provided in a standard TripleO undercloud\n      deployment, heat templates contained in an external git repository, or a\n      swift container that already contains templates.\n    input:\n      - container\n      - source_url: null\n      - queue_name: tripleo\n      - generate_passwords: true\n      - use_default_templates: false\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      container_required_check:\n        description: >\n          If using the default templates or importing templates from a git\n          repository, a new container needs to be created.  If using an existing\n          container containing templates, skip straight to create_plan.\n        on-success:\n          - verify_container_doesnt_exist: <% $.use_default_templates or $.source_url %>\n          - create_plan: <% $.use_default_templates = false and $.source_url = null %>\n\n      verify_container_doesnt_exist:\n        action: swift.head_container container=<% $.container %>\n        on-success: notify_zaqar\n        on-error: create_container\n        publish:\n          status: FAILED\n          message: \"Unable to create plan. The Swift container already exists\"\n\n      create_container:\n        action: tripleo.plan.create_container container=<% $.container %>\n        on-success: templates_source_check\n        on-error: create_container_set_status_failed\n\n      cleanup_temporary_files:\n        action: tripleo.git.clean container=<% $.container %>\n\n      templates_source_check:\n        on-success:\n          - upload_default_templates: <% $.use_default_templates = true %>\n          - clone_git_repo: <% $.source_url != null %>\n\n      clone_git_repo:\n        action: tripleo.git.clone container=<% $.container %> url=<% $.source_url %>\n        on-success: upload_templates_directory\n        on-error: clone_git_repo_set_status_failed\n\n      upload_templates_directory:\n        action: tripleo.templates.upload container=<% $.container %> templates_path=<% task(clone_git_repo).result %>\n        on-success: create_plan\n        on-complete: cleanup_temporary_files\n        on-error: upload_templates_directory_set_status_failed\n\n      upload_default_templates:\n        action: tripleo.templates.upload container=<% $.container %>\n        on-success: create_plan\n        on-error: upload_to_container_set_status_failed\n\n      create_plan:\n        on-success:\n          - ensure_passwords_exist: <% $.generate_passwords = true %>\n          - add_root_stack_name: <% $.generate_passwords != true %>\n\n      ensure_passwords_exist:\n        action: tripleo.parameters.generate_passwords container=<% $.container %>\n        on-success: add_root_stack_name\n        on-error: ensure_passwords_exist_set_status_failed\n\n      add_root_stack_name:\n        action: tripleo.parameters.update\n        input:\n          container: <% $.container %>\n          parameters:\n            RootStackName: <% $.container %>\n        on-success: container_images_prepare\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      container_images_prepare:\n        description: >\n          Populate all container image parameters with default values.\n        action: tripleo.container_images.prepare container=<% $.container %>\n        on-success: process_templates\n        on-error: container_images_prepare_set_status_failed\n\n      process_templates:\n        action: tripleo.templates.process container=<% $.container %>\n        on-success: set_status_success\n        on-error: process_templates_set_status_failed\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: 'Plan created.'\n\n      create_container_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_container).result %>\n\n      clone_git_repo_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(clone_git_repo).result %>\n\n      upload_templates_directory_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(upload_templates_directory).result %>\n\n      upload_to_container_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(upload_default_templates).result %>\n\n      ensure_passwords_exist_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(ensure_passwords_exist).result %>\n\n      process_templates_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(process_templates).result %>\n\n      container_images_prepare_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(container_images_prepare).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.create_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_deployment_plan:\n    input:\n      - container\n      - source_url: null\n      - queue_name: tripleo\n      - generate_passwords: true\n      - plan_environment: null\n    tags:\n      - tripleo-common-managed\n    tasks:\n      templates_source_check:\n        on-success:\n          - update_plan: <% $.source_url = null %>\n          - clone_git_repo: <% $.source_url != null %>\n\n      clone_git_repo:\n        action: tripleo.git.clone container=<% $.container %> url=<% $.source_url %>\n        on-success: upload_templates_directory\n        on-error: clone_git_repo_set_status_failed\n\n      upload_templates_directory:\n        action: tripleo.templates.upload container=<% $.container %> templates_path=<% task(clone_git_repo).result %>\n        on-success: create_swift_rings_backup_plan\n        on-complete: cleanup_temporary_files\n        on-error: upload_templates_directory_set_status_failed\n\n      cleanup_temporary_files:\n        action: tripleo.git.clean container=<% $.container %>\n\n      create_swift_rings_backup_plan:\n        workflow: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan\n        on-success: update_plan\n        on-error: create_swift_rings_backup_plan_set_status_failed\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n          use_default_templates: true\n\n      update_plan:\n        on-success:\n          - ensure_passwords_exist: <% $.generate_passwords = true %>\n          - container_images_prepare: <% $.generate_passwords != true %>\n\n      ensure_passwords_exist:\n        action: tripleo.parameters.generate_passwords container=<% $.container %>\n        on-success: container_images_prepare\n        on-error: ensure_passwords_exist_set_status_failed\n\n      container_images_prepare:\n        description: >\n          Populate all container image parameters with default values.\n        action: tripleo.container_images.prepare container=<% $.container %>\n        on-success: process_templates\n        on-error: container_images_prepare_set_status_failed\n\n      process_templates:\n        action: tripleo.templates.process container=<% $.container %>\n        on-success:\n          - set_status_success: <% $.plan_environment = null %>\n          - upload_plan_environment: <% $.plan_environment != null %>\n        on-error: process_templates_set_status_failed\n\n      upload_plan_environment:\n        action: tripleo.templates.upload_plan_environment container=<% $.container %> plan_environment=<% $.plan_environment %>\n        on-success: set_status_success\n        on-error: process_templates_set_status_failed\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: 'Plan updated.'\n\n      create_swift_rings_backup_plan_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_swift_rings_backup_plan).result %>\n\n      clone_git_repo_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(clone_git_repo).result %>\n\n      upload_templates_directory_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(upload_templates_directory).result %>\n\n      process_templates_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(process_templates).result %>\n\n      ensure_passwords_exist_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(ensure_passwords_exist).result %>\n\n      container_images_prepare_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(container_images_prepare).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.update_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  delete_deployment_plan:\n    description: >\n      Deletes a plan by deleting the container matching plan_name. It will\n      not delete the plan if a stack exists with the same name.\n\n    tags:\n      - tripleo-common-managed\n\n    input:\n      - container: overcloud\n      - queue_name: tripleo\n\n    tasks:\n      delete_plan:\n        action: tripleo.plan.delete container=<% $.container %>\n        on-complete: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.delete_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n\n  get_passwords:\n    description: Retrieves passwords for a given plan\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      verify_container_exists:\n        action: swift.head_container container=<% $.container %>\n        on-success: get_environment_passwords\n        on-error: verify_container_set_status_failed\n\n      get_environment_passwords:\n        action: tripleo.parameters.get_passwords container=<% $.container %>\n        on-success: get_passwords_set_status_success\n        on-error: get_passwords_set_status_failed\n\n      get_passwords_set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(get_environment_passwords).result %>\n\n      get_passwords_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(get_environment_passwords).result %>\n\n      verify_container_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(verify_container_exists).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.get_passwords\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  export_deployment_plan:\n    description: Creates an export tarball for a given plan\n    input:\n      - plan\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      export_plan:\n        action: tripleo.plan.export\n        input:\n          plan: <% $.plan %>\n          delete_after: 3600\n          exports_container: \"plan-exports\"\n        on-success: create_tempurl\n        on-error: export_plan_set_status_failed\n\n      create_tempurl:\n        action: tripleo.swift.tempurl\n        on-success: set_status_success\n        on-error: create_tempurl_set_status_failed\n        input:\n          container: \"plan-exports\"\n          obj: \"<% $.plan %>.tar.gz\"\n          valid: 3600\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(create_tempurl).result %>\n          tempurl: <% task(create_tempurl).result %>\n\n      export_plan_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(export_plan).result %>\n\n      create_tempurl_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_tempurl).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.export_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                tempurl: <% $.get('tempurl', '') %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  get_deprecated_parameters:\n    description: Gets the list of deprecated parameters in the whole of the plan including nested stack\n    input:\n      - container: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_flatten_data:\n        action: tripleo.parameters.get_flatten container=<% $.container %>\n        on-success: get_deprecated_params\n        on-error: set_status_failed_get_flatten_data\n        publish:\n          user_params: <% task().result.environment_parameters %>\n          plan_params: <% task().result.heat_resource_tree.parameters.keys() %>\n          parameter_groups: <% task().result.heat_resource_tree.resources.values().where( $.get('parameter_groups') ).select($.parameter_groups).flatten() %>\n\n      get_deprecated_params:\n        on-success: check_if_user_param_has_deprecated\n        publish:\n          deprecated_params: <% $.parameter_groups.where($.get('label') = 'deprecated').select($.parameters).flatten().distinct() %>\n\n      check_if_user_param_has_deprecated:\n        on-success: get_unused_params\n        publish:\n          deprecated_result: <% let(up => $.user_params) -> $.deprecated_params.select( dict('parameter' => $, 'deprecated' => true, 'user_defined' => $up.keys().contains($)) ) %>\n\n      # Get the list of parameters, which are defined by user via environment files's parameter_default, but not part of the plan definition\n      # It may be possible that the parameter will be used by a service, but the service is not part of the plan.\n      # In such cases, the parameter will be reported as unused, care should be take to understand whether it is really unused or not.\n      get_unused_params:\n        on-success: send_message\n        publish:\n          unused_params: <% let(plan_params => $.plan_params) ->  $.user_params.keys().where( not $plan_params.contains($) ) %>\n\n      set_status_failed_get_flatten_data:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_flatten_data).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.get_deprecated_parameters\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                deprecated: <% $.get('deprecated_result', []) %>\n                unused: <% $.get('unused_params', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  publish_ui_logs_to_swift:\n    description: >\n      This workflow drains a zaqar queue, and publish its messages into a log\n      file in swift.  This workflow is called by cron trigger.\n\n    input:\n      - logging_queue_name: tripleo-ui-logging\n      - logging_container: tripleo-ui-logs\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      # We're using a NoOp action to start the workflow.  The recursive nature\n      # of the workflow means that Mistral will refuse to execute it because it\n      # doesn't know where to begin.\n      start:\n        on-success: get_messages\n\n      get_messages:\n        action: zaqar.claim_messages\n        on-success:\n          - format_messages: <% task().result.len() > 0 %>\n        input:\n          queue_name: <% $.logging_queue_name %>\n          ttl: 60\n          grace: 60\n        publish:\n          status: SUCCESS\n          messages: <% task().result %>\n          message_ids: <% task().result.select($._id) %>\n\n      format_messages:\n        action: tripleo.logging_to_swift.format_messages\n        on-success: upload_to_swift\n        input:\n          messages: <% $.messages %>\n        publish:\n          status: SUCCESS\n          formatted_messages: <% task().result %>\n\n      upload_to_swift:\n        action: tripleo.logging_to_swift.publish_ui_log_to_swift\n        on-success: delete_messages\n        input:\n          logging_data: <% $.formatted_messages %>\n          logging_container: <% $.logging_container %>\n        publish:\n          status: SUCCESS\n\n      delete_messages:\n        action: zaqar.delete_messages\n        on-success: get_messages\n        input:\n          queue_name: <% $.logging_queue_name %>\n          messages: <% $.message_ids %>\n        publish:\n          status: SUCCESS\n\n  download_logs:\n    description: Creates a tarball with logging data\n    input:\n      - queue_name: tripleo\n      - logging_container: \"tripleo-ui-logs\"\n      - downloads_container: \"tripleo-ui-logs-downloads\"\n      - delete_after: 3600\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      publish_logs:\n        workflow: tripleo.plan_management.v1.publish_ui_logs_to_swift\n        on-success: prepare_log_download\n        on-error: publish_logs_set_status_failed\n\n      prepare_log_download:\n        action: tripleo.logging_to_swift.prepare_log_download\n        input:\n          logging_container: <% $.logging_container %>\n          downloads_container: <% $.downloads_container %>\n          delete_after: <% $.delete_after %>\n        on-success: create_tempurl\n        on-error: download_logs_set_status_failed\n        publish:\n          filename: <% task().result %>\n\n      create_tempurl:\n        action: tripleo.swift.tempurl\n        on-success: set_status_success\n        on-error: create_tempurl_set_status_failed\n        input:\n          container: <% $.downloads_container %>\n          obj: <% $.filename %>\n          valid: 3600\n        publish:\n          tempurl: <% task().result %>\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(create_tempurl).result %>\n          tempurl: <% task(create_tempurl).result %>\n\n      publish_logs_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(publish_logs).result %>\n\n      download_logs_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(prepare_log_download).result %>\n\n      create_tempurl_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_tempurl).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.download_logs\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                tempurl: <% $.get('tempurl', '') %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list_roles:\n    description: Retrieve the roles_data.yaml and return a usable object\n\n    input:\n      - container: overcloud\n      - roles_data_file: 'roles_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      roles_data: <% $.roles_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_roles_data:\n        action: swift.get_object\n        input:\n          container: <% $.container %>\n          obj: <% $.roles_data_file %>\n        publish:\n          roles_data: <% yaml_parse(task().result.last()) %>\n          status: SUCCESS\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.list_roles\n              payload:\n                status: <% $.status %>\n                roles_data: <% $.get('roles_data', {}) %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list_available_networks:\n    input:\n      - container\n      - queue_name: tripleo\n\n    output:\n      available_networks: <% $.available_networks %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_network_file_names:\n        action: swift.get_container\n        input:\n          container: <% $.container %>\n        publish:\n          network_names: <% task().result[1].where($.name.startsWith('networks/')).where($.name.endsWith('.yaml')).name %>\n        on-success: get_network_files\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_network_files:\n        with-items: network_name in <% $.network_names %>\n        action: swift.get_object\n        on-success: transform_output\n        on-error: notify_zaqar\n        input:\n          container: <% $.container %>\n          obj: <% $.network_name %>\n        publish:\n          status: SUCCESS\n          available_yaml_networks: <% task().result.select($[1]) %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      transform_output:\n        publish:\n          status: SUCCESS\n          available_networks: <% yaml_parse($.available_yaml_networks.join(\"\\n\")) %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-complete: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.list_available_networks\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                available_networks: <% $.get('available_networks', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list_networks:\n    input:\n      - container: 'overcloud'\n      - network_data_file: 'network_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_networks:\n        action: swift.get_object\n        input:\n          container: <% $.container %>\n          obj: <% $.network_data_file %>\n        on-success: notify_zaqar\n        publish:\n          network_data: <% yaml_parse(task().result.last()) %>\n          status: SUCCESS\n          message: <% task().result %>\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.list_networks\n              payload:\n                status: <% $.status %>\n                network_data: <% $.get('network_data', {}) %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_network_files:\n    description: Validate network files exist\n    input:\n      - container: overcloud\n      - network_data\n      - queue_name: tripleo\n\n    output:\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_network_names:\n        publish:\n          network_names_lower: <% $.network_data.where($.containsKey('name_lower')).name_lower %>\n          network_names: <% $.network_data.where(not $.containsKey('name_lower')).name %>\n        on-success: validate_networks\n\n      validate_networks:\n        with-items: network in <% $.network_names_lower.concat($.network_names) %>\n        action: swift.head_object\n        input:\n          container: <% $.container %>\n          obj: network/<% $.network.toLower() %>.yaml\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.validate_network_files\n              payload:\n                status: <% $.status %>\n                message: <% $.message %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_networks:\n    description: Validate network files were generated properly and exist\n    input:\n      - container: 'overcloud'\n      - network_data_file: 'network_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_network_data:\n        workflow: list_networks\n        input:\n          container: <% $.container %>\n          network_data_file: <% $.network_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().result.network_data %>\n        on-success: validate_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error:\n          notify_zaqar\n\n      validate_networks:\n        workflow: validate_network_files\n        input:\n          container: <% $.container %>\n          network_data: <% $.network_data %>\n          queue_name: <% $.queue_name %>\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.validate_networks\n              payload:\n                status: <% $.status %>\n                network_data: <% $.get('network_data', {}) %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_roles:\n    description: Vaildate roles data exists and is parsable\n\n    input:\n      - container: overcloud\n      - roles_data_file: 'roles_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      roles_data: <% $.roles_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_roles_data:\n        workflow: list_roles\n        input:\n          container: <% $.container %>\n          roles_data_file: <% $.roles_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          roles_data: <% task().result.roles_data %>\n          status: SUCCESS\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error:\n          notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.validate_networks\n              payload:\n                status: <% $.status %>\n                roles_data: <% $.get('roles_data', '') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  _validate_networks_from_roles:\n    description: Internal workflow for validating a network exists from a role\n\n    input:\n      - container: overcloud\n      - defined_networks\n      - networks_in_roles\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      validate_network_in_network_data:\n        publish:\n          networks_found: <% $.networks_in_roles.toSet().intersect($.defined_networks.toSet()) %>\n          networks_not_found: <% $.networks_in_roles.toSet().difference($.defined_networks.toSet()) %>\n        on-success:\n          - network_not_found: <% $.networks_not_found %>\n          - notify_zaqar: <% not $.networks_not_found %>\n\n      network_not_found:\n        publish:\n          message: <% \"Some networks in roles are not defined, {0}\".format($.networks_not_found.join(', ')) %>\n          status: FAILED\n        on-success: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1._validate_networks_from_role\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_roles_and_networks:\n    description: Vaidate that roles and network data are valid\n\n    input:\n      - container: overcloud\n      - roles_data_file: 'roles_data.yaml'\n      - network_data_file: 'network_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      roles_data: <% $.roles_data %>\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      validate_network_data:\n        workflow: validate_networks\n        input:\n          container: <% $.container %>\n          network_data_file: <% $.network_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().result.network_data %>\n        on-success: validate_roles_data\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      validate_roles_data:\n        workflow: validate_roles\n        input:\n          container: <% $.container %>\n          roles_data_file: <% $.roles_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          roles_data: <% task().result.roles_data %>\n          role_networks_data: <% task().result.roles_data.networks %>\n          networks_in_roles: <% task().result.roles_data.networks.flatten().distinct() %>\n        on-success: validate_roles_and_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      validate_roles_and_networks:\n        workflow: _validate_networks_from_roles\n        input:\n          container: <% $.container %>\n          defined_networks: <% $.network_data.name %>\n          networks_in_roles: <% $.networks_in_roles %>\n          queue_name: <% $.queue_name %>\n        publish:\n          status: SUCCESS\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result.message %>\n        on-error: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.validate_roles_and_networks\n              payload:\n                status: <% $.status %>\n                roles_data: <% $.get('roles_data', {}) %>\n                network_data: <% $.get('network_data', {}) %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list_available_roles:\n    input:\n      - container: overcloud\n      - queue_name: tripleo\n\n    output:\n      available_roles: <% $.available_roles %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_role_file_names:\n        action: swift.get_container\n        input:\n          container: <% $.container %>\n        publish:\n          role_names: <% task().result[1].where($.name.startsWith('roles/')).where($.name.endsWith('.yaml')).name %>\n        on-success: get_role_files\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_role_files:\n        with-items: role_name in <% $.role_names %>\n        action: swift.get_object\n        on-success: transform_output\n        on-error: notify_zaqar\n        input:\n          container: <% $.container %>\n          obj: <% $.role_name %>\n        publish:\n          status: SUCCESS\n          available_yaml_roles: <% task().result.select($[1]) %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      transform_output:\n        publish:\n          status: SUCCESS\n          available_roles: <% yaml_parse($.available_yaml_roles.join(\"\\n\")) %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-complete: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.list_available_roles\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                available_roles: <% $.get('available_roles', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_roles:\n    description: >\n      takes data in json format validates its contents and persists them in\n      roles_data.yaml, after successful update, templates are regenerated.\n    input:\n      - container\n      - roles\n      - roles_data_file: 'roles_data.yaml'\n      - replace_all: false\n      - queue_name: tripleo\n    tags:\n      - tripleo-common-managed\n    tasks:\n      get_available_roles:\n        workflow: list_available_roles\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name%>\n        publish:\n          available_roles: <% task().result.available_roles %>\n        on-success: validate_input\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      validate_input:\n        description: >\n          validate the format of input (verify that each role in input has the\n          required attributes set. check README in roles directory in t-h-t),\n          validate that roles in input exist in roles directory in t-h-t\n        action: tripleo.plan.validate_roles\n        input:\n          container: <% $.container %>\n          roles: <% $.roles %>\n          available_roles: <% $.available_roles %>\n        on-success: get_network_data\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_network_data:\n        workflow: list_networks\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().result.network_data %>\n        on-success: validate_network_names\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      validate_network_names:\n        description: >\n          validate that Network names assigned to Role exist in\n          network-data.yaml object in Swift container\n        workflow: _validate_networks_from_roles\n        input:\n          container: <% $.container %>\n          defined_networks: <% $.network_data.name %>\n          networks_in_roles: <% $.roles.networks.flatten().distinct() %>\n          queue_name: <% $.queue_name %>\n        on-success: get_current_roles\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result.message %>\n\n      get_current_roles:\n        workflow: list_roles\n        input:\n          container: <% $.container %>\n          roles_data_file: <% $.roles_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n            current_roles: <% task().result.roles_data %>\n        on-success: update_roles_data\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      update_roles_data:\n        description: >\n          update roles_data.yaml object in Swift with roles from workflow input\n        action: tripleo.plan.update_roles\n        input:\n          container: <% $.container %>\n          roles: <% $.roles %>\n          current_roles: <% $.current_roles %>\n          replace_all: <% $.replace_all %>\n        publish:\n          updated_roles_data: <% task().result.roles %>\n        on-success: update_roles_data_in_swift\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      update_roles_data_in_swift:\n        description: >\n          update roles_data.yaml object in Swift with data from workflow input\n        action: swift.put_object\n        input:\n          container: <% $.container %>\n          obj: <% $.roles_data_file %>\n          contents: <% yaml_dump($.updated_roles_data) %>\n        on-success: regenerate_templates\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      regenerate_templates:\n        action: tripleo.templates.process container=<% $.container %>\n        on-success: get_updated_roles\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_updated_roles:\n        workflow: list_roles\n        input:\n          container: <% $.container %>\n          roles_data_file:  <% $.roles_data_file %>\n        publish:\n            updated_roles: <% task().result.roles_data %>\n            status: SUCCESS\n        on-complete: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.roles.v1.update_roles\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                updated_roles: <% $.get('updated_roles', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  select_roles:\n    description: >\n      takes a list of role names as input and populates roles_data.yaml in\n      container in Swift with respective roles from 'roles directory'\n    input:\n      - container\n      - role_names\n      - roles_data_file: 'roles_data.yaml'\n      - replace_all: true\n      - queue_name: tripleo\n    tags:\n      - tripleo-common-managed\n    tasks:\n\n      get_available_roles:\n        workflow: list_available_roles\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n        publish:\n          available_roles: <% task().result.available_roles %>\n        on-success: get_current_roles\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_current_roles:\n        workflow: list_roles\n        input:\n          container: <% $.container %>\n          roles_data_file: <% $.roles_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          current_roles: <% task().result.roles_data %>\n        on-success: gather_roles\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      gather_roles:\n        description: >\n          for each role name from the input, check if it exists in\n          roles_data.yaml, if yes, use that role definition, if not, get the\n          role definition from roles directory. Use the gathered roles\n          definitions as input to updateRolesWorkflow - this ensures\n          configuration of the roles which are already in roles_data.yaml\n          will not get overridden by data from roles directory\n        action: tripleo.plan.gather_roles\n        input:\n          role_names: <% $.role_names %>\n          current_roles: <% $.current_roles %>\n          available_roles: <% $.available_roles %>\n        publish:\n          gathered_roles: <% task().result.gathered_roles %>\n        on-success: call_update_roles_workflow\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      call_update_roles_workflow:\n        workflow: update_roles\n        input:\n          container: <% $.container %>\n          roles: <% $.gathered_roles %>\n          roles_data_file: <% $.roles_data_file %>\n          replace_all: <% $.replace_all %>\n          queue_name: <% $.queue_name %>\n        on-complete: notify_zaqar\n        publish:\n          selected_roles: <% task().result.updated_roles %>\n          status: SUCCESS\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.select_roles\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                selected_roles: <% $.get('selected_roles', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1", "tags": [], "created_at": "2018-12-03 11:19:09", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "9c82ad70-39c5-48a5-b310-0e0f342222a4"}, {"definition": "---\nversion: '2.0'\nname: tripleo.skydive_ansible.v1\ndescription: TripleO manages Skydive with skydive-ansible\n\nworkflows:\n  skydive_install:\n    # allows for additional extra_vars via workflow input\n    input:\n      - ansible_playbook_verbosity: 0\n      - ansible_extra_env_variables:\n          ANSIBLE_ROLES_PATH: /usr/share/skydive-ansible/roles/\n          ANSIBLE_RETRY_FILES_ENABLED: 'False'\n          ANSIBLE_LOG_PATH: /var/log/mistral/skydive-install-workflow.log\n          ANSIBLE_HOST_KEY_CHECKING: 'False'\n      - skydive_ansible_extra_vars: {}\n      - skydive_ansible_playbook: /usr/share/skydive-ansible/playbook.yml.sample\n    tags:\n      - tripleo-common-managed\n    tasks:\n      set_blacklisted_ips:\n        publish:\n          blacklisted_ips: <% env().get('blacklisted_ip_addresses', []) %>\n        on-success: set_ip_lists\n      set_ip_lists:\n        publish:\n          agent_ips: <% let(root => $) -> env().get('service_ips', {}).get('skydive_agent_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          analyzer_ips: <% let(root => $) -> env().get('service_ips', {}).get('skydive_analyzer_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n        on-success: enable_ssh_admin\n      enable_ssh_admin:\n        workflow: tripleo.access.v1.enable_ssh_admin\n        input:\n          ssh_servers: <% ($.agent_ips + $.analyzer_ips).toSet() %>\n        on-success: get_private_key\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: set_fork_count\n      set_fork_count:\n        publish: # unique list of all IPs: make each list a set, take unions and count\n          fork_count: <% min($.agent_ips.toSet().union($.analyzer_ips.toSet()).count(), 100) %> # don't use >100 forks\n        on-success: set_role_vars\n      set_role_vars:\n        publish:\n          # NOTE(sbaubeau): collect role settings from all tht roles\n          agent_vars: <% env().get('role_merged_configs', {}).values().select($.get('skydive_agent_ansible_vars', {})).aggregate($1 + $2) %>\n          analyzer_vars: <% env().get('role_merged_configs', {}).values().select($.get('skydive_analyzer_ansible_vars', {})).aggregate($1 + $2) %>\n        on-success: build_extra_vars\n      build_extra_vars:\n        publish:\n          # NOTE(sbaubeau): merge vars from all ansible roles\n          extra_vars: <% $.agent_vars + $.analyzer_vars + $.skydive_ansible_extra_vars %>\n        on-success: skydive_install\n      skydive_install:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            agents:\n              hosts: <% $.agent_ips.toDict($, {}) %>\n            analyzers:\n              hosts: <% $.analyzer_ips.toDict($, {}) %>\n          playbook: <% $.skydive_ansible_playbook %>\n          remote_user: tripleo-admin\n          become: true\n          become_user: root\n          verbosity: <% $.ansible_playbook_verbosity %>\n          forks: <% $.fork_count %>\n          ssh_private_key: <% $.private_key %>\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          extra_vars: <% $.extra_vars %>\n        publish:\n          output: <% task().result %>\n", "name": "tripleo.skydive_ansible.v1", "tags": [], "created_at": "2018-12-03 11:19:10", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "d9eb9cb8-56d0-4006-b72e-ddd07a368e40"}, {"definition": "---\nversion: '2.0'\nname: tripleo.scale.v1\ndescription: TripleO Overcloud Deployment Workflows v1\n\nworkflows:\n\n  delete_node:\n    description: deletes given overcloud nodes and updates the stack\n\n    input:\n      - container\n      - nodes\n      - timeout: 240\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      delete_node:\n        action: tripleo.scale.delete_node nodes=<% $.nodes %> timeout=<% $.timeout %> container=<% $.container %>\n        on-success: wait_for_stack_in_progress\n        on-error: set_delete_node_failed\n\n      set_delete_node_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(delete_node).result %>\n\n      wait_for_stack_in_progress:\n        workflow: tripleo.stack.v1.wait_for_stack_in_progress stack=<% $.container %>\n        on-success: wait_for_stack_complete\n        on-error: wait_for_stack_in_progress_failed\n\n      wait_for_stack_in_progress_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(wait_for_stack_in_progress).result %>\n\n      wait_for_stack_complete:\n        workflow: tripleo.stack.v1.wait_for_stack_complete_or_failed stack=<% $.container %>\n        on-success: send_message\n        on-error: wait_for_stack_complete_failed\n\n      wait_for_stack_complete_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(wait_for_stack_complete).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.scale.v1.delete_node\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.scale.v1", "tags": [], "created_at": "2018-12-03 11:19:10", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "f2834234-458d-424e-8b41-f398dee4a3c8"}, {"definition": "---\nversion: '2.0'\nname: tripleo.stack.v1\ndescription: TripleO Stack Workflows\n\nworkflows:\n\n  wait_for_stack_complete_or_failed:\n    input:\n      - stack\n      - timeout: 14400 # 4 hours. Default timeout of stack deployment\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      wait_for_stack_status:\n        action: heat.stacks_get stack_id=<% $.stack %>\n        timeout: <% $.timeout %>\n        retry:\n          delay: 15\n          count: <% $.timeout / 15 %>\n          continue-on: <% task().result.stack_status in ['CREATE_IN_PROGRESS', 'UPDATE_IN_PROGRESS', 'DELETE_IN_PROGRESS'] %>\n\n  wait_for_stack_in_progress:\n    input:\n      - stack\n      - timeout: 600 # 10 minutes. Should not take much longer for a stack to transition to IN_PROGRESS\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      wait_for_stack_status:\n        action: heat.stacks_get stack_id=<% $.stack %>\n        timeout: <% $.timeout %>\n        retry:\n          delay: 15\n          count: <% $.timeout / 15 %>\n          continue-on: <% task().result.stack_status in ['CREATE_COMPLETE', 'CREATE_FAILED', 'UPDATE_COMPLETE', 'UPDATE_FAILED', 'DELETE_FAILED'] %>\n\n  wait_for_stack_does_not_exist:\n    input:\n      - stack\n      - timeout: 3600\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      wait_for_stack_does_not_exist:\n        action: heat.stacks_list\n        timeout: <% $.timeout %>\n        retry:\n          delay: 15\n          count: <% $.timeout / 15 %>\n          continue-on: <% $.stack in task(wait_for_stack_does_not_exist).result.select([$.stack_name, $.id]).flatten() %>\n\n  delete_stack:\n    input:\n      - stack\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      delete_the_stack:\n        action: heat.stacks_delete stack_id=<% $.stack %>\n        on-success: wait_for_stack_does_not_exist\n        on-error: delete_the_stack_failed\n\n      delete_the_stack_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(delete_the_stack).result %>\n\n      wait_for_stack_does_not_exist:\n        workflow: tripleo.stack.v1.wait_for_stack_does_not_exist stack=<% $.stack %>\n        on-success: send_message\n        on-error: wait_for_stack_does_not_exist_failed\n\n      wait_for_stack_does_not_exist_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(wait_for_stack_does_not_exist).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.scale.v1.delete_stack\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.stack.v1", "tags": [], "created_at": "2018-12-03 11:19:11", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "fb99e428-9481-4f48-b14b-64197a944fc9"}, {"definition": "---\nversion: '2.0'\nname: tripleo.swift_ring.v1\ndescription: Rebalance and distribute Swift rings using Ansible\n\n\nworkflows:\n  rebalance:\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        on-success: deploy_rings\n\n      deploy_rings:\n        action: tripleo.ansible-playbook\n        publish:\n          output: <% task().result %>\n        input:\n          ssh_private_key: <% task(get_private_key).result %>\n          ssh_common_args: '-o StrictHostKeyChecking=no'\n          ssh_extra_args: '-o UserKnownHostsFile=/dev/null'\n          verbosity: 1\n          remote_user: heat-admin\n          become: true\n          become_user: root\n          playbook: /usr/share/tripleo-common/playbooks/swift_ring_rebalance.yaml\n          inventory: /usr/bin/tripleo-ansible-inventory\n          use_openstack_credentials: true\n", "name": "tripleo.swift_ring.v1", "tags": [], "created_at": "2018-12-03 11:19:12", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "6f3140be-f8aa-470b-a519-695eaae1d6c1"}, {"definition": "---\nversion: '2.0'\nname: tripleo.support.v1\ndescription: TripleO support workflows\n\nworkflows:\n\n  collect_logs:\n    description: >\n      This workflow runs sosreport on the servers where their names match the\n      provided server_name input. The logs are stored in the provided sos_dir.\n    input:\n      - server_name\n      - sos_dir: /var/tmp/tripleo-sos\n      - sos_options: boot,cluster,hardware,kernel,memory,nfs,openstack,packagemanager,performance,services,storage,system,webserver,virt\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      collect_logs_on_servers:\n        workflow: tripleo.deployment.v1.deploy_on_servers\n        on-success: send_message\n        on-error: set_collect_logs_on_servers_failed\n        input:\n          server_name: <% $.server_name %>\n          config_name: 'run_sosreport'\n          config: |\n            #!/bin/bash\n            mkdir -p <% $.sos_dir %>\n            sosreport --batch \\\n            -p <% $.sos_options %> \\\n            --tmp-dir <% $.sos_dir %>\n\n      set_collect_logs_on_servers_failed:\n          on-complete:\n           - send_message\n          publish:\n            type: tripleo.deployment.v1.fetch_logs\n            status: FAILED\n            message: <% task().result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.collect_logs') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n  upload_logs:\n    description: >\n      This workflow uploads the sosreport files stored in the provide sos_dir\n      on the provided host (server_uuid) to a swift container on the undercloud\n    input:\n      - server_uuid\n      - server_name\n      - container\n      - sos_dir: /var/tmp/tripleo-sos\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      # actions\n      get_swift_information:\n        action: tripleo.swift.swift_information\n        on-success: do_log_upload\n        on-error: set_get_swift_information_failed\n        input:\n          container: <% $.container %>\n        publish:\n          container_url: <% task().result.container_url %>\n          auth_key: <% task().result.auth_key %>\n\n      set_get_swift_information_failed:\n        on-complete:\n          - send_message\n        publish:\n          status: FAILED\n          message: <% task(get_swift_information).result %>\n\n      do_log_upload:\n        action: tripleo.deployment.config\n        on-success: send_message\n        on-error: set_do_log_upload_failed\n        input:\n          server_id: <% $.server_uuid %>\n          name: \"upload_logs\"\n          config: |\n            #!/bin/bash\n            CONTAINER_URL=\"<% $.container_url %>\"\n            TOKEN=\"<% $.auth_key %>\"\n            SOS_DIR=\"<% $.sos_dir %>\"\n            for FILE in $(find $SOS_DIR -type f); do\n              FILENAME=$(basename $FILE)\n              curl -X PUT -i -H \"X-Auth-Token: $TOKEN\" -T $FILE $CONTAINER_URL/$FILENAME\n              if [ $? -eq 0 ]; then\n                rm -f $FILE\n              fi\n            done\n          group: \"script\"\n        publish:\n          message: \"Uploaded logs from <% $.server_name %>\"\n\n      set_do_log_upload_failed:\n        on-complete:\n          - send_message\n        publish:\n          status: FAILED\n          message: <% tag(do_log_upload).result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.upload_logs') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n  create_container:\n    description: >\n      This work flow is used to check if the container exists and creates it\n      if it does not exist.\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_container:\n        action: swift.head_container container=<% $.container %>\n        on-success: send_message\n        on-error: create_container\n\n      create_container:\n        action: swift.put_container\n        input:\n          container: <% $.container %>\n          headers:\n            x-container-meta-usage-tripleo: support\n        on-success: send_message\n        on-error: set_create_container_failed\n\n      set_create_container_failed:\n        on-complete:\n          - send_message\n        publish:\n          type: tripleo.support.v1.create_container.create_container\n          status: FAILED\n          message: <% task(create_container).result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.create_container') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n  delete_container:\n    description: >\n      This workflow deletes all the objects in a provided swift container and\n      then removes the container itself from the undercloud.\n    input:\n      - container\n      - concurrency: 5\n      - timeout: 900\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      # actions\n      check_container:\n        action: swift.head_container container=<% $.container %>\n        on-success: list_objects\n        on-error: set_check_container_failure\n\n      set_check_container_failure:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          type: tripleo.support.v1.delete_container.check_container\n          message: <% task(check_container).result %>\n\n      list_objects:\n        action: swift.get_container container=<% $.container %>\n        on-success: delete_objects\n        on-error: set_list_objects_failure\n        publish:\n          log_objects: <% task().result[1] %>\n\n      set_list_objects_failure:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          type: tripleo.support.v1.delete_container.list_objects\n          message: <% task(list_objects).result %>\n\n      delete_objects:\n        action: swift.delete_object\n        concurrency: <% $.concurrency %>\n        timeout: <% $.timeout %>\n        with-items: object in <% $.log_objects %>\n        input:\n          container: <% $.container %>\n          obj: <% $.object.name %>\n        on-success: remove_container\n        on-error: set_delete_objects_failure\n\n      set_delete_objects_failure:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          type: tripleo.support.v1.delete_container.delete_objects\n          message: <% task(delete_objects).result %>\n\n      remove_container:\n        action: swift.delete_container container=<% $.container %>\n        on-success: send_message\n        on-error: set_remove_container_failure\n\n      set_remove_container_failure:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          type: tripleo.support.v1.delete_container.remove_container\n          message: <% task(remove_container).result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        wait-before: 5\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.delete_container') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n  fetch_logs:\n    description: >\n      This workflow creates a container on the undercloud, executes the log\n      collection on the servers whose names match the provided server_name, and\n      executes the log upload process on all the servers to the container on\n      the undercloud.\n    input:\n      - server_name\n      - container\n      - concurrency: 5\n      - timeout: 1800\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      # actions\n      create_container:\n        workflow: tripleo.support.v1.create_container\n        on-success: get_servers_matching\n        on-error: set_create_container_failed\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n\n      set_create_container_failed:\n          on-complete: send_message\n          publish:\n            type: tripleo.support.v1.fetch_logs.create_container\n            status: FAILED\n            message: <% task(create_container).result %>\n\n      get_servers_matching:\n        action: nova.servers_list\n        on-success: collect_logs_on_servers\n        publish:\n          servers_with_name: <% task().result._info.where($.name.indexOf(execution().input.server_name) > -1) %>\n\n      collect_logs_on_servers:\n        workflow: tripleo.support.v1.collect_logs\n        timeout: <% $.timeout %>\n        on-success: upload_logs_on_servers\n        on-error: set_collect_logs_on_servers_failed\n        input:\n          server_name: <% $.server_name %>\n          queue_name: <% $.queue_name %>\n\n      set_collect_logs_on_servers_failed:\n          on-complete: send_message\n          publish:\n            type: tripleo.support.v1.fetch_logs.collect_logs_on_servers\n            status: FAILED\n            message: <% task(collect_logs_on_servers).result %>\n\n      upload_logs_on_servers:\n        on-success: send_message\n        on-error: set_upload_logs_on_servers_failed\n        with-items: server in <% $.servers_with_name %>\n        concurrency: <% $.concurrency %>\n        workflow: tripleo.support.v1.upload_logs\n        input:\n          server_name: <% $.server.name %>\n          server_uuid: <% $.server.id %>\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n\n      set_upload_logs_on_servers_failed:\n          on-complete: send_message\n          publish:\n            type: tripleo.support.v1.fetch_logs.upload_logs\n            status: FAILED\n            message: <% task(upload_logs_on_servers).result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.fetch_logs') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n", "name": "tripleo.support.v1", "tags": [], "created_at": "2018-12-03 11:19:12", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "dd244f84-80aa-4511-b2cf-78211a3c1c50"}, {"definition": "---\nversion: '2.0'\nname: tripleo.swift_rings_backup.v1\ndescription: TripleO Swift Rings backup container Deployment Workflow v1\n\nworkflows:\n\n  create_swift_rings_backup_container_plan:\n    description: >\n      This plan ensures existence of container for Swift Rings backup.\n    input:\n      - container\n      - queue_name: tripleo\n    tags:\n      - tripleo-common-managed\n    tasks:\n\n      swift_rings_container:\n        publish:\n          swift_rings_container: \"<% $.container %>-swift-rings\"\n          swift_rings_tar: \"swift-rings.tar.gz\"\n        on-complete: check_container\n\n      check_container:\n        action: swift.head_container container=<% $.swift_rings_container %>\n        on-success: get_tempurl\n        on-error: create_container\n\n      create_container:\n        action: swift.put_container container=<% $.swift_rings_container %>\n        on-error: set_create_container_failed\n        on-success: get_tempurl\n\n      get_tempurl:\n        action: tripleo.swift.tempurl\n        on-success: set_get_tempurl\n        input:\n          container: <% $.swift_rings_container %>\n          obj: <% $.swift_rings_tar %>\n\n      set_get_tempurl:\n        action: tripleo.parameters.update\n        input:\n          parameters:\n            SwiftRingGetTempurl: <% task(get_tempurl).result %>\n          container: <% $.container %>\n        on-success: put_tempurl\n\n      put_tempurl:\n        action: tripleo.swift.tempurl\n        on-success: set_put_tempurl\n        input:\n          container: <% $.swift_rings_container %>\n          obj: <% $.swift_rings_tar %>\n          method: \"PUT\"\n\n      set_put_tempurl:\n        action: tripleo.parameters.update\n        input:\n          parameters:\n            SwiftRingPutTempurl: <% task(put_tempurl).result %>\n          container: <% $.container %>\n        on-success: set_status_success\n        on-error: set_put_tempurl_failed\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(set_put_tempurl).result %>\n\n      set_put_tempurl_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(set_put_tempurl).result %>\n\n      set_create_container_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_container).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.swift_rings_backup.v1", "tags": [], "created_at": "2018-12-03 11:19:13", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "846f1be7-0559-43c1-9c16-b7e395da8430"}, {"definition": "---\nversion: '2.0'\nname: tripleo.undercloud_backup.v1\ndescription: TripleO Undercloud backup workflows\n\nworkflows:\n\n  backup:\n    description: This workflow will launch the Undercloud backup\n    tags:\n      - tripleo-common-managed\n    input:\n      - sources_path: '/home/stack/'\n      - queue_name: tripleo\n    tasks:\n      # Action to know if there is enough available space\n      # to run the Undercloud backup\n      get_free_space:\n        action: tripleo.undercloud.get_free_space\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n            free_space: <% task().result %>\n        on-success: create_backup_dir\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # We create a temp directory to store the Undercloud\n      # backup\n      create_backup_dir:\n        action: tripleo.undercloud.create_backup_dir\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n            backup_path: <% task().result %>\n        on-success: get_database_credentials\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # The Undercloud database password for the root\n      # user is stored in a Mistral environment, we\n      # need the password in order to run the database dump\n      get_database_credentials:\n        action: mistral.environments_get name='tripleo.undercloud-config'\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n            undercloud_db_password: <% task(get_database_credentials).result.variables.undercloud_db_password %>\n        on-success: create_database_backup\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # Run the DB dump of all the databases and store the result\n      # in the temporary folder\n      create_database_backup:\n        input:\n            path: <% $.backup_path.path %>\n            dbuser: root\n            dbpassword: <% $.undercloud_db_password %>\n        action: tripleo.undercloud.create_database_backup\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n        on-success: create_fs_backup\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # This action will run the fs backup\n      create_fs_backup:\n        input:\n            sources_path: <% $.sources_path %>\n            path: <% $.backup_path.path %>\n        action: tripleo.undercloud.create_file_system_backup\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n        on-success: upload_backup\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # This action will push the backup to swift\n      upload_backup:\n        input:\n            backup_path: <% $.backup_path.path %>\n        action: tripleo.undercloud.upload_backup_to_swift\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n        on-success: cleanup_backup\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # This action will remove the backup temp folder\n      cleanup_backup:\n        input:\n            path: <% $.backup_path.path %>\n        action: tripleo.undercloud.remove_temp_dir\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n        on-success: send_message\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # Sending a message to show that the backup finished\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.undercloud_backup.v1.launch\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                execution: <% execution() %>\n                message: <% $.get('message', '') %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.undercloud_backup.v1", "tags": [], "created_at": "2018-12-03 11:19:13", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "8e569687-eb18-4386-9230-65effd901b25"}, {"definition": "---\nversion: '2.0'\nname: tripleo.validations.v1\ndescription: TripleO Validations Workflows v1\n\nworkflows:\n\n  run_validation:\n    input:\n      - validation_name\n      - plan: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      notify_running:\n        on-complete: run_validation\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validation\n              payload:\n                validation_name: <% $.validation_name %>\n                plan: <% $.plan %>\n                status: RUNNING\n                execution: <% execution() %>\n\n      run_validation:\n        on-success: send_message\n        on-error: set_status_failed\n        action: tripleo.validations.run_validation validation=<% $.validation_name %> plan=<% $.plan %>\n        publish:\n          status: SUCCESS\n          stdout: <% task().result.stdout %>\n          stderr: <% task().result.stderr %>\n\n      set_status_failed:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          stdout: <% task(run_validation).result.stdout %>\n          stderr: <% task(run_validation).result.stderr %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validation\n              payload:\n                validation_name: <% $.validation_name %>\n                plan: <% $.plan %>\n                status: <% $.get('status', 'SUCCESS') %>\n                stdout: <% $.stdout %>\n                stderr: <% $.stderr %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  run_validations:\n    input:\n      - validation_names: []\n      - plan: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      notify_running:\n        on-complete: run_validations\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validations\n              payload:\n                validation_names: <% $.validation_names %>\n                plan: <% $.plan %>\n                status: RUNNING\n                execution: <% execution() %>\n\n      run_validations:\n        on-success: send_message\n        on-error: set_status_failed\n        workflow: tripleo.validations.v1.run_validation validation_name=<% $.validation %> plan=<% $.plan %> queue_name=<% $.queue_name %>\n        with-items: validation in <% $.validation_names %>\n        publish:\n          status: SUCCESS\n\n      set_status_failed:\n        on-complete: send_message\n        publish:\n          status: FAILED\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validations\n              payload:\n                validation_names: <% $.validation_names %>\n                plan: <% $.plan %>\n                status: <% $.get('status', 'SUCCESS') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  run_groups:\n    input:\n      - group_names: []\n      - plan: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      find_validations:\n        on-success: notify_running\n        action: tripleo.validations.list_validations groups=<% $.group_names %>\n        publish:\n          validations: <% task().result %>\n\n      notify_running:\n        on-complete: run_validation_group\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validations\n              payload:\n                group_names: <% $.group_names %>\n                validation_names: <% $.validations.id %>\n                plan: <% $.plan %>\n                status: RUNNING\n                execution: <% execution() %>\n\n      run_validation_group:\n        on-success: send_message\n        on-error: set_status_failed\n        workflow: tripleo.validations.v1.run_validation validation_name=<% $.validation %> plan=<% $.plan %> queue_name=<% $.queue_name %>\n        with-items: validation in <% $.validations.id %>\n        publish:\n          status: SUCCESS\n\n      set_status_failed:\n        on-complete: send_message\n        publish:\n          status: FAILED\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_groups\n              payload:\n                group_names: <% $.group_names %>\n                validation_names: <% $.validations.id %>\n                plan: <% $.plan %>\n                status: <% $.get('status', 'SUCCESS') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list:\n    input:\n      - group_names: []\n    tags:\n      - tripleo-common-managed\n    tasks:\n      find_validations:\n        action: tripleo.validations.list_validations groups=<% $.group_names %>\n\n  list_groups:\n    tags:\n      - tripleo-common-managed\n    tasks:\n      find_groups:\n        action: tripleo.validations.list_groups\n\n  add_validation_ssh_key_parameter:\n    input:\n     - container\n     - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      test_validations_enabled:\n        action: tripleo.validations.enabled\n        on-success: get_pubkey\n        on-error: unset_validation_key_parameter\n\n      get_pubkey:\n        action: tripleo.validations.get_pubkey\n        on-success: set_validation_key_parameter\n        publish:\n          pubkey: <% task().result %>\n\n      set_validation_key_parameter:\n        action: tripleo.parameters.update\n        input:\n          parameters:\n            node_admin_extra_ssh_keys: <% $.pubkey %>\n          container: <% $.container %>\n\n      # NOTE(shadower): We need to clear keys from a previous deployment\n      unset_validation_key_parameter:\n        action: tripleo.parameters.update\n        input:\n          parameters:\n            node_admin_extra_ssh_keys: \"\"\n          container: <% $.container %>\n\n  copy_ssh_key:\n    input:\n      # FIXME: we should stop using heat-admin as e.g. split-stack\n      # environments (where Nova didn't create overcloud nodes) don't\n      # have it present\n      - overcloud_admin: heat-admin\n      - queue_name: tripleo\n    tags:\n      - tripleo-common-managed\n    tasks:\n      get_servers:\n        action: nova.servers_list\n        on-success: get_pubkey\n        publish:\n          servers: <% task().result._info %>\n\n      get_pubkey:\n        action: tripleo.validations.get_pubkey\n        on-success: deploy_ssh_key\n        publish:\n          pubkey: <% task().result %>\n\n      deploy_ssh_key:\n        workflow: tripleo.deployment.v1.deploy_on_server\n        with-items: server in <% $.servers %>\n        input:\n          server_name: <% $.server.name %>\n          server_uuid: <% $.server.id %>\n          config: |\n            #!/bin/bash\n            if ! grep \"<% $.pubkey %>\" /home/<% $.overcloud_admin %>/.ssh/authorized_keys; then\n              echo \"<% $.pubkey %>\" >> /home/<% $.overcloud_admin %>/.ssh/authorized_keys\n            fi\n          config_name: copy_ssh_key\n          group: script\n          queue_name: <% $.queue_name %>\n\n  check_boot_images:\n    input:\n      - deploy_kernel_name: 'bm-deploy-kernel'\n      - deploy_ramdisk_name: 'bm-deploy-ramdisk'\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n      kernel_id: <% $.kernel_id %>\n      ramdisk_id: <% $.ramdisk_id %>\n    tags:\n      - tripleo-common-managed\n    tasks:\n      check_run_validations:\n        on-complete:\n          - get_images: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      get_images:\n        action: glance.images_list\n        on-success: check_images\n        publish:\n          images: <% task().result %>\n\n      check_images:\n        action: tripleo.validations.check_boot_images\n        input:\n          images: <% $.images %>\n          deploy_kernel_name: <% $.deploy_kernel_name %>\n          deploy_ramdisk_name: <% $.deploy_ramdisk_name %>\n        on-success: send_message\n        publish:\n          kernel_id: <% task().result.kernel_id %>\n          ramdisk_id: <% task().result.ramdisk_id %>\n          warnings: <% task().result.warnings %>\n          errors: <% task().result.errors %>\n        on-error: send_message\n        publish-on-error:\n          kernel_id: <% task().result.kernel_id %>\n          ramdisk_id: <% task().result.ramdisk_id %>\n          warnings: <% task().result.warnings %>\n          errors: <% task().result.errors %>\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.check_boot_images\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                kernel_id: <% $.kernel_id %>\n                ramdisk_id: <% $.ramdisk_id %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  collect_flavors:\n    input:\n      - roles_info: {}\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n      flavors: <% $.flavors %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_run_validations:\n        on-complete:\n          - check_flavors: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      check_flavors:\n        action: tripleo.validations.check_flavors\n        input:\n          roles_info: <% $.roles_info %>\n        on-success: send_message\n        publish:\n          flavors: <% task().result.flavors %>\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n        on-error: send_message\n        publish-on-error:\n          flavors: {}\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.collect_flavors\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                flavors: <% $.flavors %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  check_ironic_boot_configuration:\n    input:\n      - kernel_id: null\n      - ramdisk_id: null\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_run_validations:\n        on-complete:\n          - get_ironic_nodes: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      get_ironic_nodes:\n        action: ironic.node_list\n        input:\n          provision_state: available\n          maintenance: false\n          detail: true\n        on-success: check_node_boot_configuration\n        publish:\n          nodes: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      check_node_boot_configuration:\n        action: tripleo.validations.check_node_boot_configuration\n        input:\n          node: <% $.node %>\n          kernel_id: <% $.kernel_id %>\n          ramdisk_id: <% $.ramdisk_id %>\n        with-items: node in <% $.nodes %>\n        on-success: send_message\n        publish:\n          errors: <% task().result.errors.flatten() %>\n          warnings: <% task().result.warnings.flatten() %>\n        on-error: send_message\n        publish-on-error:\n          errors: <% task().result.errors.flatten() %>\n          warnings: <% task().result.warnings.flatten() %>\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.check_ironic_boot_configuration\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  verify_profiles:\n    input:\n      - flavors: []\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_run_validations:\n        on-complete:\n          - get_ironic_nodes: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      get_ironic_nodes:\n        action: ironic.node_list\n        input:\n          maintenance: false\n          detail: true\n        on-success: verify_profiles\n        publish:\n          nodes: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      verify_profiles:\n        action: tripleo.validations.verify_profiles\n        input:\n          nodes: <% $.nodes %>\n          flavors: <% $.flavors %>\n        on-success: send_message\n        publish:\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n        on-error: send_message\n        publish-on-error:\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.verify_profiles\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  check_default_nodes_count:\n    input:\n      - stack_id: overcloud\n      - parameters: {}\n      - default_role_counts: {}\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      statistics: <% $.statistics %>\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_run_validations:\n        on-complete:\n          - get_hypervisor_statistics: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      get_hypervisor_statistics:\n        action: nova.hypervisors_statistics\n        on-success: get_stack\n        publish:\n          statistics: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n          errors: []\n          warnings: []\n          statistics: null\n\n      get_stack:\n        action: heat.stacks_get\n        input:\n          stack_id: <% $.stack_id %>\n        on-success: get_associated_nodes\n        publish:\n          stack: <% task().result %>\n        on-error: get_associated_nodes\n        publish-on-error:\n          stack: null\n\n      get_associated_nodes:\n        action: ironic.node_list\n        input:\n          associated: true\n        on-success: get_available_nodes\n        publish:\n          associated_nodes: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n          errors: []\n          warnings: []\n\n      get_available_nodes:\n        action: ironic.node_list\n        input:\n          provision_state: available\n          associated: false\n          maintenance: false\n        on-success: check_nodes_count\n        publish:\n          available_nodes: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n          errors: []\n          warnings: []\n\n      check_nodes_count:\n        action: tripleo.validations.check_nodes_count\n        input:\n          statistics: <% $.statistics %>\n          stack: <% $.stack %>\n          associated_nodes: <% $.associated_nodes %>\n          available_nodes: <% $.available_nodes %>\n          parameters: <% $.parameters %>\n          default_role_counts: <% $.default_role_counts %>\n        on-success: send_message\n        publish:\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n          statistics: null\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.check_hypervisor_stats\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                statistics: <% $.statistics %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  check_pre_deployment_validations:\n    input:\n      - deploy_kernel_name: 'bm-deploy-kernel'\n      - deploy_ramdisk_name: 'bm-deploy-ramdisk'\n      - roles_info: {}\n      - stack_id: overcloud\n      - parameters: {}\n      - default_role_counts: {}\n      - run_validations: true\n      - queue_name: tripleo\n\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n      kernel_id: <% $.kernel_id %>\n      ramdisk_id: <% $.ramdisk_id %>\n      flavors: <% $.flavors %>\n      statistics: <% $.statistics %>\n    tags:\n      - tripleo-common-managed\n    tasks:\n      init_messages:\n        on-success: check_boot_images\n        publish:\n          errors: []\n          warnings: []\n\n      check_boot_images:\n        workflow: check_boot_images\n        input:\n          deploy_kernel_name: <% $.deploy_kernel_name %>\n          deploy_ramdisk_name: <% $.deploy_ramdisk_name %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          kernel_id: <% task().result.get('kernel_id') %>\n          ramdisk_id: <% task().result.get('ramdisk_id') %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          kernel_id: <% task().result.get('kernel_id') %>\n          ramdisk_id: <% task().result.get('ramdisk_id') %>\n          status: FAILED\n        on-success: collect_flavors\n        on-error: collect_flavors\n\n      collect_flavors:\n        workflow: collect_flavors\n        input:\n          roles_info: <% $.roles_info %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          flavors: <% task().result.get('flavors') %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          flavors: <% task().result.get('flavors') %>\n          status: FAILED\n        on-success: check_ironic_boot_configuration\n        on-error: check_ironic_boot_configuration\n\n      check_ironic_boot_configuration:\n        workflow: check_ironic_boot_configuration\n        input:\n          kernel_id: <% $.kernel_id %>\n          ramdisk_id: <% $.ramdisk_id %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          status: FAILED\n        on-success: check_default_nodes_count\n        on-error: check_default_nodes_count\n\n      check_default_nodes_count:\n        workflow: check_default_nodes_count\n        # ironic-nova sync happens once in two minutes\n        retry: count=12 delay=10\n        input:\n          stack_id: <% $.stack_id %>\n          parameters: <% $.parameters %>\n          default_role_counts: <% $.default_role_counts %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          statistics: <% task().result.get('statistics') %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          statistics: <% task().result.get('statistics') %>\n          status: FAILED\n        on-success: verify_profiles\n        # Do not confuse user with info about profiles if the nodes\n        # count is off in the first place. Skip directly to\n        # send_message. (bug 1703942)\n        on-error: send_message\n\n      verify_profiles:\n        workflow: verify_profiles\n        input:\n          flavors: <% $.flavors %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          status: FAILED\n        on-success: send_message\n        on-error: send_message\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.check_hypervisor_stats\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                kernel_id: <% $.kernel_id %>\n                ramdisk_id: <% $.ramdisk_id %>\n                flavors: <% $.flavors %>\n                statistics: <% $.statistics %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.validations.v1", "tags": [], "created_at": "2018-12-03 11:19:15", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "799d1f1f-5bdd-49a9-b114-7aa3ebb06f1f"}]}

2018-12-04 06:25:48,722 DEBUG: HTTP GET https://172.16.0.10:13989/v2/workbooks 200
2018-12-04 06:25:48,728 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.access.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:48,746 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.access.v1 HTTP/1.1" 204 0
2018-12-04 06:25:48,747 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:48,747 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.access.v1 204
2018-12-04 06:25:48,747 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.baremetal.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:48,765 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.baremetal.v1 HTTP/1.1" 204 0
2018-12-04 06:25:48,766 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:48,767 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.baremetal.v1 204
2018-12-04 06:25:48,767 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.storage.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:48,785 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.storage.v1 HTTP/1.1" 204 0
2018-12-04 06:25:48,786 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:48,786 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.storage.v1 204
2018-12-04 06:25:48,787 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.deployment.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:48,808 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.deployment.v1 HTTP/1.1" 204 0
2018-12-04 06:25:48,809 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:48,809 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.deployment.v1 204
2018-12-04 06:25:48,809 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.derive_params.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:48,826 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.derive_params.v1 HTTP/1.1" 204 0
2018-12-04 06:25:48,827 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:48,827 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.derive_params.v1 204
2018-12-04 06:25:48,828 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.fernet_keys.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:48,845 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.fernet_keys.v1 HTTP/1.1" 204 0
2018-12-04 06:25:48,845 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:48,846 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.fernet_keys.v1 204
2018-12-04 06:25:48,846 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.derive_params_formulas.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:48,863 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.derive_params_formulas.v1 HTTP/1.1" 204 0
2018-12-04 06:25:48,864 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:48,864 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.derive_params_formulas.v1 204
2018-12-04 06:25:48,864 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.octavia_post.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:48,884 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.octavia_post.v1 HTTP/1.1" 204 0
2018-12-04 06:25:48,885 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:48,885 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.octavia_post.v1 204
2018-12-04 06:25:48,885 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.networks.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:48,902 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.networks.v1 HTTP/1.1" 204 0
2018-12-04 06:25:48,902 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:48,903 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.networks.v1 204
2018-12-04 06:25:48,903 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.package_update.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:48,920 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.package_update.v1 HTTP/1.1" 204 0
2018-12-04 06:25:48,921 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:48,922 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.package_update.v1 204
2018-12-04 06:25:48,922 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.plan_management.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:48,939 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.plan_management.v1 HTTP/1.1" 204 0
2018-12-04 06:25:48,940 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:48,940 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.plan_management.v1 204
2018-12-04 06:25:48,941 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.skydive_ansible.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:48,958 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.skydive_ansible.v1 HTTP/1.1" 204 0
2018-12-04 06:25:48,959 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:48,959 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.skydive_ansible.v1 204
2018-12-04 06:25:48,960 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.scale.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:48,977 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.scale.v1 HTTP/1.1" 204 0
2018-12-04 06:25:48,978 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:48,978 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.scale.v1 204
2018-12-04 06:25:48,979 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.stack.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:48,995 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.stack.v1 HTTP/1.1" 204 0
2018-12-04 06:25:48,995 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:48 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:48,996 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.stack.v1 204
2018-12-04 06:25:48,996 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.swift_ring.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:49,013 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.swift_ring.v1 HTTP/1.1" 204 0
2018-12-04 06:25:49,014 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:49 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:49,014 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.swift_ring.v1 204
2018-12-04 06:25:49,015 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.support.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:49,031 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.support.v1 HTTP/1.1" 204 0
2018-12-04 06:25:49,031 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:49 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:49,032 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.support.v1 204
2018-12-04 06:25:49,032 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.swift_rings_backup.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:49,050 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.swift_rings_backup.v1 HTTP/1.1" 204 0
2018-12-04 06:25:49,050 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:49 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:49,051 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.swift_rings_backup.v1 204
2018-12-04 06:25:49,051 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.undercloud_backup.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:49,068 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.undercloud_backup.v1 HTTP/1.1" 204 0
2018-12-04 06:25:49,069 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:49 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:49,069 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.undercloud_backup.v1 204
2018-12-04 06:25:49,070 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.validations.v1 -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:49,087 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workbooks/tripleo.validations.v1 HTTP/1.1" 204 0
2018-12-04 06:25:49,088 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:49 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:49,088 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workbooks/tripleo.validations.v1 204
2018-12-04 06:25:49,089 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/workflows -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:49,221 DEBUG: https://172.16.0.10:13989 "GET /v2/workflows HTTP/1.1" 200 272802
2018-12-04 06:25:49,274 DEBUG: RESP: [200] Content-Length: 272802 Content-Type: application/json Date: Tue, 04 Dec 2018 11:25:49 GMT Connection: keep-alive 
RESP BODY: {"workflows": [{"definition": "create_admin_via_ssh:\n  workflow: tripleo.access.v1.create_admin_via_ssh\n  input:\n    ssh_private_key: <% $.ssh_private_key %>\n    ssh_user: <% $.ssh_user %>\n    ssh_servers: <% $.ssh_servers %>\n    tasks: <% $.create_admin_tasks %>\n", "name": "tripleo.access.v1.create_admin_via_ssh", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:49", "namespace": "", "updated_at": null, "scope": "private", "input": "tasks, ssh_private_key, ssh_user, ssh_servers, ansible_extra_env_variables={u'ANSIBLE_HOST_KEY_CHECKING': u'False'}", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "0ccd8a5a-a5a3-4d4a-ace2-96f6e246228b"}, {"definition": "create_admin_via_nova:\n  workflow: tripleo.access.v1.create_admin_via_nova\n  input:\n    queue_name: <% $.queue_name %>\n    ssh_servers: <% $.ssh_servers %>\n    tasks: <% $.create_admin_tasks %>\n    overcloud_admin: <% $.overcloud_admin %>\n\n# SSH variant\n", "name": "tripleo.access.v1.create_admin_via_nova", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:49", "namespace": "", "updated_at": null, "scope": "private", "input": "tasks, queue_name=tripleo, ssh_servers=[], overcloud_admin=tripleo-admin, ansible_extra_env_variables={u'ANSIBLE_HOST_KEY_CHECKING': u'False'}", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "d1e48f83-eccb-429a-ab05-7cb1d43e410d"}, {"definition": "enable_ssh_admin:\n  description: >-\n    This workflow creates an admin user on the overcloud nodes,\n    which can then be used for connecting for automated\n    administrative or deployment tasks, e.g. via Ansible. The\n    workflow can be used both for Nova-managed and split-stack\n    deployments, assuming the correct input values are passed\n    in. The workflow defaults to Nova-managed approach, for which no\n    additional parameters need to be supplied. In case of\n    split-stack, temporary ssh connection details (user, key, list\n    of servers) need to be provided -- these are only used\n    temporarily to create the actual ssh admin user for use by\n    Mistral.\n  tags:\n    - tripleo-common-managed\n  input:\n    - ssh_private_key: null\n    - ssh_user: null\n    - ssh_servers: []\n    - overcloud_admin: tripleo-admin\n    - queue_name: tripleo\n  tasks:\n    get_pubkey:\n      action: tripleo.validations.get_pubkey\n      on-success: generate_playbook\n      publish:\n        pubkey: <% task().result %>\n\n    generate_playbook:\n      on-success:\n        - create_admin_via_nova: <% $.ssh_private_key = null %>\n        - create_admin_via_ssh: <% $.ssh_private_key != null %>\n      publish:\n        create_admin_tasks:\n          - name: create user <% $.overcloud_admin %>\n            user:\n              name: '<% $.overcloud_admin %>'\n          - name: grant admin rights to user <% $.overcloud_admin %>\n            copy:\n              dest: /etc/sudoers.d/<% $.overcloud_admin %>\n              content: |\n                <% $.overcloud_admin %> ALL=(ALL) NOPASSWD:ALL\n              mode: 0440\n          - name: ensure .ssh dir exists for user <% $.overcloud_admin %>\n            file:\n              path: /home/<% $.overcloud_admin %>/.ssh\n              state: directory\n              owner: <% $.overcloud_admin %>\n              group: <% $.overcloud_admin %>\n              mode: 0700\n          - name: ensure authorized_keys file exists for user <% $.overcloud_admin %>\n            file:\n              path: /home/<% $.overcloud_admin %>/.ssh/authorized_keys\n              state: touch\n              owner: <% $.overcloud_admin %>\n              group: <% $.overcloud_admin %>\n              mode: 0700\n          - name: authorize TripleO Mistral key for user <% $.overcloud_admin %>\n            lineinfile:\n              path: /home/<% $.overcloud_admin %>/.ssh/authorized_keys\n              line: <% $.pubkey %>\n              regexp: \"Generated by TripleO\"\n\n    # Nova variant\n    create_admin_via_nova:\n      workflow: tripleo.access.v1.create_admin_via_nova\n      input:\n        queue_name: <% $.queue_name %>\n        ssh_servers: <% $.ssh_servers %>\n        tasks: <% $.create_admin_tasks %>\n        overcloud_admin: <% $.overcloud_admin %>\n\n    # SSH variant\n    create_admin_via_ssh:\n      workflow: tripleo.access.v1.create_admin_via_ssh\n      input:\n        ssh_private_key: <% $.ssh_private_key %>\n        ssh_user: <% $.ssh_user %>\n        ssh_servers: <% $.ssh_servers %>\n        tasks: <% $.create_admin_tasks %>\n", "name": "tripleo.access.v1.enable_ssh_admin", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:49", "namespace": "", "updated_at": null, "scope": "private", "input": "ssh_private_key=None, ssh_user=None, ssh_servers=[], overcloud_admin=tripleo-admin, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "d78aee7a-d292-4722-a17f-fe114a8ca140"}, {"definition": "configure_manageable_nodes:\n  description: Update the boot configuration of all nodes in 'manageable' state.\n\n  input:\n    - queue_name: tripleo\n    - kernel_name: 'bm-deploy-kernel'\n    - ramdisk_name: 'bm-deploy-ramdisk'\n    - instance_boot_option: null\n    - root_device: null\n    - root_device_minimum_size: 4\n    - overwrite_root_device_hints: False\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    get_manageable_nodes:\n      action: ironic.node_list maintenance=False associated=False\n      on-success: configure_manageable\n      on-error: set_status_failed_get_manageable_nodes\n      publish:\n        managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>\n\n    configure_manageable:\n      on-success: send_message\n      on-error: set_status_failed_configure_manageable\n      workflow: tripleo.baremetal.v1.configure\n      input:\n        node_uuids: <% $.managed_nodes %>\n        queue_name: <% $.queue_name %>\n        kernel_name: <% $.kernel_name %>\n        ramdisk_name: <% $.ramdisk_name %>\n        instance_boot_option: <% $.instance_boot_option %>\n        root_device: <% $.root_device %>\n        root_device_minimum_size: <% $.root_device_minimum_size %>\n        overwrite_root_device_hints: <% $.overwrite_root_device_hints %>\n      publish:\n        message: 'Manageable nodes configured successfully.'\n\n    set_status_failed_configure_manageable:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(configure_manageable).result %>\n\n    set_status_failed_get_manageable_nodes:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(get_manageable_nodes).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.configure_manageable_nodes\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.configure_manageable_nodes", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "queue_name=tripleo, kernel_name=bm-deploy-kernel, ramdisk_name=bm-deploy-ramdisk, instance_boot_option=None, root_device=None, root_device_minimum_size=4, overwrite_root_device_hints=False", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "03c9f1f3-084f-4cf2-b6a3-b0c7b632dce6"}, {"definition": "cellv2_discovery:\n  description: Run cell_v2 host discovery\n\n  input:\n    - node_uuids\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    cell_v2_discover_hosts:\n      on-success: wait_for_nova_resources\n      on-error: cell_v2_discover_hosts_failed\n      action: tripleo.baremetal.cell_v2_discover_hosts\n\n    cell_v2_discover_hosts_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(cell_v2_discover_hosts).result %>\n\n    wait_for_nova_resources:\n      on-success: send_message\n      on-error: wait_for_nova_resources_failed\n      with-items: node_uuid in <% $.node_uuids %>\n      action: nova.hypervisors_find hypervisor_hostname=<% $.node_uuid %>\n\n    wait_for_nova_resources_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(wait_for_nova_resources).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.cellv2_discovery\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.cellv2_discovery", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "node_uuids, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "0539c042-546d-4e52-9848-dad6a7551daf"}, {"definition": "introspect:\n  description: >\n      Take a list of nodes and move them through introspection.\n\n      By default each node will attempt introspection up to 3 times (two\n      retries plus the initial attemp) if it fails. This behaviour can be\n      modified by changing the max_retry_attempts input.\n\n      The workflow will assume the node has timed out after 20 minutes (1200\n      seconds). This can be changed by passing the node_timeout input in\n      seconds.\n\n  input:\n    - node_uuids\n    - run_validations: False\n    - queue_name: tripleo\n    - concurrency: 20\n    - max_retry_attempts: 2\n    - node_timeout: 1200\n\n  tags:\n    - tripleo-common-managed\n\n  task-defaults:\n    on-error: unhandled_error\n\n  tasks:\n    initialize:\n      publish:\n        introspection_attempt: 1\n      on-complete:\n        - run_validations: <% $.run_validations %>\n        - introspect_nodes: <% not $.run_validations %>\n\n    run_validations:\n      workflow: tripleo.validations.v1.run_groups\n      input:\n        group_names:\n          - 'pre-introspection'\n        queue_name: <% $.queue_name %>\n      on-success: introspect_nodes\n      on-error: set_validations_failed\n\n    set_validations_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(run_validations).result %>\n\n    introspect_nodes:\n      with-items: uuid in <% $.node_uuids %>\n      concurrency: <% $.concurrency %>\n      workflow: _introspect\n      input:\n        node_uuid: <% $.uuid %>\n        queue_name: <% $.queue_name %>\n        timeout: <% $.node_timeout %>\n      # on-error is triggered if one or more nodes failed introspection. We\n      # still go to get_introspection_status as it will collect the result\n      # for each node. Unless we hit the retry limit.\n      on-error:\n       - get_introspection_status: <% $.introspection_attempt <= $.max_retry_attempts %>\n       - max_retry_attempts_reached: <% $.introspection_attempt > $.max_retry_attempts %>\n      on-success: get_introspection_status\n\n    get_introspection_status:\n      with-items: uuid in <% $.node_uuids %>\n      action: baremetal_introspection.get_status\n      input:\n        uuid: <% $.uuid %>\n      publish:\n        introspected_nodes: <% task().result.toDict($.uuid, $) %>\n        # Currently there is no way for us to ignore user introspection\n        # aborts. This means we will retry aborted nodes until the Ironic API\n        # gives us more details (error code or a boolean to show aborts etc.)\n        # If a node hasn't finished, we consider it to be failed.\n        # TODO(d0ugal): When possible, don't retry introspection of nodes\n        #               that a user manually aborted.\n        failed_introspection: <% task().result.where($.finished = true and $.error != null).select($.uuid) + task().result.where($.finished = false).select($.uuid) %>\n      publish-on-error:\n        # If a node fails to start introspection, getting the status can fail.\n        # When that happens, the result is a string and the nodes need to be\n        # filtered out.\n        introspected_nodes: <% task().result.where(isDict($)).toDict($.uuid, $) %>\n        # If there was an error, the exception string we get doesn't give us\n        # the UUID. So we use a set difference to find the UUIDs missing in\n        # the results. These are then added to the failed nodes.\n        failed_introspection: <% ($.node_uuids.toSet() - task().result.where(isDict($)).select($.uuid).toSet()) + task().result.where(isDict($)).where($.finished = true and $.error != null).toSet() + task().result.where(isDict($)).where($.finished = false).toSet() %>\n      on-error: increase_attempt_counter\n      on-success:\n        - successful_introspection: <% $.failed_introspection.len() = 0 %>\n        - increase_attempt_counter: <% $.failed_introspection.len() > 0 %>\n\n    increase_attempt_counter:\n      publish:\n        introspection_attempt: <% $.introspection_attempt + 1 %>\n      on-complete:\n        retry_failed_nodes\n\n    retry_failed_nodes:\n      publish:\n        status: RUNNING\n        message: <% 'Retrying {0} nodes that failed introspection. Attempt {1} of {2} '.format($.failed_introspection.len(), $.introspection_attempt, $.max_retry_attempts + 1) %>\n        # We are about to retry, update the tracking stats.\n        node_uuids: <% $.failed_introspection %>\n      on-success:\n        - send_message\n        - introspect_nodes\n\n    max_retry_attempts_reached:\n      publish:\n        status: FAILED\n        message: <% 'Retry limit reached with {0} nodes still failing introspection'.format($.failed_introspection.len()) %>\n      on-complete: send_message\n\n    successful_introspection:\n      publish:\n        status: SUCCESS\n        message: Successfully introspected <% $.introspected_nodes.len() %> node(s).\n      on-complete: send_message\n\n    unhandled_error:\n      publish:\n        status: FAILED\n        message: \"Unhandled workflow error\"\n      on-complete: send_message\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.introspect\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              introspected_nodes: <% $.get('introspected_nodes', []) %>\n              failed_introspection: <% $.get('failed_introspection', []) %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.introspect", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "node_uuids, run_validations=False, queue_name=tripleo, concurrency=20, max_retry_attempts=2, node_timeout=1200", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "0ef592f2-0d69-4d5d-8f66-6e92e8f97108"}, {"definition": "nodes_with_hint:\n  description: Find nodes matching a hint regex\n  input:\n    - hint_regex\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_nodes:\n      with-items: provision_state in <% ['available', 'active'] %>\n      action: ironic.node_list maintenance=false provision_state=<% $.provision_state %> detail=true\n      on-success: get_matching_nodes\n      on-error: set_status_failed_get_nodes\n\n    get_matching_nodes:\n      with-items: node in <% task(get_nodes).result.flatten() %>\n      action: tripleo.baremetal.get_node_hint node=<% $.node %>\n      on-success: send_message\n      on-error: set_status_failed_get_matching_nodes\n      publish:\n        matching_nodes: <% let(hint_regex => $.hint_regex) -> task().result.where($.hint and $.hint.matches($hint_regex)).uuid %>\n\n    set_status_failed_get_nodes:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(get_nodes).result %>\n\n    set_status_failed_get_matching_nodes:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(get_matching_nodes).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.nodes_with_hint\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              matching_nodes: <% $.matching_nodes or [] %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.nodes_with_hint", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "hint_regex, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "2b55fc55-3e8a-4a73-ada2-33a9aca9feda"}, {"definition": "validate_nodes:\n  description: Validate nodes JSON\n\n  input:\n   - nodes_json\n   - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    validate_nodes:\n      action: tripleo.baremetal.validate_nodes\n      on-success: send_message\n      on-error: validation_failed\n      input:\n         nodes_json: <% $.nodes_json %>\n\n    validation_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(validate_nodes).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.validate_nodes\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.validate_nodes", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "nodes_json, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "347ecaf3-f7f2-4cce-bd68-f9fcb23311ec"}, {"definition": "register_or_update:\n  description: Take nodes JSON and create nodes in a \"manageable\" state\n\n  input:\n   - nodes_json\n   - remove: False\n   - queue_name: tripleo\n   - kernel_name: null\n   - ramdisk_name: null\n   - instance_boot_option: local\n   - initial_state: manageable\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    validate_input:\n      workflow: tripleo.baremetal.v1.validate_nodes\n      on-success: register_or_update_nodes\n      on-error: validation_failed\n      input:\n        nodes_json: <% $.nodes_json %>\n        queue_name: <% $.queue_name %>\n\n    validation_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(validate_input).result %>\n        registered_nodes: []\n\n    register_or_update_nodes:\n      action: tripleo.baremetal.register_or_update_nodes\n      on-success:\n        - set_nodes_managed: <% $.initial_state != \"enroll\" %>\n        - send_message: <% $.initial_state = \"enroll\" %>\n      on-error: set_status_failed_register_or_update_nodes\n      input:\n         nodes_json: <% $.nodes_json %>\n         remove: <% $.remove %>\n         kernel_name: <% $.kernel_name %>\n         ramdisk_name: <% $.ramdisk_name %>\n         instance_boot_option: <% $.instance_boot_option %>\n      publish:\n        registered_nodes: <% task().result %>\n        new_nodes: <% task().result.where($.provision_state = 'enroll') %>\n\n    set_status_failed_register_or_update_nodes:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(register_or_update_nodes).result %>\n        registered_nodes: []\n\n    set_nodes_managed:\n      on-success:\n        - set_nodes_available: <% $.initial_state = \"available\" %>\n        - send_message: <% $.initial_state != \"available\" %>\n      on-error: set_status_failed_nodes_managed\n      workflow: tripleo.baremetal.v1.manage\n      input:\n        node_uuids: <% $.new_nodes.uuid %>\n        queue_name: <% $.queue_name %>\n      publish:\n        status: SUCCESS\n        message: <% $.new_nodes.len() %> node(s) successfully moved to the \"manageable\" state.\n\n    set_status_failed_nodes_managed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(set_nodes_managed).result %>\n\n    set_nodes_available:\n      on-success: send_message\n      on-error: set_status_failed_nodes_available\n      workflow: tripleo.baremetal.v1.provide node_uuids=<% $.new_nodes.uuid %> queue_name=<% $.queue_name %>\n      publish:\n        status: SUCCESS\n        message: <% $.new_nodes.len() %> node(s) successfully moved to the \"available\" state.\n\n    set_status_failed_nodes_available:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(set_nodes_available).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.register_or_update\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              registered_nodes: <% $.registered_nodes or [] %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.register_or_update", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "nodes_json, remove=False, queue_name=tripleo, kernel_name=None, ramdisk_name=None, instance_boot_option=local, initial_state=manageable", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "4f071579-0f64-4291-b929-163d191c2f15"}, {"definition": "create_raid_configuration:\n  description: Create and apply RAID configuration for given nodes\n  input:\n    - node_uuids\n    - configuration\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    set_configuration:\n      with-items: node_uuid in <% $.node_uuids %>\n      action: ironic.node_set_target_raid_config node_ident=<% $.node_uuid %> target_raid_config=<% $.configuration %>\n      on-success: apply_configuration\n      on-error: set_configuration_failed\n\n    set_configuration_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(set_configuration).result %>\n\n    apply_configuration:\n      with-items: node_uuid in <% $.node_uuids %>\n      workflow: tripleo.baremetal.v1.manual_cleaning\n      input:\n        node_uuid: <% $.node_uuid %>\n        clean_steps:\n          - interface: raid\n            step: delete_configuration\n          - interface: raid\n            step: create_configuration\n        timeout: 1800  # building RAID should be fast than general cleaning\n        retry_count: 180\n        retry_delay: 10\n      on-success: send_message\n      on-error: apply_configuration_failed\n      publish:\n        message: <% task().result %>\n        status: SUCCESS\n\n    apply_configuration_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(apply_configuration).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.create_raid_configuration\n            payload:\n              status: <% $.get('status', 'FAILED') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.create_raid_configuration", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "node_uuids, configuration, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "5e87ca39-f7ab-4ad2-8bc0-07c625449fb5"}, {"definition": "discover_nodes:\n  description: Run nodes discovery over the given IP range\n\n  input:\n    - ip_addresses\n    - credentials\n    - ports: [623]\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    get_all_nodes:\n      action: ironic.node_list\n      input:\n        fields: [\"uuid\", \"driver\", \"driver_info\"]\n        limit: 0\n      on-success: get_candidate_nodes\n      on-error: get_all_nodes_failed\n      publish:\n        existing_nodes: <% task().result %>\n\n    get_all_nodes_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(get_all_nodes).result %>\n\n    get_candidate_nodes:\n      action: tripleo.baremetal.get_candidate_nodes\n      input:\n        ip_addresses: <% $.ip_addresses %>\n        credentials: <% $.credentials %>\n        ports: <% $.ports %>\n        existing_nodes: <% $.existing_nodes %>\n      on-success: probe_nodes\n      on-error: get_candidate_nodes_failed\n      publish:\n        candidates: <% task().result %>\n\n    get_candidate_nodes_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(get_candidate_nodes).result %>\n\n    probe_nodes:\n      action: tripleo.baremetal.probe_node\n      on-success: send_message\n      on-error: probe_nodes_failed\n      input:\n        ip: <% $.node.ip %>\n        port: <% $.node.port %>\n        username: <% $.node.username %>\n        password: <% $.node.password %>\n      with-items:\n        - node in <% $.candidates %>\n      publish:\n        nodes_json: <% task().result.where($ != null) %>\n\n    probe_nodes_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(probe_nodes).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.discover_nodes\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              nodes_json: <% $.get('nodes_json', []) %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.discover_nodes", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "ip_addresses, credentials, ports=[623], queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "6074313a-4065-40fc-9e01-34d37d6e93fd"}, {"definition": "introspect_manageable_nodes:\n  description: Introspect all nodes in a 'manageable' state.\n\n  input:\n    - run_validations: False\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    get_manageable_nodes:\n      action: ironic.node_list maintenance=False associated=False\n      on-success: validate_nodes\n      on-error: set_status_failed_get_manageable_nodes\n      publish:\n        managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>\n\n    set_status_failed_get_manageable_nodes:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(get_manageable_nodes).result %>\n\n    validate_nodes:\n      on-success:\n        - introspect_manageable: <% $.managed_nodes.len() > 0 %>\n        - set_status_failed_no_nodes: <% $.managed_nodes.len() = 0 %>\n\n    set_status_failed_no_nodes:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: No manageable nodes to introspect. Check node states and maintenance.\n\n    introspect_manageable:\n      on-success: send_message\n      on-error: set_status_introspect_manageable\n      workflow: tripleo.baremetal.v1.introspect\n      input:\n        node_uuids: <% $.managed_nodes %>\n        run_validations: <% $.run_validations %>\n        queue_name: <% $.queue_name %>\n      publish:\n        introspected_nodes: <% task().result.introspected_nodes %>\n\n    set_status_introspect_manageable:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(introspect_manageable).result %>\n        introspected_nodes: []\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.introspect_manageable_nodes\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              introspected_nodes: <% $.get('introspected_nodes', []) %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.introspect_manageable_nodes", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "run_validations=False, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "726c4740-16a5-4117-a619-dadde465fdad"}, {"definition": "configure:\n  description: Take a list of manageable nodes and update their boot configuration.\n\n  input:\n    - node_uuids\n    - queue_name: tripleo\n    - kernel_name: bm-deploy-kernel\n    - ramdisk_name: bm-deploy-ramdisk\n    - instance_boot_option: null\n    - root_device: null\n    - root_device_minimum_size: 4\n    - overwrite_root_device_hints: False\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    configure_boot:\n      on-success: configure_root_device\n      on-error: set_status_failed_configure_boot\n      with-items: node_uuid in <% $.node_uuids %>\n      action: tripleo.baremetal.configure_boot node_uuid=<% $.node_uuid %> kernel_name=<% $.kernel_name %> ramdisk_name=<% $.ramdisk_name %> instance_boot_option=<% $.instance_boot_option %>\n\n    configure_root_device:\n      on-success: send_message\n      on-error: set_status_failed_configure_root_device\n      with-items: node_uuid in <% $.node_uuids %>\n      action: tripleo.baremetal.configure_root_device node_uuid=<% $.node_uuid %> root_device=<% $.root_device %> minimum_size=<% $.root_device_minimum_size %> overwrite=<% $.overwrite_root_device_hints %>\n      publish:\n        status: SUCCESS\n        message: 'Successfully configured the nodes.'\n\n    set_status_failed_configure_boot:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(configure_boot).result %>\n\n    set_status_failed_configure_root_device:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(configure_root_device).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.configure\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.configure", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "node_uuids, queue_name=tripleo, kernel_name=bm-deploy-kernel, ramdisk_name=bm-deploy-ramdisk, instance_boot_option=None, root_device=None, root_device_minimum_size=4, overwrite_root_device_hints=False", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7358d877-b770-4ab8-ad76-466944389308"}, {"definition": "set_node_state:\n  input:\n    - node_uuid\n    - state_action\n    - target_state\n    - error_states:\n        # The default includes all failure states, even unused by TripleO.\n        - 'error'\n        - 'adopt failed'\n        - 'clean failed'\n        - 'deploy failed'\n        - 'inspect failed'\n        - 'rescue failed'\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    set_provision_state:\n      on-success: wait_for_provision_state\n      on-error: set_provision_state_failed\n      action: ironic.node_set_provision_state node_uuid=<% $.node_uuid %> state=<% $.state_action %>\n\n    set_provision_state_failed:\n      publish:\n        message: <% task(set_provision_state).result %>\n      on-complete: fail\n\n    wait_for_provision_state:\n      action: ironic.node_get\n      input:\n        node_id: <% $.node_uuid %>\n        fields: ['provision_state', 'last_error']\n      timeout: 1200 #20 minutes\n      retry:\n        delay: 3\n        count: 400\n        continue-on: <% not task().result.provision_state in [$.target_state] + $.error_states %>\n      on-complete:\n        - state_not_reached: <% task().result.provision_state != $.target_state %>\n\n    state_not_reached:\n      publish:\n        message: >-\n          Node <% $.node_uuid %> did not reach state \"<% $.target_state %>\",\n          the state is \"<% task(wait_for_provision_state).result.provision_state %>\",\n          error: <% task(wait_for_provision_state).result.last_error %>\n      on-complete: fail\n\n  output-on-error:\n    result: <% $.message %>\n", "name": "tripleo.baremetal.v1.set_node_state", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "node_uuid, state_action, target_state, error_states=[u'error', u'adopt failed', u'clean failed', u'deploy failed', u'inspect failed', u'rescue failed']", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "8ffb56a4-2462-42fa-9daa-c89fca1445a3"}, {"definition": "tag_nodes:\n  description: Runs the tag_node workflow in a loop\n  input:\n    - tag_node_uuids\n    - untag_node_uuids\n    - role\n    - plan: overcloud\n    - queue_name: tripleo\n\n  task-defaults:\n    on-error: send_message\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    tag_nodes:\n      with-items: node_uuid in <% $.tag_node_uuids %>\n      workflow: tripleo.baremetal.v1.tag_node\n      input:\n        node_uuid: <% $.node_uuid %>\n        queue_name: <% $.queue_name %>\n        role: <% $.role %>\n      concurrency: 1\n      on-success: untag_nodes\n\n    untag_nodes:\n      with-items: node_uuid in <% $.untag_node_uuids %>\n      workflow: tripleo.baremetal.v1.tag_node\n      input:\n        node_uuid: <% $.node_uuid %>\n        queue_name: <% $.queue_name %>\n      concurrency: 1\n      on-success: update_role_parameters\n\n    update_role_parameters:\n      on-success: send_message\n      action: tripleo.parameters.update_role role=<% $.role %> container=<% $.plan %>\n      publish:\n        message: <% task().result %>\n        status: SUCCESS\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.tag_nodes\n            payload:\n              status: <% $.get('status', 'FAILED') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.tag_nodes", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "tag_node_uuids, untag_node_uuids, role, plan=overcloud, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "aaa48d84-a79c-42c4-8e8a-cedf897399c9"}, {"definition": "provide:\n  description: Take a list of nodes and move them to \"available\"\n\n  input:\n    - node_uuids\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    set_nodes_available:\n      on-success: cell_v2_discover_hosts\n      on-error: set_status_failed_nodes_available\n      with-items: uuid in <% $.node_uuids %>\n      workflow: tripleo.baremetal.v1.set_node_state\n      input:\n        node_uuid: <% $.uuid %>\n        queue_name: <% $.queue_name %>\n        state_action: 'provide'\n        target_state: 'available'\n\n    set_status_failed_nodes_available:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(set_nodes_available).result %>\n\n    cell_v2_discover_hosts:\n      on-success: try_power_off\n      on-error: cell_v2_discover_hosts_failed\n      workflow: tripleo.baremetal.v1.cellv2_discovery\n      input:\n        node_uuids: <% $.node_uuids %>\n        queue_name: <% $.queue_name %>\n      timeout: 900 #15 minutes\n      retry:\n        delay: 30\n        count: 30\n\n    cell_v2_discover_hosts_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(cell_v2_discover_hosts).result %>\n\n    try_power_off:\n      on-success: send_message\n      on-error: power_off_failed\n      with-items: uuid in <% $.node_uuids %>\n      workflow: tripleo.baremetal.v1.set_power_state\n      input:\n        node_uuid: <% $.uuid %>\n        queue_name: <% $.queue_name %>\n        state_action: 'off'\n        target_state: 'power off'\n      publish:\n        status: SUCCESS\n        message: <% $.node_uuids.len() %> node(s) successfully moved to the \"available\" state.\n\n    power_off_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(try_power_off).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.provide\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.provide", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "node_uuids, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "af9573de-2601-48b4-b7ce-67081c6d7d59"}, {"definition": "tag_node:\n  description: Tag a node with a role\n  input:\n    - node_uuid\n    - role: null\n    - queue_name: tripleo\n\n  task-defaults:\n    on-error: send_message\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    update_node:\n      on-success: send_message\n      action: tripleo.baremetal.update_node_capability node_uuid=<% $.node_uuid %> capability='profile' value=<% $.role %>\n      publish:\n        message: <% task().result %>\n        status: SUCCESS\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.tag_node\n            payload:\n              status: <% $.get('status', 'FAILED') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.tag_node", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "node_uuid, role=None, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "b329c03c-548f-4833-8d84-b0678dfa24dd"}, {"definition": "manage:\n  description: Set a list of nodes to 'manageable' state\n\n  input:\n    - node_uuids\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    set_nodes_manageable:\n      on-success: send_message\n      on-error: set_status_failed_nodes_manageable\n      with-items: uuid in <% $.node_uuids %>\n      workflow: tripleo.baremetal.v1.set_node_state\n      input:\n        node_uuid: <% $.uuid %>\n        state_action: 'manage'\n        target_state: 'manageable'\n        error_states:\n          # node going back to enroll designates power credentials failure\n          - 'enroll'\n          - 'error'\n\n    set_status_failed_nodes_manageable:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(set_nodes_manageable).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.manage\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.manage", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "node_uuids, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "b7ef081d-3e01-4576-ba63-bdaa0a7eae11"}, {"definition": "provide_manageable_nodes:\n  description: Provide all nodes in a 'manageable' state.\n\n  input:\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    get_manageable_nodes:\n      action: ironic.node_list maintenance=False associated=False\n      on-success: provide_manageable\n      on-error: set_status_failed_get_manageable_nodes\n      publish:\n        managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>\n\n    set_status_failed_get_manageable_nodes:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(get_manageable_nodes).result %>\n\n    provide_manageable:\n      on-success: send_message\n      workflow: tripleo.baremetal.v1.provide\n      input:\n        node_uuids: <% $.managed_nodes %>\n        queue_name: <% $.queue_name %>\n      publish:\n        status: SUCCESS\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.provide_manageable_nodes\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.provide_manageable_nodes", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "bf8cc86d-df72-4b06-93cc-01ab9e6ac3b3"}, {"definition": "set_power_state:\n  input:\n    - node_uuid\n    - state_action\n    - target_state\n    - error_state: 'error'\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    set_power_state:\n      on-success: wait_for_power_state\n      on-error: set_power_state_failed\n      action: ironic.node_set_power_state node_id=<% $.node_uuid %> state=<% $.state_action %>\n\n    set_power_state_failed:\n      publish:\n        message: <% task(set_power_state).result %>\n      on-complete: fail\n\n    wait_for_power_state:\n      action: ironic.node_get\n      input:\n        node_id: <% $.node_uuid %>\n        fields: ['power_state', 'last_error']\n      timeout: 120 #2 minutes\n      retry:\n        delay: 6\n        count: 20\n        continue-on: <% not task().result.power_state in [$.target_state, $.error_state] %>\n      on-complete:\n        - state_not_reached: <% task().result.power_state != $.target_state %>\n\n    state_not_reached:\n      publish:\n        message: >-\n          Node <% $.node_uuid %> did not reach power state \"<% $.target_state %>\",\n          the state is \"<% task(wait_for_power_state).result.power_state %>\",\n          error: <% task(wait_for_power_state).result.last_error %>\n      on-complete: fail\n\n  output-on-error:\n    result: <% $.message %>\n", "name": "tripleo.baremetal.v1.set_power_state", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "node_uuid, state_action, target_state, error_state=error", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "d92a27e5-098e-4bef-bb26-16873179502a"}, {"definition": "discover_and_enroll_nodes:\n  description: Run nodes discovery over the given IP range and enroll nodes\n\n  input:\n    - ip_addresses\n    - credentials\n    - ports: [623]\n    - kernel_name: null\n    - ramdisk_name: null\n    - instance_boot_option: local\n    - initial_state: manageable\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    discover_nodes:\n      workflow: tripleo.baremetal.v1.discover_nodes\n      input:\n        ip_addresses: <% $.ip_addresses %>\n        ports: <% $.ports %>\n        credentials: <% $.credentials %>\n        queue_name: <% $.queue_name %>\n      on-success: enroll_nodes\n      on-error: discover_nodes_failed\n      publish:\n        nodes_json: <% task().result.nodes_json %>\n\n    discover_nodes_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(discover_nodes).result %>\n\n    enroll_nodes:\n      workflow: tripleo.baremetal.v1.register_or_update\n      input:\n        nodes_json: <% $.nodes_json %>\n        kernel_name: <% $.kernel_name %>\n        ramdisk_name: <% $.ramdisk_name %>\n        instance_boot_option: <% $.instance_boot_option %>\n        initial_state: <% $.initial_state %>\n      on-success: send_message\n      on-error: enroll_nodes_failed\n      publish:\n        registered_nodes: <% task().result.registered_nodes %>\n\n    enroll_nodes_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(enroll_nodes).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.discover_and_enroll_nodes\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              registered_nodes: <% $.get('registered_nodes', []) %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.discover_and_enroll_nodes", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "ip_addresses, credentials, ports=[623], kernel_name=None, ramdisk_name=None, instance_boot_option=local, initial_state=manageable, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "dcc11fe9-4fe4-4f55-b732-f5aaf86e276f"}, {"definition": "nodes_with_profile:\n  description: Find nodes with a specific profile\n  input:\n    - profile\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_active_nodes:\n      action: ironic.node_list maintenance=false provision_state='active' detail=true\n      on-success: get_available_nodes\n      on-error: set_status_failed_get_active_nodes\n\n    get_available_nodes:\n      action: ironic.node_list maintenance=false provision_state='available' detail=true\n      on-success: get_matching_nodes\n      on-error: set_status_failed_get_available_nodes\n\n    get_matching_nodes:\n      with-items: node in <% task(get_available_nodes).result + task(get_active_nodes).result %>\n      action: tripleo.baremetal.get_profile node=<% $.node %>\n      on-success: send_message\n      on-error: set_status_failed_get_matching_nodes\n      publish:\n        matching_nodes: <% let(input_profile_name => $.profile) -> task().result.where($.profile = $input_profile_name).uuid %>\n\n    set_status_failed_get_active_nodes:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(get_active_nodes).result %>\n\n    set_status_failed_get_available_nodes:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(get_available_nodes).result %>\n\n    set_status_failed_get_matching_nodes:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(get_matching_nodes).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.nodes_with_profile\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              matching_nodes: <% $.matching_nodes or [] %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.nodes_with_profile", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "profile, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "ea15b645-a837-4a08-9ff0-0f365a096cb0"}, {"definition": "manual_cleaning:\n  input:\n    - node_uuid\n    - clean_steps\n    - timeout: 7200  # 2 hours (cleaning can take really long)\n    - retry_delay: 10\n    - retry_count: 720\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    set_provision_state:\n      on-success: wait_for_provision_state\n      on-error: set_provision_state_failed\n      action: ironic.node_set_provision_state node_uuid=<% $.node_uuid %> state='clean' cleansteps=<% $.clean_steps %>\n\n    set_provision_state_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(set_provision_state).result %>\n\n    wait_for_provision_state:\n      on-success: send_message\n      action: ironic.node_get node_id=<% $.node_uuid %>\n      timeout: <% $.timeout %>\n      retry:\n        delay: <% $.retry_delay %>\n        count: <% $.retry_count %>\n        continue-on: <% task().result.provision_state != 'manageable' %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1.manual_cleaning\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1.manual_cleaning", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "node_uuid, clean_steps, timeout=7200, retry_delay=10, retry_count=720, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "f792232b-d634-4793-aaac-80ca1f6b1e53"}, {"definition": "_introspect:\n  description: >\n      An internal workflow. The tripleo.baremetal.v1.introspect workflow\n      should be used for introspection.\n\n  input:\n    - node_uuid\n    - timeout\n    - queue_name\n\n  output:\n    result: <% task(start_introspection).result %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    start_introspection:\n      action: baremetal_introspection.introspect uuid=<% $.node_uuid %>\n      on-success: wait_for_introspection_to_finish\n      on-error: set_status_failed_start_introspection\n\n    set_status_failed_start_introspection:\n      publish:\n        status: FAILED\n        message: <% task(start_introspection).result %>\n        introspected_nodes: []\n      on-success: send_message\n\n    wait_for_introspection_to_finish:\n      action: baremetal_introspection.wait_for_finish\n      input:\n        uuids: <% [$.node_uuid] %>\n        # The interval is 10 seconds, so divide to make the overall timeout\n        # in seconds correct.\n        max_retries: <% $.timeout / 10 %>\n        retry_interval: 10\n      publish:\n        introspected_node: <% task().result.values().first() %>\n        status: <% bool(task().result.values().first().error) and \"FAILED\" or \"SUCCESS\" %>\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-success: wait_for_introspection_to_finish_success\n      on-error: wait_for_introspection_to_finish_error\n\n    wait_for_introspection_to_finish_success:\n      publish:\n        message: <% \"Introspection of node {0} completed. Status:{1}. Errors:{2}\".format($.introspected_node.uuid, $.status, $.introspected_node.error) %>\n      on-success: send_message\n\n    wait_for_introspection_to_finish_error:\n      publish:\n        message: <% \"Introspection of node {0} timed out.\".format($.node_uuid) %>\n      on-success: send_message\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.baremetal.v1._introspect\n            payload:\n              status: <% $.status %>\n              message: <% $.message %>\n              introspected_node: <% $.get('introspected_node') %>\n              node_uuid: <% $.node_uuid %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1._introspect", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:54", "namespace": "", "updated_at": null, "scope": "private", "input": "node_uuid, timeout, queue_name", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "fd4b8118-ce62-4aa2-ba73-c88e01b6aef5"}, {"definition": "ceph-install:\n  # allows for additional extra_vars via workflow input\n  input:\n    - ansible_playbook_verbosity: 0\n    - ansible_skip_tags: 'package-install,with_pkg'\n    - ansible_env_variables: {}\n    - ansible_extra_env_variables:\n        ANSIBLE_CONFIG: /usr/share/ceph-ansible/ansible.cfg\n        ANSIBLE_ACTION_PLUGINS: /usr/share/ceph-ansible/plugins/actions/\n        ANSIBLE_ROLES_PATH: /usr/share/ceph-ansible/roles/\n        ANSIBLE_RETRY_FILES_ENABLED: 'False'\n        ANSIBLE_LOG_PATH: /var/log/mistral/ceph-install-workflow.log\n        ANSIBLE_LIBRARY: /usr/share/ceph-ansible/library/\n        ANSIBLE_SSH_RETRIES: '3'\n        ANSIBLE_HOST_KEY_CHECKING: 'False'\n        DEFAULT_FORKS: '25'\n    - ceph_ansible_extra_vars: {}\n    - ceph_ansible_playbook: /usr/share/ceph-ansible/site-docker.yml.sample\n    - node_data_lookup: '{}'\n    - swift_container: 'ceph_ansible_fetch_dir'\n  tags:\n    - tripleo-common-managed\n  tasks:\n    set_swift_container:\n      publish:\n        swift_container: <% concat(env().get('heat_stack_name', ''), '_', $.swift_container) %>\n      on-complete: collect_puppet_hieradata\n    collect_puppet_hieradata:\n      on-success: check_hieradata\n      publish:\n        hieradata: <% env().get('role_merged_configs', {}).values().select($.keys()).flatten().select(regex('^ceph::profile::params::osds$').search($)).where($ != null).toSet() %>\n    check_hieradata:\n      on-success:\n      - set_blacklisted_ips: <% not bool($.hieradata) %>\n      - fail(msg=<% 'Ceph deployment stopped, puppet-ceph hieradata found. Convert it into ceph-ansible variables. {0}'.format($.hieradata) %>): <% bool($.hieradata) %>\n    set_blacklisted_ips:\n      publish:\n        blacklisted_ips: <% env().get('blacklisted_ip_addresses', []) %>\n      on-success: set_ip_lists\n    set_ip_lists:\n      publish:\n        mgr_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mgr_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n        mon_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mon_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n        osd_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_osd_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n        mds_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mds_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n        rgw_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_rgw_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n        nfs_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_nfs_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n        rbdmirror_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_rbdmirror_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n        client_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_client_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n      on-success: merge_ip_lists\n    merge_ip_lists:\n      publish:\n        ips_list: <% ($.mgr_ips + $.mon_ips + $.osd_ips + $.mds_ips + $.rgw_ips + $.nfs_ips + $.rbdmirror_ips + $.client_ips).toSet() %>\n      on-success: enable_ssh_admin\n    enable_ssh_admin:\n      workflow: tripleo.access.v1.enable_ssh_admin\n      input:\n        ssh_servers: <% $.ips_list %>\n      on-success: get_private_key\n    get_private_key:\n      action: tripleo.validations.get_privkey\n      publish:\n        private_key: <% task().result %>\n      on-success: make_fetch_directory\n    make_fetch_directory:\n      action: tripleo.files.make_temp_dir\n      publish:\n        fetch_directory: <% task().result.path %>\n      on-success: verify_container_exists\n    verify_container_exists:\n      action: swift.head_container container=<% $.swift_container %>\n      on-success: restore_fetch_directory\n      on-error: create_container\n    restore_fetch_directory:\n      action: tripleo.files.restore_temp_dir_from_swift\n      input:\n        container: <% $.swift_container %>\n        path: <% $.fetch_directory %>\n      on-success: collect_nodes_uuid\n    create_container:\n      action: swift.put_container container=<% $.swift_container %>\n      on-success: collect_nodes_uuid\n    collect_nodes_uuid:\n      action: tripleo.ansible-playbook\n      input:\n        inventory:\n          overcloud:\n            hosts: <% $.ips_list.toDict($, {}) %>\n        remote_user: tripleo-admin\n        become: true\n        become_user: root\n        verbosity: 0\n        ssh_private_key: <% $.private_key %>\n        #NOTE(gfidente): set ANSIBLE_CALLBACK_WHITELIST to empty string to avoid spurious output\n        #in the json output. The publish: directive will in fact parse the output.\n        extra_env_variables:\n          ANSIBLE_CALLBACK_WHITELIST: ''\n          ANSIBLE_HOST_KEY_CHECKING: 'False'\n          ANSIBLE_STDOUT_CALLBACK: 'json'\n        playbook:\n          - hosts: overcloud\n            gather_facts: no\n            tasks:\n              - name: collect machine id\n                command: dmidecode -s system-uuid\n                register: dmidecode\n                # NOTE(tonyb): 0 == no error, 1 == -EPERM or bad data and 2 == Command not found\n                # 1 and 2 aren't great but shouldn't cause the deploy to fail.  If we're using\n                # the node specific data we'll fail then.  If we aren't then lets keep moving\n                failed_when: dmidecode.rc not in [0, 1, 2]\n      publish:\n        ansible_output: <% json_parse(task().result.stderr) %>\n      on-success: set_ip_uuids\n    set_ip_uuids:\n      publish:\n        ip_uuids: <% let(root => $.ansible_output.get('plays')[0].get('tasks')[0].get('hosts')) -> $.ips_list.toDict($, $root.get($).get('stdout')) %>\n      on-success: parse_node_data_lookup\n    parse_node_data_lookup:\n      publish:\n        json_node_data_lookup: <% json_parse($.node_data_lookup) %>\n      on-error:\n        - fail(msg=<% 'Ceph deployment stopped, NodeDataLookup (node_data_lookup) is not valid JSON. {0}'.format($.node_data_lookup) %>)\n      on-success: map_node_data_lookup\n    map_node_data_lookup:\n      publish:\n        ips_data: <% let(uuids => $.ip_uuids, root => $) -> $.ips_list.toDict($, $root.json_node_data_lookup.get($uuids.get($, \"NO-UUID-FOUND\"), {})) %>\n      on-success: set_role_vars\n    set_role_vars:\n      publish:\n        # NOTE(gfidente): collect role settings from all tht roles\n        mgr_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mgr_ansible_vars', {})).aggregate($1 + $2) %>\n        mon_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mon_ansible_vars', {})).aggregate($1 + $2) %>\n        osd_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_osd_ansible_vars', {})).aggregate($1 + $2) %>\n        mds_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mds_ansible_vars', {})).aggregate($1 + $2) %>\n        rgw_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_rgw_ansible_vars', {})).aggregate($1 + $2) %>\n        nfs_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_nfs_ansible_vars', {})).aggregate($1 + $2) %>\n        rbdmirror_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_rbdmirror_ansible_vars', {})).aggregate($1 + $2) %>\n        client_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_client_ansible_vars', {})).aggregate($1 + $2) %>\n      on-success: build_extra_vars\n    build_extra_vars:\n      publish:\n        # NOTE(gfidente): merge vars from all ansible roles\n        extra_vars: <% {'fetch_directory'=> $.fetch_directory} + $.mgr_vars + $.mon_vars + $.osd_vars + $.mds_vars + $.rgw_vars + $.nfs_vars + $.client_vars + $.rbdmirror_vars + $.ceph_ansible_extra_vars %>\n      on-success: ceph_install\n    ceph_install:\n      with-items: playbook in <% list($.ceph_ansible_playbook).flatten() %>\n      concurrency: 1\n      action: tripleo.ansible-playbook\n      input:\n        trash_output: true\n        inventory:\n          mgrs:\n            hosts: <% let(root => $) -> $.mgr_ips.toDict($, $root.ips_data.get($, {})) %>\n          mons:\n            hosts: <% let(root => $) -> $.mon_ips.toDict($, $root.ips_data.get($, {})) %>\n          osds:\n            hosts: <% let(root => $) -> $.osd_ips.toDict($, $root.ips_data.get($, {})) %>\n          mdss:\n            hosts: <% let(root => $) -> $.mds_ips.toDict($, $root.ips_data.get($, {})) %>\n          rgws:\n            hosts: <% let(root => $) -> $.rgw_ips.toDict($, $root.ips_data.get($, {})) %>\n          nfss:\n            hosts: <% let(root => $) -> $.nfs_ips.toDict($, $root.ips_data.get($, {})) %>\n          rbdmirrors:\n            hosts: <% let(root => $) -> $.rbdmirror_ips.toDict($, $root.ips_data.get($, {})) %>\n          clients:\n            hosts: <% let(root => $) -> $.client_ips.toDict($, $root.ips_data.get($, {})) %>\n          all:\n            vars: <% $.extra_vars %>\n        playbook: <% $.playbook %>\n        remote_user: tripleo-admin\n        become: true\n        become_user: root\n        verbosity: <% $.ansible_playbook_verbosity %>\n        ssh_private_key: <% $.private_key %>\n        skip_tags: <% $.ansible_skip_tags %>\n        extra_env_variables: <% $.ansible_extra_env_variables.mergeWith($.ansible_env_variables) %>\n        extra_vars:\n          ireallymeanit: 'yes'\n      publish:\n        output: <% task().result %>\n      on-complete: save_fetch_directory\n    save_fetch_directory:\n      action: tripleo.files.save_temp_dir_to_swift\n      input:\n        container: <% $.swift_container %>\n        path: <% $.fetch_directory %>\n      on-success: purge_fetch_directory\n    purge_fetch_directory:\n      action: tripleo.files.remove_temp_dir path=<% $.fetch_directory %>\n", "name": "tripleo.storage.v1.ceph-install", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:55", "namespace": "", "updated_at": null, "scope": "private", "input": "ansible_playbook_verbosity=0, ansible_skip_tags=package-install,with_pkg, ansible_env_variables={}, ansible_extra_env_variables={u'ANSIBLE_LIBRARY': u'/usr/share/ceph-ansible/library/', u'ANSIBLE_RETRY_FILES_ENABLED': u'False', u'ANSIBLE_CONFIG': u'/usr/share/ceph-ansible/ansible.cfg', u'ANSIBLE_LOG_PATH': u'/var/log/mistral/ceph-install-workflow.log', u'DEFAULT_FORKS': u'25', u'ANSIBLE_ROLES_PATH': u'/usr/share/ceph-ansible/roles/', u'ANSIBLE_ACTION_PLUGINS': u'/usr/share/ceph-ansible/plugins/actions/', u'ANSIBLE_SSH_RETRIES': u'3', u'ANSIBLE_HOST_KEY_CHECKING': u'False'}, ceph_ansible_extra_vars={}, ceph_ansible_playbook=/usr/share/ceph-ansible/site-docker.yml.sample, node_data_lookup={}, swift_container=ceph_ansible_fetch_dir", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "440a838d-d1f3-4aef-b4e4-a5d9acb59a3d"}, {"definition": "deploy_plan:\n\n  description: >\n    Deploy the overcloud for a plan.\n\n  input:\n    - container\n    - run_validations: False\n    - timeout: 240\n    - skip_deploy_identifier: False\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    add_validation_ssh_key:\n      workflow: tripleo.validations.v1.add_validation_ssh_key_parameter\n      input:\n        container: <% $.container %>\n        queue_name: <% $.queue_name %>\n      on-complete:\n        - run_validations: <% $.run_validations %>\n        - create_swift_rings_backup_plan: <% not $.run_validations %>\n\n    run_validations:\n      workflow: tripleo.validations.v1.run_groups\n      input:\n        group_names:\n          - 'pre-deployment'\n        plan: <% $.container %>\n        queue_name: <% $.queue_name %>\n      on-success: create_swift_rings_backup_plan\n      on-error: set_validations_failed\n\n    set_validations_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(run_validations).result %>\n\n    create_swift_rings_backup_plan:\n      workflow: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan\n      on-success: cell_v2_discover_hosts\n      on-error: create_swift_rings_backup_plan_set_status_failed\n      input:\n        container: <% $.container %>\n        queue_name: <% $.queue_name %>\n        use_default_templates: true\n\n    cell_v2_discover_hosts:\n      on-success: deploy\n      on-error: cell_v2_discover_hosts_failed\n      action: tripleo.baremetal.cell_v2_discover_hosts\n\n    cell_v2_discover_hosts_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(cell_v2_discover_hosts).result %>\n\n    deploy:\n      action: tripleo.deployment.deploy\n      input:\n        timeout: <% $.timeout %>\n        container: <% $.container %>\n        skip_deploy_identifier: <% $.skip_deploy_identifier %>\n      on-success: send_message\n      on-error: set_deployment_failed\n\n    create_swift_rings_backup_plan_set_status_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(create_swift_rings_backup_plan).result %>\n\n    set_deployment_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(deploy).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.deployment.v1.deploy_plan\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.deployment.v1.deploy_plan", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:56", "namespace": "", "updated_at": null, "scope": "private", "input": "container, run_validations=False, timeout=240, skip_deploy_identifier=False, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "389f64f3-df63-46c3-97fa-e7a2d0be0b9d"}, {"definition": "get_horizon_url:\n\n  description: >\n    Retrieve the Horizon URL from the Overcloud stack.\n\n  input:\n    - stack: overcloud\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  output:\n    horizon_url: <% $.horizon_url %>\n\n  tasks:\n    get_horizon_url:\n      action: heat.stacks_get\n      input:\n        stack_id: <% $.stack %>\n      publish:\n        horizon_url: <% task().result.outputs.where($.output_key = \"EndpointMap\").output_value.HorizonPublic.uri.single() %>\n      on-success: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.deployment.v1.get_horizon_url\n            payload:\n              horizon_url: <% $.get('horizon_url', '') %>\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.deployment.v1.get_horizon_url", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:56", "namespace": "", "updated_at": null, "scope": "private", "input": "stack=overcloud, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "70871175-4864-4712-9389-617e31a1b526"}, {"definition": "deploy_on_servers:\n\n  input:\n    - server_name\n    - config_name\n    - config\n    - group: script\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    check_if_all_servers:\n      on-success:\n        - get_servers_matching: <% $.server_name != \"all\" %>\n        - get_all_servers: <% $.server_name = \"all\" %>\n\n    get_servers_matching:\n      action: nova.servers_list\n      on-success: deploy_on_servers\n      publish:\n        servers_with_name: <% task().result._info.where($.name.indexOf(execution().input.server_name) > -1) %>\n\n    get_all_servers:\n      action: nova.servers_list\n      on-success: deploy_on_servers\n      publish:\n        servers_with_name: <% task().result._info %>\n\n    deploy_on_servers:\n      on-success: send_success_message\n      on-error: send_failed_message\n      with-items: server in <% $.servers_with_name %>\n      workflow: tripleo.deployment.v1.deploy_on_server\n      input:\n        server_name: <% $.server.name %>\n        server_uuid: <% $.server.id %>\n        config: <% $.config %>\n        config_name: <% $.config_name %>\n        group: <% $.group %>\n        queue_name: <% $.queue_name %>\n\n    send_success_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.deployment.v1.deploy_on_servers\n            payload:\n              status: SUCCESS\n              execution: <% execution() %>\n\n    send_failed_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.deployment.v1.deploy_on_servers\n            payload:\n              status: FAILED\n              message: <% task(deploy_on_servers).result %>\n              execution: <% execution() %>\n      on-success: fail\n", "name": "tripleo.deployment.v1.deploy_on_servers", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:56", "namespace": "", "updated_at": null, "scope": "private", "input": "server_name, config_name, config, group=script, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "8ac25907-28b6-43a2-9536-5ce35b163bfc"}, {"definition": "config_download_deploy:\n\n  description: >\n    Configure the overcloud with config-download.\n\n  input:\n    - timeout: 240\n    - queue_name: tripleo\n    - plan_name: overcloud\n    - work_dir: /var/lib/mistral\n    - verbosity: 1\n    - blacklist: []\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    get_blacklisted_hostnames:\n      action: heat.stacks_output_show\n      input:\n        stack_id: <% $.plan_name %>\n        output_key: BlacklistedHostnames\n      publish:\n        blacklisted_hostnames: <% task().result.output.output_value.where($ != \"\") %>\n      on-success: get_config\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    get_config:\n      action: tripleo.config.get_overcloud_config\n      input:\n        container: <% $.get('plan_name') %>\n      on-success: download_config\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    download_config:\n      action: tripleo.config.download_config\n      input:\n        work_dir: <% $.get('work_dir') %>/<% execution().id %>\n      on-success: send_msg_config_download\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    send_msg_config_download:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.deployment.v1.config_download\n            payload:\n              status: <% $.get('status', 'RUNNING') %>\n              message: Config downloaded at <% $.get('work_dir') %>/<% execution().id %>\n              execution: <% execution() %>\n      on-success: get_private_key\n\n    get_private_key:\n      action: tripleo.validations.get_privkey\n      publish:\n        private_key: <% task().result %>\n      on-success: generate_inventory\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    generate_inventory:\n      action: tripleo.ansible-generate-inventory\n      input:\n        ansible_ssh_user: tripleo-admin\n        work_dir: <% $.get('work_dir') %>/<% execution().id %>\n        plan_name: <% $.get('plan_name') %>\n      publish:\n        inventory: <% task().result %>\n      on-success: send_msg_generate_inventory\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    send_msg_generate_inventory:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.deployment.v1.config_download\n            payload:\n              status: <% $.get('status', 'RUNNING') %>\n              message: Inventory generated at <% $.get('inventory') %>\n              execution: <% execution() %>\n      on-success: send_msg_run_ansible\n\n    send_msg_run_ansible:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.deployment.v1.config_download\n            payload:\n              status: <% $.get('status', 'RUNNING') %>\n              message: >\n                Running ansible playbook at <% $.get('work_dir') %>/<% execution().id %>/deploy_steps_playbook.yaml.\n                See log file at <% $.get('work_dir') %>/<% execution().id %>/ansible.log for progress.\n                ...\n              execution: <% execution() %>\n      on-success: run_ansible\n\n    run_ansible:\n      action: tripleo.ansible-playbook\n      input:\n        inventory: <% $.inventory %>\n        playbook: <% $.get('work_dir') %>/<% execution().id %>/deploy_steps_playbook.yaml\n        remote_user: tripleo-admin\n        ssh_extra_args: '-o StrictHostKeyChecking=no'\n        ssh_private_key: <% $.private_key %>\n        use_openstack_credentials: true\n        verbosity: <% $.get('verbosity') %>\n        become: true\n        timeout: <% $.timeout %>\n        work_dir: <% $.get('work_dir') %>/<% execution().id %>\n        queue_name: <% $.queue_name %>\n        reproduce_command: true\n        trash_output: true\n        blacklisted_hostnames: <% $.blacklisted_hostnames %>\n      publish:\n        log_path: <% task(run_ansible).result.get('log_path') %>\n      on-success:\n        - ansible_passed: <% task().result.returncode = 0 %>\n        - ansible_failed: <% task().result.returncode != 0 %>\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/ansible.log.\n\n    ansible_passed:\n      on-success: send_message\n      publish:\n        status: SUCCESS\n        message: Ansible passed.\n\n    ansible_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/ansible.log.\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.deployment.v1.config_download\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.deployment.v1.config_download_deploy", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:56", "namespace": "", "updated_at": null, "scope": "private", "input": "timeout=240, queue_name=tripleo, plan_name=overcloud, work_dir=/var/lib/mistral, verbosity=1, blacklist=[]", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "a37e22b1-1068-4c80-bf31-5bb78d37651e"}, {"definition": "deploy_on_server:\n\n  input:\n    - server_uuid\n    - server_name\n    - config\n    - config_name\n    - group\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    deploy_config:\n      action: tripleo.deployment.config\n      on-complete: send_message\n      input:\n        server_id: <% $.server_uuid %>\n        name: <% $.config_name %>\n        config: <% $.config %>\n        group: <% $.group %>\n      publish:\n        stdout: <% task().result.deploy_stdout %>\n        stderr: <% task().result.deploy_stderr %>\n        status_code: <% task().result.deploy_status_code %>\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.deployment.v1.deploy_on_server\n            payload:\n              status: <% $.get(\"status\", \"SUCCESS\") %>\n              message: <% $.get(\"message\", \"\") %>\n              server_uuid: <% $.server_uuid %>\n              server_name: <% $.server_name %>\n              config_name: <% $.config_name %>\n              status_code: <% $.get(\"status_code\", \"\") %>\n              stdout: <% $.get(\"stdout\", \"\") %>\n              stderr: <% $.get(\"stderr\", \"\") %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.deployment.v1.deploy_on_server", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:56", "namespace": "", "updated_at": null, "scope": "private", "input": "server_uuid, server_name, config, config_name, group, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "dd723b47-78e3-490f-9fee-a5ad5f675e54"}, {"definition": "_get_role_info:\n  description: >\n    Workflow that determines the list of derived parameter features (DPDK,\n    HCI, etc.) for a role based on the services assigned to the role.\n\n  input:\n    - role_name\n    - heat_resource_tree\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_resource_chains:\n      publish:\n        resource_chains: <% $.heat_resource_tree.resources.values().where($.get('type', '') = 'OS::Heat::ResourceChain') %>\n      on-success:\n        - get_role_chain: <% $.resource_chains %>\n        - set_status_failed_get_resource_chains: <% not $.resource_chains %>\n\n    get_role_chain:\n      publish:\n        role_chain: <% let(chain_name => concat($.role_name, 'ServiceChain'))-> $.heat_resource_tree.resources.values().where($.name = $chain_name).first({}) %>\n      on-success:\n        - get_service_chain: <% $.role_chain %>\n        - set_status_failed_get_role_chain: <% not $.role_chain %>\n\n    get_service_chain:\n      publish:\n        service_chain: <% let(resources => $.role_chain.resources)-> $.resource_chains.where($resources.contains($.id)).first('') %>\n      on-success:\n        - get_role_services: <% $.service_chain %>\n        - set_status_failed_get_service_chain: <% not $.service_chain %>\n\n    get_role_services:\n      publish:\n        role_services: <% let(resources => $.heat_resource_tree.resources)-> $.service_chain.resources.select($resources.get($)) %>\n      on-success:\n        - check_features: <% $.role_services %>\n        - set_status_failed_get_role_services: <% not $.role_services %>\n\n    check_features:\n      on-success: build_feature_dict\n      publish:\n        # The role supports the DPDK feature if the NeutronDatapathType parameter is present\n        dpdk: <% let(resources => $.heat_resource_tree.resources) -> $.role_services.any($.get('parameters', []).contains('NeutronDatapathType') or $.get('resources', []).select($resources.get($)).any($.get('parameters', []).contains('NeutronDatapathType'))) %>\n\n        # The role supports the DPDK feature in ODL if the OvsEnableDpdk parameter value is true in role parameters.\n        odl_dpdk: <% let(role => $.role_name) -> $.heat_resource_tree.parameters.get(concat($role, 'Parameters'), {}).get('default', {}).get('OvsEnableDpdk', false) %>\n\n        # The role supports the SRIOV feature if it includes NeutronSriovAgent services.\n        sriov: <% $.role_services.any($.get('type', '').endsWith('::NeutronSriovAgent')) %>\n\n        # The role supports the HCI feature if it includes both NovaCompute and CephOSD services.\n        hci: <% $.role_services.any($.get('type', '').endsWith('::NovaCompute')) and $.role_services.any($.get('type', '').endsWith('::CephOSD')) %>\n\n    build_feature_dict:\n      on-success: filter_features\n      publish:\n        feature_dict: <% dict(DPDK => ($.dpdk or $.odl_dpdk), SRIOV => $.sriov, HOST => ($.dpdk or $.odl_dpdk or $.sriov), HCI => $.hci) %>\n\n    filter_features:\n      publish:\n        # The list of features that are enabled (i.e. are true in the feature_dict).\n        role_features: <% let(feature_dict => $.feature_dict)-> $feature_dict.keys().where($feature_dict[$]) %>\n\n    set_status_failed_get_resource_chains:\n      publish:\n        message: <% 'Unable to locate any resource chains in the heat resource tree' %>\n      on-success: fail\n\n    set_status_failed_get_role_chain:\n      publish:\n        message: <% \"Unable to determine the service chain resource for role '{0}'\".format($.role_name) %>\n      on-success: fail\n\n    set_status_failed_get_service_chain:\n      publish:\n        message: <% \"Unable to determine the service chain for role '{0}'\".format($.role_name) %>\n      on-success: fail\n\n    set_status_failed_get_role_services:\n      publish:\n        message: <% \"Unable to determine list of services for role '{0}'\".format($.role_name) %>\n      on-success: fail\n", "name": "tripleo.derive_params.v1._get_role_info", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:58", "namespace": "", "updated_at": null, "scope": "private", "input": "role_name, heat_resource_tree", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "146c7a09-4497-445e-9a9e-be55d31ca546"}, {"definition": "derive_parameters:\n  description: The main workflow for deriving parameters from the introspected data\n\n  input:\n    - plan: overcloud\n    - queue_name: tripleo\n    - user_inputs: {}\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_flattened_parameters:\n      action: tripleo.parameters.get_flatten container=<% $.plan %>\n      publish:\n        environment_parameters: <% task().result.environment_parameters %>\n        heat_resource_tree: <% task().result.heat_resource_tree %>\n      on-success:\n        - get_roles: <% $.environment_parameters and $.heat_resource_tree %>\n        - set_status_failed_get_flattened_parameters: <% (not $.environment_parameters) or (not $.heat_resource_tree) %>\n      on-error: set_status_failed_get_flattened_parameters\n\n    get_roles:\n      action: tripleo.role.list container=<% $.plan %>\n      publish:\n        role_name_list: <% task().result %>\n      on-success:\n        - get_valid_roles: <% $.role_name_list %>\n        - set_status_failed_get_roles: <% not $.role_name_list %>\n      on-error: set_status_failed_on_error_get_roles\n\n    # Obtain only the roles which has count > 0, by checking <RoleName>Count parameter, like ComputeCount\n    get_valid_roles:\n      publish:\n        valid_role_name_list: <% let(hr => $.heat_resource_tree.parameters) -> $.role_name_list.where(int($hr.get(concat($, 'Count'), {}).get('default', 0)) > 0) %>\n      on-success:\n        - for_each_role: <% $.valid_role_name_list %>\n        - set_status_failed_get_valid_roles: <% not $.valid_role_name_list %>\n\n    # Execute the basic preparation workflow for each role to get introspection data\n    for_each_role:\n      with-items: role_name in <% $.valid_role_name_list %>\n      concurrency: 1\n      workflow: _derive_parameters_per_role\n      input:\n        plan: <% $.plan %>\n        role_name: <% $.role_name %>\n        environment_parameters: <% $.environment_parameters %>\n        heat_resource_tree: <% $.heat_resource_tree %>\n        user_inputs: <% $.user_inputs %>\n      publish:\n        # Gets all the roles derived parameters as dictionary\n        result: <% task().result.select($.get('derived_parameters', {})).sum() %>\n      on-success: reset_derive_parameters_in_plan\n      on-error: set_status_failed_for_each_role\n\n    reset_derive_parameters_in_plan:\n      action: tripleo.parameters.reset\n      input:\n        container: <% $.plan %>\n        key: 'derived_parameters'\n      on-success:\n        # Add the derived parameters to the deployment plan only when $.result\n        # (the derived parameters) is non-empty. Otherwise, we're done.\n        - update_derive_parameters_in_plan: <% $.result %>\n        - send_message: <% not $.result %>\n      on-error: set_status_failed_reset_derive_parameters_in_plan\n\n    update_derive_parameters_in_plan:\n      action: tripleo.parameters.update\n      input:\n        container: <% $.plan %>\n        key: 'derived_parameters'\n        parameters: <% $.get('result', {}) %>\n      on-success: send_message\n      on-error: set_status_failed_update_derive_parameters_in_plan\n\n    set_status_failed_get_flattened_parameters:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(get_flattened_parameters).result %>\n\n    set_status_failed_get_roles:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: \"Unable to determine the list of roles in the deployment plan\"\n\n    set_status_failed_on_error_get_roles:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(get_roles).result %>\n\n    set_status_failed_get_valid_roles:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: 'Unable to determine the list of valid roles in the deployment plan.'\n\n    set_status_failed_for_each_role:\n      on-success: update_message_format\n      publish:\n        status: FAILED\n        # gets the status and message for all roles from task result.\n        message: <% task(for_each_role).result.select(dict('role_name' => $.role_name, 'status' => $.get('status', 'SUCCESS'), 'message' => $.get('message', ''))) %>\n\n    update_message_format:\n      on-success: send_message\n      publish:\n        # updates the message format(Role 'role name': message) for each roles which are failed and joins the message list as string with ', ' separator.\n        message: <% $.message.where($.get('status', 'SUCCESS') != 'SUCCESS').select(concat(\"Role '{}':\".format($.role_name), \" \", $.get('message', '(error unknown)'))).join(', ') %>\n\n    set_status_failed_reset_derive_parameters_in_plan:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(reset_derive_parameters_in_plan).result %>\n\n    set_status_failed_update_derive_parameters_in_plan:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(update_derive_parameters_in_plan).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.derive_params.v1.derive_parameters\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              result: <% $.get('result', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = 'FAILED' %>\n", "name": "tripleo.derive_params.v1.derive_parameters", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:58", "namespace": "", "updated_at": null, "scope": "private", "input": "plan=overcloud, queue_name=tripleo, user_inputs={}", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "f9643d7a-bc43-4627-ba22-dd27a64316d1"}, {"definition": "_derive_parameters_per_role:\n  description: >\n    Workflow which runs per role to get the introspection data on the first matching node assigned to role.\n    Once introspection data is fetched, this worklow will trigger the actual derive parameters workflow\n  input:\n    - plan\n    - role_name\n    - environment_parameters\n    - heat_resource_tree\n    - user_inputs\n\n  output:\n    derived_parameters: <% $.get('derived_parameters', {}) %>\n    # Need role_name in output parameter to display the status for all roles in main workflow when any role fails here.\n    role_name: <% $.role_name %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_role_info:\n      workflow: _get_role_info\n      input:\n        role_name: <% $.role_name %>\n        heat_resource_tree: <% $.heat_resource_tree %>\n      publish:\n        role_features: <% task().result.get('role_features', []) %>\n        role_services: <% task().result.get('role_services', []) %>\n      on-success:\n        # Continue only if there are features associated with this role. Otherwise, we're done.\n        - get_scheduler_hints: <% $.role_features %>\n      on-error: set_status_failed_get_role_info\n\n    # Find a node associated with this role. Look for nodes matching any scheduler hints\n    # associated with the role, and if there are no scheduler hints then locate nodes\n    # with a profile matching the role's flavor.\n    get_scheduler_hints:\n      publish:\n        scheduler_hints: <% let(param_name => concat($.role_name, 'SchedulerHints')) -> $.heat_resource_tree.parameters.get($param_name, {}).get('default', {}) %>\n      on-success:\n        - get_hint_regex: <% $.scheduler_hints %>\n        # If there are no scheduler hints then move on to use the flavor\n        - get_flavor_name: <% not $.scheduler_hints %>\n\n    get_hint_regex:\n      publish:\n        hint_regex: <% $.scheduler_hints.get('capabilities:node', '').replace('%index%', '(\\d+)') %>\n      on-success:\n        - get_node_with_hint: <% $.hint_regex %>\n        # If there is no 'capabilities:node' hint then move on to use the flavor\n        - get_flavor_name: <% not $.hint_regex %>\n\n    get_node_with_hint:\n      workflow: tripleo.baremetal.v1.nodes_with_hint\n      input:\n        hint_regex: <% concat('^', $.hint_regex, '$') %>\n      publish:\n        role_node_uuid: <% task().result.matching_nodes.first('') %>\n      on-success:\n        - get_introspection_data: <% $.role_node_uuid %>\n        # If no nodes match the scheduler hint then move on to use the flavor\n        - get_flavor_name: <% not $.role_node_uuid %>\n      on-error: set_status_failed_on_error_get_node_with_hint\n\n    get_flavor_name:\n      publish:\n        flavor_name: <% let(param_name => concat('Overcloud', $.role_name, 'Flavor').replace('OvercloudControllerFlavor', 'OvercloudControlFlavor')) -> $.heat_resource_tree.parameters.get($param_name, {}).get('default', '') %>\n      on-success:\n        - get_profile_name: <% $.flavor_name %>\n        - set_status_failed_get_flavor_name: <% not $.flavor_name %>\n\n    get_profile_name:\n      action: tripleo.parameters.get_profile_of_flavor flavor_name=<% $.flavor_name %>\n      publish:\n        profile_name: <% task().result %>\n      on-success: get_profile_node\n      on-error: set_status_failed_get_profile_name\n\n    get_profile_node:\n      workflow: tripleo.baremetal.v1.nodes_with_profile\n      input:\n        profile: <% $.profile_name %>\n      publish:\n        role_node_uuid: <% task().result.matching_nodes.first('') %>\n      on-success:\n        - get_introspection_data: <% $.role_node_uuid %>\n        - set_status_failed_no_matching_node_get_profile_node: <% not $.role_node_uuid %>\n      on-error: set_status_failed_on_error_get_profile_node\n\n    get_introspection_data:\n      action: baremetal_introspection.get_data uuid=<% $.role_node_uuid %>\n      publish:\n        hw_data: <% task().result %>\n        # Establish an empty dictionary of derived_parameters prior to\n        # invoking the individual \"feature\" algorithms\n        derived_parameters: <% dict() %>\n      on-success: handle_dpdk_feature\n      on-error: set_status_failed_get_introspection_data\n\n    handle_dpdk_feature:\n      on-success:\n        - get_dpdk_derive_params: <% $.role_features.contains('DPDK') %>\n        - handle_sriov_feature: <% not $.role_features.contains('DPDK') %>\n\n    get_dpdk_derive_params:\n      workflow: tripleo.derive_params_formulas.v1.dpdk_derive_params\n      input:\n        plan: <% $.plan %>\n        role_name: <% $.role_name %>\n        hw_data: <% $.hw_data %>\n        user_inputs: <% $.user_inputs %>\n      publish:\n        derived_parameters: <% task().result.get('derived_parameters', {}) %>\n      on-success: handle_sriov_feature\n      on-error: set_status_failed_get_dpdk_derive_params\n\n    handle_sriov_feature:\n      on-success:\n        - get_sriov_derive_params: <% $.role_features.contains('SRIOV') %>\n        - handle_host_feature: <% not $.role_features.contains('SRIOV') %>\n\n    get_sriov_derive_params:\n      workflow: tripleo.derive_params_formulas.v1.sriov_derive_params\n      input:\n        role_name: <% $.role_name %>\n        hw_data: <% $.hw_data %>\n        derived_parameters: <% $.derived_parameters %>\n      publish:\n        derived_parameters: <% task().result.get('derived_parameters', {}) %>\n      on-success: handle_host_feature\n      on-error: set_status_failed_get_sriov_derive_params\n\n    handle_host_feature:\n      on-success:\n        - get_host_derive_params: <% $.role_features.contains('HOST') %>\n        - handle_hci_feature: <% not $.role_features.contains('HOST') %>\n\n    get_host_derive_params:\n      workflow: tripleo.derive_params_formulas.v1.host_derive_params\n      input:\n        role_name: <% $.role_name %>\n        hw_data: <% $.hw_data %>\n        user_inputs: <% $.user_inputs %>\n        derived_parameters: <% $.derived_parameters %>\n      publish:\n        derived_parameters: <% task().result.get('derived_parameters', {}) %>\n      on-success: handle_hci_feature\n      on-error: set_status_failed_get_host_derive_params\n\n    handle_hci_feature:\n      on-success:\n        - get_hci_derive_params: <% $.role_features.contains('HCI') %>\n\n    get_hci_derive_params:\n      workflow: tripleo.derive_params_formulas.v1.hci_derive_params\n      input:\n        role_name: <% $.role_name %>\n        environment_parameters: <% $.environment_parameters %>\n        heat_resource_tree: <% $.heat_resource_tree %>\n        introspection_data: <% $.hw_data %>\n        user_inputs: <% $.user_inputs %>\n        derived_parameters: <% $.derived_parameters %>\n      publish:\n        derived_parameters: <% task().result.get('derived_parameters', {}) %>\n      on-error: set_status_failed_get_hci_derive_params\n      # Done (no more derived parameter features)\n\n    set_status_failed_get_role_info:\n      publish:\n        role_name: <% $.role_name %>\n        status: FAILED\n        message: <% task(get_role_info).result.get('message', '') %>\n      on-success: fail\n\n    set_status_failed_get_flavor_name:\n      publish:\n        role_name: <% $.role_name %>\n        status: FAILED\n        message: <% \"Unable to determine flavor for role '{0}'\".format($.role_name) %>\n      on-success: fail\n\n    set_status_failed_get_profile_name:\n      publish:\n        role_name: <% $.role_name %>\n        status: FAILED\n        message: <% task(get_profile_name).result %>\n      on-success: fail\n\n    set_status_failed_no_matching_node_get_profile_node:\n      publish:\n        role_name: <% $.role_name %>\n        status: FAILED\n        message: <% \"Unable to determine matching node for profile '{0}'\".format($.profile_name) %>\n      on-success: fail\n\n    set_status_failed_on_error_get_profile_node:\n      publish:\n        role_name: <% $.role_name %>\n        status: FAILED\n        message: <% task(get_profile_node).result %>\n      on-success: fail\n\n    set_status_failed_on_error_get_node_with_hint:\n      publish:\n        role_name: <% $.role_name %>\n        status: FAILED\n        message: <% task(get_node_with_hint).result %>\n      on-success: fail\n\n    set_status_failed_get_introspection_data:\n      publish:\n        role_name: <% $.role_name %>\n        status: FAILED\n        message: <% task(get_introspection_data).result %>\n      on-success: fail\n\n    set_status_failed_get_dpdk_derive_params:\n      publish:\n        role_name: <% $.role_name %>\n        status: FAILED\n        message: <% task(get_dpdk_derive_params).result.message %>\n      on-success: fail\n\n    set_status_failed_get_sriov_derive_params:\n      publish:\n        role_name: <% $.role_name %>\n        status: FAILED\n        message: <% task(get_sriov_derive_params).result.message %>\n      on-success: fail\n\n    set_status_failed_get_host_derive_params:\n      publish:\n        role_name: <% $.role_name %>\n        status: FAILED\n        message: <% task(get_host_derive_params).result.message %>\n      on-success: fail\n\n    set_status_failed_get_hci_derive_params:\n      publish:\n        role_name: <% $.role_name %>\n        status: FAILED\n        message: <% task(get_hci_derive_params).result.message %>\n      on-success: fail\n", "name": "tripleo.derive_params.v1._derive_parameters_per_role", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:18:58", "namespace": "", "updated_at": null, "scope": "private", "input": "plan, role_name, environment_parameters, heat_resource_tree, user_inputs", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "f98efbc4-a39f-4e41-b8da-fedafc6eb224"}, {"definition": "host_derive_params:\n  description: >\n    This workflow derives parameters for the Host process, and is mainly associated with CPU pinning and huge memory pages.\n    This workflow can be dependent on any feature or also can be invoked individually as well.\n\n  input:\n    - role_name\n    - hw_data # introspection data\n    - user_inputs\n    - derived_parameters: {}\n\n  output:\n    derived_parameters: <% $.derived_parameters.mergeWith($.get('host_parameters', {})) %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_cpus:\n      publish:\n        cpus: <% $.hw_data.numa_topology.cpus %>\n      on-success:\n        - get_role_derive_params: <% $.cpus %>\n        - set_status_failed_get_cpus: <% not $.cpus %>\n\n    get_role_derive_params:\n      publish:\n        role_derive_params: <% $.derived_parameters.get(concat($.role_name, 'Parameters'), {}) %>\n        # removing the role parameters (eg. ComputeParameters) in derived_parameters dictionary since already copied in role_derive_params.\n        derived_parameters: <% $.derived_parameters.delete(concat($.role_name, 'Parameters')) %>\n      on-success: get_host_cpus\n\n    get_host_cpus:\n      publish:\n        host_cpus: <% $.role_derive_params.get('OvsDpdkCoreList', '') or $.role_derive_params.get('SriovHostCpusList', '') %>\n        # SriovHostCpusList parameter is added temporarily for host_cpus and not needed in derived_parameters result.\n        # SriovHostCpusList parameter is deleted in derived_parameters list and adding the updated role parameters\n        # back in the derived_parameters.\n        derived_parameters: <% $.derived_parameters + dict(concat($.role_name, 'Parameters') => $.role_derive_params.delete('SriovHostCpusList')) %>\n      on-success: get_host_dpdk_combined_cpus\n\n    get_host_dpdk_combined_cpus:\n      publish:\n        host_dpdk_combined_cpus: <% let(pmd_cpus => $.role_derive_params.get('OvsPmdCoreList', '')) -> switch($pmd_cpus => concat($pmd_cpus, ',', $.host_cpus), not $pmd_cpus => $.host_cpus) %>\n        reserved_cpus: []\n      on-success:\n        - get_host_dpdk_combined_cpus_num_list: <% $.host_dpdk_combined_cpus %>\n        - set_status_failed_get_host_dpdk_combined_cpus: <% not $.host_dpdk_combined_cpus %>\n\n    get_host_dpdk_combined_cpus_num_list:\n      action: tripleo.derive_params.convert_range_to_number_list\n      input:\n        range_list: <% $.host_dpdk_combined_cpus %>\n      publish:\n        host_dpdk_combined_cpus: <% task().result %>\n        reserved_cpus: <% task().result.split(',') %>\n      on-success: get_nova_cpus\n      on-error: set_status_failed_get_host_dpdk_combined_cpus_num_list\n\n    get_nova_cpus:\n      publish:\n        nova_cpus: <% let(reserved_cpus => $.reserved_cpus) -> $.cpus.select($.thread_siblings).flatten().where(not (str($) in $reserved_cpus)).join(',') %>\n      on-success:\n        - get_isol_cpus: <% $.nova_cpus %>\n        - set_status_failed_get_nova_cpus: <% not $.nova_cpus %>\n\n    # concatinates OvsPmdCoreList range format and NovaVcpuPinSet in range format. it may not be in perfect range format.\n    # example: concatinates '12-15,19' and 16-18' ranges '12-15,19,16-18'\n    get_isol_cpus:\n      publish:\n        isol_cpus: <% let(pmd_cpus => $.role_derive_params.get('OvsPmdCoreList','')) -> switch($pmd_cpus => concat($pmd_cpus, ',', $.nova_cpus), not $pmd_cpus => $.nova_cpus) %>\n      on-success: get_isol_cpus_num_list\n\n    # Gets the isol_cpus in the number list\n    # example: '12-15,19,16-18' into '12,13,14,15,16,17,18,19'\n    get_isol_cpus_num_list:\n      action: tripleo.derive_params.convert_range_to_number_list\n      input:\n        range_list: <% $.isol_cpus %>\n      publish:\n        isol_cpus: <% task().result %>\n      on-success: get_nova_cpus_range_list\n      on-error: set_status_failed_get_isol_cpus_num_list\n\n    get_nova_cpus_range_list:\n      action: tripleo.derive_params.convert_number_to_range_list\n      input:\n        num_list: <% $.nova_cpus %>\n      publish:\n        nova_cpus: <% task().result %>\n      on-success: get_isol_cpus_range_list\n      on-error: set_status_failed_get_nova_cpus_range_list\n\n    # converts number format isol_cpus into range format\n    # example: '12,13,14,15,16,17,18,19' into '12-19'\n    get_isol_cpus_range_list:\n      action: tripleo.derive_params.convert_number_to_range_list\n      input:\n        num_list: <% $.isol_cpus %>\n      publish:\n        isol_cpus: <% task().result %>\n      on-success: get_host_mem\n      on-error: set_status_failed_get_isol_cpus_range_list\n\n    get_host_mem:\n      publish:\n        host_mem: <% $.user_inputs.get('host_mem_default', 4096) %>\n      on-success: check_default_hugepage_supported\n\n    check_default_hugepage_supported:\n      publish:\n        default_hugepage_supported: <% $.hw_data.get('inventory', {}).get('cpu', {}).get('flags', []).contains('pdpe1gb') %>\n      on-success:\n        - get_total_memory: <% $.default_hugepage_supported %>\n        - set_status_failed_check_default_hugepage_supported: <% not $.default_hugepage_supported %>\n\n    get_total_memory:\n      publish:\n        total_memory: <% $.hw_data.get('inventory', {}).get('memory', {}).get('physical_mb', 0) %>\n      on-success:\n        - get_hugepage_allocation_percentage: <% $.total_memory %>\n        - set_status_failed_get_total_memory: <% not $.total_memory %>\n\n    get_hugepage_allocation_percentage:\n      publish:\n        huge_page_allocation_percentage: <% $.user_inputs.get('huge_page_allocation_percentage', 0) %>\n      on-success:\n        - get_hugepages: <% isInteger($.huge_page_allocation_percentage) and $.huge_page_allocation_percentage > 0 %>\n        - set_status_failed_get_hugepage_allocation_percentage_invalid: <% not isInteger($.huge_page_allocation_percentage) %>\n        - set_status_failed_get_hugepage_allocation_percentage_not_provided: <% $.huge_page_allocation_percentage = 0 %>\n\n    get_hugepages:\n      publish:\n        hugepages: <% let(huge_page_perc => float($.huge_page_allocation_percentage)/100)-> int((($.total_memory/1024)-4) * $huge_page_perc) %>\n      on-success:\n        - get_cpu_model: <% $.hugepages %>\n        - set_status_failed_get_hugepages: <% not $.hugepages %>\n\n    get_cpu_model:\n      publish:\n        intel_cpu_model: <% $.hw_data.get('inventory', {}).get('cpu', {}).get('model_name', '').startsWith('Intel') %>\n      on-success: get_iommu_info\n\n    get_iommu_info:\n      publish:\n        iommu_info: <% switch($.intel_cpu_model => 'intel_iommu=on iommu=pt', not $.intel_cpu_model => '') %>\n      on-success: get_kernel_args\n\n    get_kernel_args:\n      publish:\n        kernel_args: <% concat('default_hugepagesz=1GB hugepagesz=1G ', 'hugepages=', str($.hugepages), ' ',  $.iommu_info, ' isolcpus=', $.isol_cpus) %>\n      on-success: get_host_parameters\n\n    get_host_parameters:\n      publish:\n        host_parameters: <% dict(concat($.role_name, 'Parameters') => dict('NovaVcpuPinSet' => $.get('nova_cpus', ''), 'NovaReservedHostMemory' => $.get('host_mem', ''), 'KernelArgs' => $.get('kernel_args', ''), 'IsolCpusList' => $.get('isol_cpus', ''))) %>\n\n    set_status_failed_get_cpus:\n      publish:\n        status: FAILED\n        message: \"Unable to determine CPU's on NUMA nodes\"\n      on-success: fail\n\n    set_status_failed_get_host_dpdk_combined_cpus:\n      publish:\n        status: FAILED\n        message: 'Unable to combine host and dpdk cpus list'\n      on-success: fail\n\n    set_status_failed_get_host_dpdk_combined_cpus_num_list:\n      publish:\n        status: FAILED\n        message: <% task(get_host_dpdk_combined_cpus_num_list).result %>\n      on-success: fail\n\n    set_status_failed_get_nova_cpus:\n      publish:\n        status: FAILED\n        message: 'Unable to determine nova vcpu pin set'\n      on-success: fail\n\n    set_status_failed_get_nova_cpus_range_list:\n      publish:\n        status: FAILED\n        message: <% task(get_nova_cpus_range_list).result %>\n      on-success: fail\n\n    set_status_failed_get_isol_cpus_num_list:\n      publish:\n        status: FAILED\n        message: <% task(get_isol_cpus_num_list).result %>\n      on-success: fail\n\n    set_status_failed_get_isol_cpus_range_list:\n      publish:\n        status: FAILED\n        message: <% task(get_isol_cpus_range_list).result %>\n      on-success: fail\n\n    set_status_failed_check_default_hugepage_supported:\n      publish:\n        status: FAILED\n        message: 'default huge page size 1GB is not supported'\n      on-success: fail\n\n    set_status_failed_get_total_memory:\n      publish:\n        status: FAILED\n        message: 'Unable to determine total memory'\n      on-success: fail\n\n    set_status_failed_get_hugepage_allocation_percentage_invalid:\n      publish:\n        status: FAILED\n        message: <% \"huge_page_allocation_percentage user input '{0}' is invalid\".format($.huge_page_allocation_percentage) %>\n      on-success: fail\n\n    set_status_failed_get_hugepage_allocation_percentage_not_provided:\n      publish:\n        status: FAILED\n        message: 'huge_page_allocation_percentage user input is not provided'\n      on-success: fail\n\n    set_status_failed_get_hugepages:\n      publish:\n        status: FAILED\n        message: 'Unable to determine huge pages'\n      on-success: fail\n", "name": "tripleo.derive_params_formulas.v1.host_derive_params", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:02", "namespace": "", "updated_at": null, "scope": "private", "input": "role_name, hw_data, user_inputs, derived_parameters={}", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "66928d82-e0b9-4633-9c50-29f1f1286e6b"}, {"definition": "dpdk_derive_params:\n  description: >\n    Workflow to derive parameters for DPDK service.\n  input:\n    - plan\n    - role_name\n    - hw_data # introspection data\n    - user_inputs\n    - derived_parameters: {}\n\n  output:\n    derived_parameters: <% $.derived_parameters.mergeWith($.get('dpdk_parameters', {})) %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_network_config:\n      action: tripleo.parameters.get_network_config\n      input:\n        container: <% $.plan %>\n        role_name: <% $.role_name %>\n      publish:\n        network_configs: <% task().result.get('network_config', []) %>\n      on-success: get_dpdk_nics_numa_info\n      on-error: set_status_failed_get_network_config\n\n    get_dpdk_nics_numa_info:\n      action: tripleo.derive_params.get_dpdk_nics_numa_info\n      input:\n        network_configs: <% $.network_configs %>\n        inspect_data: <% $.hw_data %>\n      publish:\n        dpdk_nics_numa_info: <% task().result %>\n      on-success:\n        # TODO: Need to remove condtions here\n        # adding condition and throw error in action for empty check\n        - get_dpdk_nics_numa_nodes: <% $.dpdk_nics_numa_info %>\n        - set_status_failed_get_dpdk_nics_numa_info: <% not $.dpdk_nics_numa_info %>\n      on-error: set_status_failed_on_error_get_dpdk_nics_numa_info\n\n    get_dpdk_nics_numa_nodes:\n      publish:\n        dpdk_nics_numa_nodes: <% $.dpdk_nics_numa_info.groupBy($.numa_node).select($[0]).orderBy($) %>\n      on-success:\n        - get_numa_nodes: <% $.dpdk_nics_numa_nodes %>\n        - set_status_failed_get_dpdk_nics_numa_nodes: <% not $.dpdk_nics_numa_nodes %>\n\n    get_numa_nodes:\n      publish:\n        numa_nodes: <% $.hw_data.numa_topology.ram.select($.numa_node).orderBy($) %>\n      on-success:\n        - get_num_phy_cores_per_numa_for_pmd: <% $.numa_nodes %>\n        - set_status_failed_get_numa_nodes: <% not $.numa_nodes %>\n\n    get_num_phy_cores_per_numa_for_pmd:\n      publish:\n        num_phy_cores_per_numa_node_for_pmd: <% $.user_inputs.get('num_phy_cores_per_numa_node_for_pmd', 0) %>\n      on-success:\n        - get_num_cores_per_numa_nodes: <% isInteger($.num_phy_cores_per_numa_node_for_pmd) and $.num_phy_cores_per_numa_node_for_pmd > 0 %>\n        - set_status_failed_get_num_phy_cores_per_numa_for_pmd_invalid: <% not isInteger($.num_phy_cores_per_numa_node_for_pmd) %>\n        - set_status_failed_get_num_phy_cores_per_numa_for_pmd_not_provided: <% $.num_phy_cores_per_numa_node_for_pmd = 0 %>\n\n    # For NUMA node with DPDK nic, number of cores should be used from user input\n    # For NUMA node without DPDK nic, number of cores should be 1\n    get_num_cores_per_numa_nodes:\n      publish:\n        num_cores_per_numa_nodes: <% let(dpdk_nics_nodes => $.dpdk_nics_numa_nodes, cores => $.num_phy_cores_per_numa_node_for_pmd) -> $.numa_nodes.select(switch($ in $dpdk_nics_nodes => $cores, not $ in $dpdk_nics_nodes => 1)) %>\n      on-success: get_pmd_cpus\n\n    get_pmd_cpus:\n      action: tripleo.derive_params.get_dpdk_core_list\n      input:\n        inspect_data: <% $.hw_data %>\n        numa_nodes_cores_count: <% $.num_cores_per_numa_nodes %>\n      publish:\n        pmd_cpus: <% task().result %>\n      on-success:\n        - get_pmd_cpus_range_list: <% $.pmd_cpus %>\n        - set_status_failed_get_pmd_cpus: <% not $.pmd_cpus %>\n      on-error: set_status_failed_on_error_get_pmd_cpus\n\n    get_pmd_cpus_range_list:\n      action: tripleo.derive_params.convert_number_to_range_list\n      input:\n        num_list: <% $.pmd_cpus %>\n      publish:\n        pmd_cpus: <% task().result %>\n      on-success: get_host_cpus\n      on-error: set_status_failed_get_pmd_cpus_range_list\n\n    get_host_cpus:\n      workflow: tripleo.derive_params_formulas.v1.get_host_cpus\n      input:\n        role_name: <% $.role_name %>\n        hw_data: <% $.hw_data %>\n      publish:\n        host_cpus: <% task().result.get('host_cpus', '') %>\n      on-success: get_sock_mem\n      on-error: set_status_failed_get_host_cpus\n\n    get_sock_mem:\n      action: tripleo.derive_params.get_dpdk_socket_memory\n      input:\n        dpdk_nics_numa_info: <% $.dpdk_nics_numa_info %>\n        numa_nodes: <% $.numa_nodes %>\n        overhead: <% $.user_inputs.get('overhead', 800) %>\n        packet_size_in_buffer: <% 4096*64 %>\n      publish:\n        sock_mem: <% task().result %>\n      on-success:\n        - get_dpdk_parameters: <% $.sock_mem %>\n        - set_status_failed_get_sock_mem: <% not $.sock_mem %>\n      on-error: set_status_failed_on_error_get_sock_mem\n\n    get_dpdk_parameters:\n      publish:\n        dpdk_parameters: <% dict(concat($.role_name, 'Parameters') => dict('OvsPmdCoreList' => $.get('pmd_cpus', ''), 'OvsDpdkCoreList' => $.get('host_cpus', ''), 'OvsDpdkSocketMemory' => $.get('sock_mem', ''))) %>\n\n    set_status_failed_get_network_config:\n      publish:\n        status: FAILED\n        message: <% task(get_network_config).result %>\n      on-success: fail\n\n    set_status_failed_get_dpdk_nics_numa_info:\n      publish:\n        status: FAILED\n        message: \"Unable to determine DPDK NIC's NUMA information\"\n      on-success: fail\n\n    set_status_failed_on_error_get_dpdk_nics_numa_info:\n      publish:\n        status: FAILED\n        message: <% task(get_dpdk_nics_numa_info).result %>\n      on-success: fail\n\n    set_status_failed_get_dpdk_nics_numa_nodes:\n      publish:\n        status: FAILED\n        message: \"Unable to determine DPDK NIC's numa nodes\"\n      on-success: fail\n\n    set_status_failed_get_numa_nodes:\n      publish:\n        status: FAILED\n        message: 'Unable to determine available NUMA nodes'\n      on-success: fail\n\n    set_status_failed_get_num_phy_cores_per_numa_for_pmd_invalid:\n      publish:\n        status: FAILED\n        message: <% \"num_phy_cores_per_numa_node_for_pmd user input '{0}' is invalid\".format($.num_phy_cores_per_numa_node_for_pmd) %>\n      on-success: fail\n\n    set_status_failed_get_num_phy_cores_per_numa_for_pmd_not_provided:\n      publish:\n        status: FAILED\n        message: 'num_phy_cores_per_numa_node_for_pmd user input is not provided'\n      on-success: fail\n\n    set_status_failed_get_pmd_cpus:\n      publish:\n        status: FAILED\n        message: 'Unable to determine OvsPmdCoreList parameter'\n      on-success: fail\n\n    set_status_failed_on_error_get_pmd_cpus:\n      publish:\n        status: FAILED\n        message: <% task(get_pmd_cpus).result %>\n      on-success: fail\n\n    set_status_failed_get_pmd_cpus_range_list:\n      publish:\n        status: FAILED\n        message: <% task(get_pmd_cpus_range_list).result %>\n      on-success: fail\n\n    set_status_failed_get_host_cpus:\n      publish:\n        status: FAILED\n        message: <% task(get_host_cpus).result.get('message', '') %>\n      on-success: fail\n\n    set_status_failed_get_sock_mem:\n      publish:\n        status: FAILED\n        message: 'Unable to determine OvsDpdkSocketMemory parameter'\n      on-success: fail\n\n    set_status_failed_on_error_get_sock_mem:\n      publish:\n        status: FAILED\n        message: <% task(get_sock_mem).result %>\n      on-success: fail\n", "name": "tripleo.derive_params_formulas.v1.dpdk_derive_params", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:02", "namespace": "", "updated_at": null, "scope": "private", "input": "plan, role_name, hw_data, user_inputs, derived_parameters={}", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "82d77f49-293b-45ee-8ab9-6b3aca4e648e"}, {"definition": "hci_derive_params:\n  description: Derive the deployment parameters for HCI\n  input:\n    - role_name\n    - environment_parameters\n    - heat_resource_tree\n    - introspection_data\n    - user_inputs\n    - derived_parameters: {}\n\n  output:\n    derived_parameters: <% $.derived_parameters.mergeWith($.get('hci_parameters', {})) %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_hci_inputs:\n      publish:\n        hci_profile: <% $.user_inputs.get('hci_profile', '') %>\n        hci_profile_config: <% $.user_inputs.get('hci_profile_config', {}) %>\n        MB_PER_GB: 1024\n      on-success:\n        - get_average_guest_memory_size_in_mb: <% $.hci_profile and $.hci_profile_config.get($.hci_profile, {}) %>\n        - set_failed_invalid_hci_profile: <% $.hci_profile and not $.hci_profile_config.get($.hci_profile, {}) %>\n        # When no hci_profile is specified, the workflow terminates without deriving any HCI parameters.\n\n    get_average_guest_memory_size_in_mb:\n      publish:\n        average_guest_memory_size_in_mb: <% $.hci_profile_config.get($.hci_profile, {}).get('average_guest_memory_size_in_mb', 0) %>\n      on-success:\n        - get_average_guest_cpu_utilization_percentage: <% isInteger($.average_guest_memory_size_in_mb) %>\n        - set_failed_invalid_average_guest_memory_size_in_mb: <% not isInteger($.average_guest_memory_size_in_mb) %>\n\n    get_average_guest_cpu_utilization_percentage:\n      publish:\n        average_guest_cpu_utilization_percentage: <% $.hci_profile_config.get($.hci_profile, {}).get('average_guest_cpu_utilization_percentage', 0) %>\n      on-success:\n        - get_gb_overhead_per_guest: <% isInteger($.average_guest_cpu_utilization_percentage) %>\n        - set_failed_invalid_average_guest_cpu_utilization_percentage: <% not isInteger($.average_guest_cpu_utilization_percentage) %>\n\n    get_gb_overhead_per_guest:\n      publish:\n        gb_overhead_per_guest: <% $.user_inputs.get('gb_overhead_per_guest', 0.5) %>\n      on-success:\n        - get_gb_per_osd: <% isNumber($.gb_overhead_per_guest) %>\n        - set_failed_invalid_gb_overhead_per_guest: <% not isNumber($.gb_overhead_per_guest) %>\n\n    get_gb_per_osd:\n      publish:\n        gb_per_osd: <% $.user_inputs.get('gb_per_osd', 5) %>\n      on-success:\n        - get_cores_per_osd: <% isNumber($.gb_per_osd) %>\n        - set_failed_invalid_gb_per_osd: <% not isNumber($.gb_per_osd) %>\n\n    get_cores_per_osd:\n      publish:\n        cores_per_osd: <% $.user_inputs.get('cores_per_osd', 1.0) %>\n      on-success:\n        - get_extra_configs: <% isNumber($.cores_per_osd) %>\n        - set_failed_invalid_cores_per_osd: <% not isNumber($.cores_per_osd) %>\n\n    get_extra_configs:\n      publish:\n        extra_config: <% $.environment_parameters.get('ExtraConfig', {}) %>\n        role_extra_config: <% $.environment_parameters.get(concat($.role_name, 'ExtraConfig'), {}) %>\n        role_env_params: <% $.environment_parameters.get(concat($.role_name, 'Parameters'), {}) %>\n        role_derive_params: <% $.derived_parameters.get(concat($.role_name, 'Parameters'), {}) %>\n      on-success: get_num_osds\n\n    get_num_osds:\n      publish:\n        num_osds: <% $.heat_resource_tree.parameters.get('CephAnsibleDisksConfig', {}).get('default', {}).get('devices', []).count() %>\n      on-success:\n        - get_memory_mb: <% $.num_osds %>\n        # If there's no CephAnsibleDisksConfig then look for OSD configuration in hiera data\n        - get_num_osds_from_hiera: <% not $.num_osds %>\n\n    get_num_osds_from_hiera:\n      publish:\n        num_osds: <% $.role_extra_config.get('ceph::profile::params::osds', $.extra_config.get('ceph::profile::params::osds', {})).keys().count() %>\n      on-success:\n        - get_memory_mb: <% $.num_osds %>\n        - set_failed_no_osds: <% not $.num_osds %>\n\n    get_memory_mb:\n      publish:\n        memory_mb: <% $.introspection_data.get('memory_mb', 0) %>\n      on-success:\n        - get_nova_vcpu_pin_set: <% $.memory_mb %>\n        - set_failed_get_memory_mb: <% not $.memory_mb %>\n\n    # Determine the number of CPU cores available to Nova and Ceph. If\n    # NovaVcpuPinSet is defined then use the number of vCPUs in the set,\n    # otherwise use all of the cores identified in the introspection data.\n\n    get_nova_vcpu_pin_set:\n      publish:\n        # NovaVcpuPinSet can be defined in multiple locations, and it's\n        # important to select the value in order of precedence:\n        #\n        # 1) User specified value for this role\n        # 2) User specified default value for all roles\n        # 3) Value derived by another derived parameters workflow\n        nova_vcpu_pin_set: <% $.role_env_params.get('NovaVcpuPinSet', $.environment_parameters.get('NovaVcpuPinSet', $.role_derive_params.get('NovaVcpuPinSet', ''))) %>\n      on-success:\n        - get_nova_vcpu_count: <% $.nova_vcpu_pin_set %>\n        - get_num_cores: <% not $.nova_vcpu_pin_set %>\n\n    get_nova_vcpu_count:\n      action: tripleo.derive_params.convert_range_to_number_list\n      input:\n        range_list: <% $.nova_vcpu_pin_set %>\n      publish:\n        num_cores: <% task().result.split(',').count() %>\n      on-success: calculate_nova_parameters\n      on-error: set_failed_get_nova_vcpu_count\n\n    get_num_cores:\n      publish:\n        num_cores: <% $.introspection_data.get('cpus', 0) %>\n      on-success:\n        - calculate_nova_parameters: <% $.num_cores %>\n        - set_failed_get_num_cores: <% not $.num_cores %>\n\n    # HCI calculations are broken into multiple steps. This is necessary\n    # because variables published by a Mistral task are not available\n    # for use by that same task. Variables computed and published in a task\n    # are only available in subsequent tasks.\n    #\n    # The HCI calculations compute two Nova parameters:\n    # - reserved_host_memory\n    # - cpu_allocation_ratio\n    #\n    # The reserved_host_memory calculation computes the amount of memory\n    # that needs to be reserved for Ceph and the total amount of \"guest\n    # overhead\" memory that is based on the anticipated number of guests.\n    # Psuedo-code for the calculation (disregarding MB and GB units) is\n    # as follows:\n    #\n    #   ceph_memory = mem_per_osd * num_osds\n    #   nova_memory = total_memory - ceph_memory\n    #   num_guests = nova_memory /\n    #                (average_guest_memory_size + overhead_per_guest)\n    #   reserved_memory = ceph_memory + (num_guests * overhead_per_guest)\n    #\n    # The cpu_allocation_ratio calculation is similar in that it takes into\n    # account the number of cores that must be reserved for Ceph.\n    #\n    #   ceph_cores = cores_per_osd * num_osds\n    #   guest_cores = num_cores - ceph_cores\n    #   guest_vcpus = guest_cores / average_guest_utilization\n    #   cpu_allocation_ratio = guest_vcpus / num_cores\n\n    calculate_nova_parameters:\n      publish:\n        avg_guest_util: <% $.average_guest_cpu_utilization_percentage / 100.0 %>\n        avg_guest_size_gb: <% $.average_guest_memory_size_in_mb / float($.MB_PER_GB) %>\n        memory_gb: <% $.memory_mb / float($.MB_PER_GB) %>\n        ceph_mem_gb: <% $.gb_per_osd * $.num_osds %>\n        nonceph_cores: <% $.num_cores - int($.cores_per_osd * $.num_osds) %>\n      on-success: calc_step_2\n\n    calc_step_2:\n      publish:\n        num_guests: <% int(($.memory_gb - $.ceph_mem_gb) / ($.avg_guest_size_gb + $.gb_overhead_per_guest)) %>\n        guest_vcpus: <% $.nonceph_cores / $.avg_guest_util %>\n      on-success: calc_step_3\n\n    calc_step_3:\n      publish:\n        reserved_host_memory: <% $.MB_PER_GB * int($.ceph_mem_gb + ($.num_guests * $.gb_overhead_per_guest)) %>\n        cpu_allocation_ratio: <% $.guest_vcpus / $.num_cores %>\n      on-success: validate_results\n\n    validate_results:\n      publish:\n        # Verify whether HCI is viable:\n        # - At least 80% of the memory is reserved for Ceph and guest overhead\n        # - At least half of the CPU cores must be available to Nova\n        mem_ok: <% $.reserved_host_memory <= ($.memory_mb * 0.8) %>\n        cpu_ok: <% $.cpu_allocation_ratio >= 0.5 %>\n      on-success:\n        - set_failed_insufficient_mem: <% not $.mem_ok %>\n        - set_failed_insufficient_cpu: <% not $.cpu_ok %>\n        - publish_hci_parameters: <% $.mem_ok and $.cpu_ok %>\n\n    publish_hci_parameters:\n      publish:\n        # TODO(abishop): Update this when the cpu_allocation_ratio can be set\n        # via a THT parameter (no such parameter currently exists). Until a\n        # THT parameter exists, use hiera data to set the cpu_allocation_ratio.\n        hci_parameters: <% dict(concat($.role_name, 'Parameters') => dict('NovaReservedHostMemory' => $.reserved_host_memory)) + dict(concat($.role_name, 'ExtraConfig') => dict('nova::cpu_allocation_ratio' => $.cpu_allocation_ratio)) %>\n\n    set_failed_invalid_hci_profile:\n      publish:\n        message: \"'<% $.hci_profile %>' is not a valid HCI profile.\"\n      on-success: fail\n\n    set_failed_invalid_average_guest_memory_size_in_mb:\n      publish:\n        message: \"'<% $.average_guest_memory_size_in_mb %>' is not a valid average_guest_memory_size_in_mb value.\"\n      on-success: fail\n\n    set_failed_invalid_gb_overhead_per_guest:\n      publish:\n        message: \"'<% $.gb_overhead_per_guest %>' is not a valid gb_overhead_per_guest value.\"\n      on-success: fail\n\n    set_failed_invalid_gb_per_osd:\n      publish:\n        message: \"'<% $.gb_per_osd %>' is not a valid gb_per_osd value.\"\n      on-success: fail\n\n    set_failed_invalid_cores_per_osd:\n      publish:\n        message: \"'<% $.cores_per_osd %>' is not a valid cores_per_osd value.\"\n      on-success: fail\n\n    set_failed_invalid_average_guest_cpu_utilization_percentage:\n      publish:\n        message: \"'<% $.average_guest_cpu_utilization_percentage %>' is not a valid average_guest_cpu_utilization_percentage value.\"\n      on-success: fail\n\n    set_failed_no_osds:\n      publish:\n        message: \"No Ceph OSDs found in the overcloud definition ('ceph::profile::params::osds').\"\n      on-success: fail\n\n    set_failed_get_memory_mb:\n      publish:\n        message: \"Unable to determine the amount of physical memory (no 'memory_mb' found in introspection_data).\"\n      on-success: fail\n\n    set_failed_get_nova_vcpu_count:\n      publish:\n        message: <% task(get_nova_vcpu_count).result %>\n      on-success: fail\n\n    set_failed_get_num_cores:\n      publish:\n        message: \"Unable to determine the number of CPU cores (no 'cpus' found in introspection_data).\"\n      on-success: fail\n\n    set_failed_insufficient_mem:\n      publish:\n        message: \"<% $.memory_mb %> MB is not enough memory to run hyperconverged.\"\n      on-success: fail\n\n    set_failed_insufficient_cpu:\n      publish:\n        message: \"<% $.num_cores %> CPU cores are not enough to run hyperconverged.\"\n      on-success: fail\n", "name": "tripleo.derive_params_formulas.v1.hci_derive_params", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:02", "namespace": "", "updated_at": null, "scope": "private", "input": "role_name, environment_parameters, heat_resource_tree, introspection_data, user_inputs, derived_parameters={}", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "b3d7a1e5-7871-4aa8-8784-7da67b4130d5"}, {"definition": "get_host_cpus:\n  workflow: tripleo.derive_params_formulas.v1.get_host_cpus\n  input:\n    role_name: <% $.role_name %>\n    hw_data: <% $.hw_data %>\n  publish:\n    host_cpus: <% task().result.get('host_cpus', '') %>\n  on-success: get_sock_mem\n  on-error: set_status_failed_get_host_cpus\n", "name": "tripleo.derive_params_formulas.v1.get_host_cpus", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:02", "namespace": "", "updated_at": null, "scope": "private", "input": "hw_data", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "bffb81c5-0be8-44bd-8ff9-b2573ecd3782"}, {"definition": "sriov_derive_params:\n  description: >\n    This workflow derives parameters for the SRIOV feature.\n\n  input:\n    - role_name\n    - hw_data # introspection data\n    - derived_parameters: {}\n\n  output:\n    derived_parameters: <% $.derived_parameters.mergeWith($.get('sriov_parameters', {})) %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_host_cpus:\n      workflow: tripleo.derive_params_formulas.v1.get_host_cpus\n      input:\n        role_name: <% $.role_name %>\n        hw_data: <% $.hw_data %>\n      publish:\n        host_cpus: <% task().result.get('host_cpus', '') %>\n      on-success: get_sriov_parameters\n      on-error: set_status_failed_get_host_cpus\n\n    get_sriov_parameters:\n      publish:\n        # SriovHostCpusList parameter is added temporarily and it's removed later from derived parameters result.\n        sriov_parameters: <% dict(concat($.role_name, 'Parameters') => dict('SriovHostCpusList' => $.get('host_cpus', ''))) %>\n\n    set_status_failed_get_host_cpus:\n      publish:\n        status: FAILED\n        message: <% task(get_host_cpus).result.get('message', '') %>\n      on-success: fail\n", "name": "tripleo.derive_params_formulas.v1.sriov_derive_params", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:02", "namespace": "", "updated_at": null, "scope": "private", "input": "role_name, hw_data, derived_parameters={}", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "c408ac8d-b8fb-4a57-8e47-4c62871210c2"}, {"definition": "rotate_fernet_keys:\n\n  input:\n    - container\n    - queue_name: tripleo\n    - ansible_extra_env_variables:\n          ANSIBLE_HOST_KEY_CHECKING: 'False'\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    rotate_keys:\n      action: tripleo.parameters.rotate_fernet_keys container=<% $.container %>\n      on-success: deploy_ssh_key\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    deploy_ssh_key:\n      workflow: tripleo.validations.v1.copy_ssh_key\n      on-success: get_privkey\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    get_privkey:\n      action: tripleo.validations.get_privkey\n      on-success: deploy_keys\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    deploy_keys:\n      action: tripleo.ansible-playbook\n      input:\n        hosts: keystone\n        inventory: /usr/bin/tripleo-ansible-inventory\n        ssh_private_key: <% task(get_privkey).result %>\n        extra_env_variables: <% $.ansible_extra_env_variables + dict(TRIPLEO_PLAN_NAME=>$.container) %>\n        verbosity: 0\n        remote_user: heat-admin\n        become: true\n        extra_vars:\n          fernet_keys: <% task(rotate_keys).result %>\n        use_openstack_credentials: true\n        playbook: /usr/share/tripleo-common/playbooks/rotate-keys.yaml\n      on-success: notify_zaqar\n      publish:\n        status: SUCCESS\n        message: <% task().result %>\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.fernet_keys.v1.rotate_fernet_keys\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.fernet_keys.v1.rotate_fernet_keys", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:02", "namespace": "", "updated_at": null, "scope": "private", "input": "container, queue_name=tripleo, ansible_extra_env_variables={u'ANSIBLE_HOST_KEY_CHECKING': u'False'}", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "fff11434-d957-4ee8-8c48-efa966bb5e8a"}, {"definition": "validate_networks_input:\n  description: >\n    Validate that required fields are present.\n\n  input:\n    - networks\n    - queue_name: tripleo\n\n  output:\n    result: <% task(validate_network_names).result %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    validate_network_names:\n      publish:\n        network_name_present: <% $.networks.all($.containsKey('name')) %>\n      on-success:\n        - set_status_success: <% $.network_name_present = true %>\n        - set_status_error: <% $.network_name_present = false %>\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    set_status_success:\n      on-success: notify_zaqar\n      publish:\n        status: SUCCESS\n        message: <% task(validate_network_names).result %>\n\n    set_status_error:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: \"One or more entries did not contain the required field 'name'\"\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.networks.v1.validate_networks_input\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.networks.v1.validate_networks_input", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:03", "namespace": "", "updated_at": null, "scope": "private", "input": "networks, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "0418367f-6470-4af7-81d2-27e95610eeb5"}, {"definition": "update_networks:\n  description: >\n    Takes data in networks parameter in json format, validates its contents,\n    and persists them in network_data.yaml. After successful update,\n    templates are regenerated.\n\n  input:\n    - container: overcloud\n    - networks\n    - network_data_file: 'network_data.yaml'\n    - queue_name: tripleo\n\n  output:\n    network_data: <% $.network_data %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    validate_input:\n      description: >\n        validate the format of input (input includes required fields for\n        each network)\n      workflow: validate_networks_input\n      input:\n        networks: <% $.networks %>\n      on-success: validate_network_files\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    validate_network_files:\n      description: >\n        validate that Network names exist in Swift container\n      workflow: tripleo.plan_management.v1.validate_network_files\n      input:\n        container: <% $.container %>\n        network_data: <% $.networks %>\n        queue_name: <% $.queue_name %>\n      publish:\n        network_data: <% task().network_data %>\n      on-success: get_available_networks\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    get_available_networks:\n      workflow: tripleo.plan_management.v1.list_available_networks\n      input:\n        container: <% $.container %>\n        queue_name: <% $.queue_name %>\n      publish:\n        available_networks: <% task().result.available_networks %>\n      on-success: get_current_networks\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    get_current_networks:\n      workflow: tripleo.plan_management.v1.get_network_data\n      input:\n        container: <% $.container %>\n        network_data_file: <% $.network_data_file %>\n        queue_name: <% $.queue_name %>\n      publish:\n        current_networks: <% task().result.network_data %>\n      on-success: update_network_data\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    update_network_data:\n      description: >\n        Combine (or replace) the network data\n      action: tripleo.plan.update_networks\n      input:\n        networks: <% $.available_networks %>\n        current_networks: <% $.current_networks %>\n        remove_all: false\n      publish:\n        new_network_data: <% task().result.network_data %>\n      on-success: update_network_data_in_swift\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    update_network_data_in_swift:\n      description: >\n        update network_data.yaml object in Swift with data from workflow input\n      action: swift.put_object\n      input:\n        container: <% $.container %>\n        obj: <% $.network_data_file %>\n        contents: <% yaml_dump($.new_network_data) %>\n      on-success: regenerate_templates\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    regenerate_templates:\n      action: tripleo.templates.process container=<% $.container %>\n      on-success: get_networks\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    get_networks:\n      description: >\n        run GetNetworksAction to get updated contents of network_data.yaml and\n        provide it as output\n      workflow: tripleo.plan_management.v1.get_network_data\n      input:\n        container: <% $.container %>\n        network_data_file: <% $.network_data_file %>\n        queue_name: <% $.queue_name %>\n      publish:\n        network_data: <% task().network_data %>\n      on-success: set_status_success\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    set_status_success:\n      on-success: notify_zaqar\n      publish:\n        status: SUCCESS\n        message: <% task(get_networks).result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.networks.v1.update_networks\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.networks.v1.update_networks", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:03", "namespace": "", "updated_at": null, "scope": "private", "input": "container=overcloud, networks, network_data_file=network_data.yaml, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "5f40d214-3a4b-491c-8663-4bbf453bf5a8"}, {"definition": "octavia_post_deploy:\n  description: Octavia post deployment\n  input:\n    - amp_image_name\n    - amp_image_filename\n    - amp_image_tag\n    - amp_ssh_key_name\n    - amp_ssh_key_path\n    - amp_ssh_key_data\n    - auth_username\n    - auth_password\n    - auth_project_name\n    - lb_mgmt_net_name\n    - lb_mgmt_subnet_name\n    - lb_sec_group_name\n    - lb_mgmt_subnet_cidr\n    - lb_mgmt_subnet_gateway\n    - lb_mgmt_subnet_pool_start\n    - lb_mgmt_subnet_pool_end\n    - generate_certs\n    - octavia_ansible_playbook\n    - overcloud_admin\n    - ca_cert_path\n    - ca_private_key_path\n    - ca_passphrase\n    - client_cert_path\n    - mgmt_port_dev\n    - overcloud_password\n    - overcloud_project\n    - overcloud_pub_auth_uri\n    - ansible_extra_env_variables:\n        ANSIBLE_HOST_KEY_CHECKING: 'False'\n        ANSIBLE_SSH_RETRIES: '3'\n  tags:\n    - tripleo-common-managed\n  tasks:\n    get_overcloud_stack_details:\n      publish:\n        # TODO(beagles), we are making an assumption about the octavia heatlh manager and\n        # controller worker needing\n        #\n        octavia_controller_ips: <% env().get('service_ips', {}).get('octavia_worker_ctlplane_node_ips', []) %>\n      on-success: enable_ssh_admin\n\n    enable_ssh_admin:\n      workflow: tripleo.access.v1.enable_ssh_admin\n      input:\n        ssh_servers: <% $.octavia_controller_ips %>\n      on-success: get_private_key\n\n    get_private_key:\n      action: tripleo.validations.get_privkey\n      publish:\n        private_key: <% task().result %>\n      on-success: make_local_temp_directory\n\n    make_local_temp_directory:\n      action: tripleo.files.make_temp_dir\n      publish:\n        undercloud_local_dir: <% task().result.path %>\n      on-success: make_remote_temp_directory\n\n    make_remote_temp_directory:\n      action: tripleo.files.make_temp_dir\n      publish:\n        undercloud_remote_dir: <% task().result.path %>\n      on-success: build_local_connection_environment_vars\n\n    build_local_connection_environment_vars:\n      publish:\n        ansible_local_connection_variables: <% dict('ANSIBLE_REMOTE_TEMP' => $.undercloud_remote_dir, 'ANSIBLE_LOCAL_TEMP' => $.undercloud_local_dir) + $.ansible_extra_env_variables %>\n      on-success: upload_amphora\n\n    upload_amphora:\n      action: tripleo.ansible-playbook\n      input:\n        inventory:\n          undercloud:\n            hosts:\n              localhost:\n                ansible_connection: local\n\n        playbook: <% $.octavia_ansible_playbook %>\n        remote_user: stack\n        extra_env_variables: <% $.ansible_local_connection_variables %>\n        extra_vars:\n          os_password: <% $.overcloud_password %>\n          os_username: <% $.overcloud_admin %>\n          os_project_name: <% $.overcloud_project %>\n          os_auth_url: <% $.overcloud_pub_auth_uri %>\n          os_auth_type: \"password\"\n          os_identity_api_version: \"3\"\n          amp_image_name: <% $.amp_image_name %>\n          amp_image_filename: <% $.amp_image_filename %>\n          amp_image_tag: <% $.amp_image_tag %>\n          amp_ssh_key_name: <% $.amp_ssh_key_name %>\n          amp_ssh_key_path: <% $.amp_ssh_key_path %>\n          amp_ssh_key_data: <% $.amp_ssh_key_data %>\n          auth_username: <% $.auth_username %>\n          auth_password: <% $.auth_password %>\n          auth_project_name: <% $.auth_project_name %>\n      on-success: config_octavia\n\n    config_octavia:\n      action: tripleo.ansible-playbook\n      input:\n        inventory:\n          octavia_nodes:\n            hosts: <% $.octavia_controller_ips.toDict($, {}) %>\n        verbosity: 0\n        playbook: <% $.octavia_ansible_playbook %>\n        remote_user: tripleo-admin\n        become: true\n        become_user: root\n        ssh_private_key: <% $.private_key %>\n        ssh_common_args: '-o StrictHostKeyChecking=no'\n        ssh_extra_args: '-o UserKnownHostsFile=/dev/null'\n        extra_env_variables: <% $.ansible_extra_env_variables %>\n        extra_vars:\n          os_password: <% $.overcloud_password %>\n          os_username: <% $.overcloud_admin %>\n          os_project_name: <% $.overcloud_project %>\n          os_auth_url: <% $.overcloud_pub_auth_uri %>\n          os_auth_type: \"password\"\n          os_identity_api_version: \"3\"\n          amp_image_tag: <% $.amp_image_tag %>\n          lb_mgmt_net_name: <% $.lb_mgmt_net_name %>\n          lb_mgmt_subnet_name: <% $.lb_mgmt_subnet_name %>\n          lb_sec_group_name: <% $.lb_sec_group_name %>\n          lb_mgmt_subnet_cidr: <% $.lb_mgmt_subnet_cidr %>\n          lb_mgmt_subnet_gateway: <% $.lb_mgmt_subnet_gateway %>\n          lb_mgmt_subnet_pool_start: <% $.lb_mgmt_subnet_pool_start %>\n          lb_mgmt_subnet_pool_end: <% $.lb_mgmt_subnet_pool_end %>\n          ca_cert_path: <% $.ca_cert_path %>\n          ca_private_key_path: <% $.ca_private_key_path %>\n          ca_passphrase: <% $.ca_passphrase %>\n          client_cert_path: <% $.client_cert_path %>\n          generate_certs: <% $.generate_certs %>\n          mgmt_port_dev: <% $.mgmt_port_dev %>\n          auth_project_name: <% $.auth_project_name %>\n      on-complete: purge_local_temp_dir\n    purge_local_temp_dir:\n      action: tripleo.files.remove_temp_dir path=<% $.undercloud_local_dir %>\n      on-complete: purge_remote_temp_dir\n    purge_remote_temp_dir:\n      action: tripleo.files.remove_temp_dir path=<% $.undercloud_remote_dir %>\n", "name": "tripleo.octavia_post.v1.octavia_post_deploy", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:03", "namespace": "", "updated_at": null, "scope": "private", "input": "amp_image_name, amp_image_filename, amp_image_tag, amp_ssh_key_name, amp_ssh_key_path, amp_ssh_key_data, auth_username, auth_password, auth_project_name, lb_mgmt_net_name, lb_mgmt_subnet_name, lb_sec_group_name, lb_mgmt_subnet_cidr, lb_mgmt_subnet_gateway, lb_mgmt_subnet_pool_start, lb_mgmt_subnet_pool_end, generate_certs, octavia_ansible_playbook, overcloud_admin, ca_cert_path, ca_private_key_path, ca_passphrase, client_cert_path, mgmt_port_dev, overcloud_password, overcloud_project, overcloud_pub_auth_uri, ansible_extra_env_variables={u'ANSIBLE_SSH_RETRIES': u'3', u'ANSIBLE_HOST_KEY_CHECKING': u'False'}", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "ce427b80-dc80-4747-93f0-d08a489c49bf"}, {"definition": "converge_upgrade_plan:\n  description: Take a container and perform the converge step of a major upgrade\n\n  input:\n    - container\n    - timeout: 240\n    - queue_name: tripleo\n    - skip_deploy_identifier: False\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    remove_noop:\n      action: tripleo.plan.remove_noop_deploystep\n      input:\n        container: <% $.container %>\n      on-success: send_message\n      on-error: set_update_failed\n\n    set_update_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(upgrade_converge).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.major_upgrade.v1.converge_upgrade_plan\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.package_update.v1.converge_upgrade_plan", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:04", "namespace": "", "updated_at": null, "scope": "private", "input": "container, timeout=240, queue_name=tripleo, skip_deploy_identifier=False", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "3a2a5355-da1c-4552-b8d8-1892b1894fb9"}, {"definition": "get_config:\n  input:\n    - container\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_config:\n      action: tripleo.config.get_overcloud_config container=<% $.container %>\n      publish:\n        status: SUCCESS\n        message: <% task().result %>\n      publish-on-error:\n        status: FAILED\n        message: Init Minor update failed\n      on-complete: send_message\n\n    send_message:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.package_update.v1.package_update_plan\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.package_update.v1.get_config", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:04", "namespace": "", "updated_at": null, "scope": "private", "input": "container, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "99870648-049f-46bb-842a-4d4310c422bb"}, {"definition": "package_update_plan:\n  description: Take a container and perform a package update with possible breakpoints\n\n  input:\n    - container\n    - ceph_ansible_playbook\n    - timeout: 240\n    - queue_name: tripleo\n    - skip_deploy_identifier: False\n    - config_dir: '/tmp/'\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    update:\n      action: tripleo.package_update.update_stack\n      input:\n        timeout: <% $.timeout %>\n        container: <% $.container %>\n        ceph_ansible_playbook: <% $.ceph_ansible_playbook %>\n      on-success: clean_plan\n      on-error: set_update_failed\n\n    clean_plan:\n      action: tripleo.plan.update_plan_environment\n      input:\n        container: <% $.container %>\n        parameter: CephAnsiblePlaybook\n        env_key: parameter_defaults\n        delete: true\n      on-success: send_message\n      on-error: set_update_failed\n\n\n    set_update_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(update).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.package_update.v1.package_update_plan\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.package_update.v1.package_update_plan", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:04", "namespace": "", "updated_at": null, "scope": "private", "input": "container, ceph_ansible_playbook, timeout=240, queue_name=tripleo, skip_deploy_identifier=False, config_dir=/tmp/", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "9f092558-71d1-49e5-bf92-9c4da05c34c3"}, {"definition": "ffwd_upgrade_converge_plan:\n  description: ffwd-upgrade converge removes DeploymentSteps no-op from plan\n\n  input:\n    - container\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    remove_noop:\n      action: tripleo.plan.remove_noop_deploystep\n      input:\n        container: <% $.container %>\n      on-success: send_message\n      on-error: set_update_failed\n\n    set_update_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(remove_noop).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.package_update.v1.ffwd_upgrade_converge_plan\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.package_update.v1.ffwd_upgrade_converge_plan", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:04", "namespace": "", "updated_at": null, "scope": "private", "input": "container, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "e1805b11-9bdc-4a02-9fef-8e6081551e4b"}, {"definition": "update_nodes:\n  description: Take a container and perform an update nodes by nodes\n\n  input:\n    - node_user: heat-admin\n    - nodes\n    - playbook\n    - inventory_file\n    - ansible_queue_name: tripleo\n    - module_path: /usr/share/ansible-modules\n    - ansible_extra_env_variables:\n          ANSIBLE_LOG_PATH: /var/log/mistral/package_update.log\n          ANSIBLE_HOST_KEY_CHECKING: 'False'\n    - verbosity: 1\n    - work_dir: /var/lib/mistral\n    - skip_tags: ''\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    download_config:\n      action: tripleo.config.download_config\n      input:\n        work_dir: <% $.work_dir %>/<% execution().id %>\n      on-success: get_private_key\n      on-error: node_update_failed\n\n    get_private_key:\n      action: tripleo.validations.get_privkey\n      publish:\n        private_key: <% task().result %>\n      on-success: node_update\n\n    node_update:\n      action: tripleo.ansible-playbook\n      input:\n        inventory: <% $.inventory_file %>\n        playbook: <% $.work_dir %>/<% execution().id %>/<% $.playbook %>\n        remote_user: <% $.node_user %>\n        become: true\n        become_user: root\n        verbosity: <% $.verbosity %>\n        ssh_private_key: <% $.private_key %>\n        extra_env_variables: <% $.ansible_extra_env_variables %>\n        limit_hosts: <% $.nodes %>\n        module_path: <% $.module_path %>\n        queue_name: <% $.ansible_queue_name %>\n        execution_id: <% execution().id %>\n        skip_tags: <% $.skip_tags %>\n        trash_output: true\n      on-success:\n        - node_update_passed: <% task().result.returncode = 0 %>\n        - node_update_failed: <% task().result.returncode != 0 %>\n      on-error: node_update_failed\n      publish:\n        output: <% task().result %>\n\n    node_update_passed:\n      on-success: notify_zaqar\n      publish:\n        status: SUCCESS\n        message: Updated nodes - <% $.nodes %>\n\n    node_update_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: Failed to update nodes - <% $.nodes %>, please see the logs.\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.ansible_queue_name %>\n        messages:\n          body:\n            type: tripleo.package_update.v1.update_nodes\n            payload:\n              status: <% $.status %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.package_update.v1.update_nodes", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:04", "namespace": "", "updated_at": null, "scope": "private", "input": "node_user=heat-admin, nodes, playbook, inventory_file, ansible_queue_name=tripleo, module_path=/usr/share/ansible-modules, ansible_extra_env_variables={u'ANSIBLE_HOST_KEY_CHECKING': u'False', u'ANSIBLE_LOG_PATH': u'/var/log/mistral/package_update.log'}, verbosity=1, work_dir=/var/lib/mistral, skip_tags=", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "ec2bc4ed-87c0-43f8-9942-4926f3c94d6a"}, {"definition": "update_converge_plan:\n  description: Take a container and perform the converge for minor update\n\n  input:\n    - container\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    remove_noop:\n      action: tripleo.plan.remove_noop_deploystep\n      input:\n        container: <% $.container %>\n      on-success: send_message\n      on-error: set_update_failed\n\n    set_update_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(remove_noop).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.package_update.v1.update_converge_plan\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.package_update.v1.update_converge_plan", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:04", "namespace": "", "updated_at": null, "scope": "private", "input": "container, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "f7b631c3-321e-4947-9f7f-fc6b639d7565"}, {"definition": "update_roles:\n  description: >\n    takes data in json format validates its contents and persists them in\n    roles_data.yaml, after successful update, templates are regenerated.\n  input:\n    - container\n    - roles\n    - roles_data_file: 'roles_data.yaml'\n    - replace_all: false\n    - queue_name: tripleo\n  tags:\n    - tripleo-common-managed\n  tasks:\n    get_available_roles:\n      workflow: list_available_roles\n      input:\n        container: <% $.container %>\n        queue_name: <% $.queue_name%>\n      publish:\n        available_roles: <% task().result.available_roles %>\n      on-success: validate_input\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    validate_input:\n      description: >\n        validate the format of input (verify that each role in input has the\n        required attributes set. check README in roles directory in t-h-t),\n        validate that roles in input exist in roles directory in t-h-t\n      action: tripleo.plan.validate_roles\n      input:\n        container: <% $.container %>\n        roles: <% $.roles %>\n        available_roles: <% $.available_roles %>\n      on-success: get_network_data\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    get_network_data:\n      workflow: list_networks\n      input:\n        container: <% $.container %>\n        queue_name: <% $.queue_name %>\n      publish:\n        network_data: <% task().result.network_data %>\n      on-success: validate_network_names\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    validate_network_names:\n      description: >\n        validate that Network names assigned to Role exist in\n        network-data.yaml object in Swift container\n      workflow: _validate_networks_from_roles\n      input:\n        container: <% $.container %>\n        defined_networks: <% $.network_data.name %>\n        networks_in_roles: <% $.roles.networks.flatten().distinct() %>\n        queue_name: <% $.queue_name %>\n      on-success: get_current_roles\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result.message %>\n\n    get_current_roles:\n      workflow: list_roles\n      input:\n        container: <% $.container %>\n        roles_data_file: <% $.roles_data_file %>\n        queue_name: <% $.queue_name %>\n      publish:\n          current_roles: <% task().result.roles_data %>\n      on-success: update_roles_data\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    update_roles_data:\n      description: >\n        update roles_data.yaml object in Swift with roles from workflow input\n      action: tripleo.plan.update_roles\n      input:\n        container: <% $.container %>\n        roles: <% $.roles %>\n        current_roles: <% $.current_roles %>\n        replace_all: <% $.replace_all %>\n      publish:\n        updated_roles_data: <% task().result.roles %>\n      on-success: update_roles_data_in_swift\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    update_roles_data_in_swift:\n      description: >\n        update roles_data.yaml object in Swift with data from workflow input\n      action: swift.put_object\n      input:\n        container: <% $.container %>\n        obj: <% $.roles_data_file %>\n        contents: <% yaml_dump($.updated_roles_data) %>\n      on-success: regenerate_templates\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    regenerate_templates:\n      action: tripleo.templates.process container=<% $.container %>\n      on-success: get_updated_roles\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    get_updated_roles:\n      workflow: list_roles\n      input:\n        container: <% $.container %>\n        roles_data_file:  <% $.roles_data_file %>\n      publish:\n          updated_roles: <% task().result.roles_data %>\n          status: SUCCESS\n      on-complete: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.roles.v1.update_roles\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              updated_roles: <% $.get('updated_roles', []) %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.update_roles", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:09", "namespace": "", "updated_at": null, "scope": "private", "input": "container, roles, roles_data_file=roles_data.yaml, replace_all=False, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "02321835-ea49-4fc3-898f-45ae4c9467b3"}, {"definition": "create_default_deployment_plan:\n  description: >\n    This workflow exists to maintain backwards compatibility in pike.  This\n    workflow will likely be removed in queens in favor of create_deployment_plan.\n  input:\n    - container\n    - queue_name: tripleo\n    - generate_passwords: true\n  tags:\n    - tripleo-common-managed\n  tasks:\n    call_create_deployment_plan:\n      workflow: tripleo.plan_management.v1.create_deployment_plan\n      on-success: set_status_success\n      on-error: call_create_deployment_plan_set_status_failed\n      input:\n        container: <% $.container %>\n        queue_name: <% $.queue_name %>\n        generate_passwords: <% $.generate_passwords %>\n        use_default_templates: true\n\n    set_status_success:\n      on-success: notify_zaqar\n      publish:\n        status: SUCCESS\n        message: <% task(call_create_deployment_plan).result %>\n\n    call_create_deployment_plan_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(call_create_deployment_plan).result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.create_default_deployment_plan\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.create_default_deployment_plan", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:09", "namespace": "", "updated_at": null, "scope": "private", "input": "container, queue_name=tripleo, generate_passwords=True", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "0aabcb9e-a397-4fb1-ba04-bd3ca8e280c4"}, {"definition": "validate_roles_and_networks:\n  description: Vaidate that roles and network data are valid\n\n  input:\n    - container: overcloud\n    - roles_data_file: 'roles_data.yaml'\n    - network_data_file: 'network_data.yaml'\n    - queue_name: tripleo\n\n  output:\n    roles_data: <% $.roles_data %>\n    network_data: <% $.network_data %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    validate_network_data:\n      workflow: validate_networks\n      input:\n        container: <% $.container %>\n        network_data_file: <% $.network_data_file %>\n        queue_name: <% $.queue_name %>\n      publish:\n        network_data: <% task().result.network_data %>\n      on-success: validate_roles_data\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    validate_roles_data:\n      workflow: validate_roles\n      input:\n        container: <% $.container %>\n        roles_data_file: <% $.roles_data_file %>\n        queue_name: <% $.queue_name %>\n      publish:\n        roles_data: <% task().result.roles_data %>\n        role_networks_data: <% task().result.roles_data.networks %>\n        networks_in_roles: <% task().result.roles_data.networks.flatten().distinct() %>\n      on-success: validate_roles_and_networks\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    validate_roles_and_networks:\n      workflow: _validate_networks_from_roles\n      input:\n        container: <% $.container %>\n        defined_networks: <% $.network_data.name %>\n        networks_in_roles: <% $.networks_in_roles %>\n        queue_name: <% $.queue_name %>\n      publish:\n        status: SUCCESS\n      on-success: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result.message %>\n      on-error: notify_zaqar\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.validate_roles_and_networks\n            payload:\n              status: <% $.status %>\n              roles_data: <% $.get('roles_data', {}) %>\n              network_data: <% $.get('network_data', {}) %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.validate_roles_and_networks", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:09", "namespace": "", "updated_at": null, "scope": "private", "input": "container=overcloud, roles_data_file=roles_data.yaml, network_data_file=network_data.yaml, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "2fdce1ee-c22e-4d29-83fe-b5516613ef86"}, {"definition": "validate_roles:\n  description: Vaildate roles data exists and is parsable\n\n  input:\n    - container: overcloud\n    - roles_data_file: 'roles_data.yaml'\n    - queue_name: tripleo\n\n  output:\n    roles_data: <% $.roles_data %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_roles_data:\n      workflow: list_roles\n      input:\n        container: <% $.container %>\n        roles_data_file: <% $.roles_data_file %>\n        queue_name: <% $.queue_name %>\n      publish:\n        roles_data: <% task().result.roles_data %>\n        status: SUCCESS\n      on-success: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error:\n        notify_zaqar\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.validate_networks\n            payload:\n              status: <% $.status %>\n              roles_data: <% $.get('roles_data', '') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.validate_roles", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:09", "namespace": "", "updated_at": null, "scope": "private", "input": "container=overcloud, roles_data_file=roles_data.yaml, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "635ec1d3-04e6-4099-a5f9-5997b60eeaa2"}, {"definition": "_validate_networks_from_roles:\n  description: Internal workflow for validating a network exists from a role\n\n  input:\n    - container: overcloud\n    - defined_networks\n    - networks_in_roles\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    validate_network_in_network_data:\n      publish:\n        networks_found: <% $.networks_in_roles.toSet().intersect($.defined_networks.toSet()) %>\n        networks_not_found: <% $.networks_in_roles.toSet().difference($.defined_networks.toSet()) %>\n      on-success:\n        - network_not_found: <% $.networks_not_found %>\n        - notify_zaqar: <% not $.networks_not_found %>\n\n    network_not_found:\n      publish:\n        message: <% \"Some networks in roles are not defined, {0}\".format($.networks_not_found.join(', ')) %>\n        status: FAILED\n      on-success: notify_zaqar\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1._validate_networks_from_role\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1._validate_networks_from_roles", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:09", "namespace": "", "updated_at": null, "scope": "private", "input": "container=overcloud, defined_networks, networks_in_roles, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "afc82b73-5753-413e-963c-55f8590c09c0"}, {"definition": "select_roles:\n  description: >\n    takes a list of role names as input and populates roles_data.yaml in\n    container in Swift with respective roles from 'roles directory'\n  input:\n    - container\n    - role_names\n    - roles_data_file: 'roles_data.yaml'\n    - replace_all: true\n    - queue_name: tripleo\n  tags:\n    - tripleo-common-managed\n  tasks:\n\n    get_available_roles:\n      workflow: list_available_roles\n      input:\n        container: <% $.container %>\n        queue_name: <% $.queue_name %>\n      publish:\n        available_roles: <% task().result.available_roles %>\n      on-success: get_current_roles\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    get_current_roles:\n      workflow: list_roles\n      input:\n        container: <% $.container %>\n        roles_data_file: <% $.roles_data_file %>\n        queue_name: <% $.queue_name %>\n      publish:\n        current_roles: <% task().result.roles_data %>\n      on-success: gather_roles\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    gather_roles:\n      description: >\n        for each role name from the input, check if it exists in\n        roles_data.yaml, if yes, use that role definition, if not, get the\n        role definition from roles directory. Use the gathered roles\n        definitions as input to updateRolesWorkflow - this ensures\n        configuration of the roles which are already in roles_data.yaml\n        will not get overridden by data from roles directory\n      action: tripleo.plan.gather_roles\n      input:\n        role_names: <% $.role_names %>\n        current_roles: <% $.current_roles %>\n        available_roles: <% $.available_roles %>\n      publish:\n        gathered_roles: <% task().result.gathered_roles %>\n      on-success: call_update_roles_workflow\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    call_update_roles_workflow:\n      workflow: update_roles\n      input:\n        container: <% $.container %>\n        roles: <% $.gathered_roles %>\n        roles_data_file: <% $.roles_data_file %>\n        replace_all: <% $.replace_all %>\n        queue_name: <% $.queue_name %>\n      on-complete: notify_zaqar\n      publish:\n        selected_roles: <% task().result.updated_roles %>\n        status: SUCCESS\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.select_roles\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              selected_roles: <% $.get('selected_roles', []) %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.select_roles", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:09", "namespace": "", "updated_at": null, "scope": "private", "input": "container, role_names, roles_data_file=roles_data.yaml, replace_all=True, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "bf0fde29-eefd-4935-8293-f8caf54d337e"}, {"definition": "validate_networks:\n  with-items: network in <% $.network_names_lower.concat($.network_names) %>\n  action: swift.head_object\n  input:\n    container: <% $.container %>\n    obj: network/<% $.network.toLower() %>.yaml\n  publish:\n    status: SUCCESS\n    message: <% task().result %>\n  on-success: notify_zaqar\n  publish-on-error:\n    status: FAILED\n    message: <% task().result %>\n", "name": "tripleo.plan_management.v1.validate_networks", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:09", "namespace": "", "updated_at": null, "scope": "private", "input": "container=overcloud, network_data_file=network_data.yaml, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "f59c88d4-9c49-45a2-bb8c-a72c487caa27"}, {"definition": "list_networks:\n  input:\n    - container: 'overcloud'\n    - network_data_file: 'network_data.yaml'\n    - queue_name: tripleo\n\n  output:\n    network_data: <% $.network_data %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_networks:\n      action: swift.get_object\n      input:\n        container: <% $.container %>\n        obj: <% $.network_data_file %>\n      on-success: notify_zaqar\n      publish:\n        network_data: <% yaml_parse(task().result.last()) %>\n        status: SUCCESS\n        message: <% task().result %>\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.list_networks\n            payload:\n              status: <% $.status %>\n              network_data: <% $.get('network_data', {}) %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.list_networks", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "container=overcloud, network_data_file=network_data.yaml, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "2c4c9800-03eb-4dcd-870f-a081826cf4e6"}, {"definition": "download_logs:\n  description: Creates a tarball with logging data\n  input:\n    - queue_name: tripleo\n    - logging_container: \"tripleo-ui-logs\"\n    - downloads_container: \"tripleo-ui-logs-downloads\"\n    - delete_after: 3600\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    publish_logs:\n      workflow: tripleo.plan_management.v1.publish_ui_logs_to_swift\n      on-success: prepare_log_download\n      on-error: publish_logs_set_status_failed\n\n    prepare_log_download:\n      action: tripleo.logging_to_swift.prepare_log_download\n      input:\n        logging_container: <% $.logging_container %>\n        downloads_container: <% $.downloads_container %>\n        delete_after: <% $.delete_after %>\n      on-success: create_tempurl\n      on-error: download_logs_set_status_failed\n      publish:\n        filename: <% task().result %>\n\n    create_tempurl:\n      action: tripleo.swift.tempurl\n      on-success: set_status_success\n      on-error: create_tempurl_set_status_failed\n      input:\n        container: <% $.downloads_container %>\n        obj: <% $.filename %>\n        valid: 3600\n      publish:\n        tempurl: <% task().result %>\n\n    set_status_success:\n      on-success: notify_zaqar\n      publish:\n        status: SUCCESS\n        message: <% task(create_tempurl).result %>\n        tempurl: <% task(create_tempurl).result %>\n\n    publish_logs_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(publish_logs).result %>\n\n    download_logs_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(prepare_log_download).result %>\n\n    create_tempurl_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(create_tempurl).result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.download_logs\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              tempurl: <% $.get('tempurl', '') %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.download_logs", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "queue_name=tripleo, logging_container=tripleo-ui-logs, downloads_container=tripleo-ui-logs-downloads, delete_after=3600", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "2d037ec7-441f-486c-b4d0-36a9e29c07b6"}, {"definition": "get_deprecated_parameters:\n  description: Gets the list of deprecated parameters in the whole of the plan including nested stack\n  input:\n    - container: overcloud\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_flatten_data:\n      action: tripleo.parameters.get_flatten container=<% $.container %>\n      on-success: get_deprecated_params\n      on-error: set_status_failed_get_flatten_data\n      publish:\n        user_params: <% task().result.environment_parameters %>\n        plan_params: <% task().result.heat_resource_tree.parameters.keys() %>\n        parameter_groups: <% task().result.heat_resource_tree.resources.values().where( $.get('parameter_groups') ).select($.parameter_groups).flatten() %>\n\n    get_deprecated_params:\n      on-success: check_if_user_param_has_deprecated\n      publish:\n        deprecated_params: <% $.parameter_groups.where($.get('label') = 'deprecated').select($.parameters).flatten().distinct() %>\n\n    check_if_user_param_has_deprecated:\n      on-success: get_unused_params\n      publish:\n        deprecated_result: <% let(up => $.user_params) -> $.deprecated_params.select( dict('parameter' => $, 'deprecated' => true, 'user_defined' => $up.keys().contains($)) ) %>\n\n    # Get the list of parameters, which are defined by user via environment files's parameter_default, but not part of the plan definition\n    # It may be possible that the parameter will be used by a service, but the service is not part of the plan.\n    # In such cases, the parameter will be reported as unused, care should be take to understand whether it is really unused or not.\n    get_unused_params:\n      on-success: send_message\n      publish:\n        unused_params: <% let(plan_params => $.plan_params) ->  $.user_params.keys().where( not $plan_params.contains($) ) %>\n\n    set_status_failed_get_flatten_data:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(get_flatten_data).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.get_deprecated_parameters\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              deprecated: <% $.get('deprecated_result', []) %>\n              unused: <% $.get('unused_params', []) %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.get_deprecated_parameters", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "container=overcloud, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "38aa948e-e378-4a6f-bb13-01254a73cf9c"}, {"definition": "publish_ui_logs_to_swift:\n  description: >\n    This workflow drains a zaqar queue, and publish its messages into a log\n    file in swift.  This workflow is called by cron trigger.\n\n  input:\n    - logging_queue_name: tripleo-ui-logging\n    - logging_container: tripleo-ui-logs\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    # We're using a NoOp action to start the workflow.  The recursive nature\n    # of the workflow means that Mistral will refuse to execute it because it\n    # doesn't know where to begin.\n    start:\n      on-success: get_messages\n\n    get_messages:\n      action: zaqar.claim_messages\n      on-success:\n        - format_messages: <% task().result.len() > 0 %>\n      input:\n        queue_name: <% $.logging_queue_name %>\n        ttl: 60\n        grace: 60\n      publish:\n        status: SUCCESS\n        messages: <% task().result %>\n        message_ids: <% task().result.select($._id) %>\n\n    format_messages:\n      action: tripleo.logging_to_swift.format_messages\n      on-success: upload_to_swift\n      input:\n        messages: <% $.messages %>\n      publish:\n        status: SUCCESS\n        formatted_messages: <% task().result %>\n\n    upload_to_swift:\n      action: tripleo.logging_to_swift.publish_ui_log_to_swift\n      on-success: delete_messages\n      input:\n        logging_data: <% $.formatted_messages %>\n        logging_container: <% $.logging_container %>\n      publish:\n        status: SUCCESS\n\n    delete_messages:\n      action: zaqar.delete_messages\n      on-success: get_messages\n      input:\n        queue_name: <% $.logging_queue_name %>\n        messages: <% $.message_ids %>\n      publish:\n        status: SUCCESS\n", "name": "tripleo.plan_management.v1.publish_ui_logs_to_swift", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "logging_queue_name=tripleo-ui-logging, logging_container=tripleo-ui-logs", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "392c6fbb-b06e-4734-97e1-aae7ad23345b"}, {"definition": "list_available_networks:\n  input:\n    - container\n    - queue_name: tripleo\n\n  output:\n    available_networks: <% $.available_networks %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_network_file_names:\n      action: swift.get_container\n      input:\n        container: <% $.container %>\n      publish:\n        network_names: <% task().result[1].where($.name.startsWith('networks/')).where($.name.endsWith('.yaml')).name %>\n      on-success: get_network_files\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    get_network_files:\n      with-items: network_name in <% $.network_names %>\n      action: swift.get_object\n      on-success: transform_output\n      on-error: notify_zaqar\n      input:\n        container: <% $.container %>\n        obj: <% $.network_name %>\n      publish:\n        status: SUCCESS\n        available_yaml_networks: <% task().result.select($[1]) %>\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    transform_output:\n      publish:\n        status: SUCCESS\n        available_networks: <% yaml_parse($.available_yaml_networks.join(\"\\n\")) %>\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-complete: notify_zaqar\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.list_available_networks\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              available_networks: <% $.get('available_networks', []) %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.list_available_networks", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "container, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "5ffdc32b-a9ce-4d5b-a9b9-c4d05b5afa96"}, {"definition": "list_roles:\n  description: Retrieve the roles_data.yaml and return a usable object\n\n  input:\n    - container: overcloud\n    - roles_data_file: 'roles_data.yaml'\n    - queue_name: tripleo\n\n  output:\n    roles_data: <% $.roles_data %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_roles_data:\n      action: swift.get_object\n      input:\n        container: <% $.container %>\n        obj: <% $.roles_data_file %>\n      publish:\n        roles_data: <% yaml_parse(task().result.last()) %>\n        status: SUCCESS\n      on-success: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.list_roles\n            payload:\n              status: <% $.status %>\n              roles_data: <% $.get('roles_data', {}) %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.list_roles", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "container=overcloud, roles_data_file=roles_data.yaml, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "8fefde13-71cf-4010-a99d-f9e60188c4b3"}, {"definition": "delete_node:\n  description: deletes given overcloud nodes and updates the stack\n\n  input:\n    - container\n    - nodes\n    - timeout: 240\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    delete_node:\n      action: tripleo.scale.delete_node nodes=<% $.nodes %> timeout=<% $.timeout %> container=<% $.container %>\n      on-success: wait_for_stack_in_progress\n      on-error: set_delete_node_failed\n\n    set_delete_node_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(delete_node).result %>\n\n    wait_for_stack_in_progress:\n      workflow: tripleo.stack.v1.wait_for_stack_in_progress stack=<% $.container %>\n      on-success: wait_for_stack_complete\n      on-error: wait_for_stack_in_progress_failed\n\n    wait_for_stack_in_progress_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(wait_for_stack_in_progress).result %>\n\n    wait_for_stack_complete:\n      workflow: tripleo.stack.v1.wait_for_stack_complete_or_failed stack=<% $.container %>\n      on-success: send_message\n      on-error: wait_for_stack_complete_failed\n\n    wait_for_stack_complete_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(wait_for_stack_complete).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.scale.v1.delete_node\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.scale.v1.delete_node", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "container, nodes, timeout=240, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "90a3a9de-f75d-47ed-a5a0-264aec4a0ba9"}, {"definition": "update_deployment_plan:\n  input:\n    - container\n    - source_url: null\n    - queue_name: tripleo\n    - generate_passwords: true\n    - plan_environment: null\n  tags:\n    - tripleo-common-managed\n  tasks:\n    templates_source_check:\n      on-success:\n        - update_plan: <% $.source_url = null %>\n        - clone_git_repo: <% $.source_url != null %>\n\n    clone_git_repo:\n      action: tripleo.git.clone container=<% $.container %> url=<% $.source_url %>\n      on-success: upload_templates_directory\n      on-error: clone_git_repo_set_status_failed\n\n    upload_templates_directory:\n      action: tripleo.templates.upload container=<% $.container %> templates_path=<% task(clone_git_repo).result %>\n      on-success: create_swift_rings_backup_plan\n      on-complete: cleanup_temporary_files\n      on-error: upload_templates_directory_set_status_failed\n\n    cleanup_temporary_files:\n      action: tripleo.git.clean container=<% $.container %>\n\n    create_swift_rings_backup_plan:\n      workflow: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan\n      on-success: update_plan\n      on-error: create_swift_rings_backup_plan_set_status_failed\n      input:\n        container: <% $.container %>\n        queue_name: <% $.queue_name %>\n        use_default_templates: true\n\n    update_plan:\n      on-success:\n        - ensure_passwords_exist: <% $.generate_passwords = true %>\n        - container_images_prepare: <% $.generate_passwords != true %>\n\n    ensure_passwords_exist:\n      action: tripleo.parameters.generate_passwords container=<% $.container %>\n      on-success: container_images_prepare\n      on-error: ensure_passwords_exist_set_status_failed\n\n    container_images_prepare:\n      description: >\n        Populate all container image parameters with default values.\n      action: tripleo.container_images.prepare container=<% $.container %>\n      on-success: process_templates\n      on-error: container_images_prepare_set_status_failed\n\n    process_templates:\n      action: tripleo.templates.process container=<% $.container %>\n      on-success:\n        - set_status_success: <% $.plan_environment = null %>\n        - upload_plan_environment: <% $.plan_environment != null %>\n      on-error: process_templates_set_status_failed\n\n    upload_plan_environment:\n      action: tripleo.templates.upload_plan_environment container=<% $.container %> plan_environment=<% $.plan_environment %>\n      on-success: set_status_success\n      on-error: process_templates_set_status_failed\n\n    set_status_success:\n      on-success: notify_zaqar\n      publish:\n        status: SUCCESS\n        message: 'Plan updated.'\n\n    create_swift_rings_backup_plan_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(create_swift_rings_backup_plan).result %>\n\n    clone_git_repo_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(clone_git_repo).result %>\n\n    upload_templates_directory_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(upload_templates_directory).result %>\n\n    process_templates_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(process_templates).result %>\n\n    ensure_passwords_exist_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(ensure_passwords_exist).result %>\n\n    container_images_prepare_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(container_images_prepare).result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.update_deployment_plan\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.update_deployment_plan", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "container, source_url=None, queue_name=tripleo, generate_passwords=True, plan_environment=None", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "91bea41b-448f-467f-b7e3-6967d3c62881"}, {"definition": "export_deployment_plan:\n  description: Creates an export tarball for a given plan\n  input:\n    - plan\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    export_plan:\n      action: tripleo.plan.export\n      input:\n        plan: <% $.plan %>\n        delete_after: 3600\n        exports_container: \"plan-exports\"\n      on-success: create_tempurl\n      on-error: export_plan_set_status_failed\n\n    create_tempurl:\n      action: tripleo.swift.tempurl\n      on-success: set_status_success\n      on-error: create_tempurl_set_status_failed\n      input:\n        container: \"plan-exports\"\n        obj: \"<% $.plan %>.tar.gz\"\n        valid: 3600\n\n    set_status_success:\n      on-success: notify_zaqar\n      publish:\n        status: SUCCESS\n        message: <% task(create_tempurl).result %>\n        tempurl: <% task(create_tempurl).result %>\n\n    export_plan_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(export_plan).result %>\n\n    create_tempurl_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(create_tempurl).result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.export_deployment_plan\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              tempurl: <% $.get('tempurl', '') %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.export_deployment_plan", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "plan, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "9c7fc44c-1322-4f8c-a7c9-4fadc020a46f"}, {"definition": "delete_deployment_plan:\n  description: >\n    Deletes a plan by deleting the container matching plan_name. It will\n    not delete the plan if a stack exists with the same name.\n\n  tags:\n    - tripleo-common-managed\n\n  input:\n    - container: overcloud\n    - queue_name: tripleo\n\n  tasks:\n    delete_plan:\n      action: tripleo.plan.delete container=<% $.container %>\n      on-complete: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      publish:\n        status: SUCCESS\n        message: <% task().result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.delete_deployment_plan\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.delete_deployment_plan", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "container=overcloud, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "a43e1828-382f-4905-91c3-957a55d19d43"}, {"definition": "list_available_roles:\n  input:\n    - container: overcloud\n    - queue_name: tripleo\n\n  output:\n    available_roles: <% $.available_roles %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_role_file_names:\n      action: swift.get_container\n      input:\n        container: <% $.container %>\n      publish:\n        role_names: <% task().result[1].where($.name.startsWith('roles/')).where($.name.endsWith('.yaml')).name %>\n      on-success: get_role_files\n      on-error: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    get_role_files:\n      with-items: role_name in <% $.role_names %>\n      action: swift.get_object\n      on-success: transform_output\n      on-error: notify_zaqar\n      input:\n        container: <% $.container %>\n        obj: <% $.role_name %>\n      publish:\n        status: SUCCESS\n        available_yaml_roles: <% task().result.select($[1]) %>\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    transform_output:\n      publish:\n        status: SUCCESS\n        available_roles: <% yaml_parse($.available_yaml_roles.join(\"\\n\")) %>\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-complete: notify_zaqar\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.list_available_roles\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              available_roles: <% $.get('available_roles', []) %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.list_available_roles", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "container=overcloud, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "b6ee165b-98e1-4542-a984-c287b32b9e69"}, {"definition": "validate_network_files:\n  description: Validate network files exist\n  input:\n    - container: overcloud\n    - network_data\n    - queue_name: tripleo\n\n  output:\n    network_data: <% $.network_data %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_network_names:\n      publish:\n        network_names_lower: <% $.network_data.where($.containsKey('name_lower')).name_lower %>\n        network_names: <% $.network_data.where(not $.containsKey('name_lower')).name %>\n      on-success: validate_networks\n\n    validate_networks:\n      with-items: network in <% $.network_names_lower.concat($.network_names) %>\n      action: swift.head_object\n      input:\n        container: <% $.container %>\n        obj: network/<% $.network.toLower() %>.yaml\n      publish:\n        status: SUCCESS\n        message: <% task().result %>\n      on-success: notify_zaqar\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.validate_network_files\n            payload:\n              status: <% $.status %>\n              message: <% $.message %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.validate_network_files", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "container=overcloud, network_data, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "ccf12840-bb89-4f79-9d7a-d837244ace4c"}, {"definition": "skydive_install:\n  # allows for additional extra_vars via workflow input\n  input:\n    - ansible_playbook_verbosity: 0\n    - ansible_extra_env_variables:\n        ANSIBLE_ROLES_PATH: /usr/share/skydive-ansible/roles/\n        ANSIBLE_RETRY_FILES_ENABLED: 'False'\n        ANSIBLE_LOG_PATH: /var/log/mistral/skydive-install-workflow.log\n        ANSIBLE_HOST_KEY_CHECKING: 'False'\n    - skydive_ansible_extra_vars: {}\n    - skydive_ansible_playbook: /usr/share/skydive-ansible/playbook.yml.sample\n  tags:\n    - tripleo-common-managed\n  tasks:\n    set_blacklisted_ips:\n      publish:\n        blacklisted_ips: <% env().get('blacklisted_ip_addresses', []) %>\n      on-success: set_ip_lists\n    set_ip_lists:\n      publish:\n        agent_ips: <% let(root => $) -> env().get('service_ips', {}).get('skydive_agent_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n        analyzer_ips: <% let(root => $) -> env().get('service_ips', {}).get('skydive_analyzer_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n      on-success: enable_ssh_admin\n    enable_ssh_admin:\n      workflow: tripleo.access.v1.enable_ssh_admin\n      input:\n        ssh_servers: <% ($.agent_ips + $.analyzer_ips).toSet() %>\n      on-success: get_private_key\n    get_private_key:\n      action: tripleo.validations.get_privkey\n      publish:\n        private_key: <% task().result %>\n      on-success: set_fork_count\n    set_fork_count:\n      publish: # unique list of all IPs: make each list a set, take unions and count\n        fork_count: <% min($.agent_ips.toSet().union($.analyzer_ips.toSet()).count(), 100) %> # don't use >100 forks\n      on-success: set_role_vars\n    set_role_vars:\n      publish:\n        # NOTE(sbaubeau): collect role settings from all tht roles\n        agent_vars: <% env().get('role_merged_configs', {}).values().select($.get('skydive_agent_ansible_vars', {})).aggregate($1 + $2) %>\n        analyzer_vars: <% env().get('role_merged_configs', {}).values().select($.get('skydive_analyzer_ansible_vars', {})).aggregate($1 + $2) %>\n      on-success: build_extra_vars\n    build_extra_vars:\n      publish:\n        # NOTE(sbaubeau): merge vars from all ansible roles\n        extra_vars: <% $.agent_vars + $.analyzer_vars + $.skydive_ansible_extra_vars %>\n      on-success: skydive_install\n    skydive_install:\n      action: tripleo.ansible-playbook\n      input:\n        inventory:\n          agents:\n            hosts: <% $.agent_ips.toDict($, {}) %>\n          analyzers:\n            hosts: <% $.analyzer_ips.toDict($, {}) %>\n        playbook: <% $.skydive_ansible_playbook %>\n        remote_user: tripleo-admin\n        become: true\n        become_user: root\n        verbosity: <% $.ansible_playbook_verbosity %>\n        forks: <% $.fork_count %>\n        ssh_private_key: <% $.private_key %>\n        extra_env_variables: <% $.ansible_extra_env_variables %>\n        extra_vars: <% $.extra_vars %>\n      publish:\n        output: <% task().result %>\n", "name": "tripleo.skydive_ansible.v1.skydive_install", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "ansible_playbook_verbosity=0, ansible_extra_env_variables={u'ANSIBLE_ROLES_PATH': u'/usr/share/skydive-ansible/roles/', u'ANSIBLE_HOST_KEY_CHECKING': u'False', u'ANSIBLE_RETRY_FILES_ENABLED': u'False', u'ANSIBLE_LOG_PATH': u'/var/log/mistral/skydive-install-workflow.log'}, skydive_ansible_extra_vars={}, skydive_ansible_playbook=/usr/share/skydive-ansible/playbook.yml.sample", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "d08d029a-cfe4-41b8-8d44-71bc521f416a"}, {"definition": "create_deployment_plan:\n  description: >\n    This workflow provides the capability to create a deployment plan using\n    the default heat templates provided in a standard TripleO undercloud\n    deployment, heat templates contained in an external git repository, or a\n    swift container that already contains templates.\n  input:\n    - container\n    - source_url: null\n    - queue_name: tripleo\n    - generate_passwords: true\n    - use_default_templates: false\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    container_required_check:\n      description: >\n        If using the default templates or importing templates from a git\n        repository, a new container needs to be created.  If using an existing\n        container containing templates, skip straight to create_plan.\n      on-success:\n        - verify_container_doesnt_exist: <% $.use_default_templates or $.source_url %>\n        - create_plan: <% $.use_default_templates = false and $.source_url = null %>\n\n    verify_container_doesnt_exist:\n      action: swift.head_container container=<% $.container %>\n      on-success: notify_zaqar\n      on-error: create_container\n      publish:\n        status: FAILED\n        message: \"Unable to create plan. The Swift container already exists\"\n\n    create_container:\n      action: tripleo.plan.create_container container=<% $.container %>\n      on-success: templates_source_check\n      on-error: create_container_set_status_failed\n\n    cleanup_temporary_files:\n      action: tripleo.git.clean container=<% $.container %>\n\n    templates_source_check:\n      on-success:\n        - upload_default_templates: <% $.use_default_templates = true %>\n        - clone_git_repo: <% $.source_url != null %>\n\n    clone_git_repo:\n      action: tripleo.git.clone container=<% $.container %> url=<% $.source_url %>\n      on-success: upload_templates_directory\n      on-error: clone_git_repo_set_status_failed\n\n    upload_templates_directory:\n      action: tripleo.templates.upload container=<% $.container %> templates_path=<% task(clone_git_repo).result %>\n      on-success: create_plan\n      on-complete: cleanup_temporary_files\n      on-error: upload_templates_directory_set_status_failed\n\n    upload_default_templates:\n      action: tripleo.templates.upload container=<% $.container %>\n      on-success: create_plan\n      on-error: upload_to_container_set_status_failed\n\n    create_plan:\n      on-success:\n        - ensure_passwords_exist: <% $.generate_passwords = true %>\n        - add_root_stack_name: <% $.generate_passwords != true %>\n\n    ensure_passwords_exist:\n      action: tripleo.parameters.generate_passwords container=<% $.container %>\n      on-success: add_root_stack_name\n      on-error: ensure_passwords_exist_set_status_failed\n\n    add_root_stack_name:\n      action: tripleo.parameters.update\n      input:\n        container: <% $.container %>\n        parameters:\n          RootStackName: <% $.container %>\n      on-success: container_images_prepare\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n      on-error: notify_zaqar\n\n    container_images_prepare:\n      description: >\n        Populate all container image parameters with default values.\n      action: tripleo.container_images.prepare container=<% $.container %>\n      on-success: process_templates\n      on-error: container_images_prepare_set_status_failed\n\n    process_templates:\n      action: tripleo.templates.process container=<% $.container %>\n      on-success: set_status_success\n      on-error: process_templates_set_status_failed\n\n    set_status_success:\n      on-success: notify_zaqar\n      publish:\n        status: SUCCESS\n        message: 'Plan created.'\n\n    create_container_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(create_container).result %>\n\n    clone_git_repo_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(clone_git_repo).result %>\n\n    upload_templates_directory_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(upload_templates_directory).result %>\n\n    upload_to_container_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(upload_default_templates).result %>\n\n    ensure_passwords_exist_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(ensure_passwords_exist).result %>\n\n    process_templates_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(process_templates).result %>\n\n    container_images_prepare_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(container_images_prepare).result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.create_deployment_plan\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.create_deployment_plan", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "container, source_url=None, queue_name=tripleo, generate_passwords=True, use_default_templates=False", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "d6ca81c4-4342-4b70-a699-a9e5a298f80c"}, {"definition": "get_passwords:\n  description: Retrieves passwords for a given plan\n  input:\n    - container\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    verify_container_exists:\n      action: swift.head_container container=<% $.container %>\n      on-success: get_environment_passwords\n      on-error: verify_container_set_status_failed\n\n    get_environment_passwords:\n      action: tripleo.parameters.get_passwords container=<% $.container %>\n      on-success: get_passwords_set_status_success\n      on-error: get_passwords_set_status_failed\n\n    get_passwords_set_status_success:\n      on-success: notify_zaqar\n      publish:\n        status: SUCCESS\n        message: <% task(get_environment_passwords).result %>\n\n    get_passwords_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(get_environment_passwords).result %>\n\n    verify_container_set_status_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(verify_container_exists).result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.plan_management.v1.get_passwords\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1.get_passwords", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:10", "namespace": "", "updated_at": null, "scope": "private", "input": "container, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "fdf84ef9-187d-4046-8d55-1328a8097ba3"}, {"definition": "delete_stack:\n  input:\n    - stack\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    delete_the_stack:\n      action: heat.stacks_delete stack_id=<% $.stack %>\n      on-success: wait_for_stack_does_not_exist\n      on-error: delete_the_stack_failed\n\n    delete_the_stack_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(delete_the_stack).result %>\n\n    wait_for_stack_does_not_exist:\n      workflow: tripleo.stack.v1.wait_for_stack_does_not_exist stack=<% $.stack %>\n      on-success: send_message\n      on-error: wait_for_stack_does_not_exist_failed\n\n    wait_for_stack_does_not_exist_failed:\n      on-success: send_message\n      publish:\n        status: FAILED\n        message: <% task(wait_for_stack_does_not_exist).result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.scale.v1.delete_stack\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.stack.v1.delete_stack", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:11", "namespace": "", "updated_at": null, "scope": "private", "input": "stack, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "3443be8a-2375-49be-a0f1-2ec2d6063e81"}, {"definition": "wait_for_stack_in_progress:\n  input:\n    - stack\n    - timeout: 600 # 10 minutes. Should not take much longer for a stack to transition to IN_PROGRESS\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    wait_for_stack_status:\n      action: heat.stacks_get stack_id=<% $.stack %>\n      timeout: <% $.timeout %>\n      retry:\n        delay: 15\n        count: <% $.timeout / 15 %>\n        continue-on: <% task().result.stack_status in ['CREATE_COMPLETE', 'CREATE_FAILED', 'UPDATE_COMPLETE', 'UPDATE_FAILED', 'DELETE_FAILED'] %>\n", "name": "tripleo.stack.v1.wait_for_stack_in_progress", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:11", "namespace": "", "updated_at": null, "scope": "private", "input": "stack, timeout=600", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "6b1b193e-16ad-4a73-a573-711dc894aea8"}, {"definition": "wait_for_stack_does_not_exist:\n  input:\n    - stack\n    - timeout: 3600\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    wait_for_stack_does_not_exist:\n      action: heat.stacks_list\n      timeout: <% $.timeout %>\n      retry:\n        delay: 15\n        count: <% $.timeout / 15 %>\n        continue-on: <% $.stack in task(wait_for_stack_does_not_exist).result.select([$.stack_name, $.id]).flatten() %>\n", "name": "tripleo.stack.v1.wait_for_stack_does_not_exist", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:11", "namespace": "", "updated_at": null, "scope": "private", "input": "stack, timeout=3600", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "9b1d3937-b8c7-4d40-b35d-6848a13a4cc3"}, {"definition": "wait_for_stack_complete_or_failed:\n  input:\n    - stack\n    - timeout: 14400 # 4 hours. Default timeout of stack deployment\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    wait_for_stack_status:\n      action: heat.stacks_get stack_id=<% $.stack %>\n      timeout: <% $.timeout %>\n      retry:\n        delay: 15\n        count: <% $.timeout / 15 %>\n        continue-on: <% task().result.stack_status in ['CREATE_IN_PROGRESS', 'UPDATE_IN_PROGRESS', 'DELETE_IN_PROGRESS'] %>\n", "name": "tripleo.stack.v1.wait_for_stack_complete_or_failed", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:11", "namespace": "", "updated_at": null, "scope": "private", "input": "stack, timeout=14400", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "f9ef305d-fcf3-40fe-83da-d91afdcb3f33"}, {"definition": "collect_logs:\n  description: >\n    This workflow runs sosreport on the servers where their names match the\n    provided server_name input. The logs are stored in the provided sos_dir.\n  input:\n    - server_name\n    - sos_dir: /var/tmp/tripleo-sos\n    - sos_options: boot,cluster,hardware,kernel,memory,nfs,openstack,packagemanager,performance,services,storage,system,webserver,virt\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    collect_logs_on_servers:\n      workflow: tripleo.deployment.v1.deploy_on_servers\n      on-success: send_message\n      on-error: set_collect_logs_on_servers_failed\n      input:\n        server_name: <% $.server_name %>\n        config_name: 'run_sosreport'\n        config: |\n          #!/bin/bash\n          mkdir -p <% $.sos_dir %>\n          sosreport --batch \\\n          -p <% $.sos_options %> \\\n          --tmp-dir <% $.sos_dir %>\n\n    set_collect_logs_on_servers_failed:\n        on-complete:\n         - send_message\n        publish:\n          type: tripleo.deployment.v1.fetch_logs\n          status: FAILED\n          message: <% task().result %>\n\n    # status messaging\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: <% $.get('type', 'tripleo.support.v1.collect_logs') %>\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = 'FAILED' %>\n", "name": "tripleo.support.v1.collect_logs", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:12", "namespace": "", "updated_at": null, "scope": "private", "input": "server_name, sos_dir=/var/tmp/tripleo-sos, sos_options=boot,cluster,hardware,kernel,memory,nfs,openstack,packagemanager,performance,services,storage,system,webserver,virt, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "0780e67b-a348-45bb-ab9d-d00e486fd442"}, {"definition": "delete_container:\n  description: >\n    This workflow deletes all the objects in a provided swift container and\n    then removes the container itself from the undercloud.\n  input:\n    - container\n    - concurrency: 5\n    - timeout: 900\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    # actions\n    check_container:\n      action: swift.head_container container=<% $.container %>\n      on-success: list_objects\n      on-error: set_check_container_failure\n\n    set_check_container_failure:\n      on-complete: send_message\n      publish:\n        status: FAILED\n        type: tripleo.support.v1.delete_container.check_container\n        message: <% task(check_container).result %>\n\n    list_objects:\n      action: swift.get_container container=<% $.container %>\n      on-success: delete_objects\n      on-error: set_list_objects_failure\n      publish:\n        log_objects: <% task().result[1] %>\n\n    set_list_objects_failure:\n      on-complete: send_message\n      publish:\n        status: FAILED\n        type: tripleo.support.v1.delete_container.list_objects\n        message: <% task(list_objects).result %>\n\n    delete_objects:\n      action: swift.delete_object\n      concurrency: <% $.concurrency %>\n      timeout: <% $.timeout %>\n      with-items: object in <% $.log_objects %>\n      input:\n        container: <% $.container %>\n        obj: <% $.object.name %>\n      on-success: remove_container\n      on-error: set_delete_objects_failure\n\n    set_delete_objects_failure:\n      on-complete: send_message\n      publish:\n        status: FAILED\n        type: tripleo.support.v1.delete_container.delete_objects\n        message: <% task(delete_objects).result %>\n\n    remove_container:\n      action: swift.delete_container container=<% $.container %>\n      on-success: send_message\n      on-error: set_remove_container_failure\n\n    set_remove_container_failure:\n      on-complete: send_message\n      publish:\n        status: FAILED\n        type: tripleo.support.v1.delete_container.remove_container\n        message: <% task(remove_container).result %>\n\n    # status messaging\n    send_message:\n      action: zaqar.queue_post\n      wait-before: 5\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: <% $.get('type', 'tripleo.support.v1.delete_container') %>\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = 'FAILED' %>\n", "name": "tripleo.support.v1.delete_container", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:12", "namespace": "", "updated_at": null, "scope": "private", "input": "container, concurrency=5, timeout=900, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "4a912a7e-c16a-44e5-9efb-503e1c8435c6"}, {"definition": "upload_logs:\n  description: >\n    This workflow uploads the sosreport files stored in the provide sos_dir\n    on the provided host (server_uuid) to a swift container on the undercloud\n  input:\n    - server_uuid\n    - server_name\n    - container\n    - sos_dir: /var/tmp/tripleo-sos\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    # actions\n    get_swift_information:\n      action: tripleo.swift.swift_information\n      on-success: do_log_upload\n      on-error: set_get_swift_information_failed\n      input:\n        container: <% $.container %>\n      publish:\n        container_url: <% task().result.container_url %>\n        auth_key: <% task().result.auth_key %>\n\n    set_get_swift_information_failed:\n      on-complete:\n        - send_message\n      publish:\n        status: FAILED\n        message: <% task(get_swift_information).result %>\n\n    do_log_upload:\n      action: tripleo.deployment.config\n      on-success: send_message\n      on-error: set_do_log_upload_failed\n      input:\n        server_id: <% $.server_uuid %>\n        name: \"upload_logs\"\n        config: |\n          #!/bin/bash\n          CONTAINER_URL=\"<% $.container_url %>\"\n          TOKEN=\"<% $.auth_key %>\"\n          SOS_DIR=\"<% $.sos_dir %>\"\n          for FILE in $(find $SOS_DIR -type f); do\n            FILENAME=$(basename $FILE)\n            curl -X PUT -i -H \"X-Auth-Token: $TOKEN\" -T $FILE $CONTAINER_URL/$FILENAME\n            if [ $? -eq 0 ]; then\n              rm -f $FILE\n            fi\n          done\n        group: \"script\"\n      publish:\n        message: \"Uploaded logs from <% $.server_name %>\"\n\n    set_do_log_upload_failed:\n      on-complete:\n        - send_message\n      publish:\n        status: FAILED\n        message: <% tag(do_log_upload).result %>\n\n    # status messaging\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: <% $.get('type', 'tripleo.support.v1.upload_logs') %>\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = 'FAILED' %>\n", "name": "tripleo.support.v1.upload_logs", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:12", "namespace": "", "updated_at": null, "scope": "private", "input": "server_uuid, server_name, container, sos_dir=/var/tmp/tripleo-sos, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "886c5d5e-00a8-4d27-871f-2a6598265ccd"}, {"definition": "fetch_logs:\n  description: >\n    This workflow creates a container on the undercloud, executes the log\n    collection on the servers whose names match the provided server_name, and\n    executes the log upload process on all the servers to the container on\n    the undercloud.\n  input:\n    - server_name\n    - container\n    - concurrency: 5\n    - timeout: 1800\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    # actions\n    create_container:\n      workflow: tripleo.support.v1.create_container\n      on-success: get_servers_matching\n      on-error: set_create_container_failed\n      input:\n        container: <% $.container %>\n        queue_name: <% $.queue_name %>\n\n    set_create_container_failed:\n        on-complete: send_message\n        publish:\n          type: tripleo.support.v1.fetch_logs.create_container\n          status: FAILED\n          message: <% task(create_container).result %>\n\n    get_servers_matching:\n      action: nova.servers_list\n      on-success: collect_logs_on_servers\n      publish:\n        servers_with_name: <% task().result._info.where($.name.indexOf(execution().input.server_name) > -1) %>\n\n    collect_logs_on_servers:\n      workflow: tripleo.support.v1.collect_logs\n      timeout: <% $.timeout %>\n      on-success: upload_logs_on_servers\n      on-error: set_collect_logs_on_servers_failed\n      input:\n        server_name: <% $.server_name %>\n        queue_name: <% $.queue_name %>\n\n    set_collect_logs_on_servers_failed:\n        on-complete: send_message\n        publish:\n          type: tripleo.support.v1.fetch_logs.collect_logs_on_servers\n          status: FAILED\n          message: <% task(collect_logs_on_servers).result %>\n\n    upload_logs_on_servers:\n      on-success: send_message\n      on-error: set_upload_logs_on_servers_failed\n      with-items: server in <% $.servers_with_name %>\n      concurrency: <% $.concurrency %>\n      workflow: tripleo.support.v1.upload_logs\n      input:\n        server_name: <% $.server.name %>\n        server_uuid: <% $.server.id %>\n        container: <% $.container %>\n        queue_name: <% $.queue_name %>\n\n    set_upload_logs_on_servers_failed:\n        on-complete: send_message\n        publish:\n          type: tripleo.support.v1.fetch_logs.upload_logs\n          status: FAILED\n          message: <% task(upload_logs_on_servers).result %>\n\n    # status messaging\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: <% $.get('type', 'tripleo.support.v1.fetch_logs') %>\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = 'FAILED' %>\n", "name": "tripleo.support.v1.fetch_logs", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:12", "namespace": "", "updated_at": null, "scope": "private", "input": "server_name, container, concurrency=5, timeout=1800, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "b9ed892e-2c86-4120-8abe-ca41fdd331fc"}, {"definition": "create_container:\n  description: >\n    This work flow is used to check if the container exists and creates it\n    if it does not exist.\n  input:\n    - container\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    check_container:\n      action: swift.head_container container=<% $.container %>\n      on-success: send_message\n      on-error: create_container\n\n    create_container:\n      action: swift.put_container\n      input:\n        container: <% $.container %>\n        headers:\n          x-container-meta-usage-tripleo: support\n      on-success: send_message\n      on-error: set_create_container_failed\n\n    set_create_container_failed:\n      on-complete:\n        - send_message\n      publish:\n        type: tripleo.support.v1.create_container.create_container\n        status: FAILED\n        message: <% task(create_container).result %>\n\n    # status messaging\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: <% $.get('type', 'tripleo.support.v1.create_container') %>\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = 'FAILED' %>\n", "name": "tripleo.support.v1.create_container", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:12", "namespace": "", "updated_at": null, "scope": "private", "input": "container, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "d843d39e-df34-4301-bc07-237b3e307803"}, {"definition": "rebalance:\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    get_private_key:\n      action: tripleo.validations.get_privkey\n      on-success: deploy_rings\n\n    deploy_rings:\n      action: tripleo.ansible-playbook\n      publish:\n        output: <% task().result %>\n      input:\n        ssh_private_key: <% task(get_private_key).result %>\n        ssh_common_args: '-o StrictHostKeyChecking=no'\n        ssh_extra_args: '-o UserKnownHostsFile=/dev/null'\n        verbosity: 1\n        remote_user: heat-admin\n        become: true\n        become_user: root\n        playbook: /usr/share/tripleo-common/playbooks/swift_ring_rebalance.yaml\n        inventory: /usr/bin/tripleo-ansible-inventory\n        use_openstack_credentials: true\n", "name": "tripleo.swift_ring.v1.rebalance", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:12", "namespace": "", "updated_at": null, "scope": "private", "input": "", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "dfc079f0-e2d6-4bac-b0a8-0aa80227f8fa"}, {"definition": "create_swift_rings_backup_container_plan:\n  description: >\n    This plan ensures existence of container for Swift Rings backup.\n  input:\n    - container\n    - queue_name: tripleo\n  tags:\n    - tripleo-common-managed\n  tasks:\n\n    swift_rings_container:\n      publish:\n        swift_rings_container: \"<% $.container %>-swift-rings\"\n        swift_rings_tar: \"swift-rings.tar.gz\"\n      on-complete: check_container\n\n    check_container:\n      action: swift.head_container container=<% $.swift_rings_container %>\n      on-success: get_tempurl\n      on-error: create_container\n\n    create_container:\n      action: swift.put_container container=<% $.swift_rings_container %>\n      on-error: set_create_container_failed\n      on-success: get_tempurl\n\n    get_tempurl:\n      action: tripleo.swift.tempurl\n      on-success: set_get_tempurl\n      input:\n        container: <% $.swift_rings_container %>\n        obj: <% $.swift_rings_tar %>\n\n    set_get_tempurl:\n      action: tripleo.parameters.update\n      input:\n        parameters:\n          SwiftRingGetTempurl: <% task(get_tempurl).result %>\n        container: <% $.container %>\n      on-success: put_tempurl\n\n    put_tempurl:\n      action: tripleo.swift.tempurl\n      on-success: set_put_tempurl\n      input:\n        container: <% $.swift_rings_container %>\n        obj: <% $.swift_rings_tar %>\n        method: \"PUT\"\n\n    set_put_tempurl:\n      action: tripleo.parameters.update\n      input:\n        parameters:\n          SwiftRingPutTempurl: <% task(put_tempurl).result %>\n        container: <% $.container %>\n      on-success: set_status_success\n      on-error: set_put_tempurl_failed\n\n    set_status_success:\n      on-success: notify_zaqar\n      publish:\n        status: SUCCESS\n        message: <% task(set_put_tempurl).result %>\n\n    set_put_tempurl_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(set_put_tempurl).result %>\n\n    set_create_container_failed:\n      on-success: notify_zaqar\n      publish:\n        status: FAILED\n        message: <% task(create_container).result %>\n\n    notify_zaqar:\n      action: zaqar.queue_post\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan\n            payload:\n              status: <% $.status %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:13", "namespace": "", "updated_at": null, "scope": "private", "input": "container, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "1776eefa-406c-42df-a23b-7bfb8f4a335b"}, {"definition": "backup:\n  description: This workflow will launch the Undercloud backup\n  tags:\n    - tripleo-common-managed\n  input:\n    - sources_path: '/home/stack/'\n    - queue_name: tripleo\n  tasks:\n    # Action to know if there is enough available space\n    # to run the Undercloud backup\n    get_free_space:\n      action: tripleo.undercloud.get_free_space\n      publish:\n          status: SUCCESS\n          message: <% task().result %>\n          free_space: <% task().result %>\n      on-success: create_backup_dir\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    # We create a temp directory to store the Undercloud\n    # backup\n    create_backup_dir:\n      action: tripleo.undercloud.create_backup_dir\n      publish:\n          status: SUCCESS\n          message: <% task().result %>\n          backup_path: <% task().result %>\n      on-success: get_database_credentials\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    # The Undercloud database password for the root\n    # user is stored in a Mistral environment, we\n    # need the password in order to run the database dump\n    get_database_credentials:\n      action: mistral.environments_get name='tripleo.undercloud-config'\n      publish:\n          status: SUCCESS\n          message: <% task().result %>\n          undercloud_db_password: <% task(get_database_credentials).result.variables.undercloud_db_password %>\n      on-success: create_database_backup\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    # Run the DB dump of all the databases and store the result\n    # in the temporary folder\n    create_database_backup:\n      input:\n          path: <% $.backup_path.path %>\n          dbuser: root\n          dbpassword: <% $.undercloud_db_password %>\n      action: tripleo.undercloud.create_database_backup\n      publish:\n          status: SUCCESS\n          message: <% task().result %>\n      on-success: create_fs_backup\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    # This action will run the fs backup\n    create_fs_backup:\n      input:\n          sources_path: <% $.sources_path %>\n          path: <% $.backup_path.path %>\n      action: tripleo.undercloud.create_file_system_backup\n      publish:\n          status: SUCCESS\n          message: <% task().result %>\n      on-success: upload_backup\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    # This action will push the backup to swift\n    upload_backup:\n      input:\n          backup_path: <% $.backup_path.path %>\n      action: tripleo.undercloud.upload_backup_to_swift\n      publish:\n          status: SUCCESS\n          message: <% task().result %>\n      on-success: cleanup_backup\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    # This action will remove the backup temp folder\n    cleanup_backup:\n      input:\n          path: <% $.backup_path.path %>\n      action: tripleo.undercloud.remove_temp_dir\n      publish:\n          status: SUCCESS\n          message: <% task().result %>\n      on-success: send_message\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    # Sending a message to show that the backup finished\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.undercloud_backup.v1.launch\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              execution: <% execution() %>\n              message: <% $.get('message', '') %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.undercloud_backup.v1.backup", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:13", "namespace": "", "updated_at": null, "scope": "private", "input": "sources_path=/home/stack/, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "f3079885-3e1f-4eb5-b3bf-9c1701a7edcd"}, {"definition": "run_validations:\n  input:\n    - validation_names: []\n    - plan: overcloud\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    notify_running:\n      on-complete: run_validations\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.validations.v1.run_validations\n            payload:\n              validation_names: <% $.validation_names %>\n              plan: <% $.plan %>\n              status: RUNNING\n              execution: <% execution() %>\n\n    run_validations:\n      on-success: send_message\n      on-error: set_status_failed\n      workflow: tripleo.validations.v1.run_validation validation_name=<% $.validation %> plan=<% $.plan %> queue_name=<% $.queue_name %>\n      with-items: validation in <% $.validation_names %>\n      publish:\n        status: SUCCESS\n\n    set_status_failed:\n      on-complete: send_message\n      publish:\n        status: FAILED\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.validations.v1.run_validations\n            payload:\n              validation_names: <% $.validation_names %>\n              plan: <% $.plan %>\n              status: <% $.get('status', 'SUCCESS') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.validations.v1.run_validations", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:15", "namespace": "", "updated_at": null, "scope": "private", "input": "validation_names=[], plan=overcloud, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "03ef1c8a-52ef-4c38-82ae-c5b2da22d4d6"}, {"definition": "collect_flavors:\n  input:\n    - roles_info: {}\n    - run_validations: true\n    - queue_name: tripleo\n  output:\n    errors: <% $.errors %>\n    warnings: <% $.warnings %>\n    flavors: <% $.flavors %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    check_run_validations:\n      on-complete:\n        - check_flavors: <% $.run_validations %>\n        - send_message: <% not $.run_validations %>\n\n    check_flavors:\n      action: tripleo.validations.check_flavors\n      input:\n        roles_info: <% $.roles_info %>\n      on-success: send_message\n      publish:\n        flavors: <% task().result.flavors %>\n        errors: <% task().result.errors %>\n        warnings: <% task().result.warnings %>\n      on-error: send_message\n      publish-on-error:\n        flavors: {}\n        errors: <% task().result.errors %>\n        warnings: <% task().result.warnings %>\n        status: FAILED\n        message: <% task().result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.validations.v1.collect_flavors\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              flavors: <% $.flavors %>\n              errors: <% $.errors %>\n              warnings: <% $.warnings %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.validations.v1.collect_flavors", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:15", "namespace": "", "updated_at": null, "scope": "private", "input": "roles_info={}, run_validations=True, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "278fbf79-a6ba-422f-ba5d-8c358b58238a"}, {"definition": "list_groups:\n  tags:\n    - tripleo-common-managed\n  tasks:\n    find_groups:\n      action: tripleo.validations.list_groups\n", "name": "tripleo.validations.v1.list_groups", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:15", "namespace": "", "updated_at": null, "scope": "private", "input": "", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "29fd8f2c-7165-45e0-9fb7-9c536cb6bdad"}, {"definition": "run_groups:\n  input:\n    - group_names: []\n    - plan: overcloud\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    find_validations:\n      on-success: notify_running\n      action: tripleo.validations.list_validations groups=<% $.group_names %>\n      publish:\n        validations: <% task().result %>\n\n    notify_running:\n      on-complete: run_validation_group\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.validations.v1.run_validations\n            payload:\n              group_names: <% $.group_names %>\n              validation_names: <% $.validations.id %>\n              plan: <% $.plan %>\n              status: RUNNING\n              execution: <% execution() %>\n\n    run_validation_group:\n      on-success: send_message\n      on-error: set_status_failed\n      workflow: tripleo.validations.v1.run_validation validation_name=<% $.validation %> plan=<% $.plan %> queue_name=<% $.queue_name %>\n      with-items: validation in <% $.validations.id %>\n      publish:\n        status: SUCCESS\n\n    set_status_failed:\n      on-complete: send_message\n      publish:\n        status: FAILED\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.validations.v1.run_groups\n            payload:\n              group_names: <% $.group_names %>\n              validation_names: <% $.validations.id %>\n              plan: <% $.plan %>\n              status: <% $.get('status', 'SUCCESS') %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.validations.v1.run_groups", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:15", "namespace": "", "updated_at": null, "scope": "private", "input": "group_names=[], plan=overcloud, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "4cfc17bb-122b-476e-ae8e-9fbfbdd8309a"}, {"definition": "list:\n  input:\n    - group_names: []\n  tags:\n    - tripleo-common-managed\n  tasks:\n    find_validations:\n      action: tripleo.validations.list_validations groups=<% $.group_names %>\n", "name": "tripleo.validations.v1.list", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:15", "namespace": "", "updated_at": null, "scope": "private", "input": "group_names=[]", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7dbc7df6-4f6b-4317-95a4-28d2313e53a4"}, {"definition": "run_validation:\n  input:\n    - validation_name\n    - plan: overcloud\n    - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n\n    notify_running:\n      on-complete: run_validation\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.validations.v1.run_validation\n            payload:\n              validation_name: <% $.validation_name %>\n              plan: <% $.plan %>\n              status: RUNNING\n              execution: <% execution() %>\n\n    run_validation:\n      on-success: send_message\n      on-error: set_status_failed\n      action: tripleo.validations.run_validation validation=<% $.validation_name %> plan=<% $.plan %>\n      publish:\n        status: SUCCESS\n        stdout: <% task().result.stdout %>\n        stderr: <% task().result.stderr %>\n\n    set_status_failed:\n      on-complete: send_message\n      publish:\n        status: FAILED\n        stdout: <% task(run_validation).result.stdout %>\n        stderr: <% task(run_validation).result.stderr %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.validations.v1.run_validation\n            payload:\n              validation_name: <% $.validation_name %>\n              plan: <% $.plan %>\n              status: <% $.get('status', 'SUCCESS') %>\n              stdout: <% $.stdout %>\n              stderr: <% $.stderr %>\n              execution: <% execution() %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.validations.v1.run_validation", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:15", "namespace": "", "updated_at": null, "scope": "private", "input": "validation_name, plan=overcloud, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "a47ab785-bc05-4163-9db3-a2fc810ee563"}, {"definition": "check_pre_deployment_validations:\n  input:\n    - deploy_kernel_name: 'bm-deploy-kernel'\n    - deploy_ramdisk_name: 'bm-deploy-ramdisk'\n    - roles_info: {}\n    - stack_id: overcloud\n    - parameters: {}\n    - default_role_counts: {}\n    - run_validations: true\n    - queue_name: tripleo\n\n  output:\n    errors: <% $.errors %>\n    warnings: <% $.warnings %>\n    kernel_id: <% $.kernel_id %>\n    ramdisk_id: <% $.ramdisk_id %>\n    flavors: <% $.flavors %>\n    statistics: <% $.statistics %>\n  tags:\n    - tripleo-common-managed\n  tasks:\n    init_messages:\n      on-success: check_boot_images\n      publish:\n        errors: []\n        warnings: []\n\n    check_boot_images:\n      workflow: check_boot_images\n      input:\n        deploy_kernel_name: <% $.deploy_kernel_name %>\n        deploy_ramdisk_name: <% $.deploy_ramdisk_name %>\n        run_validations: <% $.run_validations %>\n        queue_name: <% $.queue_name %>\n      publish:\n        errors: <% $.errors + task().result.get('errors', []) %>\n        warnings: <% $.warnings + task().result.get('warnings', []) %>\n        kernel_id: <% task().result.get('kernel_id') %>\n        ramdisk_id: <% task().result.get('ramdisk_id') %>\n      publish-on-error:\n        errors: <% $.errors + task().result.get('errors', []) %>\n        warnings: <% $.warnings + task().result.get('warnings', []) %>\n        kernel_id: <% task().result.get('kernel_id') %>\n        ramdisk_id: <% task().result.get('ramdisk_id') %>\n        status: FAILED\n      on-success: collect_flavors\n      on-error: collect_flavors\n\n    collect_flavors:\n      workflow: collect_flavors\n      input:\n        roles_info: <% $.roles_info %>\n        run_validations: <% $.run_validations %>\n        queue_name: <% $.queue_name %>\n      publish:\n        errors: <% $.errors + task().result.get('errors', []) %>\n        warnings: <% $.warnings + task().result.get('warnings', []) %>\n        flavors: <% task().result.get('flavors') %>\n      publish-on-error:\n        errors: <% $.errors + task().result.get('errors', []) %>\n        warnings: <% $.warnings + task().result.get('warnings', []) %>\n        flavors: <% task().result.get('flavors') %>\n        status: FAILED\n      on-success: check_ironic_boot_configuration\n      on-error: check_ironic_boot_configuration\n\n    check_ironic_boot_configuration:\n      workflow: check_ironic_boot_configuration\n      input:\n        kernel_id: <% $.kernel_id %>\n        ramdisk_id: <% $.ramdisk_id %>\n        run_validations: <% $.run_validations %>\n        queue_name: <% $.queue_name %>\n      publish:\n        errors: <% $.errors + task().result.get('errors', []) %>\n        warnings: <% $.warnings + task().result.get('warnings', []) %>\n      publish-on-error:\n        errors: <% $.errors + task().result.get('errors', []) %>\n        warnings: <% $.warnings + task().result.get('warnings', []) %>\n        status: FAILED\n      on-success: check_default_nodes_count\n      on-error: check_default_nodes_count\n\n    check_default_nodes_count:\n      workflow: check_default_nodes_count\n      # ironic-nova sync happens once in two minutes\n      retry: count=12 delay=10\n      input:\n        stack_id: <% $.stack_id %>\n        parameters: <% $.parameters %>\n        default_role_counts: <% $.default_role_counts %>\n        run_validations: <% $.run_validations %>\n        queue_name: <% $.queue_name %>\n      publish:\n        errors: <% $.errors + task().result.get('errors', []) %>\n        warnings: <% $.warnings + task().result.get('warnings', []) %>\n        statistics: <% task().result.get('statistics') %>\n      publish-on-error:\n        errors: <% $.errors + task().result.get('errors', []) %>\n        warnings: <% $.warnings + task().result.get('warnings', []) %>\n        statistics: <% task().result.get('statistics') %>\n        status: FAILED\n      on-success: verify_profiles\n      # Do not confuse user with info about profiles if the nodes\n      # count is off in the first place. Skip directly to\n      # send_message. (bug 1703942)\n      on-error: send_message\n\n    verify_profiles:\n      workflow: verify_profiles\n      input:\n        flavors: <% $.flavors %>\n        run_validations: <% $.run_validations %>\n        queue_name: <% $.queue_name %>\n      publish:\n        errors: <% $.errors + task().result.get('errors', []) %>\n        warnings: <% $.warnings + task().result.get('warnings', []) %>\n      publish-on-error:\n        errors: <% $.errors + task().result.get('errors', []) %>\n        warnings: <% $.warnings + task().result.get('warnings', []) %>\n        status: FAILED\n      on-success: send_message\n      on-error: send_message\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.validations.v1.check_hypervisor_stats\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              kernel_id: <% $.kernel_id %>\n              ramdisk_id: <% $.ramdisk_id %>\n              flavors: <% $.flavors %>\n              statistics: <% $.statistics %>\n              errors: <% $.errors %>\n              warnings: <% $.warnings %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.validations.v1.check_pre_deployment_validations", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:15", "namespace": "", "updated_at": null, "scope": "private", "input": "deploy_kernel_name=bm-deploy-kernel, deploy_ramdisk_name=bm-deploy-ramdisk, roles_info={}, stack_id=overcloud, parameters={}, default_role_counts={}, run_validations=True, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "c8c50954-260e-4cfa-a251-98e95b688c8a"}, {"definition": "check_boot_images:\n  input:\n    - deploy_kernel_name: 'bm-deploy-kernel'\n    - deploy_ramdisk_name: 'bm-deploy-ramdisk'\n    - run_validations: true\n    - queue_name: tripleo\n  output:\n    errors: <% $.errors %>\n    warnings: <% $.warnings %>\n    kernel_id: <% $.kernel_id %>\n    ramdisk_id: <% $.ramdisk_id %>\n  tags:\n    - tripleo-common-managed\n  tasks:\n    check_run_validations:\n      on-complete:\n        - get_images: <% $.run_validations %>\n        - send_message: <% not $.run_validations %>\n\n    get_images:\n      action: glance.images_list\n      on-success: check_images\n      publish:\n        images: <% task().result %>\n\n    check_images:\n      action: tripleo.validations.check_boot_images\n      input:\n        images: <% $.images %>\n        deploy_kernel_name: <% $.deploy_kernel_name %>\n        deploy_ramdisk_name: <% $.deploy_ramdisk_name %>\n      on-success: send_message\n      publish:\n        kernel_id: <% task().result.kernel_id %>\n        ramdisk_id: <% task().result.ramdisk_id %>\n        warnings: <% task().result.warnings %>\n        errors: <% task().result.errors %>\n      on-error: send_message\n      publish-on-error:\n        kernel_id: <% task().result.kernel_id %>\n        ramdisk_id: <% task().result.ramdisk_id %>\n        warnings: <% task().result.warnings %>\n        errors: <% task().result.errors %>\n        status: FAILED\n        message: <% task().result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.validations.v1.check_boot_images\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              kernel_id: <% $.kernel_id %>\n              ramdisk_id: <% $.ramdisk_id %>\n              errors: <% $.errors %>\n              warnings: <% $.warnings %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.validations.v1.check_boot_images", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:16", "namespace": "", "updated_at": null, "scope": "private", "input": "deploy_kernel_name=bm-deploy-kernel, deploy_ramdisk_name=bm-deploy-ramdisk, run_validations=True, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "1119bf90-c576-49ce-87e1-dc5a15aca6d9"}, {"definition": "add_validation_ssh_key_parameter:\n  input:\n   - container\n   - queue_name: tripleo\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    test_validations_enabled:\n      action: tripleo.validations.enabled\n      on-success: get_pubkey\n      on-error: unset_validation_key_parameter\n\n    get_pubkey:\n      action: tripleo.validations.get_pubkey\n      on-success: set_validation_key_parameter\n      publish:\n        pubkey: <% task().result %>\n\n    set_validation_key_parameter:\n      action: tripleo.parameters.update\n      input:\n        parameters:\n          node_admin_extra_ssh_keys: <% $.pubkey %>\n        container: <% $.container %>\n\n    # NOTE(shadower): We need to clear keys from a previous deployment\n    unset_validation_key_parameter:\n      action: tripleo.parameters.update\n      input:\n        parameters:\n          node_admin_extra_ssh_keys: \"\"\n        container: <% $.container %>\n", "name": "tripleo.validations.v1.add_validation_ssh_key_parameter", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:16", "namespace": "", "updated_at": null, "scope": "private", "input": "container, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "139bef65-ab45-4cc4-abf5-f0a762b805d2"}, {"definition": "check_ironic_boot_configuration:\n  input:\n    - kernel_id: null\n    - ramdisk_id: null\n    - run_validations: true\n    - queue_name: tripleo\n  output:\n    errors: <% $.errors %>\n    warnings: <% $.warnings %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    check_run_validations:\n      on-complete:\n        - get_ironic_nodes: <% $.run_validations %>\n        - send_message: <% not $.run_validations %>\n\n    get_ironic_nodes:\n      action: ironic.node_list\n      input:\n        provision_state: available\n        maintenance: false\n        detail: true\n      on-success: check_node_boot_configuration\n      publish:\n        nodes: <% task().result %>\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    check_node_boot_configuration:\n      action: tripleo.validations.check_node_boot_configuration\n      input:\n        node: <% $.node %>\n        kernel_id: <% $.kernel_id %>\n        ramdisk_id: <% $.ramdisk_id %>\n      with-items: node in <% $.nodes %>\n      on-success: send_message\n      publish:\n        errors: <% task().result.errors.flatten() %>\n        warnings: <% task().result.warnings.flatten() %>\n      on-error: send_message\n      publish-on-error:\n        errors: <% task().result.errors.flatten() %>\n        warnings: <% task().result.warnings.flatten() %>\n        status: FAILED\n        message: <% task().result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.validations.v1.check_ironic_boot_configuration\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              errors: <% $.errors %>\n              warnings: <% $.warnings %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.validations.v1.check_ironic_boot_configuration", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:16", "namespace": "", "updated_at": null, "scope": "private", "input": "kernel_id=None, ramdisk_id=None, run_validations=True, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "3e3377c6-f9fb-4de2-a95e-4990f509eb7b"}, {"definition": "check_default_nodes_count:\n  input:\n    - stack_id: overcloud\n    - parameters: {}\n    - default_role_counts: {}\n    - run_validations: true\n    - queue_name: tripleo\n  output:\n    statistics: <% $.statistics %>\n    errors: <% $.errors %>\n    warnings: <% $.warnings %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    check_run_validations:\n      on-complete:\n        - get_hypervisor_statistics: <% $.run_validations %>\n        - send_message: <% not $.run_validations %>\n\n    get_hypervisor_statistics:\n      action: nova.hypervisors_statistics\n      on-success: get_stack\n      publish:\n        statistics: <% task().result %>\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n        errors: []\n        warnings: []\n        statistics: null\n\n    get_stack:\n      action: heat.stacks_get\n      input:\n        stack_id: <% $.stack_id %>\n      on-success: get_associated_nodes\n      publish:\n        stack: <% task().result %>\n      on-error: get_associated_nodes\n      publish-on-error:\n        stack: null\n\n    get_associated_nodes:\n      action: ironic.node_list\n      input:\n        associated: true\n      on-success: get_available_nodes\n      publish:\n        associated_nodes: <% task().result %>\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n        errors: []\n        warnings: []\n\n    get_available_nodes:\n      action: ironic.node_list\n      input:\n        provision_state: available\n        associated: false\n        maintenance: false\n      on-success: check_nodes_count\n      publish:\n        available_nodes: <% task().result %>\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n        errors: []\n        warnings: []\n\n    check_nodes_count:\n      action: tripleo.validations.check_nodes_count\n      input:\n        statistics: <% $.statistics %>\n        stack: <% $.stack %>\n        associated_nodes: <% $.associated_nodes %>\n        available_nodes: <% $.available_nodes %>\n        parameters: <% $.parameters %>\n        default_role_counts: <% $.default_role_counts %>\n      on-success: send_message\n      publish:\n        errors: <% task().result.errors %>\n        warnings: <% task().result.warnings %>\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n        statistics: null\n        errors: <% task().result.errors %>\n        warnings: <% task().result.warnings %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.validations.v1.check_hypervisor_stats\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              statistics: <% $.statistics %>\n              errors: <% $.errors %>\n              warnings: <% $.warnings %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.validations.v1.check_default_nodes_count", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:16", "namespace": "", "updated_at": null, "scope": "private", "input": "stack_id=overcloud, parameters={}, default_role_counts={}, run_validations=True, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "4c68fde2-089d-45fd-b978-a4348229e1ec"}, {"definition": "copy_ssh_key:\n  input:\n    # FIXME: we should stop using heat-admin as e.g. split-stack\n    # environments (where Nova didn't create overcloud nodes) don't\n    # have it present\n    - overcloud_admin: heat-admin\n    - queue_name: tripleo\n  tags:\n    - tripleo-common-managed\n  tasks:\n    get_servers:\n      action: nova.servers_list\n      on-success: get_pubkey\n      publish:\n        servers: <% task().result._info %>\n\n    get_pubkey:\n      action: tripleo.validations.get_pubkey\n      on-success: deploy_ssh_key\n      publish:\n        pubkey: <% task().result %>\n\n    deploy_ssh_key:\n      workflow: tripleo.deployment.v1.deploy_on_server\n      with-items: server in <% $.servers %>\n      input:\n        server_name: <% $.server.name %>\n        server_uuid: <% $.server.id %>\n        config: |\n          #!/bin/bash\n          if ! grep \"<% $.pubkey %>\" /home/<% $.overcloud_admin %>/.ssh/authorized_keys; then\n            echo \"<% $.pubkey %>\" >> /home/<% $.overcloud_admin %>/.ssh/authorized_keys\n          fi\n        config_name: copy_ssh_key\n        group: script\n        queue_name: <% $.queue_name %>\n", "name": "tripleo.validations.v1.copy_ssh_key", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:16", "namespace": "", "updated_at": null, "scope": "private", "input": "overcloud_admin=heat-admin, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "5eea10c6-3279-469d-9c80-9feef8096e66"}, {"definition": "verify_profiles:\n  input:\n    - flavors: []\n    - run_validations: true\n    - queue_name: tripleo\n  output:\n    errors: <% $.errors %>\n    warnings: <% $.warnings %>\n\n  tags:\n    - tripleo-common-managed\n\n  tasks:\n    check_run_validations:\n      on-complete:\n        - get_ironic_nodes: <% $.run_validations %>\n        - send_message: <% not $.run_validations %>\n\n    get_ironic_nodes:\n      action: ironic.node_list\n      input:\n        maintenance: false\n        detail: true\n      on-success: verify_profiles\n      publish:\n        nodes: <% task().result %>\n      on-error: send_message\n      publish-on-error:\n        status: FAILED\n        message: <% task().result %>\n\n    verify_profiles:\n      action: tripleo.validations.verify_profiles\n      input:\n        nodes: <% $.nodes %>\n        flavors: <% $.flavors %>\n      on-success: send_message\n      publish:\n        errors: <% task().result.errors %>\n        warnings: <% task().result.warnings %>\n      on-error: send_message\n      publish-on-error:\n        errors: <% task().result.errors %>\n        warnings: <% task().result.warnings %>\n        status: FAILED\n        message: <% task().result %>\n\n    send_message:\n      action: zaqar.queue_post\n      retry: count=5 delay=1\n      input:\n        queue_name: <% $.queue_name %>\n        messages:\n          body:\n            type: tripleo.validations.v1.verify_profiles\n            payload:\n              status: <% $.get('status', 'SUCCESS') %>\n              message: <% $.get('message', '') %>\n              execution: <% execution() %>\n              errors: <% $.errors %>\n              warnings: <% $.warnings %>\n      on-success:\n        - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.validations.v1.verify_profiles", "tags": ["tripleo-common-managed"], "created_at": "2018-12-03 11:19:16", "namespace": "", "updated_at": null, "scope": "private", "input": "flavors=[], run_validations=True, queue_name=tripleo", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "f3e2537f-79be-4a84-94b4-af5651ed2184"}, {"definition": "---\nversion: \"2.0\"\n\nstd.delete_instance:\n  type: direct\n\n  input:\n    - instance_id\n\n  description: Deletes VM.\n\n  tasks:\n    delete_vm:\n      description: Destroy VM.\n      action: nova.servers_delete server=<% $.instance_id %>\n      wait-after: 10\n      on-success:\n        - find_given_vm\n\n    find_given_vm:\n      description: Checks that VM is already deleted.\n      action: nova.servers_find id=<% $.instance_id %>\n      on-error:\n        - succeed\n\n", "name": "std.delete_instance", "tags": [], "created_at": "2018-12-04 11:24:23", "namespace": "", "updated_at": null, "scope": "public", "input": "instance_id", "project_id": "<default-project>", "id": "7644555e-74e8-4c8b-9aa4-17934f829738"}, {"definition": "---\nversion: '2.0'\n\nstd.create_instance:\n  type: direct\n\n  description: |\n    Creates VM and waits till VM OS is up and running.\n\n  input:\n    - name\n    - image_id\n    - flavor_id\n    - ssh_username: null\n    - ssh_password: null\n\n    # Name of previously created keypair to inject into the instance.\n    # Either ssh credentials or keypair must be provided.\n    - key_name: null\n\n    # Security_groups: A list of security group names\n    - security_groups: null\n\n    # An ordered list of nics to be added to this server, with information about connected networks, fixed IPs, port etc.\n    # Example: nics: [{\"net-id\": \"27aa8c1c-d6b8-4474-b7f7-6cdcf63ac856\"}]\n    - nics: null\n\n  task-defaults:\n    on-error:\n      - delete_vm\n\n  output:\n    ip: <% $.vm_ip %>\n    id: <% $.vm_id %>\n    name: <% $.name %>\n    status: <% $.status %>\n\n  tasks:\n    create_vm:\n      description: Initial request to create a VM.\n      action: nova.servers_create name=<% $.name %> image=<% $.image_id %> flavor=<% $.flavor_id %>\n      input:\n        key_name: <% $.key_name %>\n        security_groups: <% $.security_groups %>\n        nics: <% $.nics %>\n      publish:\n        vm_id: <% task(create_vm).result.id %>\n      on-success:\n        - search_for_ip\n\n    search_for_ip:\n      description: Gets first free ip from Nova floating IPs.\n      action: nova.floating_ips_findall instance_id=null\n      publish:\n        vm_ip: <% task(search_for_ip).result[0].ip %>\n      on-success:\n        - wait_vm_active\n\n    wait_vm_active:\n      description: Waits till VM is ACTIVE.\n      action: nova.servers_find id=<% $.vm_id %> status=\"ACTIVE\"\n      retry:\n        count: 10\n        delay: 10\n      publish:\n        status: <% task(wait_vm_active).result.status %>\n      on-success:\n        - associate_ip\n\n    associate_ip:\n      description: Associate server with one of floating IPs.\n      action: nova.servers_add_floating_ip server=<% $.vm_id %> address=<% $.vm_ip %>\n      wait-after: 5\n      on-success:\n        - wait_ssh\n\n    wait_ssh:\n      description: Wait till operating system on the VM is up (SSH command).\n      action: std.wait_ssh username=<% $.ssh_username %> password=<% $.ssh_password %> host=<% $.vm_ip %>\n      retry:\n        count: 10\n        delay: 10\n\n    delete_vm:\n      description: Destroy VM.\n      workflow: std.delete_instance instance_id=<% $.vm_id %>\n      on-complete:\n        - fail\n", "name": "std.create_instance", "tags": [], "created_at": "2018-12-04 11:24:23", "namespace": "", "updated_at": null, "scope": "public", "input": "name, image_id, flavor_id, ssh_username=None, ssh_password=None, key_name=None, security_groups=None, nics=None", "project_id": "<default-project>", "id": "fb056a6e-6760-4109-8ca2-66333be09821"}]}

2018-12-04 06:25:49,277 DEBUG: HTTP GET https://172.16.0.10:13989/v2/workflows 200
2018-12-04 06:25:49,282 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/cron_triggers -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:49,314 DEBUG: https://172.16.0.10:13989 "GET /v2/cron_triggers HTTP/1.1" 200 551
2018-12-04 06:25:49,315 DEBUG: RESP: [200] Content-Length: 551 Content-Type: application/json Date: Tue, 04 Dec 2018 11:25:49 GMT Connection: keep-alive 
RESP BODY: {"cron_triggers": [{"created_at": "2018-12-03 11:21:15", "project_id": "f41e7348daff4254892e5cd759aa8806", "name": "publish-ui-logs-hourly", "pattern": "0 * * * *", "workflow_name": "tripleo.plan_management.v1.publish_ui_logs_to_swift", "updated_at": "2018-12-04 11:24:28", "workflow_input": "{}", "workflow_id": "392c6fbb-b06e-4734-97e1-aae7ad23345b", "first_execution_time": null, "remaining_executions": null, "scope": "private", "workflow_params": "{}", "id": "7ba61b6c-8685-40a9-802a-a329a1973c1b", "next_execution_time": "2018-12-04 11:00:00"}]}

2018-12-04 06:25:49,315 DEBUG: HTTP GET https://172.16.0.10:13989/v2/cron_triggers 200
2018-12-04 06:25:49,316 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/cron_triggers/publish-ui-logs-hourly -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:50,613 DEBUG: https://172.16.0.10:13989 "DELETE /v2/cron_triggers/publish-ui-logs-hourly HTTP/1.1" 204 0
2018-12-04 06:25:50,614 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:50 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:50,614 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/cron_triggers/publish-ui-logs-hourly 204
2018-12-04 06:25:50,614 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.access.v1.create_admin_via_ssh -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:50,688 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.access.v1.create_admin_via_ssh HTTP/1.1" 204 0
2018-12-04 06:25:50,689 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:50 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:50,690 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.access.v1.create_admin_via_ssh 204
2018-12-04 06:25:50,690 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.access.v1.create_admin_via_nova -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:50,728 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.access.v1.create_admin_via_nova HTTP/1.1" 204 0
2018-12-04 06:25:50,729 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:50 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:50,729 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.access.v1.create_admin_via_nova 204
2018-12-04 06:25:50,730 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.access.v1.enable_ssh_admin -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:50,769 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.access.v1.enable_ssh_admin HTTP/1.1" 204 0
2018-12-04 06:25:50,770 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:50 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:50,770 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.access.v1.enable_ssh_admin 204
2018-12-04 06:25:50,770 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.configure_manageable_nodes -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:50,803 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.configure_manageable_nodes HTTP/1.1" 204 0
2018-12-04 06:25:50,804 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:50 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:50,804 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.configure_manageable_nodes 204
2018-12-04 06:25:50,805 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.cellv2_discovery -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:50,838 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.cellv2_discovery HTTP/1.1" 204 0
2018-12-04 06:25:50,839 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:50 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:50,839 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.cellv2_discovery 204
2018-12-04 06:25:50,839 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.introspect -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:50,881 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.introspect HTTP/1.1" 204 0
2018-12-04 06:25:50,882 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:50 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:50,882 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.introspect 204
2018-12-04 06:25:50,883 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.nodes_with_hint -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:50,921 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.nodes_with_hint HTTP/1.1" 204 0
2018-12-04 06:25:50,922 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:50 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:50,922 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.nodes_with_hint 204
2018-12-04 06:25:50,923 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.validate_nodes -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:50,965 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.validate_nodes HTTP/1.1" 204 0
2018-12-04 06:25:50,966 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:50 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:50,966 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.validate_nodes 204
2018-12-04 06:25:50,966 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.register_or_update -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,003 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.register_or_update HTTP/1.1" 204 0
2018-12-04 06:25:51,004 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,004 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.register_or_update 204
2018-12-04 06:25:51,005 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.create_raid_configuration -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,042 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.create_raid_configuration HTTP/1.1" 204 0
2018-12-04 06:25:51,044 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,044 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.create_raid_configuration 204
2018-12-04 06:25:51,045 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.discover_nodes -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,086 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.discover_nodes HTTP/1.1" 204 0
2018-12-04 06:25:51,087 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,088 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.discover_nodes 204
2018-12-04 06:25:51,088 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.introspect_manageable_nodes -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,129 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.introspect_manageable_nodes HTTP/1.1" 204 0
2018-12-04 06:25:51,130 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,130 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.introspect_manageable_nodes 204
2018-12-04 06:25:51,131 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.configure -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,170 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.configure HTTP/1.1" 204 0
2018-12-04 06:25:51,171 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,171 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.configure 204
2018-12-04 06:25:51,171 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.set_node_state -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,208 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.set_node_state HTTP/1.1" 204 0
2018-12-04 06:25:51,209 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,209 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.set_node_state 204
2018-12-04 06:25:51,210 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.tag_nodes -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,242 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.tag_nodes HTTP/1.1" 204 0
2018-12-04 06:25:51,243 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,244 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.tag_nodes 204
2018-12-04 06:25:51,244 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.provide -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,277 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.provide HTTP/1.1" 204 0
2018-12-04 06:25:51,278 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,278 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.provide 204
2018-12-04 06:25:51,279 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.tag_node -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,314 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.tag_node HTTP/1.1" 204 0
2018-12-04 06:25:51,315 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,316 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.tag_node 204
2018-12-04 06:25:51,316 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.manage -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,349 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.manage HTTP/1.1" 204 0
2018-12-04 06:25:51,350 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,350 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.manage 204
2018-12-04 06:25:51,351 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.provide_manageable_nodes -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,385 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.provide_manageable_nodes HTTP/1.1" 204 0
2018-12-04 06:25:51,387 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,387 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.provide_manageable_nodes 204
2018-12-04 06:25:51,388 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.set_power_state -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,421 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.set_power_state HTTP/1.1" 204 0
2018-12-04 06:25:51,423 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,423 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.set_power_state 204
2018-12-04 06:25:51,424 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.discover_and_enroll_nodes -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,458 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.discover_and_enroll_nodes HTTP/1.1" 204 0
2018-12-04 06:25:51,459 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,459 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.discover_and_enroll_nodes 204
2018-12-04 06:25:51,460 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.nodes_with_profile -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,512 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.nodes_with_profile HTTP/1.1" 204 0
2018-12-04 06:25:51,513 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,514 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.nodes_with_profile 204
2018-12-04 06:25:51,515 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.manual_cleaning -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,564 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1.manual_cleaning HTTP/1.1" 204 0
2018-12-04 06:25:51,565 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,565 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1.manual_cleaning 204
2018-12-04 06:25:51,566 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1._introspect -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,615 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.baremetal.v1._introspect HTTP/1.1" 204 0
2018-12-04 06:25:51,616 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,616 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.baremetal.v1._introspect 204
2018-12-04 06:25:51,616 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.storage.v1.ceph-install -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,670 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.storage.v1.ceph-install HTTP/1.1" 204 0
2018-12-04 06:25:51,671 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,671 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.storage.v1.ceph-install 204
2018-12-04 06:25:51,672 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.deployment.v1.deploy_plan -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,720 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.deployment.v1.deploy_plan HTTP/1.1" 204 0
2018-12-04 06:25:51,721 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,721 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.deployment.v1.deploy_plan 204
2018-12-04 06:25:51,722 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.deployment.v1.get_horizon_url -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,770 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.deployment.v1.get_horizon_url HTTP/1.1" 204 0
2018-12-04 06:25:51,771 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,771 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.deployment.v1.get_horizon_url 204
2018-12-04 06:25:51,772 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.deployment.v1.deploy_on_servers -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,804 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.deployment.v1.deploy_on_servers HTTP/1.1" 204 0
2018-12-04 06:25:51,805 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,805 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.deployment.v1.deploy_on_servers 204
2018-12-04 06:25:51,806 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.deployment.v1.config_download_deploy -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,837 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.deployment.v1.config_download_deploy HTTP/1.1" 204 0
2018-12-04 06:25:51,838 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,839 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.deployment.v1.config_download_deploy 204
2018-12-04 06:25:51,839 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.deployment.v1.deploy_on_server -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,873 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.deployment.v1.deploy_on_server HTTP/1.1" 204 0
2018-12-04 06:25:51,874 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,874 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.deployment.v1.deploy_on_server 204
2018-12-04 06:25:51,875 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params.v1._get_role_info -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,910 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.derive_params.v1._get_role_info HTTP/1.1" 204 0
2018-12-04 06:25:51,911 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,911 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params.v1._get_role_info 204
2018-12-04 06:25:51,912 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params.v1.derive_parameters -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,945 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.derive_params.v1.derive_parameters HTTP/1.1" 204 0
2018-12-04 06:25:51,946 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,946 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params.v1.derive_parameters 204
2018-12-04 06:25:51,947 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params.v1._derive_parameters_per_role -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:51,981 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.derive_params.v1._derive_parameters_per_role HTTP/1.1" 204 0
2018-12-04 06:25:51,982 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:51 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:51,982 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params.v1._derive_parameters_per_role 204
2018-12-04 06:25:51,983 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params_formulas.v1.host_derive_params -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,017 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.derive_params_formulas.v1.host_derive_params HTTP/1.1" 204 0
2018-12-04 06:25:52,018 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,018 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params_formulas.v1.host_derive_params 204
2018-12-04 06:25:52,019 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params_formulas.v1.dpdk_derive_params -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,053 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.derive_params_formulas.v1.dpdk_derive_params HTTP/1.1" 204 0
2018-12-04 06:25:52,054 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,054 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params_formulas.v1.dpdk_derive_params 204
2018-12-04 06:25:52,055 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params_formulas.v1.hci_derive_params -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,087 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.derive_params_formulas.v1.hci_derive_params HTTP/1.1" 204 0
2018-12-04 06:25:52,088 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,088 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params_formulas.v1.hci_derive_params 204
2018-12-04 06:25:52,088 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params_formulas.v1.get_host_cpus -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,122 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.derive_params_formulas.v1.get_host_cpus HTTP/1.1" 204 0
2018-12-04 06:25:52,123 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,123 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params_formulas.v1.get_host_cpus 204
2018-12-04 06:25:52,124 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params_formulas.v1.sriov_derive_params -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,155 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.derive_params_formulas.v1.sriov_derive_params HTTP/1.1" 204 0
2018-12-04 06:25:52,156 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,156 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.derive_params_formulas.v1.sriov_derive_params 204
2018-12-04 06:25:52,156 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.fernet_keys.v1.rotate_fernet_keys -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,186 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.fernet_keys.v1.rotate_fernet_keys HTTP/1.1" 204 0
2018-12-04 06:25:52,187 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,188 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.fernet_keys.v1.rotate_fernet_keys 204
2018-12-04 06:25:52,188 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.networks.v1.validate_networks_input -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,218 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.networks.v1.validate_networks_input HTTP/1.1" 204 0
2018-12-04 06:25:52,219 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,219 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.networks.v1.validate_networks_input 204
2018-12-04 06:25:52,220 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.networks.v1.update_networks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,250 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.networks.v1.update_networks HTTP/1.1" 204 0
2018-12-04 06:25:52,251 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,252 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.networks.v1.update_networks 204
2018-12-04 06:25:52,252 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.octavia_post.v1.octavia_post_deploy -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,283 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.octavia_post.v1.octavia_post_deploy HTTP/1.1" 204 0
2018-12-04 06:25:52,284 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,284 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.octavia_post.v1.octavia_post_deploy 204
2018-12-04 06:25:52,285 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.package_update.v1.converge_upgrade_plan -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,317 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.package_update.v1.converge_upgrade_plan HTTP/1.1" 204 0
2018-12-04 06:25:52,318 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,318 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.package_update.v1.converge_upgrade_plan 204
2018-12-04 06:25:52,318 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.package_update.v1.get_config -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,350 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.package_update.v1.get_config HTTP/1.1" 204 0
2018-12-04 06:25:52,351 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,352 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.package_update.v1.get_config 204
2018-12-04 06:25:52,352 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.package_update.v1.package_update_plan -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,387 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.package_update.v1.package_update_plan HTTP/1.1" 204 0
2018-12-04 06:25:52,389 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,389 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.package_update.v1.package_update_plan 204
2018-12-04 06:25:52,390 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.package_update.v1.ffwd_upgrade_converge_plan -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,425 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.package_update.v1.ffwd_upgrade_converge_plan HTTP/1.1" 204 0
2018-12-04 06:25:52,426 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,427 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.package_update.v1.ffwd_upgrade_converge_plan 204
2018-12-04 06:25:52,428 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.package_update.v1.update_nodes -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,464 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.package_update.v1.update_nodes HTTP/1.1" 204 0
2018-12-04 06:25:52,465 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,465 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.package_update.v1.update_nodes 204
2018-12-04 06:25:52,466 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.package_update.v1.update_converge_plan -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,518 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.package_update.v1.update_converge_plan HTTP/1.1" 204 0
2018-12-04 06:25:52,519 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,519 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.package_update.v1.update_converge_plan 204
2018-12-04 06:25:52,520 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.update_roles -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,556 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.update_roles HTTP/1.1" 204 0
2018-12-04 06:25:52,557 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,557 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.update_roles 204
2018-12-04 06:25:52,557 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.create_default_deployment_plan -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,591 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.create_default_deployment_plan HTTP/1.1" 204 0
2018-12-04 06:25:52,592 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,592 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.create_default_deployment_plan 204
2018-12-04 06:25:52,592 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.validate_roles_and_networks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,625 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.validate_roles_and_networks HTTP/1.1" 204 0
2018-12-04 06:25:52,626 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,626 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.validate_roles_and_networks 204
2018-12-04 06:25:52,627 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.validate_roles -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,684 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.validate_roles HTTP/1.1" 204 0
2018-12-04 06:25:52,685 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,686 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.validate_roles 204
2018-12-04 06:25:52,687 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1._validate_networks_from_roles -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,739 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1._validate_networks_from_roles HTTP/1.1" 204 0
2018-12-04 06:25:52,740 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,740 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1._validate_networks_from_roles 204
2018-12-04 06:25:52,741 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.select_roles -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,776 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.select_roles HTTP/1.1" 204 0
2018-12-04 06:25:52,777 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,778 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.select_roles 204
2018-12-04 06:25:52,778 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.validate_networks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,811 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.validate_networks HTTP/1.1" 204 0
2018-12-04 06:25:52,812 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,813 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.validate_networks 204
2018-12-04 06:25:52,813 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.list_networks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,849 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.list_networks HTTP/1.1" 204 0
2018-12-04 06:25:52,850 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,850 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.list_networks 204
2018-12-04 06:25:52,851 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.download_logs -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,887 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.download_logs HTTP/1.1" 204 0
2018-12-04 06:25:52,888 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,889 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.download_logs 204
2018-12-04 06:25:52,889 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.get_deprecated_parameters -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,925 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.get_deprecated_parameters HTTP/1.1" 204 0
2018-12-04 06:25:52,926 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,926 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.get_deprecated_parameters 204
2018-12-04 06:25:52,927 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.publish_ui_logs_to_swift -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,963 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.publish_ui_logs_to_swift HTTP/1.1" 204 0
2018-12-04 06:25:52,964 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:52,964 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.publish_ui_logs_to_swift 204
2018-12-04 06:25:52,965 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.list_available_networks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:52,999 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.list_available_networks HTTP/1.1" 204 0
2018-12-04 06:25:53,000 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:52 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,000 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.list_available_networks 204
2018-12-04 06:25:53,001 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.list_roles -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,036 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.list_roles HTTP/1.1" 204 0
2018-12-04 06:25:53,037 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,038 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.list_roles 204
2018-12-04 06:25:53,038 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.scale.v1.delete_node -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,070 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.scale.v1.delete_node HTTP/1.1" 204 0
2018-12-04 06:25:53,071 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,071 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.scale.v1.delete_node 204
2018-12-04 06:25:53,071 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.update_deployment_plan -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,115 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.update_deployment_plan HTTP/1.1" 204 0
2018-12-04 06:25:53,116 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,116 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.update_deployment_plan 204
2018-12-04 06:25:53,116 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.export_deployment_plan -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,149 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.export_deployment_plan HTTP/1.1" 204 0
2018-12-04 06:25:53,150 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,151 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.export_deployment_plan 204
2018-12-04 06:25:53,151 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.delete_deployment_plan -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,184 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.delete_deployment_plan HTTP/1.1" 204 0
2018-12-04 06:25:53,185 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,185 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.delete_deployment_plan 204
2018-12-04 06:25:53,185 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.list_available_roles -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,219 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.list_available_roles HTTP/1.1" 204 0
2018-12-04 06:25:53,220 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,220 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.list_available_roles 204
2018-12-04 06:25:53,221 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.validate_network_files -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,254 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.validate_network_files HTTP/1.1" 204 0
2018-12-04 06:25:53,255 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,255 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.validate_network_files 204
2018-12-04 06:25:53,255 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.skydive_ansible.v1.skydive_install -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,290 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.skydive_ansible.v1.skydive_install HTTP/1.1" 204 0
2018-12-04 06:25:53,291 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,291 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.skydive_ansible.v1.skydive_install 204
2018-12-04 06:25:53,292 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.create_deployment_plan -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,326 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.create_deployment_plan HTTP/1.1" 204 0
2018-12-04 06:25:53,327 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,327 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.create_deployment_plan 204
2018-12-04 06:25:53,327 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.get_passwords -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,362 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.plan_management.v1.get_passwords HTTP/1.1" 204 0
2018-12-04 06:25:53,363 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,363 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.plan_management.v1.get_passwords 204
2018-12-04 06:25:53,364 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.stack.v1.delete_stack -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,398 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.stack.v1.delete_stack HTTP/1.1" 204 0
2018-12-04 06:25:53,399 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,399 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.stack.v1.delete_stack 204
2018-12-04 06:25:53,400 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.stack.v1.wait_for_stack_in_progress -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,432 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.stack.v1.wait_for_stack_in_progress HTTP/1.1" 204 0
2018-12-04 06:25:53,433 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,433 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.stack.v1.wait_for_stack_in_progress 204
2018-12-04 06:25:53,434 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.stack.v1.wait_for_stack_does_not_exist -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,468 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.stack.v1.wait_for_stack_does_not_exist HTTP/1.1" 204 0
2018-12-04 06:25:53,469 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,470 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.stack.v1.wait_for_stack_does_not_exist 204
2018-12-04 06:25:53,470 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.stack.v1.wait_for_stack_complete_or_failed -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,503 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.stack.v1.wait_for_stack_complete_or_failed HTTP/1.1" 204 0
2018-12-04 06:25:53,504 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,504 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.stack.v1.wait_for_stack_complete_or_failed 204
2018-12-04 06:25:53,505 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.support.v1.collect_logs -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,537 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.support.v1.collect_logs HTTP/1.1" 204 0
2018-12-04 06:25:53,538 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,538 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.support.v1.collect_logs 204
2018-12-04 06:25:53,539 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.support.v1.delete_container -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,571 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.support.v1.delete_container HTTP/1.1" 204 0
2018-12-04 06:25:53,572 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,572 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.support.v1.delete_container 204
2018-12-04 06:25:53,573 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.support.v1.upload_logs -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,605 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.support.v1.upload_logs HTTP/1.1" 204 0
2018-12-04 06:25:53,606 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,606 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.support.v1.upload_logs 204
2018-12-04 06:25:53,606 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.support.v1.fetch_logs -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,638 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.support.v1.fetch_logs HTTP/1.1" 204 0
2018-12-04 06:25:53,639 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,639 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.support.v1.fetch_logs 204
2018-12-04 06:25:53,639 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.support.v1.create_container -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,672 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.support.v1.create_container HTTP/1.1" 204 0
2018-12-04 06:25:53,673 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,673 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.support.v1.create_container 204
2018-12-04 06:25:53,674 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.swift_ring.v1.rebalance -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,707 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.swift_ring.v1.rebalance HTTP/1.1" 204 0
2018-12-04 06:25:53,708 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,708 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.swift_ring.v1.rebalance 204
2018-12-04 06:25:53,708 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,741 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan HTTP/1.1" 204 0
2018-12-04 06:25:53,741 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,742 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan 204
2018-12-04 06:25:53,742 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.undercloud_backup.v1.backup -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,774 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.undercloud_backup.v1.backup HTTP/1.1" 204 0
2018-12-04 06:25:53,775 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,776 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.undercloud_backup.v1.backup 204
2018-12-04 06:25:53,776 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.run_validations -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,808 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.validations.v1.run_validations HTTP/1.1" 204 0
2018-12-04 06:25:53,809 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,809 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.run_validations 204
2018-12-04 06:25:53,810 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.collect_flavors -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,841 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.validations.v1.collect_flavors HTTP/1.1" 204 0
2018-12-04 06:25:53,842 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,843 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.collect_flavors 204
2018-12-04 06:25:53,843 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.list_groups -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,877 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.validations.v1.list_groups HTTP/1.1" 204 0
2018-12-04 06:25:53,878 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,878 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.list_groups 204
2018-12-04 06:25:53,879 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.run_groups -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,910 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.validations.v1.run_groups HTTP/1.1" 204 0
2018-12-04 06:25:53,911 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,911 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.run_groups 204
2018-12-04 06:25:53,911 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.list -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,942 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.validations.v1.list HTTP/1.1" 204 0
2018-12-04 06:25:53,943 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,943 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.list 204
2018-12-04 06:25:53,943 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.run_validation -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:53,976 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.validations.v1.run_validation HTTP/1.1" 204 0
2018-12-04 06:25:53,977 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:53 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:53,977 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.run_validation 204
2018-12-04 06:25:53,978 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.check_pre_deployment_validations -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:54,010 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.validations.v1.check_pre_deployment_validations HTTP/1.1" 204 0
2018-12-04 06:25:54,011 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:54 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:54,011 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.check_pre_deployment_validations 204
2018-12-04 06:25:54,012 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.check_boot_images -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:54,044 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.validations.v1.check_boot_images HTTP/1.1" 204 0
2018-12-04 06:25:54,044 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:54 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:54,045 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.check_boot_images 204
2018-12-04 06:25:54,045 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.add_validation_ssh_key_parameter -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:54,078 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.validations.v1.add_validation_ssh_key_parameter HTTP/1.1" 204 0
2018-12-04 06:25:54,079 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:54 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:54,080 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.add_validation_ssh_key_parameter 204
2018-12-04 06:25:54,080 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.check_ironic_boot_configuration -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:54,113 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.validations.v1.check_ironic_boot_configuration HTTP/1.1" 204 0
2018-12-04 06:25:54,114 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:54 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:54,114 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.check_ironic_boot_configuration 204
2018-12-04 06:25:54,115 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.check_default_nodes_count -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:54,147 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.validations.v1.check_default_nodes_count HTTP/1.1" 204 0
2018-12-04 06:25:54,147 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:54 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:54,148 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.check_default_nodes_count 204
2018-12-04 06:25:54,148 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.copy_ssh_key -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:54,180 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.validations.v1.copy_ssh_key HTTP/1.1" 204 0
2018-12-04 06:25:54,181 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:54 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:54,181 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.copy_ssh_key 204
2018-12-04 06:25:54,182 DEBUG: REQ: curl -g -i -X DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.verify_profiles -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:25:54,214 DEBUG: https://172.16.0.10:13989 "DELETE /v2/workflows/tripleo.validations.v1.verify_profiles HTTP/1.1" 204 0
2018-12-04 06:25:54,215 DEBUG: RESP: [204] Content-Length: 0 Date: Tue, 04 Dec 2018 11:25:54 GMT Connection: keep-alive 
RESP BODY: Omitted, Content-Type is set to None. Only application/json responses have their bodies logged.

2018-12-04 06:25:54,215 DEBUG: HTTP DELETE https://172.16.0.10:13989/v2/workflows/tripleo.validations.v1.verify_profiles 204
2018-12-04 06:25:54,226 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.access.v1
description: TripleO administration access workflows

workflows:

  enable_ssh_admin:
    description: >-
      This workflow creates an admin user on the overcloud nodes,
      which can then be used for connecting for automated
      administrative or deployment tasks, e.g. via Ansible. The
      workflow can be used both for Nova-managed and split-stack
      deployments, assuming the correct input values are passed
      in. The workflow defaults to Nova-managed approach, for which no
      additional parameters need to be supplied. In case of
      split-stack, temporary ssh connection details (user, key, list
      of servers) need to be provided -- these are only used
      temporarily to create the actual ssh admin user for use by
      Mistral.
    tags:
      - tripleo-common-managed
    input:
      - ssh_private_key: null
      - ssh_user: null
      - ssh_servers: []
      - overcloud_admin: tripleo-admin
      - queue_name: tripleo
    tasks:
      get_pubkey:
        action: tripleo.validations.get_pubkey
        on-success: generate_playbook
        publish:
          pubkey: <% task().result %>

      generate_playbook:
        on-success:
          - create_admin_via_nova: <% $.ssh_private_key = null %>
          - create_admin_via_ssh: <% $.ssh_private_key != null %>
        publish:
          create_admin_tasks:
            - name: create user <% $.overcloud_admin %>
              user:
                name: '<% $.overcloud_admin %>'
            - name: grant admin rights to user <% $.overcloud_admin %>
              copy:
                dest: /etc/sudoers.d/<% $.overcloud_admin %>
                content: |
                  <% $.overcloud_admin %> ALL=(ALL) NOPASSWD:ALL
                mode: 0440
            - name: ensure .ssh dir exists for user <% $.overcloud_admin %>
              file:
                path: /home/<% $.overcloud_admin %>/.ssh
                state: directory
                owner: <% $.overcloud_admin %>
                group: <% $.overcloud_admin %>
                mode: 0700
            - name: ensure authorized_keys file exists for user <% $.overcloud_admin %>
              file:
                path: /home/<% $.overcloud_admin %>/.ssh/authorized_keys
                state: touch
                owner: <% $.overcloud_admin %>
                group: <% $.overcloud_admin %>
                mode: 0700
            - name: authorize TripleO Mistral key for user <% $.overcloud_admin %>
              lineinfile:
                path: /home/<% $.overcloud_admin %>/.ssh/authorized_keys
                line: <% $.pubkey %>
                regexp: "Generated by TripleO"

      # Nova variant
      create_admin_via_nova:
        workflow: tripleo.access.v1.create_admin_via_nova
        input:
          queue_name: <% $.queue_name %>
          ssh_servers: <% $.ssh_servers %>
          tasks: <% $.create_admin_tasks %>
          overcloud_admin: <% $.overcloud_admin %>

      # SSH variant
      create_admin_via_ssh:
        workflow: tripleo.access.v1.create_admin_via_ssh
        input:
          ssh_private_key: <% $.ssh_private_key %>
          ssh_user: <% $.ssh_user %>
          ssh_servers: <% $.ssh_servers %>
          tasks: <% $.create_admin_tasks %>

  create_admin_via_nova:
    input:
      - tasks
      - queue_name: tripleo
      - ssh_servers: []
      - overcloud_admin: tripleo-admin
      - ansible_extra_env_variables:
            ANSIBLE_HOST_KEY_CHECKING: 'False'
    tags:
      - tripleo-common-managed
    tasks:
      get_servers:
        action: nova.servers_list
        on-success: create_admin
        publish:
          servers: <% let(root => $) -> task().result._info.where($.addresses.ctlplane.addr.any($ in $root.ssh_servers)) %>

      create_admin:
        workflow: tripleo.deployment.v1.deploy_on_server
        on-success: get_privkey
        with-items: server in <% $.servers %>
        input:
          server_name: <% $.server.name %>
          server_uuid: <% $.server.id %>
          queue_name: <% $.queue_name %>
          config_name: create_admin
          group: ansible
          config: |
            - hosts: localhost
              connection: local
              tasks: <% json_pp($.tasks) %>

      get_privkey:
        action: tripleo.validations.get_privkey
        on-success: wait_for_occ
        publish:
          privkey: <% task().result %>

      wait_for_occ:
        action: tripleo.ansible-playbook
        input:
          inventory:
            overcloud:
              hosts: <% $.ssh_servers.toDict($, {}) %>
          remote_user: <% $.overcloud_admin %>
          ssh_private_key: <% $.privkey %>
          extra_env_variables: <% $.ansible_extra_env_variables %>
          playbook:
            - hosts: overcloud
              gather_facts: no
              tasks:
                - name: wait for connection
                  wait_for_connection:
                    sleep: 5
                    timeout: 300

  create_admin_via_ssh:
    input:
      - tasks
      - ssh_private_key
      - ssh_user
      - ssh_servers
      - ansible_extra_env_variables:
            ANSIBLE_HOST_KEY_CHECKING: 'False'

    tags:
      - tripleo-common-managed
    tasks:
      write_tmp_playbook:
        action: tripleo.ansible-playbook
        input:
          inventory:
            overcloud:
              hosts: <% $.ssh_servers.toDict($, {}) %>
          remote_user: <% $.ssh_user %>
          ssh_private_key: <% $.ssh_private_key %>
          extra_env_variables: <% $.ansible_extra_env_variables %>
          become: true
          become_user: root
          playbook:
            - hosts: overcloud
              tasks: <% $.tasks %>
'
2018-12-04 06:25:54,647 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 6130
2018-12-04 06:25:54,649 DEBUG: RESP: [201] Content-Length: 6130 Content-Type: application/json Date: Tue, 04 Dec 2018 11:25:54 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.access.v1\ndescription: TripleO administration access workflows\n\nworkflows:\n\n  enable_ssh_admin:\n    description: >-\n      This workflow creates an admin user on the overcloud nodes,\n      which can then be used for connecting for automated\n      administrative or deployment tasks, e.g. via Ansible. The\n      workflow can be used both for Nova-managed and split-stack\n      deployments, assuming the correct input values are passed\n      in. The workflow defaults to Nova-managed approach, for which no\n      additional parameters need to be supplied. In case of\n      split-stack, temporary ssh connection details (user, key, list\n      of servers) need to be provided -- these are only used\n      temporarily to create the actual ssh admin user for use by\n      Mistral.\n    tags:\n      - tripleo-common-managed\n    input:\n      - ssh_private_key: null\n      - ssh_user: null\n      - ssh_servers: []\n      - overcloud_admin: tripleo-admin\n      - queue_name: tripleo\n    tasks:\n      get_pubkey:\n        action: tripleo.validations.get_pubkey\n        on-success: generate_playbook\n        publish:\n          pubkey: <% task().result %>\n\n      generate_playbook:\n        on-success:\n          - create_admin_via_nova: <% $.ssh_private_key = null %>\n          - create_admin_via_ssh: <% $.ssh_private_key != null %>\n        publish:\n          create_admin_tasks:\n            - name: create user <% $.overcloud_admin %>\n              user:\n                name: '<% $.overcloud_admin %>'\n            - name: grant admin rights to user <% $.overcloud_admin %>\n              copy:\n                dest: /etc/sudoers.d/<% $.overcloud_admin %>\n                content: |\n                  <% $.overcloud_admin %> ALL=(ALL) NOPASSWD:ALL\n                mode: 0440\n            - name: ensure .ssh dir exists for user <% $.overcloud_admin %>\n              file:\n                path: /home/<% $.overcloud_admin %>/.ssh\n                state: directory\n                owner: <% $.overcloud_admin %>\n                group: <% $.overcloud_admin %>\n                mode: 0700\n            - name: ensure authorized_keys file exists for user <% $.overcloud_admin %>\n              file:\n                path: /home/<% $.overcloud_admin %>/.ssh/authorized_keys\n                state: touch\n                owner: <% $.overcloud_admin %>\n                group: <% $.overcloud_admin %>\n                mode: 0700\n            - name: authorize TripleO Mistral key for user <% $.overcloud_admin %>\n              lineinfile:\n                path: /home/<% $.overcloud_admin %>/.ssh/authorized_keys\n                line: <% $.pubkey %>\n                regexp: \"Generated by TripleO\"\n\n      # Nova variant\n      create_admin_via_nova:\n        workflow: tripleo.access.v1.create_admin_via_nova\n        input:\n          queue_name: <% $.queue_name %>\n          ssh_servers: <% $.ssh_servers %>\n          tasks: <% $.create_admin_tasks %>\n          overcloud_admin: <% $.overcloud_admin %>\n\n      # SSH variant\n      create_admin_via_ssh:\n        workflow: tripleo.access.v1.create_admin_via_ssh\n        input:\n          ssh_private_key: <% $.ssh_private_key %>\n          ssh_user: <% $.ssh_user %>\n          ssh_servers: <% $.ssh_servers %>\n          tasks: <% $.create_admin_tasks %>\n\n  create_admin_via_nova:\n    input:\n      - tasks\n      - queue_name: tripleo\n      - ssh_servers: []\n      - overcloud_admin: tripleo-admin\n      - ansible_extra_env_variables:\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n    tags:\n      - tripleo-common-managed\n    tasks:\n      get_servers:\n        action: nova.servers_list\n        on-success: create_admin\n        publish:\n          servers: <% let(root => $) -> task().result._info.where($.addresses.ctlplane.addr.any($ in $root.ssh_servers)) %>\n\n      create_admin:\n        workflow: tripleo.deployment.v1.deploy_on_server\n        on-success: get_privkey\n        with-items: server in <% $.servers %>\n        input:\n          server_name: <% $.server.name %>\n          server_uuid: <% $.server.id %>\n          queue_name: <% $.queue_name %>\n          config_name: create_admin\n          group: ansible\n          config: |\n            - hosts: localhost\n              connection: local\n              tasks: <% json_pp($.tasks) %>\n\n      get_privkey:\n        action: tripleo.validations.get_privkey\n        on-success: wait_for_occ\n        publish:\n          privkey: <% task().result %>\n\n      wait_for_occ:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            overcloud:\n              hosts: <% $.ssh_servers.toDict($, {}) %>\n          remote_user: <% $.overcloud_admin %>\n          ssh_private_key: <% $.privkey %>\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          playbook:\n            - hosts: overcloud\n              gather_facts: no\n              tasks:\n                - name: wait for connection\n                  wait_for_connection:\n                    sleep: 5\n                    timeout: 300\n\n  create_admin_via_ssh:\n    input:\n      - tasks\n      - ssh_private_key\n      - ssh_user\n      - ssh_servers\n      - ansible_extra_env_variables:\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n\n    tags:\n      - tripleo-common-managed\n    tasks:\n      write_tmp_playbook:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            overcloud:\n              hosts: <% $.ssh_servers.toDict($, {}) %>\n          remote_user: <% $.ssh_user %>\n          ssh_private_key: <% $.ssh_private_key %>\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          become: true\n          become_user: root\n          playbook:\n            - hosts: overcloud\n              tasks: <% $.tasks %>\n", "name": "tripleo.access.v1", "tags": [], "created_at": "2018-12-04 11:25:54", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "9b8a1f42-e862-48b5-9ba3-577dd977e87b"}

2018-12-04 06:25:54,649 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:25:54,651 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.baremetal.v1
description: TripleO Baremetal Workflows

workflows:

  set_node_state:
    input:
      - node_uuid
      - state_action
      - target_state
      - error_states:
          # The default includes all failure states, even unused by TripleO.
          - 'error'
          - 'adopt failed'
          - 'clean failed'
          - 'deploy failed'
          - 'inspect failed'
          - 'rescue failed'

    tags:
      - tripleo-common-managed

    tasks:

      set_provision_state:
        on-success: wait_for_provision_state
        on-error: set_provision_state_failed
        action: ironic.node_set_provision_state node_uuid=<% $.node_uuid %> state=<% $.state_action %>

      set_provision_state_failed:
        publish:
          message: <% task(set_provision_state).result %>
        on-complete: fail

      wait_for_provision_state:
        action: ironic.node_get
        input:
          node_id: <% $.node_uuid %>
          fields: ['provision_state', 'last_error']
        timeout: 1200 #20 minutes
        retry:
          delay: 3
          count: 400
          continue-on: <% not task().result.provision_state in [$.target_state] + $.error_states %>
        on-complete:
          - state_not_reached: <% task().result.provision_state != $.target_state %>

      state_not_reached:
        publish:
          message: >-
            Node <% $.node_uuid %> did not reach state "<% $.target_state %>",
            the state is "<% task(wait_for_provision_state).result.provision_state %>",
            error: <% task(wait_for_provision_state).result.last_error %>
        on-complete: fail

    output-on-error:
      result: <% $.message %>

  set_power_state:
    input:
      - node_uuid
      - state_action
      - target_state
      - error_state: 'error'

    tags:
      - tripleo-common-managed

    tasks:

      set_power_state:
        on-success: wait_for_power_state
        on-error: set_power_state_failed
        action: ironic.node_set_power_state node_id=<% $.node_uuid %> state=<% $.state_action %>

      set_power_state_failed:
        publish:
          message: <% task(set_power_state).result %>
        on-complete: fail

      wait_for_power_state:
        action: ironic.node_get
        input:
          node_id: <% $.node_uuid %>
          fields: ['power_state', 'last_error']
        timeout: 120 #2 minutes
        retry:
          delay: 6
          count: 20
          continue-on: <% not task().result.power_state in [$.target_state, $.error_state] %>
        on-complete:
          - state_not_reached: <% task().result.power_state != $.target_state %>

      state_not_reached:
        publish:
          message: >-
            Node <% $.node_uuid %> did not reach power state "<% $.target_state %>",
            the state is "<% task(wait_for_power_state).result.power_state %>",
            error: <% task(wait_for_power_state).result.last_error %>
        on-complete: fail

    output-on-error:
      result: <% $.message %>

  manual_cleaning:
    input:
      - node_uuid
      - clean_steps
      - timeout: 7200  # 2 hours (cleaning can take really long)
      - retry_delay: 10
      - retry_count: 720
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      set_provision_state:
        on-success: wait_for_provision_state
        on-error: set_provision_state_failed
        action: ironic.node_set_provision_state node_uuid=<% $.node_uuid %> state='clean' cleansteps=<% $.clean_steps %>

      set_provision_state_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(set_provision_state).result %>

      wait_for_provision_state:
        on-success: send_message
        action: ironic.node_get node_id=<% $.node_uuid %>
        timeout: <% $.timeout %>
        retry:
          delay: <% $.retry_delay %>
          count: <% $.retry_count %>
          continue-on: <% task().result.provision_state != 'manageable' %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.manual_cleaning
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  validate_nodes:
    description: Validate nodes JSON

    input:
     - nodes_json
     - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      validate_nodes:
        action: tripleo.baremetal.validate_nodes
        on-success: send_message
        on-error: validation_failed
        input:
           nodes_json: <% $.nodes_json %>

      validation_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(validate_nodes).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.validate_nodes
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  register_or_update:
    description: Take nodes JSON and create nodes in a "manageable" state

    input:
     - nodes_json
     - remove: False
     - queue_name: tripleo
     - kernel_name: null
     - ramdisk_name: null
     - instance_boot_option: local
     - initial_state: manageable

    tags:
      - tripleo-common-managed

    tasks:

      validate_input:
        workflow: tripleo.baremetal.v1.validate_nodes
        on-success: register_or_update_nodes
        on-error: validation_failed
        input:
          nodes_json: <% $.nodes_json %>
          queue_name: <% $.queue_name %>

      validation_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(validate_input).result %>
          registered_nodes: []

      register_or_update_nodes:
        action: tripleo.baremetal.register_or_update_nodes
        on-success:
          - set_nodes_managed: <% $.initial_state != "enroll" %>
          - send_message: <% $.initial_state = "enroll" %>
        on-error: set_status_failed_register_or_update_nodes
        input:
           nodes_json: <% $.nodes_json %>
           remove: <% $.remove %>
           kernel_name: <% $.kernel_name %>
           ramdisk_name: <% $.ramdisk_name %>
           instance_boot_option: <% $.instance_boot_option %>
        publish:
          registered_nodes: <% task().result %>
          new_nodes: <% task().result.where($.provision_state = 'enroll') %>

      set_status_failed_register_or_update_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(register_or_update_nodes).result %>
          registered_nodes: []

      set_nodes_managed:
        on-success:
          - set_nodes_available: <% $.initial_state = "available" %>
          - send_message: <% $.initial_state != "available" %>
        on-error: set_status_failed_nodes_managed
        workflow: tripleo.baremetal.v1.manage
        input:
          node_uuids: <% $.new_nodes.uuid %>
          queue_name: <% $.queue_name %>
        publish:
          status: SUCCESS
          message: <% $.new_nodes.len() %> node(s) successfully moved to the "manageable" state.

      set_status_failed_nodes_managed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(set_nodes_managed).result %>

      set_nodes_available:
        on-success: send_message
        on-error: set_status_failed_nodes_available
        workflow: tripleo.baremetal.v1.provide node_uuids=<% $.new_nodes.uuid %> queue_name=<% $.queue_name %>
        publish:
          status: SUCCESS
          message: <% $.new_nodes.len() %> node(s) successfully moved to the "available" state.

      set_status_failed_nodes_available:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(set_nodes_available).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.register_or_update
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                registered_nodes: <% $.registered_nodes or [] %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  provide:
    description: Take a list of nodes and move them to "available"

    input:
      - node_uuids
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      set_nodes_available:
        on-success: cell_v2_discover_hosts
        on-error: set_status_failed_nodes_available
        with-items: uuid in <% $.node_uuids %>
        workflow: tripleo.baremetal.v1.set_node_state
        input:
          node_uuid: <% $.uuid %>
          queue_name: <% $.queue_name %>
          state_action: 'provide'
          target_state: 'available'

      set_status_failed_nodes_available:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(set_nodes_available).result %>

      cell_v2_discover_hosts:
        on-success: try_power_off
        on-error: cell_v2_discover_hosts_failed
        workflow: tripleo.baremetal.v1.cellv2_discovery
        input:
          node_uuids: <% $.node_uuids %>
          queue_name: <% $.queue_name %>
        timeout: 900 #15 minutes
        retry:
          delay: 30
          count: 30

      cell_v2_discover_hosts_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(cell_v2_discover_hosts).result %>

      try_power_off:
        on-success: send_message
        on-error: power_off_failed
        with-items: uuid in <% $.node_uuids %>
        workflow: tripleo.baremetal.v1.set_power_state
        input:
          node_uuid: <% $.uuid %>
          queue_name: <% $.queue_name %>
          state_action: 'off'
          target_state: 'power off'
        publish:
          status: SUCCESS
          message: <% $.node_uuids.len() %> node(s) successfully moved to the "available" state.

      power_off_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(try_power_off).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.provide
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  provide_manageable_nodes:
    description: Provide all nodes in a 'manageable' state.

    input:
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      get_manageable_nodes:
        action: ironic.node_list maintenance=False associated=False
        on-success: provide_manageable
        on-error: set_status_failed_get_manageable_nodes
        publish:
          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>

      set_status_failed_get_manageable_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_manageable_nodes).result %>

      provide_manageable:
        on-success: send_message
        workflow: tripleo.baremetal.v1.provide
        input:
          node_uuids: <% $.managed_nodes %>
          queue_name: <% $.queue_name %>
        publish:
          status: SUCCESS

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.provide_manageable_nodes
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  manage:
    description: Set a list of nodes to 'manageable' state

    input:
      - node_uuids
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      set_nodes_manageable:
        on-success: send_message
        on-error: set_status_failed_nodes_manageable
        with-items: uuid in <% $.node_uuids %>
        workflow: tripleo.baremetal.v1.set_node_state
        input:
          node_uuid: <% $.uuid %>
          state_action: 'manage'
          target_state: 'manageable'
          error_states:
            # node going back to enroll designates power credentials failure
            - 'enroll'
            - 'error'

      set_status_failed_nodes_manageable:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(set_nodes_manageable).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.manage
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  _introspect:
    description: >
        An internal workflow. The tripleo.baremetal.v1.introspect workflow
        should be used for introspection.

    input:
      - node_uuid
      - timeout
      - queue_name

    output:
      result: <% task(start_introspection).result %>

    tags:
      - tripleo-common-managed

    tasks:
      start_introspection:
        action: baremetal_introspection.introspect uuid=<% $.node_uuid %>
        on-success: wait_for_introspection_to_finish
        on-error: set_status_failed_start_introspection

      set_status_failed_start_introspection:
        publish:
          status: FAILED
          message: <% task(start_introspection).result %>
          introspected_nodes: []
        on-success: send_message

      wait_for_introspection_to_finish:
        action: baremetal_introspection.wait_for_finish
        input:
          uuids: <% [$.node_uuid] %>
          # The interval is 10 seconds, so divide to make the overall timeout
          # in seconds correct.
          max_retries: <% $.timeout / 10 %>
          retry_interval: 10
        publish:
          introspected_node: <% task().result.values().first() %>
          status: <% bool(task().result.values().first().error) and "FAILED" or "SUCCESS" %>
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-success: wait_for_introspection_to_finish_success
        on-error: wait_for_introspection_to_finish_error

      wait_for_introspection_to_finish_success:
        publish:
          message: <% "Introspection of node {0} completed. Status:{1}. Errors:{2}".format($.introspected_node.uuid, $.status, $.introspected_node.error) %>
        on-success: send_message

      wait_for_introspection_to_finish_error:
        publish:
          message: <% "Introspection of node {0} timed out.".format($.node_uuid) %>
        on-success: send_message

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1._introspect
              payload:
                status: <% $.status %>
                message: <% $.message %>
                introspected_node: <% $.get('introspected_node') %>
                node_uuid: <% $.node_uuid %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  introspect:
    description: >
        Take a list of nodes and move them through introspection.

        By default each node will attempt introspection up to 3 times (two
        retries plus the initial attemp) if it fails. This behaviour can be
        modified by changing the max_retry_attempts input.

        The workflow will assume the node has timed out after 20 minutes (1200
        seconds). This can be changed by passing the node_timeout input in
        seconds.

    input:
      - node_uuids
      - run_validations: False
      - queue_name: tripleo
      - concurrency: 20
      - max_retry_attempts: 2
      - node_timeout: 1200

    tags:
      - tripleo-common-managed

    task-defaults:
      on-error: unhandled_error

    tasks:
      initialize:
        publish:
          introspection_attempt: 1
        on-complete:
          - run_validations: <% $.run_validations %>
          - introspect_nodes: <% not $.run_validations %>

      run_validations:
        workflow: tripleo.validations.v1.run_groups
        input:
          group_names:
            - 'pre-introspection'
          queue_name: <% $.queue_name %>
        on-success: introspect_nodes
        on-error: set_validations_failed

      set_validations_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(run_validations).result %>

      introspect_nodes:
        with-items: uuid in <% $.node_uuids %>
        concurrency: <% $.concurrency %>
        workflow: _introspect
        input:
          node_uuid: <% $.uuid %>
          queue_name: <% $.queue_name %>
          timeout: <% $.node_timeout %>
        # on-error is triggered if one or more nodes failed introspection. We
        # still go to get_introspection_status as it will collect the result
        # for each node. Unless we hit the retry limit.
        on-error:
         - get_introspection_status: <% $.introspection_attempt <= $.max_retry_attempts %>
         - max_retry_attempts_reached: <% $.introspection_attempt > $.max_retry_attempts %>
        on-success: get_introspection_status

      get_introspection_status:
        with-items: uuid in <% $.node_uuids %>
        action: baremetal_introspection.get_status
        input:
          uuid: <% $.uuid %>
        publish:
          introspected_nodes: <% task().result.toDict($.uuid, $) %>
          # Currently there is no way for us to ignore user introspection
          # aborts. This means we will retry aborted nodes until the Ironic API
          # gives us more details (error code or a boolean to show aborts etc.)
          # If a node hasn't finished, we consider it to be failed.
          # TODO(d0ugal): When possible, don't retry introspection of nodes
          #               that a user manually aborted.
          failed_introspection: <% task().result.where($.finished = true and $.error != null).select($.uuid) + task().result.where($.finished = false).select($.uuid) %>
        publish-on-error:
          # If a node fails to start introspection, getting the status can fail.
          # When that happens, the result is a string and the nodes need to be
          # filtered out.
          introspected_nodes: <% task().result.where(isDict($)).toDict($.uuid, $) %>
          # If there was an error, the exception string we get doesn't give us
          # the UUID. So we use a set difference to find the UUIDs missing in
          # the results. These are then added to the failed nodes.
          failed_introspection: <% ($.node_uuids.toSet() - task().result.where(isDict($)).select($.uuid).toSet()) + task().result.where(isDict($)).where($.finished = true and $.error != null).toSet() + task().result.where(isDict($)).where($.finished = false).toSet() %>
        on-error: increase_attempt_counter
        on-success:
          - successful_introspection: <% $.failed_introspection.len() = 0 %>
          - increase_attempt_counter: <% $.failed_introspection.len() > 0 %>

      increase_attempt_counter:
        publish:
          introspection_attempt: <% $.introspection_attempt + 1 %>
        on-complete:
          retry_failed_nodes

      retry_failed_nodes:
        publish:
          status: RUNNING
          message: <% 'Retrying {0} nodes that failed introspection. Attempt {1} of {2} '.format($.failed_introspection.len(), $.introspection_attempt, $.max_retry_attempts + 1) %>
          # We are about to retry, update the tracking stats.
          node_uuids: <% $.failed_introspection %>
        on-success:
          - send_message
          - introspect_nodes

      max_retry_attempts_reached:
        publish:
          status: FAILED
          message: <% 'Retry limit reached with {0} nodes still failing introspection'.format($.failed_introspection.len()) %>
        on-complete: send_message

      successful_introspection:
        publish:
          status: SUCCESS
          message: Successfully introspected <% $.introspected_nodes.len() %> node(s).
        on-complete: send_message

      unhandled_error:
        publish:
          status: FAILED
          message: "Unhandled workflow error"
        on-complete: send_message

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.introspect
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                introspected_nodes: <% $.get('introspected_nodes', []) %>
                failed_introspection: <% $.get('failed_introspection', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  introspect_manageable_nodes:
    description: Introspect all nodes in a 'manageable' state.

    input:
      - run_validations: False
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      get_manageable_nodes:
        action: ironic.node_list maintenance=False associated=False
        on-success: validate_nodes
        on-error: set_status_failed_get_manageable_nodes
        publish:
          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>

      set_status_failed_get_manageable_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_manageable_nodes).result %>

      validate_nodes:
        on-success:
          - introspect_manageable: <% $.managed_nodes.len() > 0 %>
          - set_status_failed_no_nodes: <% $.managed_nodes.len() = 0 %>

      set_status_failed_no_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: No manageable nodes to introspect. Check node states and maintenance.

      introspect_manageable:
        on-success: send_message
        on-error: set_status_introspect_manageable
        workflow: tripleo.baremetal.v1.introspect
        input:
          node_uuids: <% $.managed_nodes %>
          run_validations: <% $.run_validations %>
          queue_name: <% $.queue_name %>
        publish:
          introspected_nodes: <% task().result.introspected_nodes %>

      set_status_introspect_manageable:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(introspect_manageable).result %>
          introspected_nodes: []

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.introspect_manageable_nodes
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                introspected_nodes: <% $.get('introspected_nodes', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  configure:
    description: Take a list of manageable nodes and update their boot configuration.

    input:
      - node_uuids
      - queue_name: tripleo
      - kernel_name: bm-deploy-kernel
      - ramdisk_name: bm-deploy-ramdisk
      - instance_boot_option: null
      - root_device: null
      - root_device_minimum_size: 4
      - overwrite_root_device_hints: False

    tags:
      - tripleo-common-managed

    tasks:

      configure_boot:
        on-success: configure_root_device
        on-error: set_status_failed_configure_boot
        with-items: node_uuid in <% $.node_uuids %>
        action: tripleo.baremetal.configure_boot node_uuid=<% $.node_uuid %> kernel_name=<% $.kernel_name %> ramdisk_name=<% $.ramdisk_name %> instance_boot_option=<% $.instance_boot_option %>

      configure_root_device:
        on-success: send_message
        on-error: set_status_failed_configure_root_device
        with-items: node_uuid in <% $.node_uuids %>
        action: tripleo.baremetal.configure_root_device node_uuid=<% $.node_uuid %> root_device=<% $.root_device %> minimum_size=<% $.root_device_minimum_size %> overwrite=<% $.overwrite_root_device_hints %>
        publish:
          status: SUCCESS
          message: 'Successfully configured the nodes.'

      set_status_failed_configure_boot:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(configure_boot).result %>

      set_status_failed_configure_root_device:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(configure_root_device).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.configure
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  configure_manageable_nodes:
    description: Update the boot configuration of all nodes in 'manageable' state.

    input:
      - queue_name: tripleo
      - kernel_name: 'bm-deploy-kernel'
      - ramdisk_name: 'bm-deploy-ramdisk'
      - instance_boot_option: null
      - root_device: null
      - root_device_minimum_size: 4
      - overwrite_root_device_hints: False

    tags:
      - tripleo-common-managed

    tasks:

      get_manageable_nodes:
        action: ironic.node_list maintenance=False associated=False
        on-success: configure_manageable
        on-error: set_status_failed_get_manageable_nodes
        publish:
          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>

      configure_manageable:
        on-success: send_message
        on-error: set_status_failed_configure_manageable
        workflow: tripleo.baremetal.v1.configure
        input:
          node_uuids: <% $.managed_nodes %>
          queue_name: <% $.queue_name %>
          kernel_name: <% $.kernel_name %>
          ramdisk_name: <% $.ramdisk_name %>
          instance_boot_option: <% $.instance_boot_option %>
          root_device: <% $.root_device %>
          root_device_minimum_size: <% $.root_device_minimum_size %>
          overwrite_root_device_hints: <% $.overwrite_root_device_hints %>
        publish:
          message: 'Manageable nodes configured successfully.'

      set_status_failed_configure_manageable:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(configure_manageable).result %>

      set_status_failed_get_manageable_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_manageable_nodes).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.configure_manageable_nodes
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  tag_node:
    description: Tag a node with a role
    input:
      - node_uuid
      - role: null
      - queue_name: tripleo

    task-defaults:
      on-error: send_message

    tags:
      - tripleo-common-managed

    tasks:

      update_node:
        on-success: send_message
        action: tripleo.baremetal.update_node_capability node_uuid=<% $.node_uuid %> capability='profile' value=<% $.role %>
        publish:
          message: <% task().result %>
          status: SUCCESS

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.tag_node
              payload:
                status: <% $.get('status', 'FAILED') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  tag_nodes:
    description: Runs the tag_node workflow in a loop
    input:
      - tag_node_uuids
      - untag_node_uuids
      - role
      - plan: overcloud
      - queue_name: tripleo

    task-defaults:
      on-error: send_message

    tags:
      - tripleo-common-managed

    tasks:

      tag_nodes:
        with-items: node_uuid in <% $.tag_node_uuids %>
        workflow: tripleo.baremetal.v1.tag_node
        input:
          node_uuid: <% $.node_uuid %>
          queue_name: <% $.queue_name %>
          role: <% $.role %>
        concurrency: 1
        on-success: untag_nodes

      untag_nodes:
        with-items: node_uuid in <% $.untag_node_uuids %>
        workflow: tripleo.baremetal.v1.tag_node
        input:
          node_uuid: <% $.node_uuid %>
          queue_name: <% $.queue_name %>
        concurrency: 1
        on-success: update_role_parameters

      update_role_parameters:
        on-success: send_message
        action: tripleo.parameters.update_role role=<% $.role %> container=<% $.plan %>
        publish:
          message: <% task().result %>
          status: SUCCESS

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.tag_nodes
              payload:
                status: <% $.get('status', 'FAILED') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  nodes_with_hint:
    description: Find nodes matching a hint regex
    input:
      - hint_regex
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      get_nodes:
        with-items: provision_state in <% ['available', 'active'] %>
        action: ironic.node_list maintenance=false provision_state=<% $.provision_state %> detail=true
        on-success: get_matching_nodes
        on-error: set_status_failed_get_nodes

      get_matching_nodes:
        with-items: node in <% task(get_nodes).result.flatten() %>
        action: tripleo.baremetal.get_node_hint node=<% $.node %>
        on-success: send_message
        on-error: set_status_failed_get_matching_nodes
        publish:
          matching_nodes: <% let(hint_regex => $.hint_regex) -> task().result.where($.hint and $.hint.matches($hint_regex)).uuid %>

      set_status_failed_get_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_nodes).result %>

      set_status_failed_get_matching_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_matching_nodes).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.nodes_with_hint
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                matching_nodes: <% $.matching_nodes or [] %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  nodes_with_profile:
    description: Find nodes with a specific profile
    input:
      - profile
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      get_active_nodes:
        action: ironic.node_list maintenance=false provision_state='active' detail=true
        on-success: get_available_nodes
        on-error: set_status_failed_get_active_nodes

      get_available_nodes:
        action: ironic.node_list maintenance=false provision_state='available' detail=true
        on-success: get_matching_nodes
        on-error: set_status_failed_get_available_nodes

      get_matching_nodes:
        with-items: node in <% task(get_available_nodes).result + task(get_active_nodes).result %>
        action: tripleo.baremetal.get_profile node=<% $.node %>
        on-success: send_message
        on-error: set_status_failed_get_matching_nodes
        publish:
          matching_nodes: <% let(input_profile_name => $.profile) -> task().result.where($.profile = $input_profile_name).uuid %>

      set_status_failed_get_active_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_active_nodes).result %>

      set_status_failed_get_available_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_available_nodes).result %>

      set_status_failed_get_matching_nodes:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_matching_nodes).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.nodes_with_profile
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                matching_nodes: <% $.matching_nodes or [] %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  create_raid_configuration:
    description: Create and apply RAID configuration for given nodes
    input:
      - node_uuids
      - configuration
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      set_configuration:
        with-items: node_uuid in <% $.node_uuids %>
        action: ironic.node_set_target_raid_config node_ident=<% $.node_uuid %> target_raid_config=<% $.configuration %>
        on-success: apply_configuration
        on-error: set_configuration_failed

      set_configuration_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(set_configuration).result %>

      apply_configuration:
        with-items: node_uuid in <% $.node_uuids %>
        workflow: tripleo.baremetal.v1.manual_cleaning
        input:
          node_uuid: <% $.node_uuid %>
          clean_steps:
            - interface: raid
              step: delete_configuration
            - interface: raid
              step: create_configuration
          timeout: 1800  # building RAID should be fast than general cleaning
          retry_count: 180
          retry_delay: 10
        on-success: send_message
        on-error: apply_configuration_failed
        publish:
          message: <% task().result %>
          status: SUCCESS

      apply_configuration_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(apply_configuration).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.create_raid_configuration
              payload:
                status: <% $.get('status', 'FAILED') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>


  cellv2_discovery:
    description: Run cell_v2 host discovery

    input:
      - node_uuids
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      cell_v2_discover_hosts:
        on-success: wait_for_nova_resources
        on-error: cell_v2_discover_hosts_failed
        action: tripleo.baremetal.cell_v2_discover_hosts

      cell_v2_discover_hosts_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(cell_v2_discover_hosts).result %>

      wait_for_nova_resources:
        on-success: send_message
        on-error: wait_for_nova_resources_failed
        with-items: node_uuid in <% $.node_uuids %>
        action: nova.hypervisors_find hypervisor_hostname=<% $.node_uuid %>

      wait_for_nova_resources_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(wait_for_nova_resources).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.cellv2_discovery
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>


  discover_nodes:
    description: Run nodes discovery over the given IP range

    input:
      - ip_addresses
      - credentials
      - ports: [623]
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      get_all_nodes:
        action: ironic.node_list
        input:
          fields: ["uuid", "driver", "driver_info"]
          limit: 0
        on-success: get_candidate_nodes
        on-error: get_all_nodes_failed
        publish:
          existing_nodes: <% task().result %>

      get_all_nodes_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_all_nodes).result %>

      get_candidate_nodes:
        action: tripleo.baremetal.get_candidate_nodes
        input:
          ip_addresses: <% $.ip_addresses %>
          credentials: <% $.credentials %>
          ports: <% $.ports %>
          existing_nodes: <% $.existing_nodes %>
        on-success: probe_nodes
        on-error: get_candidate_nodes_failed
        publish:
          candidates: <% task().result %>

      get_candidate_nodes_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_candidate_nodes).result %>

      probe_nodes:
        action: tripleo.baremetal.probe_node
        on-success: send_message
        on-error: probe_nodes_failed
        input:
          ip: <% $.node.ip %>
          port: <% $.node.port %>
          username: <% $.node.username %>
          password: <% $.node.password %>
        with-items:
          - node in <% $.candidates %>
        publish:
          nodes_json: <% task().result.where($ != null) %>

      probe_nodes_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(probe_nodes).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.discover_nodes
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                nodes_json: <% $.get('nodes_json', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  discover_and_enroll_nodes:
    description: Run nodes discovery over the given IP range and enroll nodes

    input:
      - ip_addresses
      - credentials
      - ports: [623]
      - kernel_name: null
      - ramdisk_name: null
      - instance_boot_option: local
      - initial_state: manageable
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      discover_nodes:
        workflow: tripleo.baremetal.v1.discover_nodes
        input:
          ip_addresses: <% $.ip_addresses %>
          ports: <% $.ports %>
          credentials: <% $.credentials %>
          queue_name: <% $.queue_name %>
        on-success: enroll_nodes
        on-error: discover_nodes_failed
        publish:
          nodes_json: <% task().result.nodes_json %>

      discover_nodes_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(discover_nodes).result %>

      enroll_nodes:
        workflow: tripleo.baremetal.v1.register_or_update
        input:
          nodes_json: <% $.nodes_json %>
          kernel_name: <% $.kernel_name %>
          ramdisk_name: <% $.ramdisk_name %>
          instance_boot_option: <% $.instance_boot_option %>
          initial_state: <% $.initial_state %>
        on-success: send_message
        on-error: enroll_nodes_failed
        publish:
          registered_nodes: <% task().result.registered_nodes %>

      enroll_nodes_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(enroll_nodes).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.baremetal.v1.discover_and_enroll_nodes
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                registered_nodes: <% $.get('registered_nodes', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-04 06:26:00,053 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 45010
2018-12-04 06:26:00,095 DEBUG: RESP: [201] Content-Length: 45010 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:00 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.baremetal.v1\ndescription: TripleO Baremetal Workflows\n\nworkflows:\n\n  set_node_state:\n    input:\n      - node_uuid\n      - state_action\n      - target_state\n      - error_states:\n          # The default includes all failure states, even unused by TripleO.\n          - 'error'\n          - 'adopt failed'\n          - 'clean failed'\n          - 'deploy failed'\n          - 'inspect failed'\n          - 'rescue failed'\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_provision_state:\n        on-success: wait_for_provision_state\n        on-error: set_provision_state_failed\n        action: ironic.node_set_provision_state node_uuid=<% $.node_uuid %> state=<% $.state_action %>\n\n      set_provision_state_failed:\n        publish:\n          message: <% task(set_provision_state).result %>\n        on-complete: fail\n\n      wait_for_provision_state:\n        action: ironic.node_get\n        input:\n          node_id: <% $.node_uuid %>\n          fields: ['provision_state', 'last_error']\n        timeout: 1200 #20 minutes\n        retry:\n          delay: 3\n          count: 400\n          continue-on: <% not task().result.provision_state in [$.target_state] + $.error_states %>\n        on-complete:\n          - state_not_reached: <% task().result.provision_state != $.target_state %>\n\n      state_not_reached:\n        publish:\n          message: >-\n            Node <% $.node_uuid %> did not reach state \"<% $.target_state %>\",\n            the state is \"<% task(wait_for_provision_state).result.provision_state %>\",\n            error: <% task(wait_for_provision_state).result.last_error %>\n        on-complete: fail\n\n    output-on-error:\n      result: <% $.message %>\n\n  set_power_state:\n    input:\n      - node_uuid\n      - state_action\n      - target_state\n      - error_state: 'error'\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_power_state:\n        on-success: wait_for_power_state\n        on-error: set_power_state_failed\n        action: ironic.node_set_power_state node_id=<% $.node_uuid %> state=<% $.state_action %>\n\n      set_power_state_failed:\n        publish:\n          message: <% task(set_power_state).result %>\n        on-complete: fail\n\n      wait_for_power_state:\n        action: ironic.node_get\n        input:\n          node_id: <% $.node_uuid %>\n          fields: ['power_state', 'last_error']\n        timeout: 120 #2 minutes\n        retry:\n          delay: 6\n          count: 20\n          continue-on: <% not task().result.power_state in [$.target_state, $.error_state] %>\n        on-complete:\n          - state_not_reached: <% task().result.power_state != $.target_state %>\n\n      state_not_reached:\n        publish:\n          message: >-\n            Node <% $.node_uuid %> did not reach power state \"<% $.target_state %>\",\n            the state is \"<% task(wait_for_power_state).result.power_state %>\",\n            error: <% task(wait_for_power_state).result.last_error %>\n        on-complete: fail\n\n    output-on-error:\n      result: <% $.message %>\n\n  manual_cleaning:\n    input:\n      - node_uuid\n      - clean_steps\n      - timeout: 7200  # 2 hours (cleaning can take really long)\n      - retry_delay: 10\n      - retry_count: 720\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_provision_state:\n        on-success: wait_for_provision_state\n        on-error: set_provision_state_failed\n        action: ironic.node_set_provision_state node_uuid=<% $.node_uuid %> state='clean' cleansteps=<% $.clean_steps %>\n\n      set_provision_state_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_provision_state).result %>\n\n      wait_for_provision_state:\n        on-success: send_message\n        action: ironic.node_get node_id=<% $.node_uuid %>\n        timeout: <% $.timeout %>\n        retry:\n          delay: <% $.retry_delay %>\n          count: <% $.retry_count %>\n          continue-on: <% task().result.provision_state != 'manageable' %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.manual_cleaning\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_nodes:\n    description: Validate nodes JSON\n\n    input:\n     - nodes_json\n     - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      validate_nodes:\n        action: tripleo.baremetal.validate_nodes\n        on-success: send_message\n        on-error: validation_failed\n        input:\n           nodes_json: <% $.nodes_json %>\n\n      validation_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(validate_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.validate_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  register_or_update:\n    description: Take nodes JSON and create nodes in a \"manageable\" state\n\n    input:\n     - nodes_json\n     - remove: False\n     - queue_name: tripleo\n     - kernel_name: null\n     - ramdisk_name: null\n     - instance_boot_option: local\n     - initial_state: manageable\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      validate_input:\n        workflow: tripleo.baremetal.v1.validate_nodes\n        on-success: register_or_update_nodes\n        on-error: validation_failed\n        input:\n          nodes_json: <% $.nodes_json %>\n          queue_name: <% $.queue_name %>\n\n      validation_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(validate_input).result %>\n          registered_nodes: []\n\n      register_or_update_nodes:\n        action: tripleo.baremetal.register_or_update_nodes\n        on-success:\n          - set_nodes_managed: <% $.initial_state != \"enroll\" %>\n          - send_message: <% $.initial_state = \"enroll\" %>\n        on-error: set_status_failed_register_or_update_nodes\n        input:\n           nodes_json: <% $.nodes_json %>\n           remove: <% $.remove %>\n           kernel_name: <% $.kernel_name %>\n           ramdisk_name: <% $.ramdisk_name %>\n           instance_boot_option: <% $.instance_boot_option %>\n        publish:\n          registered_nodes: <% task().result %>\n          new_nodes: <% task().result.where($.provision_state = 'enroll') %>\n\n      set_status_failed_register_or_update_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(register_or_update_nodes).result %>\n          registered_nodes: []\n\n      set_nodes_managed:\n        on-success:\n          - set_nodes_available: <% $.initial_state = \"available\" %>\n          - send_message: <% $.initial_state != \"available\" %>\n        on-error: set_status_failed_nodes_managed\n        workflow: tripleo.baremetal.v1.manage\n        input:\n          node_uuids: <% $.new_nodes.uuid %>\n          queue_name: <% $.queue_name %>\n        publish:\n          status: SUCCESS\n          message: <% $.new_nodes.len() %> node(s) successfully moved to the \"manageable\" state.\n\n      set_status_failed_nodes_managed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_nodes_managed).result %>\n\n      set_nodes_available:\n        on-success: send_message\n        on-error: set_status_failed_nodes_available\n        workflow: tripleo.baremetal.v1.provide node_uuids=<% $.new_nodes.uuid %> queue_name=<% $.queue_name %>\n        publish:\n          status: SUCCESS\n          message: <% $.new_nodes.len() %> node(s) successfully moved to the \"available\" state.\n\n      set_status_failed_nodes_available:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_nodes_available).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.register_or_update\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                registered_nodes: <% $.registered_nodes or [] %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  provide:\n    description: Take a list of nodes and move them to \"available\"\n\n    input:\n      - node_uuids\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_nodes_available:\n        on-success: cell_v2_discover_hosts\n        on-error: set_status_failed_nodes_available\n        with-items: uuid in <% $.node_uuids %>\n        workflow: tripleo.baremetal.v1.set_node_state\n        input:\n          node_uuid: <% $.uuid %>\n          queue_name: <% $.queue_name %>\n          state_action: 'provide'\n          target_state: 'available'\n\n      set_status_failed_nodes_available:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_nodes_available).result %>\n\n      cell_v2_discover_hosts:\n        on-success: try_power_off\n        on-error: cell_v2_discover_hosts_failed\n        workflow: tripleo.baremetal.v1.cellv2_discovery\n        input:\n          node_uuids: <% $.node_uuids %>\n          queue_name: <% $.queue_name %>\n        timeout: 900 #15 minutes\n        retry:\n          delay: 30\n          count: 30\n\n      cell_v2_discover_hosts_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(cell_v2_discover_hosts).result %>\n\n      try_power_off:\n        on-success: send_message\n        on-error: power_off_failed\n        with-items: uuid in <% $.node_uuids %>\n        workflow: tripleo.baremetal.v1.set_power_state\n        input:\n          node_uuid: <% $.uuid %>\n          queue_name: <% $.queue_name %>\n          state_action: 'off'\n          target_state: 'power off'\n        publish:\n          status: SUCCESS\n          message: <% $.node_uuids.len() %> node(s) successfully moved to the \"available\" state.\n\n      power_off_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(try_power_off).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.provide\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  provide_manageable_nodes:\n    description: Provide all nodes in a 'manageable' state.\n\n    input:\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_manageable_nodes:\n        action: ironic.node_list maintenance=False associated=False\n        on-success: provide_manageable\n        on-error: set_status_failed_get_manageable_nodes\n        publish:\n          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>\n\n      set_status_failed_get_manageable_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_manageable_nodes).result %>\n\n      provide_manageable:\n        on-success: send_message\n        workflow: tripleo.baremetal.v1.provide\n        input:\n          node_uuids: <% $.managed_nodes %>\n          queue_name: <% $.queue_name %>\n        publish:\n          status: SUCCESS\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.provide_manageable_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  manage:\n    description: Set a list of nodes to 'manageable' state\n\n    input:\n      - node_uuids\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_nodes_manageable:\n        on-success: send_message\n        on-error: set_status_failed_nodes_manageable\n        with-items: uuid in <% $.node_uuids %>\n        workflow: tripleo.baremetal.v1.set_node_state\n        input:\n          node_uuid: <% $.uuid %>\n          state_action: 'manage'\n          target_state: 'manageable'\n          error_states:\n            # node going back to enroll designates power credentials failure\n            - 'enroll'\n            - 'error'\n\n      set_status_failed_nodes_manageable:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_nodes_manageable).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.manage\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  _introspect:\n    description: >\n        An internal workflow. The tripleo.baremetal.v1.introspect workflow\n        should be used for introspection.\n\n    input:\n      - node_uuid\n      - timeout\n      - queue_name\n\n    output:\n      result: <% task(start_introspection).result %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      start_introspection:\n        action: baremetal_introspection.introspect uuid=<% $.node_uuid %>\n        on-success: wait_for_introspection_to_finish\n        on-error: set_status_failed_start_introspection\n\n      set_status_failed_start_introspection:\n        publish:\n          status: FAILED\n          message: <% task(start_introspection).result %>\n          introspected_nodes: []\n        on-success: send_message\n\n      wait_for_introspection_to_finish:\n        action: baremetal_introspection.wait_for_finish\n        input:\n          uuids: <% [$.node_uuid] %>\n          # The interval is 10 seconds, so divide to make the overall timeout\n          # in seconds correct.\n          max_retries: <% $.timeout / 10 %>\n          retry_interval: 10\n        publish:\n          introspected_node: <% task().result.values().first() %>\n          status: <% bool(task().result.values().first().error) and \"FAILED\" or \"SUCCESS\" %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-success: wait_for_introspection_to_finish_success\n        on-error: wait_for_introspection_to_finish_error\n\n      wait_for_introspection_to_finish_success:\n        publish:\n          message: <% \"Introspection of node {0} completed. Status:{1}. Errors:{2}\".format($.introspected_node.uuid, $.status, $.introspected_node.error) %>\n        on-success: send_message\n\n      wait_for_introspection_to_finish_error:\n        publish:\n          message: <% \"Introspection of node {0} timed out.\".format($.node_uuid) %>\n        on-success: send_message\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1._introspect\n              payload:\n                status: <% $.status %>\n                message: <% $.message %>\n                introspected_node: <% $.get('introspected_node') %>\n                node_uuid: <% $.node_uuid %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  introspect:\n    description: >\n        Take a list of nodes and move them through introspection.\n\n        By default each node will attempt introspection up to 3 times (two\n        retries plus the initial attemp) if it fails. This behaviour can be\n        modified by changing the max_retry_attempts input.\n\n        The workflow will assume the node has timed out after 20 minutes (1200\n        seconds). This can be changed by passing the node_timeout input in\n        seconds.\n\n    input:\n      - node_uuids\n      - run_validations: False\n      - queue_name: tripleo\n      - concurrency: 20\n      - max_retry_attempts: 2\n      - node_timeout: 1200\n\n    tags:\n      - tripleo-common-managed\n\n    task-defaults:\n      on-error: unhandled_error\n\n    tasks:\n      initialize:\n        publish:\n          introspection_attempt: 1\n        on-complete:\n          - run_validations: <% $.run_validations %>\n          - introspect_nodes: <% not $.run_validations %>\n\n      run_validations:\n        workflow: tripleo.validations.v1.run_groups\n        input:\n          group_names:\n            - 'pre-introspection'\n          queue_name: <% $.queue_name %>\n        on-success: introspect_nodes\n        on-error: set_validations_failed\n\n      set_validations_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(run_validations).result %>\n\n      introspect_nodes:\n        with-items: uuid in <% $.node_uuids %>\n        concurrency: <% $.concurrency %>\n        workflow: _introspect\n        input:\n          node_uuid: <% $.uuid %>\n          queue_name: <% $.queue_name %>\n          timeout: <% $.node_timeout %>\n        # on-error is triggered if one or more nodes failed introspection. We\n        # still go to get_introspection_status as it will collect the result\n        # for each node. Unless we hit the retry limit.\n        on-error:\n         - get_introspection_status: <% $.introspection_attempt <= $.max_retry_attempts %>\n         - max_retry_attempts_reached: <% $.introspection_attempt > $.max_retry_attempts %>\n        on-success: get_introspection_status\n\n      get_introspection_status:\n        with-items: uuid in <% $.node_uuids %>\n        action: baremetal_introspection.get_status\n        input:\n          uuid: <% $.uuid %>\n        publish:\n          introspected_nodes: <% task().result.toDict($.uuid, $) %>\n          # Currently there is no way for us to ignore user introspection\n          # aborts. This means we will retry aborted nodes until the Ironic API\n          # gives us more details (error code or a boolean to show aborts etc.)\n          # If a node hasn't finished, we consider it to be failed.\n          # TODO(d0ugal): When possible, don't retry introspection of nodes\n          #               that a user manually aborted.\n          failed_introspection: <% task().result.where($.finished = true and $.error != null).select($.uuid) + task().result.where($.finished = false).select($.uuid) %>\n        publish-on-error:\n          # If a node fails to start introspection, getting the status can fail.\n          # When that happens, the result is a string and the nodes need to be\n          # filtered out.\n          introspected_nodes: <% task().result.where(isDict($)).toDict($.uuid, $) %>\n          # If there was an error, the exception string we get doesn't give us\n          # the UUID. So we use a set difference to find the UUIDs missing in\n          # the results. These are then added to the failed nodes.\n          failed_introspection: <% ($.node_uuids.toSet() - task().result.where(isDict($)).select($.uuid).toSet()) + task().result.where(isDict($)).where($.finished = true and $.error != null).toSet() + task().result.where(isDict($)).where($.finished = false).toSet() %>\n        on-error: increase_attempt_counter\n        on-success:\n          - successful_introspection: <% $.failed_introspection.len() = 0 %>\n          - increase_attempt_counter: <% $.failed_introspection.len() > 0 %>\n\n      increase_attempt_counter:\n        publish:\n          introspection_attempt: <% $.introspection_attempt + 1 %>\n        on-complete:\n          retry_failed_nodes\n\n      retry_failed_nodes:\n        publish:\n          status: RUNNING\n          message: <% 'Retrying {0} nodes that failed introspection. Attempt {1} of {2} '.format($.failed_introspection.len(), $.introspection_attempt, $.max_retry_attempts + 1) %>\n          # We are about to retry, update the tracking stats.\n          node_uuids: <% $.failed_introspection %>\n        on-success:\n          - send_message\n          - introspect_nodes\n\n      max_retry_attempts_reached:\n        publish:\n          status: FAILED\n          message: <% 'Retry limit reached with {0} nodes still failing introspection'.format($.failed_introspection.len()) %>\n        on-complete: send_message\n\n      successful_introspection:\n        publish:\n          status: SUCCESS\n          message: Successfully introspected <% $.introspected_nodes.len() %> node(s).\n        on-complete: send_message\n\n      unhandled_error:\n        publish:\n          status: FAILED\n          message: \"Unhandled workflow error\"\n        on-complete: send_message\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.introspect\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                introspected_nodes: <% $.get('introspected_nodes', []) %>\n                failed_introspection: <% $.get('failed_introspection', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  introspect_manageable_nodes:\n    description: Introspect all nodes in a 'manageable' state.\n\n    input:\n      - run_validations: False\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_manageable_nodes:\n        action: ironic.node_list maintenance=False associated=False\n        on-success: validate_nodes\n        on-error: set_status_failed_get_manageable_nodes\n        publish:\n          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>\n\n      set_status_failed_get_manageable_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_manageable_nodes).result %>\n\n      validate_nodes:\n        on-success:\n          - introspect_manageable: <% $.managed_nodes.len() > 0 %>\n          - set_status_failed_no_nodes: <% $.managed_nodes.len() = 0 %>\n\n      set_status_failed_no_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: No manageable nodes to introspect. Check node states and maintenance.\n\n      introspect_manageable:\n        on-success: send_message\n        on-error: set_status_introspect_manageable\n        workflow: tripleo.baremetal.v1.introspect\n        input:\n          node_uuids: <% $.managed_nodes %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          introspected_nodes: <% task().result.introspected_nodes %>\n\n      set_status_introspect_manageable:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(introspect_manageable).result %>\n          introspected_nodes: []\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.introspect_manageable_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                introspected_nodes: <% $.get('introspected_nodes', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  configure:\n    description: Take a list of manageable nodes and update their boot configuration.\n\n    input:\n      - node_uuids\n      - queue_name: tripleo\n      - kernel_name: bm-deploy-kernel\n      - ramdisk_name: bm-deploy-ramdisk\n      - instance_boot_option: null\n      - root_device: null\n      - root_device_minimum_size: 4\n      - overwrite_root_device_hints: False\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      configure_boot:\n        on-success: configure_root_device\n        on-error: set_status_failed_configure_boot\n        with-items: node_uuid in <% $.node_uuids %>\n        action: tripleo.baremetal.configure_boot node_uuid=<% $.node_uuid %> kernel_name=<% $.kernel_name %> ramdisk_name=<% $.ramdisk_name %> instance_boot_option=<% $.instance_boot_option %>\n\n      configure_root_device:\n        on-success: send_message\n        on-error: set_status_failed_configure_root_device\n        with-items: node_uuid in <% $.node_uuids %>\n        action: tripleo.baremetal.configure_root_device node_uuid=<% $.node_uuid %> root_device=<% $.root_device %> minimum_size=<% $.root_device_minimum_size %> overwrite=<% $.overwrite_root_device_hints %>\n        publish:\n          status: SUCCESS\n          message: 'Successfully configured the nodes.'\n\n      set_status_failed_configure_boot:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(configure_boot).result %>\n\n      set_status_failed_configure_root_device:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(configure_root_device).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.configure\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  configure_manageable_nodes:\n    description: Update the boot configuration of all nodes in 'manageable' state.\n\n    input:\n      - queue_name: tripleo\n      - kernel_name: 'bm-deploy-kernel'\n      - ramdisk_name: 'bm-deploy-ramdisk'\n      - instance_boot_option: null\n      - root_device: null\n      - root_device_minimum_size: 4\n      - overwrite_root_device_hints: False\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_manageable_nodes:\n        action: ironic.node_list maintenance=False associated=False\n        on-success: configure_manageable\n        on-error: set_status_failed_get_manageable_nodes\n        publish:\n          managed_nodes: <% task().result.where($.provision_state = 'manageable').uuid %>\n\n      configure_manageable:\n        on-success: send_message\n        on-error: set_status_failed_configure_manageable\n        workflow: tripleo.baremetal.v1.configure\n        input:\n          node_uuids: <% $.managed_nodes %>\n          queue_name: <% $.queue_name %>\n          kernel_name: <% $.kernel_name %>\n          ramdisk_name: <% $.ramdisk_name %>\n          instance_boot_option: <% $.instance_boot_option %>\n          root_device: <% $.root_device %>\n          root_device_minimum_size: <% $.root_device_minimum_size %>\n          overwrite_root_device_hints: <% $.overwrite_root_device_hints %>\n        publish:\n          message: 'Manageable nodes configured successfully.'\n\n      set_status_failed_configure_manageable:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(configure_manageable).result %>\n\n      set_status_failed_get_manageable_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_manageable_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.configure_manageable_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  tag_node:\n    description: Tag a node with a role\n    input:\n      - node_uuid\n      - role: null\n      - queue_name: tripleo\n\n    task-defaults:\n      on-error: send_message\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      update_node:\n        on-success: send_message\n        action: tripleo.baremetal.update_node_capability node_uuid=<% $.node_uuid %> capability='profile' value=<% $.role %>\n        publish:\n          message: <% task().result %>\n          status: SUCCESS\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.tag_node\n              payload:\n                status: <% $.get('status', 'FAILED') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  tag_nodes:\n    description: Runs the tag_node workflow in a loop\n    input:\n      - tag_node_uuids\n      - untag_node_uuids\n      - role\n      - plan: overcloud\n      - queue_name: tripleo\n\n    task-defaults:\n      on-error: send_message\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      tag_nodes:\n        with-items: node_uuid in <% $.tag_node_uuids %>\n        workflow: tripleo.baremetal.v1.tag_node\n        input:\n          node_uuid: <% $.node_uuid %>\n          queue_name: <% $.queue_name %>\n          role: <% $.role %>\n        concurrency: 1\n        on-success: untag_nodes\n\n      untag_nodes:\n        with-items: node_uuid in <% $.untag_node_uuids %>\n        workflow: tripleo.baremetal.v1.tag_node\n        input:\n          node_uuid: <% $.node_uuid %>\n          queue_name: <% $.queue_name %>\n        concurrency: 1\n        on-success: update_role_parameters\n\n      update_role_parameters:\n        on-success: send_message\n        action: tripleo.parameters.update_role role=<% $.role %> container=<% $.plan %>\n        publish:\n          message: <% task().result %>\n          status: SUCCESS\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.tag_nodes\n              payload:\n                status: <% $.get('status', 'FAILED') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  nodes_with_hint:\n    description: Find nodes matching a hint regex\n    input:\n      - hint_regex\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_nodes:\n        with-items: provision_state in <% ['available', 'active'] %>\n        action: ironic.node_list maintenance=false provision_state=<% $.provision_state %> detail=true\n        on-success: get_matching_nodes\n        on-error: set_status_failed_get_nodes\n\n      get_matching_nodes:\n        with-items: node in <% task(get_nodes).result.flatten() %>\n        action: tripleo.baremetal.get_node_hint node=<% $.node %>\n        on-success: send_message\n        on-error: set_status_failed_get_matching_nodes\n        publish:\n          matching_nodes: <% let(hint_regex => $.hint_regex) -> task().result.where($.hint and $.hint.matches($hint_regex)).uuid %>\n\n      set_status_failed_get_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_nodes).result %>\n\n      set_status_failed_get_matching_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_matching_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.nodes_with_hint\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                matching_nodes: <% $.matching_nodes or [] %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  nodes_with_profile:\n    description: Find nodes with a specific profile\n    input:\n      - profile\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_active_nodes:\n        action: ironic.node_list maintenance=false provision_state='active' detail=true\n        on-success: get_available_nodes\n        on-error: set_status_failed_get_active_nodes\n\n      get_available_nodes:\n        action: ironic.node_list maintenance=false provision_state='available' detail=true\n        on-success: get_matching_nodes\n        on-error: set_status_failed_get_available_nodes\n\n      get_matching_nodes:\n        with-items: node in <% task(get_available_nodes).result + task(get_active_nodes).result %>\n        action: tripleo.baremetal.get_profile node=<% $.node %>\n        on-success: send_message\n        on-error: set_status_failed_get_matching_nodes\n        publish:\n          matching_nodes: <% let(input_profile_name => $.profile) -> task().result.where($.profile = $input_profile_name).uuid %>\n\n      set_status_failed_get_active_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_active_nodes).result %>\n\n      set_status_failed_get_available_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_available_nodes).result %>\n\n      set_status_failed_get_matching_nodes:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_matching_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.nodes_with_profile\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                matching_nodes: <% $.matching_nodes or [] %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  create_raid_configuration:\n    description: Create and apply RAID configuration for given nodes\n    input:\n      - node_uuids\n      - configuration\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      set_configuration:\n        with-items: node_uuid in <% $.node_uuids %>\n        action: ironic.node_set_target_raid_config node_ident=<% $.node_uuid %> target_raid_config=<% $.configuration %>\n        on-success: apply_configuration\n        on-error: set_configuration_failed\n\n      set_configuration_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(set_configuration).result %>\n\n      apply_configuration:\n        with-items: node_uuid in <% $.node_uuids %>\n        workflow: tripleo.baremetal.v1.manual_cleaning\n        input:\n          node_uuid: <% $.node_uuid %>\n          clean_steps:\n            - interface: raid\n              step: delete_configuration\n            - interface: raid\n              step: create_configuration\n          timeout: 1800  # building RAID should be fast than general cleaning\n          retry_count: 180\n          retry_delay: 10\n        on-success: send_message\n        on-error: apply_configuration_failed\n        publish:\n          message: <% task().result %>\n          status: SUCCESS\n\n      apply_configuration_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(apply_configuration).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.create_raid_configuration\n              payload:\n                status: <% $.get('status', 'FAILED') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n\n  cellv2_discovery:\n    description: Run cell_v2 host discovery\n\n    input:\n      - node_uuids\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      cell_v2_discover_hosts:\n        on-success: wait_for_nova_resources\n        on-error: cell_v2_discover_hosts_failed\n        action: tripleo.baremetal.cell_v2_discover_hosts\n\n      cell_v2_discover_hosts_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(cell_v2_discover_hosts).result %>\n\n      wait_for_nova_resources:\n        on-success: send_message\n        on-error: wait_for_nova_resources_failed\n        with-items: node_uuid in <% $.node_uuids %>\n        action: nova.hypervisors_find hypervisor_hostname=<% $.node_uuid %>\n\n      wait_for_nova_resources_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(wait_for_nova_resources).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.cellv2_discovery\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n\n  discover_nodes:\n    description: Run nodes discovery over the given IP range\n\n    input:\n      - ip_addresses\n      - credentials\n      - ports: [623]\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_all_nodes:\n        action: ironic.node_list\n        input:\n          fields: [\"uuid\", \"driver\", \"driver_info\"]\n          limit: 0\n        on-success: get_candidate_nodes\n        on-error: get_all_nodes_failed\n        publish:\n          existing_nodes: <% task().result %>\n\n      get_all_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_all_nodes).result %>\n\n      get_candidate_nodes:\n        action: tripleo.baremetal.get_candidate_nodes\n        input:\n          ip_addresses: <% $.ip_addresses %>\n          credentials: <% $.credentials %>\n          ports: <% $.ports %>\n          existing_nodes: <% $.existing_nodes %>\n        on-success: probe_nodes\n        on-error: get_candidate_nodes_failed\n        publish:\n          candidates: <% task().result %>\n\n      get_candidate_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_candidate_nodes).result %>\n\n      probe_nodes:\n        action: tripleo.baremetal.probe_node\n        on-success: send_message\n        on-error: probe_nodes_failed\n        input:\n          ip: <% $.node.ip %>\n          port: <% $.node.port %>\n          username: <% $.node.username %>\n          password: <% $.node.password %>\n        with-items:\n          - node in <% $.candidates %>\n        publish:\n          nodes_json: <% task().result.where($ != null) %>\n\n      probe_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(probe_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.discover_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                nodes_json: <% $.get('nodes_json', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  discover_and_enroll_nodes:\n    description: Run nodes discovery over the given IP range and enroll nodes\n\n    input:\n      - ip_addresses\n      - credentials\n      - ports: [623]\n      - kernel_name: null\n      - ramdisk_name: null\n      - instance_boot_option: local\n      - initial_state: manageable\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      discover_nodes:\n        workflow: tripleo.baremetal.v1.discover_nodes\n        input:\n          ip_addresses: <% $.ip_addresses %>\n          ports: <% $.ports %>\n          credentials: <% $.credentials %>\n          queue_name: <% $.queue_name %>\n        on-success: enroll_nodes\n        on-error: discover_nodes_failed\n        publish:\n          nodes_json: <% task().result.nodes_json %>\n\n      discover_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(discover_nodes).result %>\n\n      enroll_nodes:\n        workflow: tripleo.baremetal.v1.register_or_update\n        input:\n          nodes_json: <% $.nodes_json %>\n          kernel_name: <% $.kernel_name %>\n          ramdisk_name: <% $.ramdisk_name %>\n          instance_boot_option: <% $.instance_boot_option %>\n          initial_state: <% $.initial_state %>\n        on-success: send_message\n        on-error: enroll_nodes_failed\n        publish:\n          registered_nodes: <% task().result.registered_nodes %>\n\n      enroll_nodes_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(enroll_nodes).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.baremetal.v1.discover_and_enroll_nodes\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                registered_nodes: <% $.get('registered_nodes', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.baremetal.v1", "tags": [], "created_at": "2018-12-04 11:25:59", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "a0528580-772d-4569-a94c-2080e6b23b97"}

2018-12-04 06:26:00,095 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:00,097 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.storage.v1
description: TripleO manages Ceph with ceph-ansible

workflows:
  ceph-install:
    # allows for additional extra_vars via workflow input
    input:
      - ansible_playbook_verbosity: 0
      - ansible_skip_tags: 'package-install,with_pkg'
      - ansible_env_variables: {}
      - ansible_extra_env_variables:
          ANSIBLE_CONFIG: /usr/share/ceph-ansible/ansible.cfg
          ANSIBLE_ACTION_PLUGINS: /usr/share/ceph-ansible/plugins/actions/
          ANSIBLE_ROLES_PATH: /usr/share/ceph-ansible/roles/
          ANSIBLE_RETRY_FILES_ENABLED: 'False'
          ANSIBLE_LOG_PATH: /var/log/mistral/ceph-install-workflow.log
          ANSIBLE_LIBRARY: /usr/share/ceph-ansible/library/
          ANSIBLE_SSH_RETRIES: '3'
          ANSIBLE_HOST_KEY_CHECKING: 'False'
          DEFAULT_FORKS: '25'
      - ceph_ansible_extra_vars: {}
      - ceph_ansible_playbook: /usr/share/ceph-ansible/site-docker.yml.sample
      - node_data_lookup: '{}'
      - swift_container: 'ceph_ansible_fetch_dir'
    tags:
      - tripleo-common-managed
    tasks:
      set_swift_container:
        publish:
          swift_container: <% concat(env().get('heat_stack_name', ''), '_', $.swift_container) %>
        on-complete: collect_puppet_hieradata
      collect_puppet_hieradata:
        on-success: check_hieradata
        publish:
          hieradata: <% env().get('role_merged_configs', {}).values().select($.keys()).flatten().select(regex('^ceph::profile::params::osds$').search($)).where($ != null).toSet() %>
      check_hieradata:
        on-success:
        - set_blacklisted_ips: <% not bool($.hieradata) %>
        - fail(msg=<% 'Ceph deployment stopped, puppet-ceph hieradata found. Convert it into ceph-ansible variables. {0}'.format($.hieradata) %>): <% bool($.hieradata) %>
      set_blacklisted_ips:
        publish:
          blacklisted_ips: <% env().get('blacklisted_ip_addresses', []) %>
        on-success: set_ip_lists
      set_ip_lists:
        publish:
          mgr_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mgr_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          mon_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mon_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          osd_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_osd_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          mds_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mds_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          rgw_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_rgw_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          nfs_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_nfs_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          rbdmirror_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_rbdmirror_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          client_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_client_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
        on-success: merge_ip_lists
      merge_ip_lists:
        publish:
          ips_list: <% ($.mgr_ips + $.mon_ips + $.osd_ips + $.mds_ips + $.rgw_ips + $.nfs_ips + $.rbdmirror_ips + $.client_ips).toSet() %>
        on-success: enable_ssh_admin
      enable_ssh_admin:
        workflow: tripleo.access.v1.enable_ssh_admin
        input:
          ssh_servers: <% $.ips_list %>
        on-success: get_private_key
      get_private_key:
        action: tripleo.validations.get_privkey
        publish:
          private_key: <% task().result %>
        on-success: make_fetch_directory
      make_fetch_directory:
        action: tripleo.files.make_temp_dir
        publish:
          fetch_directory: <% task().result.path %>
        on-success: verify_container_exists
      verify_container_exists:
        action: swift.head_container container=<% $.swift_container %>
        on-success: restore_fetch_directory
        on-error: create_container
      restore_fetch_directory:
        action: tripleo.files.restore_temp_dir_from_swift
        input:
          container: <% $.swift_container %>
          path: <% $.fetch_directory %>
        on-success: collect_nodes_uuid
      create_container:
        action: swift.put_container container=<% $.swift_container %>
        on-success: collect_nodes_uuid
      collect_nodes_uuid:
        action: tripleo.ansible-playbook
        input:
          inventory:
            overcloud:
              hosts: <% $.ips_list.toDict($, {}) %>
          remote_user: tripleo-admin
          become: true
          become_user: root
          verbosity: 0
          ssh_private_key: <% $.private_key %>
          #NOTE(gfidente): set ANSIBLE_CALLBACK_WHITELIST to empty string to avoid spurious output
          #in the json output. The publish: directive will in fact parse the output.
          extra_env_variables:
            ANSIBLE_CALLBACK_WHITELIST: ''
            ANSIBLE_HOST_KEY_CHECKING: 'False'
            ANSIBLE_STDOUT_CALLBACK: 'json'
          playbook:
            - hosts: overcloud
              gather_facts: no
              tasks:
                - name: collect machine id
                  command: dmidecode -s system-uuid
                  register: dmidecode
                  # NOTE(tonyb): 0 == no error, 1 == -EPERM or bad data and 2 == Command not found
                  # 1 and 2 aren't great but shouldn't cause the deploy to fail.  If we're using
                  # the node specific data we'll fail then.  If we aren't then lets keep moving
                  failed_when: dmidecode.rc not in [0, 1, 2]
        publish:
          ansible_output: <% json_parse(task().result.stderr) %>
        on-success: set_ip_uuids
      set_ip_uuids:
        publish:
          ip_uuids: <% let(root => $.ansible_output.get('plays')[0].get('tasks')[0].get('hosts')) -> $.ips_list.toDict($, $root.get($).get('stdout')) %>
        on-success: parse_node_data_lookup
      parse_node_data_lookup:
        publish:
          json_node_data_lookup: <% json_parse($.node_data_lookup) %>
        on-error:
          - fail(msg=<% 'Ceph deployment stopped, NodeDataLookup (node_data_lookup) is not valid JSON. {0}'.format($.node_data_lookup) %>)
        on-success: map_node_data_lookup
      map_node_data_lookup:
        publish:
          ips_data: <% let(uuids => $.ip_uuids, root => $) -> $.ips_list.toDict($, $root.json_node_data_lookup.get($uuids.get($, "NO-UUID-FOUND"), {})) %>
        on-success: set_role_vars
      set_role_vars:
        publish:
          # NOTE(gfidente): collect role settings from all tht roles
          mgr_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mgr_ansible_vars', {})).aggregate($1 + $2) %>
          mon_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mon_ansible_vars', {})).aggregate($1 + $2) %>
          osd_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_osd_ansible_vars', {})).aggregate($1 + $2) %>
          mds_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mds_ansible_vars', {})).aggregate($1 + $2) %>
          rgw_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_rgw_ansible_vars', {})).aggregate($1 + $2) %>
          nfs_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_nfs_ansible_vars', {})).aggregate($1 + $2) %>
          rbdmirror_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_rbdmirror_ansible_vars', {})).aggregate($1 + $2) %>
          client_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_client_ansible_vars', {})).aggregate($1 + $2) %>
        on-success: build_extra_vars
      build_extra_vars:
        publish:
          # NOTE(gfidente): merge vars from all ansible roles
          extra_vars: <% {'fetch_directory'=> $.fetch_directory} + $.mgr_vars + $.mon_vars + $.osd_vars + $.mds_vars + $.rgw_vars + $.nfs_vars + $.client_vars + $.rbdmirror_vars + $.ceph_ansible_extra_vars %>
        on-success: ceph_install
      ceph_install:
        with-items: playbook in <% list($.ceph_ansible_playbook).flatten() %>
        concurrency: 1
        action: tripleo.ansible-playbook
        input:
          trash_output: true
          inventory:
            mgrs:
              hosts: <% let(root => $) -> $.mgr_ips.toDict($, $root.ips_data.get($, {})) %>
            mons:
              hosts: <% let(root => $) -> $.mon_ips.toDict($, $root.ips_data.get($, {})) %>
            osds:
              hosts: <% let(root => $) -> $.osd_ips.toDict($, $root.ips_data.get($, {})) %>
            mdss:
              hosts: <% let(root => $) -> $.mds_ips.toDict($, $root.ips_data.get($, {})) %>
            rgws:
              hosts: <% let(root => $) -> $.rgw_ips.toDict($, $root.ips_data.get($, {})) %>
            nfss:
              hosts: <% let(root => $) -> $.nfs_ips.toDict($, $root.ips_data.get($, {})) %>
            rbdmirrors:
              hosts: <% let(root => $) -> $.rbdmirror_ips.toDict($, $root.ips_data.get($, {})) %>
            clients:
              hosts: <% let(root => $) -> $.client_ips.toDict($, $root.ips_data.get($, {})) %>
            all:
              vars: <% $.extra_vars %>
          playbook: <% $.playbook %>
          remote_user: tripleo-admin
          become: true
          become_user: root
          verbosity: <% $.ansible_playbook_verbosity %>
          ssh_private_key: <% $.private_key %>
          skip_tags: <% $.ansible_skip_tags %>
          extra_env_variables: <% $.ansible_extra_env_variables.mergeWith($.ansible_env_variables) %>
          extra_vars:
            ireallymeanit: 'yes'
        publish:
          output: <% task().result %>
        on-complete: save_fetch_directory
      save_fetch_directory:
        action: tripleo.files.save_temp_dir_to_swift
        input:
          container: <% $.swift_container %>
          path: <% $.fetch_directory %>
        on-success: purge_fetch_directory
      purge_fetch_directory:
        action: tripleo.files.remove_temp_dir path=<% $.fetch_directory %>
'
2018-12-04 06:26:00,976 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 10739
2018-12-04 06:26:00,977 DEBUG: RESP: [201] Content-Length: 10739 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:00 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.storage.v1\ndescription: TripleO manages Ceph with ceph-ansible\n\nworkflows:\n  ceph-install:\n    # allows for additional extra_vars via workflow input\n    input:\n      - ansible_playbook_verbosity: 0\n      - ansible_skip_tags: 'package-install,with_pkg'\n      - ansible_env_variables: {}\n      - ansible_extra_env_variables:\n          ANSIBLE_CONFIG: /usr/share/ceph-ansible/ansible.cfg\n          ANSIBLE_ACTION_PLUGINS: /usr/share/ceph-ansible/plugins/actions/\n          ANSIBLE_ROLES_PATH: /usr/share/ceph-ansible/roles/\n          ANSIBLE_RETRY_FILES_ENABLED: 'False'\n          ANSIBLE_LOG_PATH: /var/log/mistral/ceph-install-workflow.log\n          ANSIBLE_LIBRARY: /usr/share/ceph-ansible/library/\n          ANSIBLE_SSH_RETRIES: '3'\n          ANSIBLE_HOST_KEY_CHECKING: 'False'\n          DEFAULT_FORKS: '25'\n      - ceph_ansible_extra_vars: {}\n      - ceph_ansible_playbook: /usr/share/ceph-ansible/site-docker.yml.sample\n      - node_data_lookup: '{}'\n      - swift_container: 'ceph_ansible_fetch_dir'\n    tags:\n      - tripleo-common-managed\n    tasks:\n      set_swift_container:\n        publish:\n          swift_container: <% concat(env().get('heat_stack_name', ''), '_', $.swift_container) %>\n        on-complete: collect_puppet_hieradata\n      collect_puppet_hieradata:\n        on-success: check_hieradata\n        publish:\n          hieradata: <% env().get('role_merged_configs', {}).values().select($.keys()).flatten().select(regex('^ceph::profile::params::osds$').search($)).where($ != null).toSet() %>\n      check_hieradata:\n        on-success:\n        - set_blacklisted_ips: <% not bool($.hieradata) %>\n        - fail(msg=<% 'Ceph deployment stopped, puppet-ceph hieradata found. Convert it into ceph-ansible variables. {0}'.format($.hieradata) %>): <% bool($.hieradata) %>\n      set_blacklisted_ips:\n        publish:\n          blacklisted_ips: <% env().get('blacklisted_ip_addresses', []) %>\n        on-success: set_ip_lists\n      set_ip_lists:\n        publish:\n          mgr_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mgr_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          mon_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mon_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          osd_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_osd_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          mds_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_mds_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          rgw_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_rgw_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          nfs_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_nfs_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          rbdmirror_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_rbdmirror_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          client_ips: <% let(root => $) -> env().get('service_ips', {}).get('ceph_client_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n        on-success: merge_ip_lists\n      merge_ip_lists:\n        publish:\n          ips_list: <% ($.mgr_ips + $.mon_ips + $.osd_ips + $.mds_ips + $.rgw_ips + $.nfs_ips + $.rbdmirror_ips + $.client_ips).toSet() %>\n        on-success: enable_ssh_admin\n      enable_ssh_admin:\n        workflow: tripleo.access.v1.enable_ssh_admin\n        input:\n          ssh_servers: <% $.ips_list %>\n        on-success: get_private_key\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: make_fetch_directory\n      make_fetch_directory:\n        action: tripleo.files.make_temp_dir\n        publish:\n          fetch_directory: <% task().result.path %>\n        on-success: verify_container_exists\n      verify_container_exists:\n        action: swift.head_container container=<% $.swift_container %>\n        on-success: restore_fetch_directory\n        on-error: create_container\n      restore_fetch_directory:\n        action: tripleo.files.restore_temp_dir_from_swift\n        input:\n          container: <% $.swift_container %>\n          path: <% $.fetch_directory %>\n        on-success: collect_nodes_uuid\n      create_container:\n        action: swift.put_container container=<% $.swift_container %>\n        on-success: collect_nodes_uuid\n      collect_nodes_uuid:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            overcloud:\n              hosts: <% $.ips_list.toDict($, {}) %>\n          remote_user: tripleo-admin\n          become: true\n          become_user: root\n          verbosity: 0\n          ssh_private_key: <% $.private_key %>\n          #NOTE(gfidente): set ANSIBLE_CALLBACK_WHITELIST to empty string to avoid spurious output\n          #in the json output. The publish: directive will in fact parse the output.\n          extra_env_variables:\n            ANSIBLE_CALLBACK_WHITELIST: ''\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n            ANSIBLE_STDOUT_CALLBACK: 'json'\n          playbook:\n            - hosts: overcloud\n              gather_facts: no\n              tasks:\n                - name: collect machine id\n                  command: dmidecode -s system-uuid\n                  register: dmidecode\n                  # NOTE(tonyb): 0 == no error, 1 == -EPERM or bad data and 2 == Command not found\n                  # 1 and 2 aren't great but shouldn't cause the deploy to fail.  If we're using\n                  # the node specific data we'll fail then.  If we aren't then lets keep moving\n                  failed_when: dmidecode.rc not in [0, 1, 2]\n        publish:\n          ansible_output: <% json_parse(task().result.stderr) %>\n        on-success: set_ip_uuids\n      set_ip_uuids:\n        publish:\n          ip_uuids: <% let(root => $.ansible_output.get('plays')[0].get('tasks')[0].get('hosts')) -> $.ips_list.toDict($, $root.get($).get('stdout')) %>\n        on-success: parse_node_data_lookup\n      parse_node_data_lookup:\n        publish:\n          json_node_data_lookup: <% json_parse($.node_data_lookup) %>\n        on-error:\n          - fail(msg=<% 'Ceph deployment stopped, NodeDataLookup (node_data_lookup) is not valid JSON. {0}'.format($.node_data_lookup) %>)\n        on-success: map_node_data_lookup\n      map_node_data_lookup:\n        publish:\n          ips_data: <% let(uuids => $.ip_uuids, root => $) -> $.ips_list.toDict($, $root.json_node_data_lookup.get($uuids.get($, \"NO-UUID-FOUND\"), {})) %>\n        on-success: set_role_vars\n      set_role_vars:\n        publish:\n          # NOTE(gfidente): collect role settings from all tht roles\n          mgr_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mgr_ansible_vars', {})).aggregate($1 + $2) %>\n          mon_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mon_ansible_vars', {})).aggregate($1 + $2) %>\n          osd_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_osd_ansible_vars', {})).aggregate($1 + $2) %>\n          mds_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_mds_ansible_vars', {})).aggregate($1 + $2) %>\n          rgw_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_rgw_ansible_vars', {})).aggregate($1 + $2) %>\n          nfs_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_nfs_ansible_vars', {})).aggregate($1 + $2) %>\n          rbdmirror_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_rbdmirror_ansible_vars', {})).aggregate($1 + $2) %>\n          client_vars: <% env().get('role_merged_configs', {}).values().select($.get('ceph_client_ansible_vars', {})).aggregate($1 + $2) %>\n        on-success: build_extra_vars\n      build_extra_vars:\n        publish:\n          # NOTE(gfidente): merge vars from all ansible roles\n          extra_vars: <% {'fetch_directory'=> $.fetch_directory} + $.mgr_vars + $.mon_vars + $.osd_vars + $.mds_vars + $.rgw_vars + $.nfs_vars + $.client_vars + $.rbdmirror_vars + $.ceph_ansible_extra_vars %>\n        on-success: ceph_install\n      ceph_install:\n        with-items: playbook in <% list($.ceph_ansible_playbook).flatten() %>\n        concurrency: 1\n        action: tripleo.ansible-playbook\n        input:\n          trash_output: true\n          inventory:\n            mgrs:\n              hosts: <% let(root => $) -> $.mgr_ips.toDict($, $root.ips_data.get($, {})) %>\n            mons:\n              hosts: <% let(root => $) -> $.mon_ips.toDict($, $root.ips_data.get($, {})) %>\n            osds:\n              hosts: <% let(root => $) -> $.osd_ips.toDict($, $root.ips_data.get($, {})) %>\n            mdss:\n              hosts: <% let(root => $) -> $.mds_ips.toDict($, $root.ips_data.get($, {})) %>\n            rgws:\n              hosts: <% let(root => $) -> $.rgw_ips.toDict($, $root.ips_data.get($, {})) %>\n            nfss:\n              hosts: <% let(root => $) -> $.nfs_ips.toDict($, $root.ips_data.get($, {})) %>\n            rbdmirrors:\n              hosts: <% let(root => $) -> $.rbdmirror_ips.toDict($, $root.ips_data.get($, {})) %>\n            clients:\n              hosts: <% let(root => $) -> $.client_ips.toDict($, $root.ips_data.get($, {})) %>\n            all:\n              vars: <% $.extra_vars %>\n          playbook: <% $.playbook %>\n          remote_user: tripleo-admin\n          become: true\n          become_user: root\n          verbosity: <% $.ansible_playbook_verbosity %>\n          ssh_private_key: <% $.private_key %>\n          skip_tags: <% $.ansible_skip_tags %>\n          extra_env_variables: <% $.ansible_extra_env_variables.mergeWith($.ansible_env_variables) %>\n          extra_vars:\n            ireallymeanit: 'yes'\n        publish:\n          output: <% task().result %>\n        on-complete: save_fetch_directory\n      save_fetch_directory:\n        action: tripleo.files.save_temp_dir_to_swift\n        input:\n          container: <% $.swift_container %>\n          path: <% $.fetch_directory %>\n        on-success: purge_fetch_directory\n      purge_fetch_directory:\n        action: tripleo.files.remove_temp_dir path=<% $.fetch_directory %>\n", "name": "tripleo.storage.v1", "tags": [], "created_at": "2018-12-04 11:26:00", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "8704edf7-bada-49b5-b8ac-2e3f65e82cfd"}

2018-12-04 06:26:00,978 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:00,979 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.deployment.v1
description: TripleO deployment workflows

workflows:

  deploy_on_server:

    input:
      - server_uuid
      - server_name
      - config
      - config_name
      - group
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      deploy_config:
        action: tripleo.deployment.config
        on-complete: send_message
        input:
          server_id: <% $.server_uuid %>
          name: <% $.config_name %>
          config: <% $.config %>
          group: <% $.group %>
        publish:
          stdout: <% task().result.deploy_stdout %>
          stderr: <% task().result.deploy_stderr %>
          status_code: <% task().result.deploy_status_code %>
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.deploy_on_server
              payload:
                status: <% $.get("status", "SUCCESS") %>
                message: <% $.get("message", "") %>
                server_uuid: <% $.server_uuid %>
                server_name: <% $.server_name %>
                config_name: <% $.config_name %>
                status_code: <% $.get("status_code", "") %>
                stdout: <% $.get("stdout", "") %>
                stderr: <% $.get("stderr", "") %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  deploy_on_servers:

    input:
      - server_name
      - config_name
      - config
      - group: script
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      check_if_all_servers:
        on-success:
          - get_servers_matching: <% $.server_name != "all" %>
          - get_all_servers: <% $.server_name = "all" %>

      get_servers_matching:
        action: nova.servers_list
        on-success: deploy_on_servers
        publish:
          servers_with_name: <% task().result._info.where($.name.indexOf(execution().input.server_name) > -1) %>

      get_all_servers:
        action: nova.servers_list
        on-success: deploy_on_servers
        publish:
          servers_with_name: <% task().result._info %>

      deploy_on_servers:
        on-success: send_success_message
        on-error: send_failed_message
        with-items: server in <% $.servers_with_name %>
        workflow: tripleo.deployment.v1.deploy_on_server
        input:
          server_name: <% $.server.name %>
          server_uuid: <% $.server.id %>
          config: <% $.config %>
          config_name: <% $.config_name %>
          group: <% $.group %>
          queue_name: <% $.queue_name %>

      send_success_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.deploy_on_servers
              payload:
                status: SUCCESS
                execution: <% execution() %>

      send_failed_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.deploy_on_servers
              payload:
                status: FAILED
                message: <% task(deploy_on_servers).result %>
                execution: <% execution() %>
        on-success: fail

  deploy_plan:

    description: >
      Deploy the overcloud for a plan.

    input:
      - container
      - run_validations: False
      - timeout: 240
      - skip_deploy_identifier: False
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      add_validation_ssh_key:
        workflow: tripleo.validations.v1.add_validation_ssh_key_parameter
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>
        on-complete:
          - run_validations: <% $.run_validations %>
          - create_swift_rings_backup_plan: <% not $.run_validations %>

      run_validations:
        workflow: tripleo.validations.v1.run_groups
        input:
          group_names:
            - 'pre-deployment'
          plan: <% $.container %>
          queue_name: <% $.queue_name %>
        on-success: create_swift_rings_backup_plan
        on-error: set_validations_failed

      set_validations_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(run_validations).result %>

      create_swift_rings_backup_plan:
        workflow: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan
        on-success: cell_v2_discover_hosts
        on-error: create_swift_rings_backup_plan_set_status_failed
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>
          use_default_templates: true

      cell_v2_discover_hosts:
        on-success: deploy
        on-error: cell_v2_discover_hosts_failed
        action: tripleo.baremetal.cell_v2_discover_hosts

      cell_v2_discover_hosts_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(cell_v2_discover_hosts).result %>

      deploy:
        action: tripleo.deployment.deploy
        input:
          timeout: <% $.timeout %>
          container: <% $.container %>
          skip_deploy_identifier: <% $.skip_deploy_identifier %>
        on-success: send_message
        on-error: set_deployment_failed

      create_swift_rings_backup_plan_set_status_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(create_swift_rings_backup_plan).result %>

      set_deployment_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(deploy).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.deploy_plan
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  get_horizon_url:

    description: >
      Retrieve the Horizon URL from the Overcloud stack.

    input:
      - stack: overcloud
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    output:
      horizon_url: <% $.horizon_url %>

    tasks:
      get_horizon_url:
        action: heat.stacks_get
        input:
          stack_id: <% $.stack %>
        publish:
          horizon_url: <% task().result.outputs.where($.output_key = "EndpointMap").output_value.HorizonPublic.uri.single() %>
        on-success: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      notify_zaqar:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.get_horizon_url
              payload:
                horizon_url: <% $.get('horizon_url', '') %>
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  config_download_deploy:

    description: >
      Configure the overcloud with config-download.

    input:
      - timeout: 240
      - queue_name: tripleo
      - plan_name: overcloud
      - work_dir: /var/lib/mistral
      - verbosity: 1
      - blacklist: []

    tags:
      - tripleo-common-managed

    tasks:

      get_blacklisted_hostnames:
        action: heat.stacks_output_show
        input:
          stack_id: <% $.plan_name %>
          output_key: BlacklistedHostnames
        publish:
          blacklisted_hostnames: <% task().result.output.output_value.where($ != "") %>
        on-success: get_config
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      get_config:
        action: tripleo.config.get_overcloud_config
        input:
          container: <% $.get('plan_name') %>
        on-success: download_config
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      download_config:
        action: tripleo.config.download_config
        input:
          work_dir: <% $.get('work_dir') %>/<% execution().id %>
        on-success: send_msg_config_download
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      send_msg_config_download:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.config_download
              payload:
                status: <% $.get('status', 'RUNNING') %>
                message: Config downloaded at <% $.get('work_dir') %>/<% execution().id %>
                execution: <% execution() %>
        on-success: get_private_key

      get_private_key:
        action: tripleo.validations.get_privkey
        publish:
          private_key: <% task().result %>
        on-success: generate_inventory
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      generate_inventory:
        action: tripleo.ansible-generate-inventory
        input:
          ansible_ssh_user: tripleo-admin
          work_dir: <% $.get('work_dir') %>/<% execution().id %>
          plan_name: <% $.get('plan_name') %>
        publish:
          inventory: <% task().result %>
        on-success: send_msg_generate_inventory
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      send_msg_generate_inventory:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.config_download
              payload:
                status: <% $.get('status', 'RUNNING') %>
                message: Inventory generated at <% $.get('inventory') %>
                execution: <% execution() %>
        on-success: send_msg_run_ansible

      send_msg_run_ansible:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.config_download
              payload:
                status: <% $.get('status', 'RUNNING') %>
                message: >
                  Running ansible playbook at <% $.get('work_dir') %>/<% execution().id %>/deploy_steps_playbook.yaml.
                  See log file at <% $.get('work_dir') %>/<% execution().id %>/ansible.log for progress.
                  ...
                execution: <% execution() %>
        on-success: run_ansible

      run_ansible:
        action: tripleo.ansible-playbook
        input:
          inventory: <% $.inventory %>
          playbook: <% $.get('work_dir') %>/<% execution().id %>/deploy_steps_playbook.yaml
          remote_user: tripleo-admin
          ssh_extra_args: '-o StrictHostKeyChecking=no'
          ssh_private_key: <% $.private_key %>
          use_openstack_credentials: true
          verbosity: <% $.get('verbosity') %>
          become: true
          timeout: <% $.timeout %>
          work_dir: <% $.get('work_dir') %>/<% execution().id %>
          queue_name: <% $.queue_name %>
          reproduce_command: true
          trash_output: true
          blacklisted_hostnames: <% $.blacklisted_hostnames %>
        publish:
          log_path: <% task(run_ansible).result.get('log_path') %>
        on-success:
          - ansible_passed: <% task().result.returncode = 0 %>
          - ansible_failed: <% task().result.returncode != 0 %>
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/ansible.log.

      ansible_passed:
        on-success: send_message
        publish:
          status: SUCCESS
          message: Ansible passed.

      ansible_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/ansible.log.

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.deployment.v1.config_download
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-04 06:26:02,558 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 14085
2018-12-04 06:26:02,560 DEBUG: RESP: [201] Content-Length: 14085 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:02 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.deployment.v1\ndescription: TripleO deployment workflows\n\nworkflows:\n\n  deploy_on_server:\n\n    input:\n      - server_uuid\n      - server_name\n      - config\n      - config_name\n      - group\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      deploy_config:\n        action: tripleo.deployment.config\n        on-complete: send_message\n        input:\n          server_id: <% $.server_uuid %>\n          name: <% $.config_name %>\n          config: <% $.config %>\n          group: <% $.group %>\n        publish:\n          stdout: <% task().result.deploy_stdout %>\n          stderr: <% task().result.deploy_stderr %>\n          status_code: <% task().result.deploy_status_code %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.deploy_on_server\n              payload:\n                status: <% $.get(\"status\", \"SUCCESS\") %>\n                message: <% $.get(\"message\", \"\") %>\n                server_uuid: <% $.server_uuid %>\n                server_name: <% $.server_name %>\n                config_name: <% $.config_name %>\n                status_code: <% $.get(\"status_code\", \"\") %>\n                stdout: <% $.get(\"stdout\", \"\") %>\n                stderr: <% $.get(\"stderr\", \"\") %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  deploy_on_servers:\n\n    input:\n      - server_name\n      - config_name\n      - config\n      - group: script\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      check_if_all_servers:\n        on-success:\n          - get_servers_matching: <% $.server_name != \"all\" %>\n          - get_all_servers: <% $.server_name = \"all\" %>\n\n      get_servers_matching:\n        action: nova.servers_list\n        on-success: deploy_on_servers\n        publish:\n          servers_with_name: <% task().result._info.where($.name.indexOf(execution().input.server_name) > -1) %>\n\n      get_all_servers:\n        action: nova.servers_list\n        on-success: deploy_on_servers\n        publish:\n          servers_with_name: <% task().result._info %>\n\n      deploy_on_servers:\n        on-success: send_success_message\n        on-error: send_failed_message\n        with-items: server in <% $.servers_with_name %>\n        workflow: tripleo.deployment.v1.deploy_on_server\n        input:\n          server_name: <% $.server.name %>\n          server_uuid: <% $.server.id %>\n          config: <% $.config %>\n          config_name: <% $.config_name %>\n          group: <% $.group %>\n          queue_name: <% $.queue_name %>\n\n      send_success_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.deploy_on_servers\n              payload:\n                status: SUCCESS\n                execution: <% execution() %>\n\n      send_failed_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.deploy_on_servers\n              payload:\n                status: FAILED\n                message: <% task(deploy_on_servers).result %>\n                execution: <% execution() %>\n        on-success: fail\n\n  deploy_plan:\n\n    description: >\n      Deploy the overcloud for a plan.\n\n    input:\n      - container\n      - run_validations: False\n      - timeout: 240\n      - skip_deploy_identifier: False\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      add_validation_ssh_key:\n        workflow: tripleo.validations.v1.add_validation_ssh_key_parameter\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n        on-complete:\n          - run_validations: <% $.run_validations %>\n          - create_swift_rings_backup_plan: <% not $.run_validations %>\n\n      run_validations:\n        workflow: tripleo.validations.v1.run_groups\n        input:\n          group_names:\n            - 'pre-deployment'\n          plan: <% $.container %>\n          queue_name: <% $.queue_name %>\n        on-success: create_swift_rings_backup_plan\n        on-error: set_validations_failed\n\n      set_validations_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(run_validations).result %>\n\n      create_swift_rings_backup_plan:\n        workflow: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan\n        on-success: cell_v2_discover_hosts\n        on-error: create_swift_rings_backup_plan_set_status_failed\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n          use_default_templates: true\n\n      cell_v2_discover_hosts:\n        on-success: deploy\n        on-error: cell_v2_discover_hosts_failed\n        action: tripleo.baremetal.cell_v2_discover_hosts\n\n      cell_v2_discover_hosts_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(cell_v2_discover_hosts).result %>\n\n      deploy:\n        action: tripleo.deployment.deploy\n        input:\n          timeout: <% $.timeout %>\n          container: <% $.container %>\n          skip_deploy_identifier: <% $.skip_deploy_identifier %>\n        on-success: send_message\n        on-error: set_deployment_failed\n\n      create_swift_rings_backup_plan_set_status_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(create_swift_rings_backup_plan).result %>\n\n      set_deployment_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(deploy).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.deploy_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  get_horizon_url:\n\n    description: >\n      Retrieve the Horizon URL from the Overcloud stack.\n\n    input:\n      - stack: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    output:\n      horizon_url: <% $.horizon_url %>\n\n    tasks:\n      get_horizon_url:\n        action: heat.stacks_get\n        input:\n          stack_id: <% $.stack %>\n        publish:\n          horizon_url: <% task().result.outputs.where($.output_key = \"EndpointMap\").output_value.HorizonPublic.uri.single() %>\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.get_horizon_url\n              payload:\n                horizon_url: <% $.get('horizon_url', '') %>\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  config_download_deploy:\n\n    description: >\n      Configure the overcloud with config-download.\n\n    input:\n      - timeout: 240\n      - queue_name: tripleo\n      - plan_name: overcloud\n      - work_dir: /var/lib/mistral\n      - verbosity: 1\n      - blacklist: []\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      get_blacklisted_hostnames:\n        action: heat.stacks_output_show\n        input:\n          stack_id: <% $.plan_name %>\n          output_key: BlacklistedHostnames\n        publish:\n          blacklisted_hostnames: <% task().result.output.output_value.where($ != \"\") %>\n        on-success: get_config\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_config:\n        action: tripleo.config.get_overcloud_config\n        input:\n          container: <% $.get('plan_name') %>\n        on-success: download_config\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      download_config:\n        action: tripleo.config.download_config\n        input:\n          work_dir: <% $.get('work_dir') %>/<% execution().id %>\n        on-success: send_msg_config_download\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      send_msg_config_download:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.config_download\n              payload:\n                status: <% $.get('status', 'RUNNING') %>\n                message: Config downloaded at <% $.get('work_dir') %>/<% execution().id %>\n                execution: <% execution() %>\n        on-success: get_private_key\n\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: generate_inventory\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      generate_inventory:\n        action: tripleo.ansible-generate-inventory\n        input:\n          ansible_ssh_user: tripleo-admin\n          work_dir: <% $.get('work_dir') %>/<% execution().id %>\n          plan_name: <% $.get('plan_name') %>\n        publish:\n          inventory: <% task().result %>\n        on-success: send_msg_generate_inventory\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      send_msg_generate_inventory:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.config_download\n              payload:\n                status: <% $.get('status', 'RUNNING') %>\n                message: Inventory generated at <% $.get('inventory') %>\n                execution: <% execution() %>\n        on-success: send_msg_run_ansible\n\n      send_msg_run_ansible:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.config_download\n              payload:\n                status: <% $.get('status', 'RUNNING') %>\n                message: >\n                  Running ansible playbook at <% $.get('work_dir') %>/<% execution().id %>/deploy_steps_playbook.yaml.\n                  See log file at <% $.get('work_dir') %>/<% execution().id %>/ansible.log for progress.\n                  ...\n                execution: <% execution() %>\n        on-success: run_ansible\n\n      run_ansible:\n        action: tripleo.ansible-playbook\n        input:\n          inventory: <% $.inventory %>\n          playbook: <% $.get('work_dir') %>/<% execution().id %>/deploy_steps_playbook.yaml\n          remote_user: tripleo-admin\n          ssh_extra_args: '-o StrictHostKeyChecking=no'\n          ssh_private_key: <% $.private_key %>\n          use_openstack_credentials: true\n          verbosity: <% $.get('verbosity') %>\n          become: true\n          timeout: <% $.timeout %>\n          work_dir: <% $.get('work_dir') %>/<% execution().id %>\n          queue_name: <% $.queue_name %>\n          reproduce_command: true\n          trash_output: true\n          blacklisted_hostnames: <% $.blacklisted_hostnames %>\n        publish:\n          log_path: <% task(run_ansible).result.get('log_path') %>\n        on-success:\n          - ansible_passed: <% task().result.returncode = 0 %>\n          - ansible_failed: <% task().result.returncode != 0 %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/ansible.log.\n\n      ansible_passed:\n        on-success: send_message\n        publish:\n          status: SUCCESS\n          message: Ansible passed.\n\n      ansible_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/ansible.log.\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.deployment.v1.config_download\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.deployment.v1", "tags": [], "created_at": "2018-12-04 11:26:02", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "506873da-318d-4096-9a05-f5e5bf9434ca"}

2018-12-04 06:26:02,560 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:02,562 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.derive_params.v1
description: TripleO Workflows to derive deployment parameters from the introspected data

workflows:

  derive_parameters:
    description: The main workflow for deriving parameters from the introspected data

    input:
      - plan: overcloud
      - queue_name: tripleo
      - user_inputs: {}

    tags:
      - tripleo-common-managed

    tasks:
      get_flattened_parameters:
        action: tripleo.parameters.get_flatten container=<% $.plan %>
        publish:
          environment_parameters: <% task().result.environment_parameters %>
          heat_resource_tree: <% task().result.heat_resource_tree %>
        on-success:
          - get_roles: <% $.environment_parameters and $.heat_resource_tree %>
          - set_status_failed_get_flattened_parameters: <% (not $.environment_parameters) or (not $.heat_resource_tree) %>
        on-error: set_status_failed_get_flattened_parameters

      get_roles:
        action: tripleo.role.list container=<% $.plan %>
        publish:
          role_name_list: <% task().result %>
        on-success:
          - get_valid_roles: <% $.role_name_list %>
          - set_status_failed_get_roles: <% not $.role_name_list %>
        on-error: set_status_failed_on_error_get_roles

      # Obtain only the roles which has count > 0, by checking <RoleName>Count parameter, like ComputeCount
      get_valid_roles:
        publish:
          valid_role_name_list: <% let(hr => $.heat_resource_tree.parameters) -> $.role_name_list.where(int($hr.get(concat($, 'Count'), {}).get('default', 0)) > 0) %>
        on-success:
          - for_each_role: <% $.valid_role_name_list %>
          - set_status_failed_get_valid_roles: <% not $.valid_role_name_list %>

      # Execute the basic preparation workflow for each role to get introspection data
      for_each_role:
        with-items: role_name in <% $.valid_role_name_list %>
        concurrency: 1
        workflow: _derive_parameters_per_role
        input:
          plan: <% $.plan %>
          role_name: <% $.role_name %>
          environment_parameters: <% $.environment_parameters %>
          heat_resource_tree: <% $.heat_resource_tree %>
          user_inputs: <% $.user_inputs %>
        publish:
          # Gets all the roles derived parameters as dictionary
          result: <% task().result.select($.get('derived_parameters', {})).sum() %>
        on-success: reset_derive_parameters_in_plan
        on-error: set_status_failed_for_each_role

      reset_derive_parameters_in_plan:
        action: tripleo.parameters.reset
        input:
          container: <% $.plan %>
          key: 'derived_parameters'
        on-success:
          # Add the derived parameters to the deployment plan only when $.result
          # (the derived parameters) is non-empty. Otherwise, we're done.
          - update_derive_parameters_in_plan: <% $.result %>
          - send_message: <% not $.result %>
        on-error: set_status_failed_reset_derive_parameters_in_plan

      update_derive_parameters_in_plan:
        action: tripleo.parameters.update
        input:
          container: <% $.plan %>
          key: 'derived_parameters'
          parameters: <% $.get('result', {}) %>
        on-success: send_message
        on-error: set_status_failed_update_derive_parameters_in_plan

      set_status_failed_get_flattened_parameters:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_flattened_parameters).result %>

      set_status_failed_get_roles:
        on-success: send_message
        publish:
          status: FAILED
          message: "Unable to determine the list of roles in the deployment plan"

      set_status_failed_on_error_get_roles:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_roles).result %>

      set_status_failed_get_valid_roles:
        on-success: send_message
        publish:
          status: FAILED
          message: 'Unable to determine the list of valid roles in the deployment plan.'

      set_status_failed_for_each_role:
        on-success: update_message_format
        publish:
          status: FAILED
          # gets the status and message for all roles from task result.
          message: <% task(for_each_role).result.select(dict('role_name' => $.role_name, 'status' => $.get('status', 'SUCCESS'), 'message' => $.get('message', ''))) %>

      update_message_format:
        on-success: send_message
        publish:
          # updates the message format(Role 'role name': message) for each roles which are failed and joins the message list as string with ', ' separator.
          message: <% $.message.where($.get('status', 'SUCCESS') != 'SUCCESS').select(concat("Role '{}':".format($.role_name), " ", $.get('message', '(error unknown)'))).join(', ') %>

      set_status_failed_reset_derive_parameters_in_plan:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(reset_derive_parameters_in_plan).result %>

      set_status_failed_update_derive_parameters_in_plan:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(update_derive_parameters_in_plan).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.derive_params.v1.derive_parameters
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                result: <% $.get('result', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = 'FAILED' %>


  _derive_parameters_per_role:
    description: >
      Workflow which runs per role to get the introspection data on the first matching node assigned to role.
      Once introspection data is fetched, this worklow will trigger the actual derive parameters workflow
    input:
      - plan
      - role_name
      - environment_parameters
      - heat_resource_tree
      - user_inputs

    output:
      derived_parameters: <% $.get('derived_parameters', {}) %>
      # Need role_name in output parameter to display the status for all roles in main workflow when any role fails here.
      role_name: <% $.role_name %>

    tags:
      - tripleo-common-managed

    tasks:
      get_role_info:
        workflow: _get_role_info
        input:
          role_name: <% $.role_name %>
          heat_resource_tree: <% $.heat_resource_tree %>
        publish:
          role_features: <% task().result.get('role_features', []) %>
          role_services: <% task().result.get('role_services', []) %>
        on-success:
          # Continue only if there are features associated with this role. Otherwise, we're done.
          - get_scheduler_hints: <% $.role_features %>
        on-error: set_status_failed_get_role_info

      # Find a node associated with this role. Look for nodes matching any scheduler hints
      # associated with the role, and if there are no scheduler hints then locate nodes
      # with a profile matching the role's flavor.
      get_scheduler_hints:
        publish:
          scheduler_hints: <% let(param_name => concat($.role_name, 'SchedulerHints')) -> $.heat_resource_tree.parameters.get($param_name, {}).get('default', {}) %>
        on-success:
          - get_hint_regex: <% $.scheduler_hints %>
          # If there are no scheduler hints then move on to use the flavor
          - get_flavor_name: <% not $.scheduler_hints %>

      get_hint_regex:
        publish:
          hint_regex: <% $.scheduler_hints.get('capabilities:node', '').replace('%index%', '(\d+)') %>
        on-success:
          - get_node_with_hint: <% $.hint_regex %>
          # If there is no 'capabilities:node' hint then move on to use the flavor
          - get_flavor_name: <% not $.hint_regex %>

      get_node_with_hint:
        workflow: tripleo.baremetal.v1.nodes_with_hint
        input:
          hint_regex: <% concat('^', $.hint_regex, '$') %>
        publish:
          role_node_uuid: <% task().result.matching_nodes.first('') %>
        on-success:
          - get_introspection_data: <% $.role_node_uuid %>
          # If no nodes match the scheduler hint then move on to use the flavor
          - get_flavor_name: <% not $.role_node_uuid %>
        on-error: set_status_failed_on_error_get_node_with_hint

      get_flavor_name:
        publish:
          flavor_name: <% let(param_name => concat('Overcloud', $.role_name, 'Flavor').replace('OvercloudControllerFlavor', 'OvercloudControlFlavor')) -> $.heat_resource_tree.parameters.get($param_name, {}).get('default', '') %>
        on-success:
          - get_profile_name: <% $.flavor_name %>
          - set_status_failed_get_flavor_name: <% not $.flavor_name %>

      get_profile_name:
        action: tripleo.parameters.get_profile_of_flavor flavor_name=<% $.flavor_name %>
        publish:
          profile_name: <% task().result %>
        on-success: get_profile_node
        on-error: set_status_failed_get_profile_name

      get_profile_node:
        workflow: tripleo.baremetal.v1.nodes_with_profile
        input:
          profile: <% $.profile_name %>
        publish:
          role_node_uuid: <% task().result.matching_nodes.first('') %>
        on-success:
          - get_introspection_data: <% $.role_node_uuid %>
          - set_status_failed_no_matching_node_get_profile_node: <% not $.role_node_uuid %>
        on-error: set_status_failed_on_error_get_profile_node

      get_introspection_data:
        action: baremetal_introspection.get_data uuid=<% $.role_node_uuid %>
        publish:
          hw_data: <% task().result %>
          # Establish an empty dictionary of derived_parameters prior to
          # invoking the individual "feature" algorithms
          derived_parameters: <% dict() %>
        on-success: handle_dpdk_feature
        on-error: set_status_failed_get_introspection_data

      handle_dpdk_feature:
        on-success:
          - get_dpdk_derive_params: <% $.role_features.contains('DPDK') %>
          - handle_sriov_feature: <% not $.role_features.contains('DPDK') %>

      get_dpdk_derive_params:
        workflow: tripleo.derive_params_formulas.v1.dpdk_derive_params
        input:
          plan: <% $.plan %>
          role_name: <% $.role_name %>
          hw_data: <% $.hw_data %>
          user_inputs: <% $.user_inputs %>
        publish:
          derived_parameters: <% task().result.get('derived_parameters', {}) %>
        on-success: handle_sriov_feature
        on-error: set_status_failed_get_dpdk_derive_params

      handle_sriov_feature:
        on-success:
          - get_sriov_derive_params: <% $.role_features.contains('SRIOV') %>
          - handle_host_feature: <% not $.role_features.contains('SRIOV') %>

      get_sriov_derive_params:
        workflow: tripleo.derive_params_formulas.v1.sriov_derive_params
        input:
          role_name: <% $.role_name %>
          hw_data: <% $.hw_data %>
          derived_parameters: <% $.derived_parameters %>
        publish:
          derived_parameters: <% task().result.get('derived_parameters', {}) %>
        on-success: handle_host_feature
        on-error: set_status_failed_get_sriov_derive_params

      handle_host_feature:
        on-success:
          - get_host_derive_params: <% $.role_features.contains('HOST') %>
          - handle_hci_feature: <% not $.role_features.contains('HOST') %>

      get_host_derive_params:
        workflow: tripleo.derive_params_formulas.v1.host_derive_params
        input:
          role_name: <% $.role_name %>
          hw_data: <% $.hw_data %>
          user_inputs: <% $.user_inputs %>
          derived_parameters: <% $.derived_parameters %>
        publish:
          derived_parameters: <% task().result.get('derived_parameters', {}) %>
        on-success: handle_hci_feature
        on-error: set_status_failed_get_host_derive_params

      handle_hci_feature:
        on-success:
          - get_hci_derive_params: <% $.role_features.contains('HCI') %>

      get_hci_derive_params:
        workflow: tripleo.derive_params_formulas.v1.hci_derive_params
        input:
          role_name: <% $.role_name %>
          environment_parameters: <% $.environment_parameters %>
          heat_resource_tree: <% $.heat_resource_tree %>
          introspection_data: <% $.hw_data %>
          user_inputs: <% $.user_inputs %>
          derived_parameters: <% $.derived_parameters %>
        publish:
          derived_parameters: <% task().result.get('derived_parameters', {}) %>
        on-error: set_status_failed_get_hci_derive_params
        # Done (no more derived parameter features)

      set_status_failed_get_role_info:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_role_info).result.get('message', '') %>
        on-success: fail

      set_status_failed_get_flavor_name:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% "Unable to determine flavor for role '{0}'".format($.role_name) %>
        on-success: fail

      set_status_failed_get_profile_name:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_profile_name).result %>
        on-success: fail

      set_status_failed_no_matching_node_get_profile_node:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% "Unable to determine matching node for profile '{0}'".format($.profile_name) %>
        on-success: fail

      set_status_failed_on_error_get_profile_node:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_profile_node).result %>
        on-success: fail

      set_status_failed_on_error_get_node_with_hint:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_node_with_hint).result %>
        on-success: fail

      set_status_failed_get_introspection_data:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_introspection_data).result %>
        on-success: fail

      set_status_failed_get_dpdk_derive_params:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_dpdk_derive_params).result.message %>
        on-success: fail

      set_status_failed_get_sriov_derive_params:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_sriov_derive_params).result.message %>
        on-success: fail

      set_status_failed_get_host_derive_params:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_host_derive_params).result.message %>
        on-success: fail

      set_status_failed_get_hci_derive_params:
        publish:
          role_name: <% $.role_name %>
          status: FAILED
          message: <% task(get_hci_derive_params).result.message %>
        on-success: fail


  _get_role_info:
    description: >
      Workflow that determines the list of derived parameter features (DPDK,
      HCI, etc.) for a role based on the services assigned to the role.

    input:
      - role_name
      - heat_resource_tree

    tags:
      - tripleo-common-managed

    tasks:
      get_resource_chains:
        publish:
          resource_chains: <% $.heat_resource_tree.resources.values().where($.get('type', '') = 'OS::Heat::ResourceChain') %>
        on-success:
          - get_role_chain: <% $.resource_chains %>
          - set_status_failed_get_resource_chains: <% not $.resource_chains %>

      get_role_chain:
        publish:
          role_chain: <% let(chain_name => concat($.role_name, 'ServiceChain'))-> $.heat_resource_tree.resources.values().where($.name = $chain_name).first({}) %>
        on-success:
          - get_service_chain: <% $.role_chain %>
          - set_status_failed_get_role_chain: <% not $.role_chain %>

      get_service_chain:
        publish:
          service_chain: <% let(resources => $.role_chain.resources)-> $.resource_chains.where($resources.contains($.id)).first('') %>
        on-success:
          - get_role_services: <% $.service_chain %>
          - set_status_failed_get_service_chain: <% not $.service_chain %>

      get_role_services:
        publish:
          role_services: <% let(resources => $.heat_resource_tree.resources)-> $.service_chain.resources.select($resources.get($)) %>
        on-success:
          - check_features: <% $.role_services %>
          - set_status_failed_get_role_services: <% not $.role_services %>

      check_features:
        on-success: build_feature_dict
        publish:
          # The role supports the DPDK feature if the NeutronDatapathType parameter is present
          dpdk: <% let(resources => $.heat_resource_tree.resources) -> $.role_services.any($.get('parameters', []).contains('NeutronDatapathType') or $.get('resources', []).select($resources.get($)).any($.get('parameters', []).contains('NeutronDatapathType'))) %>

          # The role supports the DPDK feature in ODL if the OvsEnableDpdk parameter value is true in role parameters.
          odl_dpdk: <% let(role => $.role_name) -> $.heat_resource_tree.parameters.get(concat($role, 'Parameters'), {}).get('default', {}).get('OvsEnableDpdk', false) %>

          # The role supports the SRIOV feature if it includes NeutronSriovAgent services.
          sriov: <% $.role_services.any($.get('type', '').endsWith('::NeutronSriovAgent')) %>

          # The role supports the HCI feature if it includes both NovaCompute and CephOSD services.
          hci: <% $.role_services.any($.get('type', '').endsWith('::NovaCompute')) and $.role_services.any($.get('type', '').endsWith('::CephOSD')) %>

      build_feature_dict:
        on-success: filter_features
        publish:
          feature_dict: <% dict(DPDK => ($.dpdk or $.odl_dpdk), SRIOV => $.sriov, HOST => ($.dpdk or $.odl_dpdk or $.sriov), HCI => $.hci) %>

      filter_features:
        publish:
          # The list of features that are enabled (i.e. are true in the feature_dict).
          role_features: <% let(feature_dict => $.feature_dict)-> $feature_dict.keys().where($feature_dict[$]) %>

      set_status_failed_get_resource_chains:
        publish:
          message: <% 'Unable to locate any resource chains in the heat resource tree' %>
        on-success: fail

      set_status_failed_get_role_chain:
        publish:
          message: <% "Unable to determine the service chain resource for role '{0}'".format($.role_name) %>
        on-success: fail

      set_status_failed_get_service_chain:
        publish:
          message: <% "Unable to determine the service chain for role '{0}'".format($.role_name) %>
        on-success: fail

      set_status_failed_get_role_services:
        publish:
          message: <% "Unable to determine list of services for role '{0}'".format($.role_name) %>
        on-success: fail
'
2018-12-04 06:26:04,717 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 20049
2018-12-04 06:26:04,758 DEBUG: RESP: [201] Content-Length: 20049 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:04 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.derive_params.v1\ndescription: TripleO Workflows to derive deployment parameters from the introspected data\n\nworkflows:\n\n  derive_parameters:\n    description: The main workflow for deriving parameters from the introspected data\n\n    input:\n      - plan: overcloud\n      - queue_name: tripleo\n      - user_inputs: {}\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_flattened_parameters:\n        action: tripleo.parameters.get_flatten container=<% $.plan %>\n        publish:\n          environment_parameters: <% task().result.environment_parameters %>\n          heat_resource_tree: <% task().result.heat_resource_tree %>\n        on-success:\n          - get_roles: <% $.environment_parameters and $.heat_resource_tree %>\n          - set_status_failed_get_flattened_parameters: <% (not $.environment_parameters) or (not $.heat_resource_tree) %>\n        on-error: set_status_failed_get_flattened_parameters\n\n      get_roles:\n        action: tripleo.role.list container=<% $.plan %>\n        publish:\n          role_name_list: <% task().result %>\n        on-success:\n          - get_valid_roles: <% $.role_name_list %>\n          - set_status_failed_get_roles: <% not $.role_name_list %>\n        on-error: set_status_failed_on_error_get_roles\n\n      # Obtain only the roles which has count > 0, by checking <RoleName>Count parameter, like ComputeCount\n      get_valid_roles:\n        publish:\n          valid_role_name_list: <% let(hr => $.heat_resource_tree.parameters) -> $.role_name_list.where(int($hr.get(concat($, 'Count'), {}).get('default', 0)) > 0) %>\n        on-success:\n          - for_each_role: <% $.valid_role_name_list %>\n          - set_status_failed_get_valid_roles: <% not $.valid_role_name_list %>\n\n      # Execute the basic preparation workflow for each role to get introspection data\n      for_each_role:\n        with-items: role_name in <% $.valid_role_name_list %>\n        concurrency: 1\n        workflow: _derive_parameters_per_role\n        input:\n          plan: <% $.plan %>\n          role_name: <% $.role_name %>\n          environment_parameters: <% $.environment_parameters %>\n          heat_resource_tree: <% $.heat_resource_tree %>\n          user_inputs: <% $.user_inputs %>\n        publish:\n          # Gets all the roles derived parameters as dictionary\n          result: <% task().result.select($.get('derived_parameters', {})).sum() %>\n        on-success: reset_derive_parameters_in_plan\n        on-error: set_status_failed_for_each_role\n\n      reset_derive_parameters_in_plan:\n        action: tripleo.parameters.reset\n        input:\n          container: <% $.plan %>\n          key: 'derived_parameters'\n        on-success:\n          # Add the derived parameters to the deployment plan only when $.result\n          # (the derived parameters) is non-empty. Otherwise, we're done.\n          - update_derive_parameters_in_plan: <% $.result %>\n          - send_message: <% not $.result %>\n        on-error: set_status_failed_reset_derive_parameters_in_plan\n\n      update_derive_parameters_in_plan:\n        action: tripleo.parameters.update\n        input:\n          container: <% $.plan %>\n          key: 'derived_parameters'\n          parameters: <% $.get('result', {}) %>\n        on-success: send_message\n        on-error: set_status_failed_update_derive_parameters_in_plan\n\n      set_status_failed_get_flattened_parameters:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_flattened_parameters).result %>\n\n      set_status_failed_get_roles:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: \"Unable to determine the list of roles in the deployment plan\"\n\n      set_status_failed_on_error_get_roles:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_roles).result %>\n\n      set_status_failed_get_valid_roles:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: 'Unable to determine the list of valid roles in the deployment plan.'\n\n      set_status_failed_for_each_role:\n        on-success: update_message_format\n        publish:\n          status: FAILED\n          # gets the status and message for all roles from task result.\n          message: <% task(for_each_role).result.select(dict('role_name' => $.role_name, 'status' => $.get('status', 'SUCCESS'), 'message' => $.get('message', ''))) %>\n\n      update_message_format:\n        on-success: send_message\n        publish:\n          # updates the message format(Role 'role name': message) for each roles which are failed and joins the message list as string with ', ' separator.\n          message: <% $.message.where($.get('status', 'SUCCESS') != 'SUCCESS').select(concat(\"Role '{}':\".format($.role_name), \" \", $.get('message', '(error unknown)'))).join(', ') %>\n\n      set_status_failed_reset_derive_parameters_in_plan:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(reset_derive_parameters_in_plan).result %>\n\n      set_status_failed_update_derive_parameters_in_plan:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(update_derive_parameters_in_plan).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.derive_params.v1.derive_parameters\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                result: <% $.get('result', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n\n  _derive_parameters_per_role:\n    description: >\n      Workflow which runs per role to get the introspection data on the first matching node assigned to role.\n      Once introspection data is fetched, this worklow will trigger the actual derive parameters workflow\n    input:\n      - plan\n      - role_name\n      - environment_parameters\n      - heat_resource_tree\n      - user_inputs\n\n    output:\n      derived_parameters: <% $.get('derived_parameters', {}) %>\n      # Need role_name in output parameter to display the status for all roles in main workflow when any role fails here.\n      role_name: <% $.role_name %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_role_info:\n        workflow: _get_role_info\n        input:\n          role_name: <% $.role_name %>\n          heat_resource_tree: <% $.heat_resource_tree %>\n        publish:\n          role_features: <% task().result.get('role_features', []) %>\n          role_services: <% task().result.get('role_services', []) %>\n        on-success:\n          # Continue only if there are features associated with this role. Otherwise, we're done.\n          - get_scheduler_hints: <% $.role_features %>\n        on-error: set_status_failed_get_role_info\n\n      # Find a node associated with this role. Look for nodes matching any scheduler hints\n      # associated with the role, and if there are no scheduler hints then locate nodes\n      # with a profile matching the role's flavor.\n      get_scheduler_hints:\n        publish:\n          scheduler_hints: <% let(param_name => concat($.role_name, 'SchedulerHints')) -> $.heat_resource_tree.parameters.get($param_name, {}).get('default', {}) %>\n        on-success:\n          - get_hint_regex: <% $.scheduler_hints %>\n          # If there are no scheduler hints then move on to use the flavor\n          - get_flavor_name: <% not $.scheduler_hints %>\n\n      get_hint_regex:\n        publish:\n          hint_regex: <% $.scheduler_hints.get('capabilities:node', '').replace('%index%', '(\\d+)') %>\n        on-success:\n          - get_node_with_hint: <% $.hint_regex %>\n          # If there is no 'capabilities:node' hint then move on to use the flavor\n          - get_flavor_name: <% not $.hint_regex %>\n\n      get_node_with_hint:\n        workflow: tripleo.baremetal.v1.nodes_with_hint\n        input:\n          hint_regex: <% concat('^', $.hint_regex, '$') %>\n        publish:\n          role_node_uuid: <% task().result.matching_nodes.first('') %>\n        on-success:\n          - get_introspection_data: <% $.role_node_uuid %>\n          # If no nodes match the scheduler hint then move on to use the flavor\n          - get_flavor_name: <% not $.role_node_uuid %>\n        on-error: set_status_failed_on_error_get_node_with_hint\n\n      get_flavor_name:\n        publish:\n          flavor_name: <% let(param_name => concat('Overcloud', $.role_name, 'Flavor').replace('OvercloudControllerFlavor', 'OvercloudControlFlavor')) -> $.heat_resource_tree.parameters.get($param_name, {}).get('default', '') %>\n        on-success:\n          - get_profile_name: <% $.flavor_name %>\n          - set_status_failed_get_flavor_name: <% not $.flavor_name %>\n\n      get_profile_name:\n        action: tripleo.parameters.get_profile_of_flavor flavor_name=<% $.flavor_name %>\n        publish:\n          profile_name: <% task().result %>\n        on-success: get_profile_node\n        on-error: set_status_failed_get_profile_name\n\n      get_profile_node:\n        workflow: tripleo.baremetal.v1.nodes_with_profile\n        input:\n          profile: <% $.profile_name %>\n        publish:\n          role_node_uuid: <% task().result.matching_nodes.first('') %>\n        on-success:\n          - get_introspection_data: <% $.role_node_uuid %>\n          - set_status_failed_no_matching_node_get_profile_node: <% not $.role_node_uuid %>\n        on-error: set_status_failed_on_error_get_profile_node\n\n      get_introspection_data:\n        action: baremetal_introspection.get_data uuid=<% $.role_node_uuid %>\n        publish:\n          hw_data: <% task().result %>\n          # Establish an empty dictionary of derived_parameters prior to\n          # invoking the individual \"feature\" algorithms\n          derived_parameters: <% dict() %>\n        on-success: handle_dpdk_feature\n        on-error: set_status_failed_get_introspection_data\n\n      handle_dpdk_feature:\n        on-success:\n          - get_dpdk_derive_params: <% $.role_features.contains('DPDK') %>\n          - handle_sriov_feature: <% not $.role_features.contains('DPDK') %>\n\n      get_dpdk_derive_params:\n        workflow: tripleo.derive_params_formulas.v1.dpdk_derive_params\n        input:\n          plan: <% $.plan %>\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n          user_inputs: <% $.user_inputs %>\n        publish:\n          derived_parameters: <% task().result.get('derived_parameters', {}) %>\n        on-success: handle_sriov_feature\n        on-error: set_status_failed_get_dpdk_derive_params\n\n      handle_sriov_feature:\n        on-success:\n          - get_sriov_derive_params: <% $.role_features.contains('SRIOV') %>\n          - handle_host_feature: <% not $.role_features.contains('SRIOV') %>\n\n      get_sriov_derive_params:\n        workflow: tripleo.derive_params_formulas.v1.sriov_derive_params\n        input:\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n          derived_parameters: <% $.derived_parameters %>\n        publish:\n          derived_parameters: <% task().result.get('derived_parameters', {}) %>\n        on-success: handle_host_feature\n        on-error: set_status_failed_get_sriov_derive_params\n\n      handle_host_feature:\n        on-success:\n          - get_host_derive_params: <% $.role_features.contains('HOST') %>\n          - handle_hci_feature: <% not $.role_features.contains('HOST') %>\n\n      get_host_derive_params:\n        workflow: tripleo.derive_params_formulas.v1.host_derive_params\n        input:\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n          user_inputs: <% $.user_inputs %>\n          derived_parameters: <% $.derived_parameters %>\n        publish:\n          derived_parameters: <% task().result.get('derived_parameters', {}) %>\n        on-success: handle_hci_feature\n        on-error: set_status_failed_get_host_derive_params\n\n      handle_hci_feature:\n        on-success:\n          - get_hci_derive_params: <% $.role_features.contains('HCI') %>\n\n      get_hci_derive_params:\n        workflow: tripleo.derive_params_formulas.v1.hci_derive_params\n        input:\n          role_name: <% $.role_name %>\n          environment_parameters: <% $.environment_parameters %>\n          heat_resource_tree: <% $.heat_resource_tree %>\n          introspection_data: <% $.hw_data %>\n          user_inputs: <% $.user_inputs %>\n          derived_parameters: <% $.derived_parameters %>\n        publish:\n          derived_parameters: <% task().result.get('derived_parameters', {}) %>\n        on-error: set_status_failed_get_hci_derive_params\n        # Done (no more derived parameter features)\n\n      set_status_failed_get_role_info:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_role_info).result.get('message', '') %>\n        on-success: fail\n\n      set_status_failed_get_flavor_name:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% \"Unable to determine flavor for role '{0}'\".format($.role_name) %>\n        on-success: fail\n\n      set_status_failed_get_profile_name:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_profile_name).result %>\n        on-success: fail\n\n      set_status_failed_no_matching_node_get_profile_node:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% \"Unable to determine matching node for profile '{0}'\".format($.profile_name) %>\n        on-success: fail\n\n      set_status_failed_on_error_get_profile_node:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_profile_node).result %>\n        on-success: fail\n\n      set_status_failed_on_error_get_node_with_hint:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_node_with_hint).result %>\n        on-success: fail\n\n      set_status_failed_get_introspection_data:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_introspection_data).result %>\n        on-success: fail\n\n      set_status_failed_get_dpdk_derive_params:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_dpdk_derive_params).result.message %>\n        on-success: fail\n\n      set_status_failed_get_sriov_derive_params:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_sriov_derive_params).result.message %>\n        on-success: fail\n\n      set_status_failed_get_host_derive_params:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_host_derive_params).result.message %>\n        on-success: fail\n\n      set_status_failed_get_hci_derive_params:\n        publish:\n          role_name: <% $.role_name %>\n          status: FAILED\n          message: <% task(get_hci_derive_params).result.message %>\n        on-success: fail\n\n\n  _get_role_info:\n    description: >\n      Workflow that determines the list of derived parameter features (DPDK,\n      HCI, etc.) for a role based on the services assigned to the role.\n\n    input:\n      - role_name\n      - heat_resource_tree\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_resource_chains:\n        publish:\n          resource_chains: <% $.heat_resource_tree.resources.values().where($.get('type', '') = 'OS::Heat::ResourceChain') %>\n        on-success:\n          - get_role_chain: <% $.resource_chains %>\n          - set_status_failed_get_resource_chains: <% not $.resource_chains %>\n\n      get_role_chain:\n        publish:\n          role_chain: <% let(chain_name => concat($.role_name, 'ServiceChain'))-> $.heat_resource_tree.resources.values().where($.name = $chain_name).first({}) %>\n        on-success:\n          - get_service_chain: <% $.role_chain %>\n          - set_status_failed_get_role_chain: <% not $.role_chain %>\n\n      get_service_chain:\n        publish:\n          service_chain: <% let(resources => $.role_chain.resources)-> $.resource_chains.where($resources.contains($.id)).first('') %>\n        on-success:\n          - get_role_services: <% $.service_chain %>\n          - set_status_failed_get_service_chain: <% not $.service_chain %>\n\n      get_role_services:\n        publish:\n          role_services: <% let(resources => $.heat_resource_tree.resources)-> $.service_chain.resources.select($resources.get($)) %>\n        on-success:\n          - check_features: <% $.role_services %>\n          - set_status_failed_get_role_services: <% not $.role_services %>\n\n      check_features:\n        on-success: build_feature_dict\n        publish:\n          # The role supports the DPDK feature if the NeutronDatapathType parameter is present\n          dpdk: <% let(resources => $.heat_resource_tree.resources) -> $.role_services.any($.get('parameters', []).contains('NeutronDatapathType') or $.get('resources', []).select($resources.get($)).any($.get('parameters', []).contains('NeutronDatapathType'))) %>\n\n          # The role supports the DPDK feature in ODL if the OvsEnableDpdk parameter value is true in role parameters.\n          odl_dpdk: <% let(role => $.role_name) -> $.heat_resource_tree.parameters.get(concat($role, 'Parameters'), {}).get('default', {}).get('OvsEnableDpdk', false) %>\n\n          # The role supports the SRIOV feature if it includes NeutronSriovAgent services.\n          sriov: <% $.role_services.any($.get('type', '').endsWith('::NeutronSriovAgent')) %>\n\n          # The role supports the HCI feature if it includes both NovaCompute and CephOSD services.\n          hci: <% $.role_services.any($.get('type', '').endsWith('::NovaCompute')) and $.role_services.any($.get('type', '').endsWith('::CephOSD')) %>\n\n      build_feature_dict:\n        on-success: filter_features\n        publish:\n          feature_dict: <% dict(DPDK => ($.dpdk or $.odl_dpdk), SRIOV => $.sriov, HOST => ($.dpdk or $.odl_dpdk or $.sriov), HCI => $.hci) %>\n\n      filter_features:\n        publish:\n          # The list of features that are enabled (i.e. are true in the feature_dict).\n          role_features: <% let(feature_dict => $.feature_dict)-> $feature_dict.keys().where($feature_dict[$]) %>\n\n      set_status_failed_get_resource_chains:\n        publish:\n          message: <% 'Unable to locate any resource chains in the heat resource tree' %>\n        on-success: fail\n\n      set_status_failed_get_role_chain:\n        publish:\n          message: <% \"Unable to determine the service chain resource for role '{0}'\".format($.role_name) %>\n        on-success: fail\n\n      set_status_failed_get_service_chain:\n        publish:\n          message: <% \"Unable to determine the service chain for role '{0}'\".format($.role_name) %>\n        on-success: fail\n\n      set_status_failed_get_role_services:\n        publish:\n          message: <% \"Unable to determine list of services for role '{0}'\".format($.role_name) %>\n        on-success: fail\n", "name": "tripleo.derive_params.v1", "tags": [], "created_at": "2018-12-04 11:26:04", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "6dd11ba0-99a6-49e3-b0c6-c786d4b68ed8"}

2018-12-04 06:26:04,758 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:04,760 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.derive_params_formulas.v1
description: TripleO Workflows to derive deployment parameters from the introspected data

workflows:


  dpdk_derive_params:
    description: >
      Workflow to derive parameters for DPDK service.
    input:
      - plan
      - role_name
      - hw_data # introspection data
      - user_inputs
      - derived_parameters: {}

    output:
      derived_parameters: <% $.derived_parameters.mergeWith($.get('dpdk_parameters', {})) %>

    tags:
      - tripleo-common-managed

    tasks:
      get_network_config:
        action: tripleo.parameters.get_network_config
        input:
          container: <% $.plan %>
          role_name: <% $.role_name %>
        publish:
          network_configs: <% task().result.get('network_config', []) %>
        on-success: get_dpdk_nics_numa_info
        on-error: set_status_failed_get_network_config

      get_dpdk_nics_numa_info:
        action: tripleo.derive_params.get_dpdk_nics_numa_info
        input:
          network_configs: <% $.network_configs %>
          inspect_data: <% $.hw_data %>
        publish:
          dpdk_nics_numa_info: <% task().result %>
        on-success:
          # TODO: Need to remove condtions here
          # adding condition and throw error in action for empty check
          - get_dpdk_nics_numa_nodes: <% $.dpdk_nics_numa_info %>
          - set_status_failed_get_dpdk_nics_numa_info: <% not $.dpdk_nics_numa_info %>
        on-error: set_status_failed_on_error_get_dpdk_nics_numa_info

      get_dpdk_nics_numa_nodes:
        publish:
          dpdk_nics_numa_nodes: <% $.dpdk_nics_numa_info.groupBy($.numa_node).select($[0]).orderBy($) %>
        on-success:
          - get_numa_nodes: <% $.dpdk_nics_numa_nodes %>
          - set_status_failed_get_dpdk_nics_numa_nodes: <% not $.dpdk_nics_numa_nodes %>

      get_numa_nodes:
        publish:
          numa_nodes: <% $.hw_data.numa_topology.ram.select($.numa_node).orderBy($) %>
        on-success:
          - get_num_phy_cores_per_numa_for_pmd: <% $.numa_nodes %>
          - set_status_failed_get_numa_nodes: <% not $.numa_nodes %>

      get_num_phy_cores_per_numa_for_pmd:
        publish:
          num_phy_cores_per_numa_node_for_pmd: <% $.user_inputs.get('num_phy_cores_per_numa_node_for_pmd', 0) %>
        on-success:
          - get_num_cores_per_numa_nodes: <% isInteger($.num_phy_cores_per_numa_node_for_pmd) and $.num_phy_cores_per_numa_node_for_pmd > 0 %>
          - set_status_failed_get_num_phy_cores_per_numa_for_pmd_invalid: <% not isInteger($.num_phy_cores_per_numa_node_for_pmd) %>
          - set_status_failed_get_num_phy_cores_per_numa_for_pmd_not_provided: <% $.num_phy_cores_per_numa_node_for_pmd = 0 %>

      # For NUMA node with DPDK nic, number of cores should be used from user input
      # For NUMA node without DPDK nic, number of cores should be 1
      get_num_cores_per_numa_nodes:
        publish:
          num_cores_per_numa_nodes: <% let(dpdk_nics_nodes => $.dpdk_nics_numa_nodes, cores => $.num_phy_cores_per_numa_node_for_pmd) -> $.numa_nodes.select(switch($ in $dpdk_nics_nodes => $cores, not $ in $dpdk_nics_nodes => 1)) %>
        on-success: get_pmd_cpus

      get_pmd_cpus:
        action: tripleo.derive_params.get_dpdk_core_list
        input:
          inspect_data: <% $.hw_data %>
          numa_nodes_cores_count: <% $.num_cores_per_numa_nodes %>
        publish:
          pmd_cpus: <% task().result %>
        on-success:
          - get_pmd_cpus_range_list: <% $.pmd_cpus %>
          - set_status_failed_get_pmd_cpus: <% not $.pmd_cpus %>
        on-error: set_status_failed_on_error_get_pmd_cpus

      get_pmd_cpus_range_list:
        action: tripleo.derive_params.convert_number_to_range_list
        input:
          num_list: <% $.pmd_cpus %>
        publish:
          pmd_cpus: <% task().result %>
        on-success: get_host_cpus
        on-error: set_status_failed_get_pmd_cpus_range_list

      get_host_cpus:
        workflow: tripleo.derive_params_formulas.v1.get_host_cpus
        input:
          role_name: <% $.role_name %>
          hw_data: <% $.hw_data %>
        publish:
          host_cpus: <% task().result.get('host_cpus', '') %>
        on-success: get_sock_mem
        on-error: set_status_failed_get_host_cpus

      get_sock_mem:
        action: tripleo.derive_params.get_dpdk_socket_memory
        input:
          dpdk_nics_numa_info: <% $.dpdk_nics_numa_info %>
          numa_nodes: <% $.numa_nodes %>
          overhead: <% $.user_inputs.get('overhead', 800) %>
          packet_size_in_buffer: <% 4096*64 %>
        publish:
          sock_mem: <% task().result %>
        on-success:
          - get_dpdk_parameters: <% $.sock_mem %>
          - set_status_failed_get_sock_mem: <% not $.sock_mem %>
        on-error: set_status_failed_on_error_get_sock_mem

      get_dpdk_parameters:
        publish:
          dpdk_parameters: <% dict(concat($.role_name, 'Parameters') => dict('OvsPmdCoreList' => $.get('pmd_cpus', ''), 'OvsDpdkCoreList' => $.get('host_cpus', ''), 'OvsDpdkSocketMemory' => $.get('sock_mem', ''))) %>

      set_status_failed_get_network_config:
        publish:
          status: FAILED
          message: <% task(get_network_config).result %>
        on-success: fail

      set_status_failed_get_dpdk_nics_numa_info:
        publish:
          status: FAILED
          message: "Unable to determine DPDK NIC's NUMA information"
        on-success: fail

      set_status_failed_on_error_get_dpdk_nics_numa_info:
        publish:
          status: FAILED
          message: <% task(get_dpdk_nics_numa_info).result %>
        on-success: fail

      set_status_failed_get_dpdk_nics_numa_nodes:
        publish:
          status: FAILED
          message: "Unable to determine DPDK NIC's numa nodes"
        on-success: fail

      set_status_failed_get_numa_nodes:
        publish:
          status: FAILED
          message: 'Unable to determine available NUMA nodes'
        on-success: fail

      set_status_failed_get_num_phy_cores_per_numa_for_pmd_invalid:
        publish:
          status: FAILED
          message: <% "num_phy_cores_per_numa_node_for_pmd user input '{0}' is invalid".format($.num_phy_cores_per_numa_node_for_pmd) %>
        on-success: fail

      set_status_failed_get_num_phy_cores_per_numa_for_pmd_not_provided:
        publish:
          status: FAILED
          message: 'num_phy_cores_per_numa_node_for_pmd user input is not provided'
        on-success: fail

      set_status_failed_get_pmd_cpus:
        publish:
          status: FAILED
          message: 'Unable to determine OvsPmdCoreList parameter'
        on-success: fail

      set_status_failed_on_error_get_pmd_cpus:
        publish:
          status: FAILED
          message: <% task(get_pmd_cpus).result %>
        on-success: fail

      set_status_failed_get_pmd_cpus_range_list:
        publish:
          status: FAILED
          message: <% task(get_pmd_cpus_range_list).result %>
        on-success: fail

      set_status_failed_get_host_cpus:
        publish:
          status: FAILED
          message: <% task(get_host_cpus).result.get('message', '') %>
        on-success: fail

      set_status_failed_get_sock_mem:
        publish:
          status: FAILED
          message: 'Unable to determine OvsDpdkSocketMemory parameter'
        on-success: fail

      set_status_failed_on_error_get_sock_mem:
        publish:
          status: FAILED
          message: <% task(get_sock_mem).result %>
        on-success: fail


  sriov_derive_params:
    description: >
      This workflow derives parameters for the SRIOV feature.

    input:
      - role_name
      - hw_data # introspection data
      - derived_parameters: {}

    output:
      derived_parameters: <% $.derived_parameters.mergeWith($.get('sriov_parameters', {})) %>

    tags:
      - tripleo-common-managed

    tasks:
      get_host_cpus:
        workflow: tripleo.derive_params_formulas.v1.get_host_cpus
        input:
          role_name: <% $.role_name %>
          hw_data: <% $.hw_data %>
        publish:
          host_cpus: <% task().result.get('host_cpus', '') %>
        on-success: get_sriov_parameters
        on-error: set_status_failed_get_host_cpus

      get_sriov_parameters:
        publish:
          # SriovHostCpusList parameter is added temporarily and it's removed later from derived parameters result.
          sriov_parameters: <% dict(concat($.role_name, 'Parameters') => dict('SriovHostCpusList' => $.get('host_cpus', ''))) %>

      set_status_failed_get_host_cpus:
        publish:
          status: FAILED
          message: <% task(get_host_cpus).result.get('message', '') %>
        on-success: fail


  get_host_cpus:
    description: >
      Fetching the host CPU list from the introspection data, and then converting the raw list into a range list.

    input:
      - hw_data # introspection data

    output:
      host_cpus: <% $.get('host_cpus', '') %>

    tags:
      - tripleo-common-managed

    tasks:
      get_host_cpus:
        action: tripleo.derive_params.get_host_cpus_list inspect_data=<% $.hw_data %>
        publish:
          host_cpus: <% task().result  %>
        on-success:
          - get_host_cpus_range_list: <% $.host_cpus %>
          - set_status_failed_get_host_cpus: <% not $.host_cpus %>
        on-error: set_status_failed_on_error_get_host_cpus

      get_host_cpus_range_list:
        action: tripleo.derive_params.convert_number_to_range_list
        input:
          num_list: <% $.host_cpus %>
        publish:
          host_cpus: <% task().result %>
        on-error: set_status_failed_get_host_cpus_range_list

      set_status_failed_get_host_cpus:
        publish:
          status: FAILED
          message: 'Unable to determine host cpus'
        on-success: fail

      set_status_failed_on_error_get_host_cpus:
        publish:
          status: FAILED
          message: <% task(get_host_cpus).result %>
        on-success: fail

      set_status_failed_get_host_cpus_range_list:
        publish:
          status: FAILED
          message: <% task(get_host_cpus_range_list).result %>
        on-success: fail


  host_derive_params:
    description: >
      This workflow derives parameters for the Host process, and is mainly associated with CPU pinning and huge memory pages.
      This workflow can be dependent on any feature or also can be invoked individually as well.

    input:
      - role_name
      - hw_data # introspection data
      - user_inputs
      - derived_parameters: {}

    output:
      derived_parameters: <% $.derived_parameters.mergeWith($.get('host_parameters', {})) %>

    tags:
      - tripleo-common-managed

    tasks:
      get_cpus:
        publish:
          cpus: <% $.hw_data.numa_topology.cpus %>
        on-success:
          - get_role_derive_params: <% $.cpus %>
          - set_status_failed_get_cpus: <% not $.cpus %>

      get_role_derive_params:
        publish:
          role_derive_params: <% $.derived_parameters.get(concat($.role_name, 'Parameters'), {}) %>
          # removing the role parameters (eg. ComputeParameters) in derived_parameters dictionary since already copied in role_derive_params.
          derived_parameters: <% $.derived_parameters.delete(concat($.role_name, 'Parameters')) %>
        on-success: get_host_cpus

      get_host_cpus:
        publish:
          host_cpus: <% $.role_derive_params.get('OvsDpdkCoreList', '') or $.role_derive_params.get('SriovHostCpusList', '') %>
          # SriovHostCpusList parameter is added temporarily for host_cpus and not needed in derived_parameters result.
          # SriovHostCpusList parameter is deleted in derived_parameters list and adding the updated role parameters
          # back in the derived_parameters.
          derived_parameters: <% $.derived_parameters + dict(concat($.role_name, 'Parameters') => $.role_derive_params.delete('SriovHostCpusList')) %>
        on-success: get_host_dpdk_combined_cpus

      get_host_dpdk_combined_cpus:
        publish:
          host_dpdk_combined_cpus: <% let(pmd_cpus => $.role_derive_params.get('OvsPmdCoreList', '')) -> switch($pmd_cpus => concat($pmd_cpus, ',', $.host_cpus), not $pmd_cpus => $.host_cpus) %>
          reserved_cpus: []
        on-success:
          - get_host_dpdk_combined_cpus_num_list: <% $.host_dpdk_combined_cpus %>
          - set_status_failed_get_host_dpdk_combined_cpus: <% not $.host_dpdk_combined_cpus %>

      get_host_dpdk_combined_cpus_num_list:
        action: tripleo.derive_params.convert_range_to_number_list
        input:
          range_list: <% $.host_dpdk_combined_cpus %>
        publish:
          host_dpdk_combined_cpus: <% task().result %>
          reserved_cpus: <% task().result.split(',') %>
        on-success: get_nova_cpus
        on-error: set_status_failed_get_host_dpdk_combined_cpus_num_list

      get_nova_cpus:
        publish:
          nova_cpus: <% let(reserved_cpus => $.reserved_cpus) -> $.cpus.select($.thread_siblings).flatten().where(not (str($) in $reserved_cpus)).join(',') %>
        on-success:
          - get_isol_cpus: <% $.nova_cpus %>
          - set_status_failed_get_nova_cpus: <% not $.nova_cpus %>

      # concatinates OvsPmdCoreList range format and NovaVcpuPinSet in range format. it may not be in perfect range format.
      # example: concatinates '12-15,19' and 16-18' ranges '12-15,19,16-18'
      get_isol_cpus:
        publish:
          isol_cpus: <% let(pmd_cpus => $.role_derive_params.get('OvsPmdCoreList','')) -> switch($pmd_cpus => concat($pmd_cpus, ',', $.nova_cpus), not $pmd_cpus => $.nova_cpus) %>
        on-success: get_isol_cpus_num_list

      # Gets the isol_cpus in the number list
      # example: '12-15,19,16-18' into '12,13,14,15,16,17,18,19'
      get_isol_cpus_num_list:
        action: tripleo.derive_params.convert_range_to_number_list
        input:
          range_list: <% $.isol_cpus %>
        publish:
          isol_cpus: <% task().result %>
        on-success: get_nova_cpus_range_list
        on-error: set_status_failed_get_isol_cpus_num_list

      get_nova_cpus_range_list:
        action: tripleo.derive_params.convert_number_to_range_list
        input:
          num_list: <% $.nova_cpus %>
        publish:
          nova_cpus: <% task().result %>
        on-success: get_isol_cpus_range_list
        on-error: set_status_failed_get_nova_cpus_range_list

      # converts number format isol_cpus into range format
      # example: '12,13,14,15,16,17,18,19' into '12-19'
      get_isol_cpus_range_list:
        action: tripleo.derive_params.convert_number_to_range_list
        input:
          num_list: <% $.isol_cpus %>
        publish:
          isol_cpus: <% task().result %>
        on-success: get_host_mem
        on-error: set_status_failed_get_isol_cpus_range_list

      get_host_mem:
        publish:
          host_mem: <% $.user_inputs.get('host_mem_default', 4096) %>
        on-success: check_default_hugepage_supported

      check_default_hugepage_supported:
        publish:
          default_hugepage_supported: <% $.hw_data.get('inventory', {}).get('cpu', {}).get('flags', []).contains('pdpe1gb') %>
        on-success:
          - get_total_memory: <% $.default_hugepage_supported %>
          - set_status_failed_check_default_hugepage_supported: <% not $.default_hugepage_supported %>

      get_total_memory:
        publish:
          total_memory: <% $.hw_data.get('inventory', {}).get('memory', {}).get('physical_mb', 0) %>
        on-success:
          - get_hugepage_allocation_percentage: <% $.total_memory %>
          - set_status_failed_get_total_memory: <% not $.total_memory %>

      get_hugepage_allocation_percentage:
        publish:
          huge_page_allocation_percentage: <% $.user_inputs.get('huge_page_allocation_percentage', 0) %>
        on-success:
          - get_hugepages: <% isInteger($.huge_page_allocation_percentage) and $.huge_page_allocation_percentage > 0 %>
          - set_status_failed_get_hugepage_allocation_percentage_invalid: <% not isInteger($.huge_page_allocation_percentage) %>
          - set_status_failed_get_hugepage_allocation_percentage_not_provided: <% $.huge_page_allocation_percentage = 0 %>

      get_hugepages:
        publish:
          hugepages: <% let(huge_page_perc => float($.huge_page_allocation_percentage)/100)-> int((($.total_memory/1024)-4) * $huge_page_perc) %>
        on-success:
          - get_cpu_model: <% $.hugepages %>
          - set_status_failed_get_hugepages: <% not $.hugepages %>

      get_cpu_model:
        publish:
          intel_cpu_model: <% $.hw_data.get('inventory', {}).get('cpu', {}).get('model_name', '').startsWith('Intel') %>
        on-success: get_iommu_info

      get_iommu_info:
        publish:
          iommu_info: <% switch($.intel_cpu_model => 'intel_iommu=on iommu=pt', not $.intel_cpu_model => '') %>
        on-success: get_kernel_args

      get_kernel_args:
        publish:
          kernel_args: <% concat('default_hugepagesz=1GB hugepagesz=1G ', 'hugepages=', str($.hugepages), ' ',  $.iommu_info, ' isolcpus=', $.isol_cpus) %>
        on-success: get_host_parameters

      get_host_parameters:
        publish:
          host_parameters: <% dict(concat($.role_name, 'Parameters') => dict('NovaVcpuPinSet' => $.get('nova_cpus', ''), 'NovaReservedHostMemory' => $.get('host_mem', ''), 'KernelArgs' => $.get('kernel_args', ''), 'IsolCpusList' => $.get('isol_cpus', ''))) %>

      set_status_failed_get_cpus:
        publish:
          status: FAILED
          message: "Unable to determine CPU's on NUMA nodes"
        on-success: fail

      set_status_failed_get_host_dpdk_combined_cpus:
        publish:
          status: FAILED
          message: 'Unable to combine host and dpdk cpus list'
        on-success: fail

      set_status_failed_get_host_dpdk_combined_cpus_num_list:
        publish:
          status: FAILED
          message: <% task(get_host_dpdk_combined_cpus_num_list).result %>
        on-success: fail

      set_status_failed_get_nova_cpus:
        publish:
          status: FAILED
          message: 'Unable to determine nova vcpu pin set'
        on-success: fail

      set_status_failed_get_nova_cpus_range_list:
        publish:
          status: FAILED
          message: <% task(get_nova_cpus_range_list).result %>
        on-success: fail

      set_status_failed_get_isol_cpus_num_list:
        publish:
          status: FAILED
          message: <% task(get_isol_cpus_num_list).result %>
        on-success: fail

      set_status_failed_get_isol_cpus_range_list:
        publish:
          status: FAILED
          message: <% task(get_isol_cpus_range_list).result %>
        on-success: fail

      set_status_failed_check_default_hugepage_supported:
        publish:
          status: FAILED
          message: 'default huge page size 1GB is not supported'
        on-success: fail

      set_status_failed_get_total_memory:
        publish:
          status: FAILED
          message: 'Unable to determine total memory'
        on-success: fail

      set_status_failed_get_hugepage_allocation_percentage_invalid:
        publish:
          status: FAILED
          message: <% "huge_page_allocation_percentage user input '{0}' is invalid".format($.huge_page_allocation_percentage) %>
        on-success: fail

      set_status_failed_get_hugepage_allocation_percentage_not_provided:
        publish:
          status: FAILED
          message: 'huge_page_allocation_percentage user input is not provided'
        on-success: fail

      set_status_failed_get_hugepages:
        publish:
          status: FAILED
          message: 'Unable to determine huge pages'
        on-success: fail


  hci_derive_params:
    description: Derive the deployment parameters for HCI
    input:
      - role_name
      - environment_parameters
      - heat_resource_tree
      - introspection_data
      - user_inputs
      - derived_parameters: {}

    output:
      derived_parameters: <% $.derived_parameters.mergeWith($.get('hci_parameters', {})) %>

    tags:
      - tripleo-common-managed

    tasks:
      get_hci_inputs:
        publish:
          hci_profile: <% $.user_inputs.get('hci_profile', '') %>
          hci_profile_config: <% $.user_inputs.get('hci_profile_config', {}) %>
          MB_PER_GB: 1024
        on-success:
          - get_average_guest_memory_size_in_mb: <% $.hci_profile and $.hci_profile_config.get($.hci_profile, {}) %>
          - set_failed_invalid_hci_profile: <% $.hci_profile and not $.hci_profile_config.get($.hci_profile, {}) %>
          # When no hci_profile is specified, the workflow terminates without deriving any HCI parameters.

      get_average_guest_memory_size_in_mb:
        publish:
          average_guest_memory_size_in_mb: <% $.hci_profile_config.get($.hci_profile, {}).get('average_guest_memory_size_in_mb', 0) %>
        on-success:
          - get_average_guest_cpu_utilization_percentage: <% isInteger($.average_guest_memory_size_in_mb) %>
          - set_failed_invalid_average_guest_memory_size_in_mb: <% not isInteger($.average_guest_memory_size_in_mb) %>

      get_average_guest_cpu_utilization_percentage:
        publish:
          average_guest_cpu_utilization_percentage: <% $.hci_profile_config.get($.hci_profile, {}).get('average_guest_cpu_utilization_percentage', 0) %>
        on-success:
          - get_gb_overhead_per_guest: <% isInteger($.average_guest_cpu_utilization_percentage) %>
          - set_failed_invalid_average_guest_cpu_utilization_percentage: <% not isInteger($.average_guest_cpu_utilization_percentage) %>

      get_gb_overhead_per_guest:
        publish:
          gb_overhead_per_guest: <% $.user_inputs.get('gb_overhead_per_guest', 0.5) %>
        on-success:
          - get_gb_per_osd: <% isNumber($.gb_overhead_per_guest) %>
          - set_failed_invalid_gb_overhead_per_guest: <% not isNumber($.gb_overhead_per_guest) %>

      get_gb_per_osd:
        publish:
          gb_per_osd: <% $.user_inputs.get('gb_per_osd', 5) %>
        on-success:
          - get_cores_per_osd: <% isNumber($.gb_per_osd) %>
          - set_failed_invalid_gb_per_osd: <% not isNumber($.gb_per_osd) %>

      get_cores_per_osd:
        publish:
          cores_per_osd: <% $.user_inputs.get('cores_per_osd', 1.0) %>
        on-success:
          - get_extra_configs: <% isNumber($.cores_per_osd) %>
          - set_failed_invalid_cores_per_osd: <% not isNumber($.cores_per_osd) %>

      get_extra_configs:
        publish:
          extra_config: <% $.environment_parameters.get('ExtraConfig', {}) %>
          role_extra_config: <% $.environment_parameters.get(concat($.role_name, 'ExtraConfig'), {}) %>
          role_env_params: <% $.environment_parameters.get(concat($.role_name, 'Parameters'), {}) %>
          role_derive_params: <% $.derived_parameters.get(concat($.role_name, 'Parameters'), {}) %>
        on-success: get_num_osds

      get_num_osds:
        publish:
          num_osds: <% $.heat_resource_tree.parameters.get('CephAnsibleDisksConfig', {}).get('default', {}).get('devices', []).count() %>
        on-success:
          - get_memory_mb: <% $.num_osds %>
          # If there's no CephAnsibleDisksConfig then look for OSD configuration in hiera data
          - get_num_osds_from_hiera: <% not $.num_osds %>

      get_num_osds_from_hiera:
        publish:
          num_osds: <% $.role_extra_config.get('ceph::profile::params::osds', $.extra_config.get('ceph::profile::params::osds', {})).keys().count() %>
        on-success:
          - get_memory_mb: <% $.num_osds %>
          - set_failed_no_osds: <% not $.num_osds %>

      get_memory_mb:
        publish:
          memory_mb: <% $.introspection_data.get('memory_mb', 0) %>
        on-success:
          - get_nova_vcpu_pin_set: <% $.memory_mb %>
          - set_failed_get_memory_mb: <% not $.memory_mb %>

      # Determine the number of CPU cores available to Nova and Ceph. If
      # NovaVcpuPinSet is defined then use the number of vCPUs in the set,
      # otherwise use all of the cores identified in the introspection data.

      get_nova_vcpu_pin_set:
        publish:
          # NovaVcpuPinSet can be defined in multiple locations, and it's
          # important to select the value in order of precedence:
          #
          # 1) User specified value for this role
          # 2) User specified default value for all roles
          # 3) Value derived by another derived parameters workflow
          nova_vcpu_pin_set: <% $.role_env_params.get('NovaVcpuPinSet', $.environment_parameters.get('NovaVcpuPinSet', $.role_derive_params.get('NovaVcpuPinSet', ''))) %>
        on-success:
          - get_nova_vcpu_count: <% $.nova_vcpu_pin_set %>
          - get_num_cores: <% not $.nova_vcpu_pin_set %>

      get_nova_vcpu_count:
        action: tripleo.derive_params.convert_range_to_number_list
        input:
          range_list: <% $.nova_vcpu_pin_set %>
        publish:
          num_cores: <% task().result.split(',').count() %>
        on-success: calculate_nova_parameters
        on-error: set_failed_get_nova_vcpu_count

      get_num_cores:
        publish:
          num_cores: <% $.introspection_data.get('cpus', 0) %>
        on-success:
          - calculate_nova_parameters: <% $.num_cores %>
          - set_failed_get_num_cores: <% not $.num_cores %>

      # HCI calculations are broken into multiple steps. This is necessary
      # because variables published by a Mistral task are not available
      # for use by that same task. Variables computed and published in a task
      # are only available in subsequent tasks.
      #
      # The HCI calculations compute two Nova parameters:
      # - reserved_host_memory
      # - cpu_allocation_ratio
      #
      # The reserved_host_memory calculation computes the amount of memory
      # that needs to be reserved for Ceph and the total amount of "guest
      # overhead" memory that is based on the anticipated number of guests.
      # Psuedo-code for the calculation (disregarding MB and GB units) is
      # as follows:
      #
      #   ceph_memory = mem_per_osd * num_osds
      #   nova_memory = total_memory - ceph_memory
      #   num_guests = nova_memory /
      #                (average_guest_memory_size + overhead_per_guest)
      #   reserved_memory = ceph_memory + (num_guests * overhead_per_guest)
      #
      # The cpu_allocation_ratio calculation is similar in that it takes into
      # account the number of cores that must be reserved for Ceph.
      #
      #   ceph_cores = cores_per_osd * num_osds
      #   guest_cores = num_cores - ceph_cores
      #   guest_vcpus = guest_cores / average_guest_utilization
      #   cpu_allocation_ratio = guest_vcpus / num_cores

      calculate_nova_parameters:
        publish:
          avg_guest_util: <% $.average_guest_cpu_utilization_percentage / 100.0 %>
          avg_guest_size_gb: <% $.average_guest_memory_size_in_mb / float($.MB_PER_GB) %>
          memory_gb: <% $.memory_mb / float($.MB_PER_GB) %>
          ceph_mem_gb: <% $.gb_per_osd * $.num_osds %>
          nonceph_cores: <% $.num_cores - int($.cores_per_osd * $.num_osds) %>
        on-success: calc_step_2

      calc_step_2:
        publish:
          num_guests: <% int(($.memory_gb - $.ceph_mem_gb) / ($.avg_guest_size_gb + $.gb_overhead_per_guest)) %>
          guest_vcpus: <% $.nonceph_cores / $.avg_guest_util %>
        on-success: calc_step_3

      calc_step_3:
        publish:
          reserved_host_memory: <% $.MB_PER_GB * int($.ceph_mem_gb + ($.num_guests * $.gb_overhead_per_guest)) %>
          cpu_allocation_ratio: <% $.guest_vcpus / $.num_cores %>
        on-success: validate_results

      validate_results:
        publish:
          # Verify whether HCI is viable:
          # - At least 80% of the memory is reserved for Ceph and guest overhead
          # - At least half of the CPU cores must be available to Nova
          mem_ok: <% $.reserved_host_memory <= ($.memory_mb * 0.8) %>
          cpu_ok: <% $.cpu_allocation_ratio >= 0.5 %>
        on-success:
          - set_failed_insufficient_mem: <% not $.mem_ok %>
          - set_failed_insufficient_cpu: <% not $.cpu_ok %>
          - publish_hci_parameters: <% $.mem_ok and $.cpu_ok %>

      publish_hci_parameters:
        publish:
          # TODO(abishop): Update this when the cpu_allocation_ratio can be set
          # via a THT parameter (no such parameter currently exists). Until a
          # THT parameter exists, use hiera data to set the cpu_allocation_ratio.
          hci_parameters: <% dict(concat($.role_name, 'Parameters') => dict('NovaReservedHostMemory' => $.reserved_host_memory)) + dict(concat($.role_name, 'ExtraConfig') => dict('nova::cpu_allocation_ratio' => $.cpu_allocation_ratio)) %>

      set_failed_invalid_hci_profile:
        publish:
          message: "'<% $.hci_profile %>' is not a valid HCI profile."
        on-success: fail

      set_failed_invalid_average_guest_memory_size_in_mb:
        publish:
          message: "'<% $.average_guest_memory_size_in_mb %>' is not a valid average_guest_memory_size_in_mb value."
        on-success: fail

      set_failed_invalid_gb_overhead_per_guest:
        publish:
          message: "'<% $.gb_overhead_per_guest %>' is not a valid gb_overhead_per_guest value."
        on-success: fail

      set_failed_invalid_gb_per_osd:
        publish:
          message: "'<% $.gb_per_osd %>' is not a valid gb_per_osd value."
        on-success: fail

      set_failed_invalid_cores_per_osd:
        publish:
          message: "'<% $.cores_per_osd %>' is not a valid cores_per_osd value."
        on-success: fail

      set_failed_invalid_average_guest_cpu_utilization_percentage:
        publish:
          message: "'<% $.average_guest_cpu_utilization_percentage %>' is not a valid average_guest_cpu_utilization_percentage value."
        on-success: fail

      set_failed_no_osds:
        publish:
          message: "No Ceph OSDs found in the overcloud definition ('ceph::profile::params::osds')."
        on-success: fail

      set_failed_get_memory_mb:
        publish:
          message: "Unable to determine the amount of physical memory (no 'memory_mb' found in introspection_data)."
        on-success: fail

      set_failed_get_nova_vcpu_count:
        publish:
          message: <% task(get_nova_vcpu_count).result %>
        on-success: fail

      set_failed_get_num_cores:
        publish:
          message: "Unable to determine the number of CPU cores (no 'cpus' found in introspection_data)."
        on-success: fail

      set_failed_insufficient_mem:
        publish:
          message: "<% $.memory_mb %> MB is not enough memory to run hyperconverged."
        on-success: fail

      set_failed_insufficient_cpu:
        publish:
          message: "<% $.num_cores %> CPU cores are not enough to run hyperconverged."
        on-success: fail
'
2018-12-04 06:26:08,435 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 32010
2018-12-04 06:26:08,476 DEBUG: RESP: [201] Content-Length: 32010 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:08 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.derive_params_formulas.v1\ndescription: TripleO Workflows to derive deployment parameters from the introspected data\n\nworkflows:\n\n\n  dpdk_derive_params:\n    description: >\n      Workflow to derive parameters for DPDK service.\n    input:\n      - plan\n      - role_name\n      - hw_data # introspection data\n      - user_inputs\n      - derived_parameters: {}\n\n    output:\n      derived_parameters: <% $.derived_parameters.mergeWith($.get('dpdk_parameters', {})) %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_network_config:\n        action: tripleo.parameters.get_network_config\n        input:\n          container: <% $.plan %>\n          role_name: <% $.role_name %>\n        publish:\n          network_configs: <% task().result.get('network_config', []) %>\n        on-success: get_dpdk_nics_numa_info\n        on-error: set_status_failed_get_network_config\n\n      get_dpdk_nics_numa_info:\n        action: tripleo.derive_params.get_dpdk_nics_numa_info\n        input:\n          network_configs: <% $.network_configs %>\n          inspect_data: <% $.hw_data %>\n        publish:\n          dpdk_nics_numa_info: <% task().result %>\n        on-success:\n          # TODO: Need to remove condtions here\n          # adding condition and throw error in action for empty check\n          - get_dpdk_nics_numa_nodes: <% $.dpdk_nics_numa_info %>\n          - set_status_failed_get_dpdk_nics_numa_info: <% not $.dpdk_nics_numa_info %>\n        on-error: set_status_failed_on_error_get_dpdk_nics_numa_info\n\n      get_dpdk_nics_numa_nodes:\n        publish:\n          dpdk_nics_numa_nodes: <% $.dpdk_nics_numa_info.groupBy($.numa_node).select($[0]).orderBy($) %>\n        on-success:\n          - get_numa_nodes: <% $.dpdk_nics_numa_nodes %>\n          - set_status_failed_get_dpdk_nics_numa_nodes: <% not $.dpdk_nics_numa_nodes %>\n\n      get_numa_nodes:\n        publish:\n          numa_nodes: <% $.hw_data.numa_topology.ram.select($.numa_node).orderBy($) %>\n        on-success:\n          - get_num_phy_cores_per_numa_for_pmd: <% $.numa_nodes %>\n          - set_status_failed_get_numa_nodes: <% not $.numa_nodes %>\n\n      get_num_phy_cores_per_numa_for_pmd:\n        publish:\n          num_phy_cores_per_numa_node_for_pmd: <% $.user_inputs.get('num_phy_cores_per_numa_node_for_pmd', 0) %>\n        on-success:\n          - get_num_cores_per_numa_nodes: <% isInteger($.num_phy_cores_per_numa_node_for_pmd) and $.num_phy_cores_per_numa_node_for_pmd > 0 %>\n          - set_status_failed_get_num_phy_cores_per_numa_for_pmd_invalid: <% not isInteger($.num_phy_cores_per_numa_node_for_pmd) %>\n          - set_status_failed_get_num_phy_cores_per_numa_for_pmd_not_provided: <% $.num_phy_cores_per_numa_node_for_pmd = 0 %>\n\n      # For NUMA node with DPDK nic, number of cores should be used from user input\n      # For NUMA node without DPDK nic, number of cores should be 1\n      get_num_cores_per_numa_nodes:\n        publish:\n          num_cores_per_numa_nodes: <% let(dpdk_nics_nodes => $.dpdk_nics_numa_nodes, cores => $.num_phy_cores_per_numa_node_for_pmd) -> $.numa_nodes.select(switch($ in $dpdk_nics_nodes => $cores, not $ in $dpdk_nics_nodes => 1)) %>\n        on-success: get_pmd_cpus\n\n      get_pmd_cpus:\n        action: tripleo.derive_params.get_dpdk_core_list\n        input:\n          inspect_data: <% $.hw_data %>\n          numa_nodes_cores_count: <% $.num_cores_per_numa_nodes %>\n        publish:\n          pmd_cpus: <% task().result %>\n        on-success:\n          - get_pmd_cpus_range_list: <% $.pmd_cpus %>\n          - set_status_failed_get_pmd_cpus: <% not $.pmd_cpus %>\n        on-error: set_status_failed_on_error_get_pmd_cpus\n\n      get_pmd_cpus_range_list:\n        action: tripleo.derive_params.convert_number_to_range_list\n        input:\n          num_list: <% $.pmd_cpus %>\n        publish:\n          pmd_cpus: <% task().result %>\n        on-success: get_host_cpus\n        on-error: set_status_failed_get_pmd_cpus_range_list\n\n      get_host_cpus:\n        workflow: tripleo.derive_params_formulas.v1.get_host_cpus\n        input:\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n        publish:\n          host_cpus: <% task().result.get('host_cpus', '') %>\n        on-success: get_sock_mem\n        on-error: set_status_failed_get_host_cpus\n\n      get_sock_mem:\n        action: tripleo.derive_params.get_dpdk_socket_memory\n        input:\n          dpdk_nics_numa_info: <% $.dpdk_nics_numa_info %>\n          numa_nodes: <% $.numa_nodes %>\n          overhead: <% $.user_inputs.get('overhead', 800) %>\n          packet_size_in_buffer: <% 4096*64 %>\n        publish:\n          sock_mem: <% task().result %>\n        on-success:\n          - get_dpdk_parameters: <% $.sock_mem %>\n          - set_status_failed_get_sock_mem: <% not $.sock_mem %>\n        on-error: set_status_failed_on_error_get_sock_mem\n\n      get_dpdk_parameters:\n        publish:\n          dpdk_parameters: <% dict(concat($.role_name, 'Parameters') => dict('OvsPmdCoreList' => $.get('pmd_cpus', ''), 'OvsDpdkCoreList' => $.get('host_cpus', ''), 'OvsDpdkSocketMemory' => $.get('sock_mem', ''))) %>\n\n      set_status_failed_get_network_config:\n        publish:\n          status: FAILED\n          message: <% task(get_network_config).result %>\n        on-success: fail\n\n      set_status_failed_get_dpdk_nics_numa_info:\n        publish:\n          status: FAILED\n          message: \"Unable to determine DPDK NIC's NUMA information\"\n        on-success: fail\n\n      set_status_failed_on_error_get_dpdk_nics_numa_info:\n        publish:\n          status: FAILED\n          message: <% task(get_dpdk_nics_numa_info).result %>\n        on-success: fail\n\n      set_status_failed_get_dpdk_nics_numa_nodes:\n        publish:\n          status: FAILED\n          message: \"Unable to determine DPDK NIC's numa nodes\"\n        on-success: fail\n\n      set_status_failed_get_numa_nodes:\n        publish:\n          status: FAILED\n          message: 'Unable to determine available NUMA nodes'\n        on-success: fail\n\n      set_status_failed_get_num_phy_cores_per_numa_for_pmd_invalid:\n        publish:\n          status: FAILED\n          message: <% \"num_phy_cores_per_numa_node_for_pmd user input '{0}' is invalid\".format($.num_phy_cores_per_numa_node_for_pmd) %>\n        on-success: fail\n\n      set_status_failed_get_num_phy_cores_per_numa_for_pmd_not_provided:\n        publish:\n          status: FAILED\n          message: 'num_phy_cores_per_numa_node_for_pmd user input is not provided'\n        on-success: fail\n\n      set_status_failed_get_pmd_cpus:\n        publish:\n          status: FAILED\n          message: 'Unable to determine OvsPmdCoreList parameter'\n        on-success: fail\n\n      set_status_failed_on_error_get_pmd_cpus:\n        publish:\n          status: FAILED\n          message: <% task(get_pmd_cpus).result %>\n        on-success: fail\n\n      set_status_failed_get_pmd_cpus_range_list:\n        publish:\n          status: FAILED\n          message: <% task(get_pmd_cpus_range_list).result %>\n        on-success: fail\n\n      set_status_failed_get_host_cpus:\n        publish:\n          status: FAILED\n          message: <% task(get_host_cpus).result.get('message', '') %>\n        on-success: fail\n\n      set_status_failed_get_sock_mem:\n        publish:\n          status: FAILED\n          message: 'Unable to determine OvsDpdkSocketMemory parameter'\n        on-success: fail\n\n      set_status_failed_on_error_get_sock_mem:\n        publish:\n          status: FAILED\n          message: <% task(get_sock_mem).result %>\n        on-success: fail\n\n\n  sriov_derive_params:\n    description: >\n      This workflow derives parameters for the SRIOV feature.\n\n    input:\n      - role_name\n      - hw_data # introspection data\n      - derived_parameters: {}\n\n    output:\n      derived_parameters: <% $.derived_parameters.mergeWith($.get('sriov_parameters', {})) %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_host_cpus:\n        workflow: tripleo.derive_params_formulas.v1.get_host_cpus\n        input:\n          role_name: <% $.role_name %>\n          hw_data: <% $.hw_data %>\n        publish:\n          host_cpus: <% task().result.get('host_cpus', '') %>\n        on-success: get_sriov_parameters\n        on-error: set_status_failed_get_host_cpus\n\n      get_sriov_parameters:\n        publish:\n          # SriovHostCpusList parameter is added temporarily and it's removed later from derived parameters result.\n          sriov_parameters: <% dict(concat($.role_name, 'Parameters') => dict('SriovHostCpusList' => $.get('host_cpus', ''))) %>\n\n      set_status_failed_get_host_cpus:\n        publish:\n          status: FAILED\n          message: <% task(get_host_cpus).result.get('message', '') %>\n        on-success: fail\n\n\n  get_host_cpus:\n    description: >\n      Fetching the host CPU list from the introspection data, and then converting the raw list into a range list.\n\n    input:\n      - hw_data # introspection data\n\n    output:\n      host_cpus: <% $.get('host_cpus', '') %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_host_cpus:\n        action: tripleo.derive_params.get_host_cpus_list inspect_data=<% $.hw_data %>\n        publish:\n          host_cpus: <% task().result  %>\n        on-success:\n          - get_host_cpus_range_list: <% $.host_cpus %>\n          - set_status_failed_get_host_cpus: <% not $.host_cpus %>\n        on-error: set_status_failed_on_error_get_host_cpus\n\n      get_host_cpus_range_list:\n        action: tripleo.derive_params.convert_number_to_range_list\n        input:\n          num_list: <% $.host_cpus %>\n        publish:\n          host_cpus: <% task().result %>\n        on-error: set_status_failed_get_host_cpus_range_list\n\n      set_status_failed_get_host_cpus:\n        publish:\n          status: FAILED\n          message: 'Unable to determine host cpus'\n        on-success: fail\n\n      set_status_failed_on_error_get_host_cpus:\n        publish:\n          status: FAILED\n          message: <% task(get_host_cpus).result %>\n        on-success: fail\n\n      set_status_failed_get_host_cpus_range_list:\n        publish:\n          status: FAILED\n          message: <% task(get_host_cpus_range_list).result %>\n        on-success: fail\n\n\n  host_derive_params:\n    description: >\n      This workflow derives parameters for the Host process, and is mainly associated with CPU pinning and huge memory pages.\n      This workflow can be dependent on any feature or also can be invoked individually as well.\n\n    input:\n      - role_name\n      - hw_data # introspection data\n      - user_inputs\n      - derived_parameters: {}\n\n    output:\n      derived_parameters: <% $.derived_parameters.mergeWith($.get('host_parameters', {})) %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_cpus:\n        publish:\n          cpus: <% $.hw_data.numa_topology.cpus %>\n        on-success:\n          - get_role_derive_params: <% $.cpus %>\n          - set_status_failed_get_cpus: <% not $.cpus %>\n\n      get_role_derive_params:\n        publish:\n          role_derive_params: <% $.derived_parameters.get(concat($.role_name, 'Parameters'), {}) %>\n          # removing the role parameters (eg. ComputeParameters) in derived_parameters dictionary since already copied in role_derive_params.\n          derived_parameters: <% $.derived_parameters.delete(concat($.role_name, 'Parameters')) %>\n        on-success: get_host_cpus\n\n      get_host_cpus:\n        publish:\n          host_cpus: <% $.role_derive_params.get('OvsDpdkCoreList', '') or $.role_derive_params.get('SriovHostCpusList', '') %>\n          # SriovHostCpusList parameter is added temporarily for host_cpus and not needed in derived_parameters result.\n          # SriovHostCpusList parameter is deleted in derived_parameters list and adding the updated role parameters\n          # back in the derived_parameters.\n          derived_parameters: <% $.derived_parameters + dict(concat($.role_name, 'Parameters') => $.role_derive_params.delete('SriovHostCpusList')) %>\n        on-success: get_host_dpdk_combined_cpus\n\n      get_host_dpdk_combined_cpus:\n        publish:\n          host_dpdk_combined_cpus: <% let(pmd_cpus => $.role_derive_params.get('OvsPmdCoreList', '')) -> switch($pmd_cpus => concat($pmd_cpus, ',', $.host_cpus), not $pmd_cpus => $.host_cpus) %>\n          reserved_cpus: []\n        on-success:\n          - get_host_dpdk_combined_cpus_num_list: <% $.host_dpdk_combined_cpus %>\n          - set_status_failed_get_host_dpdk_combined_cpus: <% not $.host_dpdk_combined_cpus %>\n\n      get_host_dpdk_combined_cpus_num_list:\n        action: tripleo.derive_params.convert_range_to_number_list\n        input:\n          range_list: <% $.host_dpdk_combined_cpus %>\n        publish:\n          host_dpdk_combined_cpus: <% task().result %>\n          reserved_cpus: <% task().result.split(',') %>\n        on-success: get_nova_cpus\n        on-error: set_status_failed_get_host_dpdk_combined_cpus_num_list\n\n      get_nova_cpus:\n        publish:\n          nova_cpus: <% let(reserved_cpus => $.reserved_cpus) -> $.cpus.select($.thread_siblings).flatten().where(not (str($) in $reserved_cpus)).join(',') %>\n        on-success:\n          - get_isol_cpus: <% $.nova_cpus %>\n          - set_status_failed_get_nova_cpus: <% not $.nova_cpus %>\n\n      # concatinates OvsPmdCoreList range format and NovaVcpuPinSet in range format. it may not be in perfect range format.\n      # example: concatinates '12-15,19' and 16-18' ranges '12-15,19,16-18'\n      get_isol_cpus:\n        publish:\n          isol_cpus: <% let(pmd_cpus => $.role_derive_params.get('OvsPmdCoreList','')) -> switch($pmd_cpus => concat($pmd_cpus, ',', $.nova_cpus), not $pmd_cpus => $.nova_cpus) %>\n        on-success: get_isol_cpus_num_list\n\n      # Gets the isol_cpus in the number list\n      # example: '12-15,19,16-18' into '12,13,14,15,16,17,18,19'\n      get_isol_cpus_num_list:\n        action: tripleo.derive_params.convert_range_to_number_list\n        input:\n          range_list: <% $.isol_cpus %>\n        publish:\n          isol_cpus: <% task().result %>\n        on-success: get_nova_cpus_range_list\n        on-error: set_status_failed_get_isol_cpus_num_list\n\n      get_nova_cpus_range_list:\n        action: tripleo.derive_params.convert_number_to_range_list\n        input:\n          num_list: <% $.nova_cpus %>\n        publish:\n          nova_cpus: <% task().result %>\n        on-success: get_isol_cpus_range_list\n        on-error: set_status_failed_get_nova_cpus_range_list\n\n      # converts number format isol_cpus into range format\n      # example: '12,13,14,15,16,17,18,19' into '12-19'\n      get_isol_cpus_range_list:\n        action: tripleo.derive_params.convert_number_to_range_list\n        input:\n          num_list: <% $.isol_cpus %>\n        publish:\n          isol_cpus: <% task().result %>\n        on-success: get_host_mem\n        on-error: set_status_failed_get_isol_cpus_range_list\n\n      get_host_mem:\n        publish:\n          host_mem: <% $.user_inputs.get('host_mem_default', 4096) %>\n        on-success: check_default_hugepage_supported\n\n      check_default_hugepage_supported:\n        publish:\n          default_hugepage_supported: <% $.hw_data.get('inventory', {}).get('cpu', {}).get('flags', []).contains('pdpe1gb') %>\n        on-success:\n          - get_total_memory: <% $.default_hugepage_supported %>\n          - set_status_failed_check_default_hugepage_supported: <% not $.default_hugepage_supported %>\n\n      get_total_memory:\n        publish:\n          total_memory: <% $.hw_data.get('inventory', {}).get('memory', {}).get('physical_mb', 0) %>\n        on-success:\n          - get_hugepage_allocation_percentage: <% $.total_memory %>\n          - set_status_failed_get_total_memory: <% not $.total_memory %>\n\n      get_hugepage_allocation_percentage:\n        publish:\n          huge_page_allocation_percentage: <% $.user_inputs.get('huge_page_allocation_percentage', 0) %>\n        on-success:\n          - get_hugepages: <% isInteger($.huge_page_allocation_percentage) and $.huge_page_allocation_percentage > 0 %>\n          - set_status_failed_get_hugepage_allocation_percentage_invalid: <% not isInteger($.huge_page_allocation_percentage) %>\n          - set_status_failed_get_hugepage_allocation_percentage_not_provided: <% $.huge_page_allocation_percentage = 0 %>\n\n      get_hugepages:\n        publish:\n          hugepages: <% let(huge_page_perc => float($.huge_page_allocation_percentage)/100)-> int((($.total_memory/1024)-4) * $huge_page_perc) %>\n        on-success:\n          - get_cpu_model: <% $.hugepages %>\n          - set_status_failed_get_hugepages: <% not $.hugepages %>\n\n      get_cpu_model:\n        publish:\n          intel_cpu_model: <% $.hw_data.get('inventory', {}).get('cpu', {}).get('model_name', '').startsWith('Intel') %>\n        on-success: get_iommu_info\n\n      get_iommu_info:\n        publish:\n          iommu_info: <% switch($.intel_cpu_model => 'intel_iommu=on iommu=pt', not $.intel_cpu_model => '') %>\n        on-success: get_kernel_args\n\n      get_kernel_args:\n        publish:\n          kernel_args: <% concat('default_hugepagesz=1GB hugepagesz=1G ', 'hugepages=', str($.hugepages), ' ',  $.iommu_info, ' isolcpus=', $.isol_cpus) %>\n        on-success: get_host_parameters\n\n      get_host_parameters:\n        publish:\n          host_parameters: <% dict(concat($.role_name, 'Parameters') => dict('NovaVcpuPinSet' => $.get('nova_cpus', ''), 'NovaReservedHostMemory' => $.get('host_mem', ''), 'KernelArgs' => $.get('kernel_args', ''), 'IsolCpusList' => $.get('isol_cpus', ''))) %>\n\n      set_status_failed_get_cpus:\n        publish:\n          status: FAILED\n          message: \"Unable to determine CPU's on NUMA nodes\"\n        on-success: fail\n\n      set_status_failed_get_host_dpdk_combined_cpus:\n        publish:\n          status: FAILED\n          message: 'Unable to combine host and dpdk cpus list'\n        on-success: fail\n\n      set_status_failed_get_host_dpdk_combined_cpus_num_list:\n        publish:\n          status: FAILED\n          message: <% task(get_host_dpdk_combined_cpus_num_list).result %>\n        on-success: fail\n\n      set_status_failed_get_nova_cpus:\n        publish:\n          status: FAILED\n          message: 'Unable to determine nova vcpu pin set'\n        on-success: fail\n\n      set_status_failed_get_nova_cpus_range_list:\n        publish:\n          status: FAILED\n          message: <% task(get_nova_cpus_range_list).result %>\n        on-success: fail\n\n      set_status_failed_get_isol_cpus_num_list:\n        publish:\n          status: FAILED\n          message: <% task(get_isol_cpus_num_list).result %>\n        on-success: fail\n\n      set_status_failed_get_isol_cpus_range_list:\n        publish:\n          status: FAILED\n          message: <% task(get_isol_cpus_range_list).result %>\n        on-success: fail\n\n      set_status_failed_check_default_hugepage_supported:\n        publish:\n          status: FAILED\n          message: 'default huge page size 1GB is not supported'\n        on-success: fail\n\n      set_status_failed_get_total_memory:\n        publish:\n          status: FAILED\n          message: 'Unable to determine total memory'\n        on-success: fail\n\n      set_status_failed_get_hugepage_allocation_percentage_invalid:\n        publish:\n          status: FAILED\n          message: <% \"huge_page_allocation_percentage user input '{0}' is invalid\".format($.huge_page_allocation_percentage) %>\n        on-success: fail\n\n      set_status_failed_get_hugepage_allocation_percentage_not_provided:\n        publish:\n          status: FAILED\n          message: 'huge_page_allocation_percentage user input is not provided'\n        on-success: fail\n\n      set_status_failed_get_hugepages:\n        publish:\n          status: FAILED\n          message: 'Unable to determine huge pages'\n        on-success: fail\n\n\n  hci_derive_params:\n    description: Derive the deployment parameters for HCI\n    input:\n      - role_name\n      - environment_parameters\n      - heat_resource_tree\n      - introspection_data\n      - user_inputs\n      - derived_parameters: {}\n\n    output:\n      derived_parameters: <% $.derived_parameters.mergeWith($.get('hci_parameters', {})) %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_hci_inputs:\n        publish:\n          hci_profile: <% $.user_inputs.get('hci_profile', '') %>\n          hci_profile_config: <% $.user_inputs.get('hci_profile_config', {}) %>\n          MB_PER_GB: 1024\n        on-success:\n          - get_average_guest_memory_size_in_mb: <% $.hci_profile and $.hci_profile_config.get($.hci_profile, {}) %>\n          - set_failed_invalid_hci_profile: <% $.hci_profile and not $.hci_profile_config.get($.hci_profile, {}) %>\n          # When no hci_profile is specified, the workflow terminates without deriving any HCI parameters.\n\n      get_average_guest_memory_size_in_mb:\n        publish:\n          average_guest_memory_size_in_mb: <% $.hci_profile_config.get($.hci_profile, {}).get('average_guest_memory_size_in_mb', 0) %>\n        on-success:\n          - get_average_guest_cpu_utilization_percentage: <% isInteger($.average_guest_memory_size_in_mb) %>\n          - set_failed_invalid_average_guest_memory_size_in_mb: <% not isInteger($.average_guest_memory_size_in_mb) %>\n\n      get_average_guest_cpu_utilization_percentage:\n        publish:\n          average_guest_cpu_utilization_percentage: <% $.hci_profile_config.get($.hci_profile, {}).get('average_guest_cpu_utilization_percentage', 0) %>\n        on-success:\n          - get_gb_overhead_per_guest: <% isInteger($.average_guest_cpu_utilization_percentage) %>\n          - set_failed_invalid_average_guest_cpu_utilization_percentage: <% not isInteger($.average_guest_cpu_utilization_percentage) %>\n\n      get_gb_overhead_per_guest:\n        publish:\n          gb_overhead_per_guest: <% $.user_inputs.get('gb_overhead_per_guest', 0.5) %>\n        on-success:\n          - get_gb_per_osd: <% isNumber($.gb_overhead_per_guest) %>\n          - set_failed_invalid_gb_overhead_per_guest: <% not isNumber($.gb_overhead_per_guest) %>\n\n      get_gb_per_osd:\n        publish:\n          gb_per_osd: <% $.user_inputs.get('gb_per_osd', 5) %>\n        on-success:\n          - get_cores_per_osd: <% isNumber($.gb_per_osd) %>\n          - set_failed_invalid_gb_per_osd: <% not isNumber($.gb_per_osd) %>\n\n      get_cores_per_osd:\n        publish:\n          cores_per_osd: <% $.user_inputs.get('cores_per_osd', 1.0) %>\n        on-success:\n          - get_extra_configs: <% isNumber($.cores_per_osd) %>\n          - set_failed_invalid_cores_per_osd: <% not isNumber($.cores_per_osd) %>\n\n      get_extra_configs:\n        publish:\n          extra_config: <% $.environment_parameters.get('ExtraConfig', {}) %>\n          role_extra_config: <% $.environment_parameters.get(concat($.role_name, 'ExtraConfig'), {}) %>\n          role_env_params: <% $.environment_parameters.get(concat($.role_name, 'Parameters'), {}) %>\n          role_derive_params: <% $.derived_parameters.get(concat($.role_name, 'Parameters'), {}) %>\n        on-success: get_num_osds\n\n      get_num_osds:\n        publish:\n          num_osds: <% $.heat_resource_tree.parameters.get('CephAnsibleDisksConfig', {}).get('default', {}).get('devices', []).count() %>\n        on-success:\n          - get_memory_mb: <% $.num_osds %>\n          # If there's no CephAnsibleDisksConfig then look for OSD configuration in hiera data\n          - get_num_osds_from_hiera: <% not $.num_osds %>\n\n      get_num_osds_from_hiera:\n        publish:\n          num_osds: <% $.role_extra_config.get('ceph::profile::params::osds', $.extra_config.get('ceph::profile::params::osds', {})).keys().count() %>\n        on-success:\n          - get_memory_mb: <% $.num_osds %>\n          - set_failed_no_osds: <% not $.num_osds %>\n\n      get_memory_mb:\n        publish:\n          memory_mb: <% $.introspection_data.get('memory_mb', 0) %>\n        on-success:\n          - get_nova_vcpu_pin_set: <% $.memory_mb %>\n          - set_failed_get_memory_mb: <% not $.memory_mb %>\n\n      # Determine the number of CPU cores available to Nova and Ceph. If\n      # NovaVcpuPinSet is defined then use the number of vCPUs in the set,\n      # otherwise use all of the cores identified in the introspection data.\n\n      get_nova_vcpu_pin_set:\n        publish:\n          # NovaVcpuPinSet can be defined in multiple locations, and it's\n          # important to select the value in order of precedence:\n          #\n          # 1) User specified value for this role\n          # 2) User specified default value for all roles\n          # 3) Value derived by another derived parameters workflow\n          nova_vcpu_pin_set: <% $.role_env_params.get('NovaVcpuPinSet', $.environment_parameters.get('NovaVcpuPinSet', $.role_derive_params.get('NovaVcpuPinSet', ''))) %>\n        on-success:\n          - get_nova_vcpu_count: <% $.nova_vcpu_pin_set %>\n          - get_num_cores: <% not $.nova_vcpu_pin_set %>\n\n      get_nova_vcpu_count:\n        action: tripleo.derive_params.convert_range_to_number_list\n        input:\n          range_list: <% $.nova_vcpu_pin_set %>\n        publish:\n          num_cores: <% task().result.split(',').count() %>\n        on-success: calculate_nova_parameters\n        on-error: set_failed_get_nova_vcpu_count\n\n      get_num_cores:\n        publish:\n          num_cores: <% $.introspection_data.get('cpus', 0) %>\n        on-success:\n          - calculate_nova_parameters: <% $.num_cores %>\n          - set_failed_get_num_cores: <% not $.num_cores %>\n\n      # HCI calculations are broken into multiple steps. This is necessary\n      # because variables published by a Mistral task are not available\n      # for use by that same task. Variables computed and published in a task\n      # are only available in subsequent tasks.\n      #\n      # The HCI calculations compute two Nova parameters:\n      # - reserved_host_memory\n      # - cpu_allocation_ratio\n      #\n      # The reserved_host_memory calculation computes the amount of memory\n      # that needs to be reserved for Ceph and the total amount of \"guest\n      # overhead\" memory that is based on the anticipated number of guests.\n      # Psuedo-code for the calculation (disregarding MB and GB units) is\n      # as follows:\n      #\n      #   ceph_memory = mem_per_osd * num_osds\n      #   nova_memory = total_memory - ceph_memory\n      #   num_guests = nova_memory /\n      #                (average_guest_memory_size + overhead_per_guest)\n      #   reserved_memory = ceph_memory + (num_guests * overhead_per_guest)\n      #\n      # The cpu_allocation_ratio calculation is similar in that it takes into\n      # account the number of cores that must be reserved for Ceph.\n      #\n      #   ceph_cores = cores_per_osd * num_osds\n      #   guest_cores = num_cores - ceph_cores\n      #   guest_vcpus = guest_cores / average_guest_utilization\n      #   cpu_allocation_ratio = guest_vcpus / num_cores\n\n      calculate_nova_parameters:\n        publish:\n          avg_guest_util: <% $.average_guest_cpu_utilization_percentage / 100.0 %>\n          avg_guest_size_gb: <% $.average_guest_memory_size_in_mb / float($.MB_PER_GB) %>\n          memory_gb: <% $.memory_mb / float($.MB_PER_GB) %>\n          ceph_mem_gb: <% $.gb_per_osd * $.num_osds %>\n          nonceph_cores: <% $.num_cores - int($.cores_per_osd * $.num_osds) %>\n        on-success: calc_step_2\n\n      calc_step_2:\n        publish:\n          num_guests: <% int(($.memory_gb - $.ceph_mem_gb) / ($.avg_guest_size_gb + $.gb_overhead_per_guest)) %>\n          guest_vcpus: <% $.nonceph_cores / $.avg_guest_util %>\n        on-success: calc_step_3\n\n      calc_step_3:\n        publish:\n          reserved_host_memory: <% $.MB_PER_GB * int($.ceph_mem_gb + ($.num_guests * $.gb_overhead_per_guest)) %>\n          cpu_allocation_ratio: <% $.guest_vcpus / $.num_cores %>\n        on-success: validate_results\n\n      validate_results:\n        publish:\n          # Verify whether HCI is viable:\n          # - At least 80% of the memory is reserved for Ceph and guest overhead\n          # - At least half of the CPU cores must be available to Nova\n          mem_ok: <% $.reserved_host_memory <= ($.memory_mb * 0.8) %>\n          cpu_ok: <% $.cpu_allocation_ratio >= 0.5 %>\n        on-success:\n          - set_failed_insufficient_mem: <% not $.mem_ok %>\n          - set_failed_insufficient_cpu: <% not $.cpu_ok %>\n          - publish_hci_parameters: <% $.mem_ok and $.cpu_ok %>\n\n      publish_hci_parameters:\n        publish:\n          # TODO(abishop): Update this when the cpu_allocation_ratio can be set\n          # via a THT parameter (no such parameter currently exists). Until a\n          # THT parameter exists, use hiera data to set the cpu_allocation_ratio.\n          hci_parameters: <% dict(concat($.role_name, 'Parameters') => dict('NovaReservedHostMemory' => $.reserved_host_memory)) + dict(concat($.role_name, 'ExtraConfig') => dict('nova::cpu_allocation_ratio' => $.cpu_allocation_ratio)) %>\n\n      set_failed_invalid_hci_profile:\n        publish:\n          message: \"'<% $.hci_profile %>' is not a valid HCI profile.\"\n        on-success: fail\n\n      set_failed_invalid_average_guest_memory_size_in_mb:\n        publish:\n          message: \"'<% $.average_guest_memory_size_in_mb %>' is not a valid average_guest_memory_size_in_mb value.\"\n        on-success: fail\n\n      set_failed_invalid_gb_overhead_per_guest:\n        publish:\n          message: \"'<% $.gb_overhead_per_guest %>' is not a valid gb_overhead_per_guest value.\"\n        on-success: fail\n\n      set_failed_invalid_gb_per_osd:\n        publish:\n          message: \"'<% $.gb_per_osd %>' is not a valid gb_per_osd value.\"\n        on-success: fail\n\n      set_failed_invalid_cores_per_osd:\n        publish:\n          message: \"'<% $.cores_per_osd %>' is not a valid cores_per_osd value.\"\n        on-success: fail\n\n      set_failed_invalid_average_guest_cpu_utilization_percentage:\n        publish:\n          message: \"'<% $.average_guest_cpu_utilization_percentage %>' is not a valid average_guest_cpu_utilization_percentage value.\"\n        on-success: fail\n\n      set_failed_no_osds:\n        publish:\n          message: \"No Ceph OSDs found in the overcloud definition ('ceph::profile::params::osds').\"\n        on-success: fail\n\n      set_failed_get_memory_mb:\n        publish:\n          message: \"Unable to determine the amount of physical memory (no 'memory_mb' found in introspection_data).\"\n        on-success: fail\n\n      set_failed_get_nova_vcpu_count:\n        publish:\n          message: <% task(get_nova_vcpu_count).result %>\n        on-success: fail\n\n      set_failed_get_num_cores:\n        publish:\n          message: \"Unable to determine the number of CPU cores (no 'cpus' found in introspection_data).\"\n        on-success: fail\n\n      set_failed_insufficient_mem:\n        publish:\n          message: \"<% $.memory_mb %> MB is not enough memory to run hyperconverged.\"\n        on-success: fail\n\n      set_failed_insufficient_cpu:\n        publish:\n          message: \"<% $.num_cores %> CPU cores are not enough to run hyperconverged.\"\n        on-success: fail\n", "name": "tripleo.derive_params_formulas.v1", "tags": [], "created_at": "2018-12-04 11:26:08", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "489dc4b1-5202-444b-89df-c840f9e07c11"}

2018-12-04 06:26:08,476 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:08,478 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.fernet_keys.v1
description: TripleO fernet key rotation workflows

workflows:

  rotate_fernet_keys:

    input:
      - container
      - queue_name: tripleo
      - ansible_extra_env_variables:
            ANSIBLE_HOST_KEY_CHECKING: 'False'

    tags:
      - tripleo-common-managed

    tasks:

      rotate_keys:
        action: tripleo.parameters.rotate_fernet_keys container=<% $.container %>
        on-success: deploy_ssh_key
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      deploy_ssh_key:
        workflow: tripleo.validations.v1.copy_ssh_key
        on-success: get_privkey
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      get_privkey:
        action: tripleo.validations.get_privkey
        on-success: deploy_keys
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      deploy_keys:
        action: tripleo.ansible-playbook
        input:
          hosts: keystone
          inventory: /usr/bin/tripleo-ansible-inventory
          ssh_private_key: <% task(get_privkey).result %>
          extra_env_variables: <% $.ansible_extra_env_variables + dict(TRIPLEO_PLAN_NAME=>$.container) %>
          verbosity: 0
          remote_user: heat-admin
          become: true
          extra_vars:
            fernet_keys: <% task(rotate_keys).result %>
          use_openstack_credentials: true
          playbook: /usr/share/tripleo-common/playbooks/rotate-keys.yaml
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task().result %>
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.fernet_keys.v1.rotate_fernet_keys
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-04 06:26:08,836 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 2609
2018-12-04 06:26:08,838 DEBUG: RESP: [201] Content-Length: 2609 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:08 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.fernet_keys.v1\ndescription: TripleO fernet key rotation workflows\n\nworkflows:\n\n  rotate_fernet_keys:\n\n    input:\n      - container\n      - queue_name: tripleo\n      - ansible_extra_env_variables:\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      rotate_keys:\n        action: tripleo.parameters.rotate_fernet_keys container=<% $.container %>\n        on-success: deploy_ssh_key\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      deploy_ssh_key:\n        workflow: tripleo.validations.v1.copy_ssh_key\n        on-success: get_privkey\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_privkey:\n        action: tripleo.validations.get_privkey\n        on-success: deploy_keys\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      deploy_keys:\n        action: tripleo.ansible-playbook\n        input:\n          hosts: keystone\n          inventory: /usr/bin/tripleo-ansible-inventory\n          ssh_private_key: <% task(get_privkey).result %>\n          extra_env_variables: <% $.ansible_extra_env_variables + dict(TRIPLEO_PLAN_NAME=>$.container) %>\n          verbosity: 0\n          remote_user: heat-admin\n          become: true\n          extra_vars:\n            fernet_keys: <% task(rotate_keys).result %>\n          use_openstack_credentials: true\n          playbook: /usr/share/tripleo-common/playbooks/rotate-keys.yaml\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.fernet_keys.v1.rotate_fernet_keys\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.fernet_keys.v1", "tags": [], "created_at": "2018-12-04 11:26:08", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "2148a37c-7e5c-475f-8de1-efb6322d4497"}

2018-12-04 06:26:08,838 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:08,839 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.networks.v1
description: TripleO Overcloud Networks Workflows v1

workflows:

  validate_networks_input:
    description: >
      Validate that required fields are present.

    input:
      - networks
      - queue_name: tripleo

    output:
      result: <% task(validate_network_names).result %>

    tags:
      - tripleo-common-managed

    tasks:
      validate_network_names:
        publish:
          network_name_present: <% $.networks.all($.containsKey('name')) %>
        on-success:
          - set_status_success: <% $.network_name_present = true %>
          - set_status_error: <% $.network_name_present = false %>
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task(validate_network_names).result %>

      set_status_error:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: "One or more entries did not contain the required field 'name'"

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.networks.v1.validate_networks_input
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  update_networks:
    description: >
      Takes data in networks parameter in json format, validates its contents,
      and persists them in network_data.yaml. After successful update,
      templates are regenerated.

    input:
      - container: overcloud
      - networks
      - network_data_file: 'network_data.yaml'
      - queue_name: tripleo

    output:
      network_data: <% $.network_data %>

    tags:
      - tripleo-common-managed

    tasks:
      validate_input:
        description: >
          validate the format of input (input includes required fields for
          each network)
        workflow: validate_networks_input
        input:
          networks: <% $.networks %>
        on-success: validate_network_files
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      validate_network_files:
        description: >
          validate that Network names exist in Swift container
        workflow: tripleo.plan_management.v1.validate_network_files
        input:
          container: <% $.container %>
          network_data: <% $.networks %>
          queue_name: <% $.queue_name %>
        publish:
          network_data: <% task().network_data %>
        on-success: get_available_networks
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      get_available_networks:
        workflow: tripleo.plan_management.v1.list_available_networks
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>
        publish:
          available_networks: <% task().result.available_networks %>
        on-success: get_current_networks
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      get_current_networks:
        workflow: tripleo.plan_management.v1.get_network_data
        input:
          container: <% $.container %>
          network_data_file: <% $.network_data_file %>
          queue_name: <% $.queue_name %>
        publish:
          current_networks: <% task().result.network_data %>
        on-success: update_network_data
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      update_network_data:
        description: >
          Combine (or replace) the network data
        action: tripleo.plan.update_networks
        input:
          networks: <% $.available_networks %>
          current_networks: <% $.current_networks %>
          remove_all: false
        publish:
          new_network_data: <% task().result.network_data %>
        on-success: update_network_data_in_swift
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      update_network_data_in_swift:
        description: >
          update network_data.yaml object in Swift with data from workflow input
        action: swift.put_object
        input:
          container: <% $.container %>
          obj: <% $.network_data_file %>
          contents: <% yaml_dump($.new_network_data) %>
        on-success: regenerate_templates
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      regenerate_templates:
        action: tripleo.templates.process container=<% $.container %>
        on-success: get_networks
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      get_networks:
        description: >
          run GetNetworksAction to get updated contents of network_data.yaml and
          provide it as output
        workflow: tripleo.plan_management.v1.get_network_data
        input:
          container: <% $.container %>
          network_data_file: <% $.network_data_file %>
          queue_name: <% $.queue_name %>
        publish:
          network_data: <% task().network_data %>
        on-success: set_status_success
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task(get_networks).result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.networks.v1.update_networks
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-04 06:26:09,569 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 6800
2018-12-04 06:26:09,570 DEBUG: RESP: [201] Content-Length: 6800 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:09 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.networks.v1\ndescription: TripleO Overcloud Networks Workflows v1\n\nworkflows:\n\n  validate_networks_input:\n    description: >\n      Validate that required fields are present.\n\n    input:\n      - networks\n      - queue_name: tripleo\n\n    output:\n      result: <% task(validate_network_names).result %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      validate_network_names:\n        publish:\n          network_name_present: <% $.networks.all($.containsKey('name')) %>\n        on-success:\n          - set_status_success: <% $.network_name_present = true %>\n          - set_status_error: <% $.network_name_present = false %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(validate_network_names).result %>\n\n      set_status_error:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: \"One or more entries did not contain the required field 'name'\"\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.networks.v1.validate_networks_input\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_networks:\n    description: >\n      Takes data in networks parameter in json format, validates its contents,\n      and persists them in network_data.yaml. After successful update,\n      templates are regenerated.\n\n    input:\n      - container: overcloud\n      - networks\n      - network_data_file: 'network_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      validate_input:\n        description: >\n          validate the format of input (input includes required fields for\n          each network)\n        workflow: validate_networks_input\n        input:\n          networks: <% $.networks %>\n        on-success: validate_network_files\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      validate_network_files:\n        description: >\n          validate that Network names exist in Swift container\n        workflow: tripleo.plan_management.v1.validate_network_files\n        input:\n          container: <% $.container %>\n          network_data: <% $.networks %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().network_data %>\n        on-success: get_available_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      get_available_networks:\n        workflow: tripleo.plan_management.v1.list_available_networks\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n        publish:\n          available_networks: <% task().result.available_networks %>\n        on-success: get_current_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      get_current_networks:\n        workflow: tripleo.plan_management.v1.get_network_data\n        input:\n          container: <% $.container %>\n          network_data_file: <% $.network_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          current_networks: <% task().result.network_data %>\n        on-success: update_network_data\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      update_network_data:\n        description: >\n          Combine (or replace) the network data\n        action: tripleo.plan.update_networks\n        input:\n          networks: <% $.available_networks %>\n          current_networks: <% $.current_networks %>\n          remove_all: false\n        publish:\n          new_network_data: <% task().result.network_data %>\n        on-success: update_network_data_in_swift\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      update_network_data_in_swift:\n        description: >\n          update network_data.yaml object in Swift with data from workflow input\n        action: swift.put_object\n        input:\n          container: <% $.container %>\n          obj: <% $.network_data_file %>\n          contents: <% yaml_dump($.new_network_data) %>\n        on-success: regenerate_templates\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      regenerate_templates:\n        action: tripleo.templates.process container=<% $.container %>\n        on-success: get_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      get_networks:\n        description: >\n          run GetNetworksAction to get updated contents of network_data.yaml and\n          provide it as output\n        workflow: tripleo.plan_management.v1.get_network_data\n        input:\n          container: <% $.container %>\n          network_data_file: <% $.network_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().network_data %>\n        on-success: set_status_success\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(get_networks).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.networks.v1.update_networks\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.networks.v1", "tags": [], "created_at": "2018-12-04 11:26:09", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "c1072e0b-427d-4fdc-a2ff-d516f48c93e9"}

2018-12-04 06:26:09,570 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:09,572 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.octavia_post.v1
description: TripleO Octavia post deployment Workflows

workflows:

  octavia_post_deploy:
    description: Octavia post deployment
    input:
      - amp_image_name
      - amp_image_filename
      - amp_image_tag
      - amp_ssh_key_name
      - amp_ssh_key_path
      - amp_ssh_key_data
      - auth_username
      - auth_password
      - auth_project_name
      - lb_mgmt_net_name
      - lb_mgmt_subnet_name
      - lb_sec_group_name
      - lb_mgmt_subnet_cidr
      - lb_mgmt_subnet_gateway
      - lb_mgmt_subnet_pool_start
      - lb_mgmt_subnet_pool_end
      - generate_certs
      - octavia_ansible_playbook
      - overcloud_admin
      - ca_cert_path
      - ca_private_key_path
      - ca_passphrase
      - client_cert_path
      - mgmt_port_dev
      - overcloud_password
      - overcloud_project
      - overcloud_pub_auth_uri
      - ansible_extra_env_variables:
          ANSIBLE_HOST_KEY_CHECKING: 'False'
          ANSIBLE_SSH_RETRIES: '3'
    tags:
      - tripleo-common-managed
    tasks:
      get_overcloud_stack_details:
        publish:
          # TODO(beagles), we are making an assumption about the octavia heatlh manager and
          # controller worker needing
          #
          octavia_controller_ips: <% env().get('service_ips', {}).get('octavia_worker_ctlplane_node_ips', []) %>
        on-success: enable_ssh_admin

      enable_ssh_admin:
        workflow: tripleo.access.v1.enable_ssh_admin
        input:
          ssh_servers: <% $.octavia_controller_ips %>
        on-success: get_private_key

      get_private_key:
        action: tripleo.validations.get_privkey
        publish:
          private_key: <% task().result %>
        on-success: make_local_temp_directory

      make_local_temp_directory:
        action: tripleo.files.make_temp_dir
        publish:
          undercloud_local_dir: <% task().result.path %>
        on-success: make_remote_temp_directory

      make_remote_temp_directory:
        action: tripleo.files.make_temp_dir
        publish:
          undercloud_remote_dir: <% task().result.path %>
        on-success: build_local_connection_environment_vars

      build_local_connection_environment_vars:
        publish:
          ansible_local_connection_variables: <% dict('ANSIBLE_REMOTE_TEMP' => $.undercloud_remote_dir, 'ANSIBLE_LOCAL_TEMP' => $.undercloud_local_dir) + $.ansible_extra_env_variables %>
        on-success: upload_amphora

      upload_amphora:
        action: tripleo.ansible-playbook
        input:
          inventory:
            undercloud:
              hosts:
                localhost:
                  ansible_connection: local

          playbook: <% $.octavia_ansible_playbook %>
          remote_user: stack
          extra_env_variables: <% $.ansible_local_connection_variables %>
          extra_vars:
            os_password: <% $.overcloud_password %>
            os_username: <% $.overcloud_admin %>
            os_project_name: <% $.overcloud_project %>
            os_auth_url: <% $.overcloud_pub_auth_uri %>
            os_auth_type: "password"
            os_identity_api_version: "3"
            amp_image_name: <% $.amp_image_name %>
            amp_image_filename: <% $.amp_image_filename %>
            amp_image_tag: <% $.amp_image_tag %>
            amp_ssh_key_name: <% $.amp_ssh_key_name %>
            amp_ssh_key_path: <% $.amp_ssh_key_path %>
            amp_ssh_key_data: <% $.amp_ssh_key_data %>
            auth_username: <% $.auth_username %>
            auth_password: <% $.auth_password %>
            auth_project_name: <% $.auth_project_name %>
        on-success: config_octavia

      config_octavia:
        action: tripleo.ansible-playbook
        input:
          inventory:
            octavia_nodes:
              hosts: <% $.octavia_controller_ips.toDict($, {}) %>
          verbosity: 0
          playbook: <% $.octavia_ansible_playbook %>
          remote_user: tripleo-admin
          become: true
          become_user: root
          ssh_private_key: <% $.private_key %>
          ssh_common_args: '-o StrictHostKeyChecking=no'
          ssh_extra_args: '-o UserKnownHostsFile=/dev/null'
          extra_env_variables: <% $.ansible_extra_env_variables %>
          extra_vars:
            os_password: <% $.overcloud_password %>
            os_username: <% $.overcloud_admin %>
            os_project_name: <% $.overcloud_project %>
            os_auth_url: <% $.overcloud_pub_auth_uri %>
            os_auth_type: "password"
            os_identity_api_version: "3"
            amp_image_tag: <% $.amp_image_tag %>
            lb_mgmt_net_name: <% $.lb_mgmt_net_name %>
            lb_mgmt_subnet_name: <% $.lb_mgmt_subnet_name %>
            lb_sec_group_name: <% $.lb_sec_group_name %>
            lb_mgmt_subnet_cidr: <% $.lb_mgmt_subnet_cidr %>
            lb_mgmt_subnet_gateway: <% $.lb_mgmt_subnet_gateway %>
            lb_mgmt_subnet_pool_start: <% $.lb_mgmt_subnet_pool_start %>
            lb_mgmt_subnet_pool_end: <% $.lb_mgmt_subnet_pool_end %>
            ca_cert_path: <% $.ca_cert_path %>
            ca_private_key_path: <% $.ca_private_key_path %>
            ca_passphrase: <% $.ca_passphrase %>
            client_cert_path: <% $.client_cert_path %>
            generate_certs: <% $.generate_certs %>
            mgmt_port_dev: <% $.mgmt_port_dev %>
            auth_project_name: <% $.auth_project_name %>
        on-complete: purge_local_temp_dir
      purge_local_temp_dir:
        action: tripleo.files.remove_temp_dir path=<% $.undercloud_local_dir %>
        on-complete: purge_remote_temp_dir
      purge_remote_temp_dir:
        action: tripleo.files.remove_temp_dir path=<% $.undercloud_remote_dir %>

'
2018-12-04 06:26:09,984 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 6113
2018-12-04 06:26:09,986 DEBUG: RESP: [201] Content-Length: 6113 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:09 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.octavia_post.v1\ndescription: TripleO Octavia post deployment Workflows\n\nworkflows:\n\n  octavia_post_deploy:\n    description: Octavia post deployment\n    input:\n      - amp_image_name\n      - amp_image_filename\n      - amp_image_tag\n      - amp_ssh_key_name\n      - amp_ssh_key_path\n      - amp_ssh_key_data\n      - auth_username\n      - auth_password\n      - auth_project_name\n      - lb_mgmt_net_name\n      - lb_mgmt_subnet_name\n      - lb_sec_group_name\n      - lb_mgmt_subnet_cidr\n      - lb_mgmt_subnet_gateway\n      - lb_mgmt_subnet_pool_start\n      - lb_mgmt_subnet_pool_end\n      - generate_certs\n      - octavia_ansible_playbook\n      - overcloud_admin\n      - ca_cert_path\n      - ca_private_key_path\n      - ca_passphrase\n      - client_cert_path\n      - mgmt_port_dev\n      - overcloud_password\n      - overcloud_project\n      - overcloud_pub_auth_uri\n      - ansible_extra_env_variables:\n          ANSIBLE_HOST_KEY_CHECKING: 'False'\n          ANSIBLE_SSH_RETRIES: '3'\n    tags:\n      - tripleo-common-managed\n    tasks:\n      get_overcloud_stack_details:\n        publish:\n          # TODO(beagles), we are making an assumption about the octavia heatlh manager and\n          # controller worker needing\n          #\n          octavia_controller_ips: <% env().get('service_ips', {}).get('octavia_worker_ctlplane_node_ips', []) %>\n        on-success: enable_ssh_admin\n\n      enable_ssh_admin:\n        workflow: tripleo.access.v1.enable_ssh_admin\n        input:\n          ssh_servers: <% $.octavia_controller_ips %>\n        on-success: get_private_key\n\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: make_local_temp_directory\n\n      make_local_temp_directory:\n        action: tripleo.files.make_temp_dir\n        publish:\n          undercloud_local_dir: <% task().result.path %>\n        on-success: make_remote_temp_directory\n\n      make_remote_temp_directory:\n        action: tripleo.files.make_temp_dir\n        publish:\n          undercloud_remote_dir: <% task().result.path %>\n        on-success: build_local_connection_environment_vars\n\n      build_local_connection_environment_vars:\n        publish:\n          ansible_local_connection_variables: <% dict('ANSIBLE_REMOTE_TEMP' => $.undercloud_remote_dir, 'ANSIBLE_LOCAL_TEMP' => $.undercloud_local_dir) + $.ansible_extra_env_variables %>\n        on-success: upload_amphora\n\n      upload_amphora:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            undercloud:\n              hosts:\n                localhost:\n                  ansible_connection: local\n\n          playbook: <% $.octavia_ansible_playbook %>\n          remote_user: stack\n          extra_env_variables: <% $.ansible_local_connection_variables %>\n          extra_vars:\n            os_password: <% $.overcloud_password %>\n            os_username: <% $.overcloud_admin %>\n            os_project_name: <% $.overcloud_project %>\n            os_auth_url: <% $.overcloud_pub_auth_uri %>\n            os_auth_type: \"password\"\n            os_identity_api_version: \"3\"\n            amp_image_name: <% $.amp_image_name %>\n            amp_image_filename: <% $.amp_image_filename %>\n            amp_image_tag: <% $.amp_image_tag %>\n            amp_ssh_key_name: <% $.amp_ssh_key_name %>\n            amp_ssh_key_path: <% $.amp_ssh_key_path %>\n            amp_ssh_key_data: <% $.amp_ssh_key_data %>\n            auth_username: <% $.auth_username %>\n            auth_password: <% $.auth_password %>\n            auth_project_name: <% $.auth_project_name %>\n        on-success: config_octavia\n\n      config_octavia:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            octavia_nodes:\n              hosts: <% $.octavia_controller_ips.toDict($, {}) %>\n          verbosity: 0\n          playbook: <% $.octavia_ansible_playbook %>\n          remote_user: tripleo-admin\n          become: true\n          become_user: root\n          ssh_private_key: <% $.private_key %>\n          ssh_common_args: '-o StrictHostKeyChecking=no'\n          ssh_extra_args: '-o UserKnownHostsFile=/dev/null'\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          extra_vars:\n            os_password: <% $.overcloud_password %>\n            os_username: <% $.overcloud_admin %>\n            os_project_name: <% $.overcloud_project %>\n            os_auth_url: <% $.overcloud_pub_auth_uri %>\n            os_auth_type: \"password\"\n            os_identity_api_version: \"3\"\n            amp_image_tag: <% $.amp_image_tag %>\n            lb_mgmt_net_name: <% $.lb_mgmt_net_name %>\n            lb_mgmt_subnet_name: <% $.lb_mgmt_subnet_name %>\n            lb_sec_group_name: <% $.lb_sec_group_name %>\n            lb_mgmt_subnet_cidr: <% $.lb_mgmt_subnet_cidr %>\n            lb_mgmt_subnet_gateway: <% $.lb_mgmt_subnet_gateway %>\n            lb_mgmt_subnet_pool_start: <% $.lb_mgmt_subnet_pool_start %>\n            lb_mgmt_subnet_pool_end: <% $.lb_mgmt_subnet_pool_end %>\n            ca_cert_path: <% $.ca_cert_path %>\n            ca_private_key_path: <% $.ca_private_key_path %>\n            ca_passphrase: <% $.ca_passphrase %>\n            client_cert_path: <% $.client_cert_path %>\n            generate_certs: <% $.generate_certs %>\n            mgmt_port_dev: <% $.mgmt_port_dev %>\n            auth_project_name: <% $.auth_project_name %>\n        on-complete: purge_local_temp_dir\n      purge_local_temp_dir:\n        action: tripleo.files.remove_temp_dir path=<% $.undercloud_local_dir %>\n        on-complete: purge_remote_temp_dir\n      purge_remote_temp_dir:\n        action: tripleo.files.remove_temp_dir path=<% $.undercloud_remote_dir %>\n\n", "name": "tripleo.octavia_post.v1", "tags": [], "created_at": "2018-12-04 11:26:09", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "548304fc-c15b-4a2f-ad78-995b571f0a89"}

2018-12-04 06:26:09,986 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:09,987 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.package_update.v1
description: TripleO update workflows

workflows:

  # Updates a workload cloud stack
  package_update_plan:
    description: Take a container and perform a package update with possible breakpoints

    input:
      - container
      - ceph_ansible_playbook
      - timeout: 240
      - queue_name: tripleo
      - skip_deploy_identifier: False
      - config_dir: '/tmp/'

    tags:
      - tripleo-common-managed

    tasks:
      update:
        action: tripleo.package_update.update_stack
        input:
          timeout: <% $.timeout %>
          container: <% $.container %>
          ceph_ansible_playbook: <% $.ceph_ansible_playbook %>
        on-success: clean_plan
        on-error: set_update_failed

      clean_plan:
        action: tripleo.plan.update_plan_environment
        input:
          container: <% $.container %>
          parameter: CephAnsiblePlaybook
          env_key: parameter_defaults
          delete: true
        on-success: send_message
        on-error: set_update_failed


      set_update_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(update).result %>

      send_message:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.package_update.v1.package_update_plan
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  get_config:
    input:
      - container
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      get_config:
        action: tripleo.config.get_overcloud_config container=<% $.container %>
        publish:
          status: SUCCESS
          message: <% task().result %>
        publish-on-error:
          status: FAILED
          message: Init Minor update failed
        on-complete: send_message

      send_message:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.package_update.v1.package_update_plan
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  update_nodes:
    description: Take a container and perform an update nodes by nodes

    input:
      - node_user: heat-admin
      - nodes
      - playbook
      - inventory_file
      - ansible_queue_name: tripleo
      - module_path: /usr/share/ansible-modules
      - ansible_extra_env_variables:
            ANSIBLE_LOG_PATH: /var/log/mistral/package_update.log
            ANSIBLE_HOST_KEY_CHECKING: 'False'
      - verbosity: 1
      - work_dir: /var/lib/mistral
      - skip_tags: ''

    tags:
      - tripleo-common-managed

    tasks:
      download_config:
        action: tripleo.config.download_config
        input:
          work_dir: <% $.work_dir %>/<% execution().id %>
        on-success: get_private_key
        on-error: node_update_failed

      get_private_key:
        action: tripleo.validations.get_privkey
        publish:
          private_key: <% task().result %>
        on-success: node_update

      node_update:
        action: tripleo.ansible-playbook
        input:
          inventory: <% $.inventory_file %>
          playbook: <% $.work_dir %>/<% execution().id %>/<% $.playbook %>
          remote_user: <% $.node_user %>
          become: true
          become_user: root
          verbosity: <% $.verbosity %>
          ssh_private_key: <% $.private_key %>
          extra_env_variables: <% $.ansible_extra_env_variables %>
          limit_hosts: <% $.nodes %>
          module_path: <% $.module_path %>
          queue_name: <% $.ansible_queue_name %>
          execution_id: <% execution().id %>
          skip_tags: <% $.skip_tags %>
          trash_output: true
        on-success:
          - node_update_passed: <% task().result.returncode = 0 %>
          - node_update_failed: <% task().result.returncode != 0 %>
        on-error: node_update_failed
        publish:
          output: <% task().result %>

      node_update_passed:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: Updated nodes - <% $.nodes %>

      node_update_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: Failed to update nodes - <% $.nodes %>, please see the logs.

      notify_zaqar:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.ansible_queue_name %>
          messages:
            body:
              type: tripleo.package_update.v1.update_nodes
              payload:
                status: <% $.status %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  update_converge_plan:
    description: Take a container and perform the converge for minor update

    input:
      - container
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      remove_noop:
        action: tripleo.plan.remove_noop_deploystep
        input:
          container: <% $.container %>
        on-success: send_message
        on-error: set_update_failed

      set_update_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(remove_noop).result %>

      send_message:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.package_update.v1.update_converge_plan
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  converge_upgrade_plan:
    description: Take a container and perform the converge step of a major upgrade

    input:
      - container
      - timeout: 240
      - queue_name: tripleo
      - skip_deploy_identifier: False

    tags:
      - tripleo-common-managed

    tasks:
      remove_noop:
        action: tripleo.plan.remove_noop_deploystep
        input:
          container: <% $.container %>
        on-success: send_message
        on-error: set_update_failed

      set_update_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(upgrade_converge).result %>

      send_message:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.major_upgrade.v1.converge_upgrade_plan
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  ffwd_upgrade_converge_plan:
    description: ffwd-upgrade converge removes DeploymentSteps no-op from plan

    input:
      - container
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      remove_noop:
        action: tripleo.plan.remove_noop_deploystep
        input:
          container: <% $.container %>
        on-success: send_message
        on-error: set_update_failed

      set_update_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(remove_noop).result %>

      send_message:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.package_update.v1.ffwd_upgrade_converge_plan
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-04 06:26:10,982 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 8860
2018-12-04 06:26:10,984 DEBUG: RESP: [201] Content-Length: 8860 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:10 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.package_update.v1\ndescription: TripleO update workflows\n\nworkflows:\n\n  # Updates a workload cloud stack\n  package_update_plan:\n    description: Take a container and perform a package update with possible breakpoints\n\n    input:\n      - container\n      - ceph_ansible_playbook\n      - timeout: 240\n      - queue_name: tripleo\n      - skip_deploy_identifier: False\n      - config_dir: '/tmp/'\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      update:\n        action: tripleo.package_update.update_stack\n        input:\n          timeout: <% $.timeout %>\n          container: <% $.container %>\n          ceph_ansible_playbook: <% $.ceph_ansible_playbook %>\n        on-success: clean_plan\n        on-error: set_update_failed\n\n      clean_plan:\n        action: tripleo.plan.update_plan_environment\n        input:\n          container: <% $.container %>\n          parameter: CephAnsiblePlaybook\n          env_key: parameter_defaults\n          delete: true\n        on-success: send_message\n        on-error: set_update_failed\n\n\n      set_update_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(update).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.package_update_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  get_config:\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_config:\n        action: tripleo.config.get_overcloud_config container=<% $.container %>\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n        publish-on-error:\n          status: FAILED\n          message: Init Minor update failed\n        on-complete: send_message\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.package_update_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_nodes:\n    description: Take a container and perform an update nodes by nodes\n\n    input:\n      - node_user: heat-admin\n      - nodes\n      - playbook\n      - inventory_file\n      - ansible_queue_name: tripleo\n      - module_path: /usr/share/ansible-modules\n      - ansible_extra_env_variables:\n            ANSIBLE_LOG_PATH: /var/log/mistral/package_update.log\n            ANSIBLE_HOST_KEY_CHECKING: 'False'\n      - verbosity: 1\n      - work_dir: /var/lib/mistral\n      - skip_tags: ''\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      download_config:\n        action: tripleo.config.download_config\n        input:\n          work_dir: <% $.work_dir %>/<% execution().id %>\n        on-success: get_private_key\n        on-error: node_update_failed\n\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: node_update\n\n      node_update:\n        action: tripleo.ansible-playbook\n        input:\n          inventory: <% $.inventory_file %>\n          playbook: <% $.work_dir %>/<% execution().id %>/<% $.playbook %>\n          remote_user: <% $.node_user %>\n          become: true\n          become_user: root\n          verbosity: <% $.verbosity %>\n          ssh_private_key: <% $.private_key %>\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          limit_hosts: <% $.nodes %>\n          module_path: <% $.module_path %>\n          queue_name: <% $.ansible_queue_name %>\n          execution_id: <% execution().id %>\n          skip_tags: <% $.skip_tags %>\n          trash_output: true\n        on-success:\n          - node_update_passed: <% task().result.returncode = 0 %>\n          - node_update_failed: <% task().result.returncode != 0 %>\n        on-error: node_update_failed\n        publish:\n          output: <% task().result %>\n\n      node_update_passed:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: Updated nodes - <% $.nodes %>\n\n      node_update_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: Failed to update nodes - <% $.nodes %>, please see the logs.\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.ansible_queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.update_nodes\n              payload:\n                status: <% $.status %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_converge_plan:\n    description: Take a container and perform the converge for minor update\n\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      remove_noop:\n        action: tripleo.plan.remove_noop_deploystep\n        input:\n          container: <% $.container %>\n        on-success: send_message\n        on-error: set_update_failed\n\n      set_update_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(remove_noop).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.update_converge_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  converge_upgrade_plan:\n    description: Take a container and perform the converge step of a major upgrade\n\n    input:\n      - container\n      - timeout: 240\n      - queue_name: tripleo\n      - skip_deploy_identifier: False\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      remove_noop:\n        action: tripleo.plan.remove_noop_deploystep\n        input:\n          container: <% $.container %>\n        on-success: send_message\n        on-error: set_update_failed\n\n      set_update_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(upgrade_converge).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.major_upgrade.v1.converge_upgrade_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  ffwd_upgrade_converge_plan:\n    description: ffwd-upgrade converge removes DeploymentSteps no-op from plan\n\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      remove_noop:\n        action: tripleo.plan.remove_noop_deploystep\n        input:\n          container: <% $.container %>\n        on-success: send_message\n        on-error: set_update_failed\n\n      set_update_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(remove_noop).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.package_update.v1.ffwd_upgrade_converge_plan\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.package_update.v1", "tags": [], "created_at": "2018-12-04 11:26:10", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "7d4f33ef-cf8d-4486-b6f5-d81cff8949ad"}

2018-12-04 06:26:10,984 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:10,986 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.plan_management.v1
description: TripleO Overcloud Deployment Workflows v1

workflows:

  create_default_deployment_plan:
    description: >
      This workflow exists to maintain backwards compatibility in pike.  This
      workflow will likely be removed in queens in favor of create_deployment_plan.
    input:
      - container
      - queue_name: tripleo
      - generate_passwords: true
    tags:
      - tripleo-common-managed
    tasks:
      call_create_deployment_plan:
        workflow: tripleo.plan_management.v1.create_deployment_plan
        on-success: set_status_success
        on-error: call_create_deployment_plan_set_status_failed
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>
          generate_passwords: <% $.generate_passwords %>
          use_default_templates: true

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task(call_create_deployment_plan).result %>

      call_create_deployment_plan_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(call_create_deployment_plan).result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.create_default_deployment_plan
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  create_deployment_plan:
    description: >
      This workflow provides the capability to create a deployment plan using
      the default heat templates provided in a standard TripleO undercloud
      deployment, heat templates contained in an external git repository, or a
      swift container that already contains templates.
    input:
      - container
      - source_url: null
      - queue_name: tripleo
      - generate_passwords: true
      - use_default_templates: false

    tags:
      - tripleo-common-managed

    tasks:
      container_required_check:
        description: >
          If using the default templates or importing templates from a git
          repository, a new container needs to be created.  If using an existing
          container containing templates, skip straight to create_plan.
        on-success:
          - verify_container_doesnt_exist: <% $.use_default_templates or $.source_url %>
          - create_plan: <% $.use_default_templates = false and $.source_url = null %>

      verify_container_doesnt_exist:
        action: swift.head_container container=<% $.container %>
        on-success: notify_zaqar
        on-error: create_container
        publish:
          status: FAILED
          message: "Unable to create plan. The Swift container already exists"

      create_container:
        action: tripleo.plan.create_container container=<% $.container %>
        on-success: templates_source_check
        on-error: create_container_set_status_failed

      cleanup_temporary_files:
        action: tripleo.git.clean container=<% $.container %>

      templates_source_check:
        on-success:
          - upload_default_templates: <% $.use_default_templates = true %>
          - clone_git_repo: <% $.source_url != null %>

      clone_git_repo:
        action: tripleo.git.clone container=<% $.container %> url=<% $.source_url %>
        on-success: upload_templates_directory
        on-error: clone_git_repo_set_status_failed

      upload_templates_directory:
        action: tripleo.templates.upload container=<% $.container %> templates_path=<% task(clone_git_repo).result %>
        on-success: create_plan
        on-complete: cleanup_temporary_files
        on-error: upload_templates_directory_set_status_failed

      upload_default_templates:
        action: tripleo.templates.upload container=<% $.container %>
        on-success: create_plan
        on-error: upload_to_container_set_status_failed

      create_plan:
        on-success:
          - ensure_passwords_exist: <% $.generate_passwords = true %>
          - add_root_stack_name: <% $.generate_passwords != true %>

      ensure_passwords_exist:
        action: tripleo.parameters.generate_passwords container=<% $.container %>
        on-success: add_root_stack_name
        on-error: ensure_passwords_exist_set_status_failed

      add_root_stack_name:
        action: tripleo.parameters.update
        input:
          container: <% $.container %>
          parameters:
            RootStackName: <% $.container %>
        on-success: container_images_prepare
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      container_images_prepare:
        description: >
          Populate all container image parameters with default values.
        action: tripleo.container_images.prepare container=<% $.container %>
        on-success: process_templates
        on-error: container_images_prepare_set_status_failed

      process_templates:
        action: tripleo.templates.process container=<% $.container %>
        on-success: set_status_success
        on-error: process_templates_set_status_failed

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: 'Plan created.'

      create_container_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(create_container).result %>

      clone_git_repo_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(clone_git_repo).result %>

      upload_templates_directory_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(upload_templates_directory).result %>

      upload_to_container_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(upload_default_templates).result %>

      ensure_passwords_exist_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(ensure_passwords_exist).result %>

      process_templates_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(process_templates).result %>

      container_images_prepare_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(container_images_prepare).result %>

      notify_zaqar:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.create_deployment_plan
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  update_deployment_plan:
    input:
      - container
      - source_url: null
      - queue_name: tripleo
      - generate_passwords: true
      - plan_environment: null
    tags:
      - tripleo-common-managed
    tasks:
      templates_source_check:
        on-success:
          - update_plan: <% $.source_url = null %>
          - clone_git_repo: <% $.source_url != null %>

      clone_git_repo:
        action: tripleo.git.clone container=<% $.container %> url=<% $.source_url %>
        on-success: upload_templates_directory
        on-error: clone_git_repo_set_status_failed

      upload_templates_directory:
        action: tripleo.templates.upload container=<% $.container %> templates_path=<% task(clone_git_repo).result %>
        on-success: create_swift_rings_backup_plan
        on-complete: cleanup_temporary_files
        on-error: upload_templates_directory_set_status_failed

      cleanup_temporary_files:
        action: tripleo.git.clean container=<% $.container %>

      create_swift_rings_backup_plan:
        workflow: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan
        on-success: update_plan
        on-error: create_swift_rings_backup_plan_set_status_failed
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>
          use_default_templates: true

      update_plan:
        on-success:
          - ensure_passwords_exist: <% $.generate_passwords = true %>
          - container_images_prepare: <% $.generate_passwords != true %>

      ensure_passwords_exist:
        action: tripleo.parameters.generate_passwords container=<% $.container %>
        on-success: container_images_prepare
        on-error: ensure_passwords_exist_set_status_failed

      container_images_prepare:
        description: >
          Populate all container image parameters with default values.
        action: tripleo.container_images.prepare container=<% $.container %>
        on-success: process_templates
        on-error: container_images_prepare_set_status_failed

      process_templates:
        action: tripleo.templates.process container=<% $.container %>
        on-success:
          - set_status_success: <% $.plan_environment = null %>
          - upload_plan_environment: <% $.plan_environment != null %>
        on-error: process_templates_set_status_failed

      upload_plan_environment:
        action: tripleo.templates.upload_plan_environment container=<% $.container %> plan_environment=<% $.plan_environment %>
        on-success: set_status_success
        on-error: process_templates_set_status_failed

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: 'Plan updated.'

      create_swift_rings_backup_plan_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(create_swift_rings_backup_plan).result %>

      clone_git_repo_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(clone_git_repo).result %>

      upload_templates_directory_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(upload_templates_directory).result %>

      process_templates_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(process_templates).result %>

      ensure_passwords_exist_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(ensure_passwords_exist).result %>

      container_images_prepare_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(container_images_prepare).result %>

      notify_zaqar:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.update_deployment_plan
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  delete_deployment_plan:
    description: >
      Deletes a plan by deleting the container matching plan_name. It will
      not delete the plan if a stack exists with the same name.

    tags:
      - tripleo-common-managed

    input:
      - container: overcloud
      - queue_name: tripleo

    tasks:
      delete_plan:
        action: tripleo.plan.delete container=<% $.container %>
        on-complete: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        publish:
          status: SUCCESS
          message: <% task().result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.delete_deployment_plan
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>


  get_passwords:
    description: Retrieves passwords for a given plan
    input:
      - container
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      verify_container_exists:
        action: swift.head_container container=<% $.container %>
        on-success: get_environment_passwords
        on-error: verify_container_set_status_failed

      get_environment_passwords:
        action: tripleo.parameters.get_passwords container=<% $.container %>
        on-success: get_passwords_set_status_success
        on-error: get_passwords_set_status_failed

      get_passwords_set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task(get_environment_passwords).result %>

      get_passwords_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(get_environment_passwords).result %>

      verify_container_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(verify_container_exists).result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.get_passwords
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  export_deployment_plan:
    description: Creates an export tarball for a given plan
    input:
      - plan
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      export_plan:
        action: tripleo.plan.export
        input:
          plan: <% $.plan %>
          delete_after: 3600
          exports_container: "plan-exports"
        on-success: create_tempurl
        on-error: export_plan_set_status_failed

      create_tempurl:
        action: tripleo.swift.tempurl
        on-success: set_status_success
        on-error: create_tempurl_set_status_failed
        input:
          container: "plan-exports"
          obj: "<% $.plan %>.tar.gz"
          valid: 3600

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task(create_tempurl).result %>
          tempurl: <% task(create_tempurl).result %>

      export_plan_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(export_plan).result %>

      create_tempurl_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(create_tempurl).result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.export_deployment_plan
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                tempurl: <% $.get('tempurl', '') %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  get_deprecated_parameters:
    description: Gets the list of deprecated parameters in the whole of the plan including nested stack
    input:
      - container: overcloud
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      get_flatten_data:
        action: tripleo.parameters.get_flatten container=<% $.container %>
        on-success: get_deprecated_params
        on-error: set_status_failed_get_flatten_data
        publish:
          user_params: <% task().result.environment_parameters %>
          plan_params: <% task().result.heat_resource_tree.parameters.keys() %>
          parameter_groups: <% task().result.heat_resource_tree.resources.values().where( $.get('parameter_groups') ).select($.parameter_groups).flatten() %>

      get_deprecated_params:
        on-success: check_if_user_param_has_deprecated
        publish:
          deprecated_params: <% $.parameter_groups.where($.get('label') = 'deprecated').select($.parameters).flatten().distinct() %>

      check_if_user_param_has_deprecated:
        on-success: get_unused_params
        publish:
          deprecated_result: <% let(up => $.user_params) -> $.deprecated_params.select( dict('parameter' => $, 'deprecated' => true, 'user_defined' => $up.keys().contains($)) ) %>

      # Get the list of parameters, which are defined by user via environment files's parameter_default, but not part of the plan definition
      # It may be possible that the parameter will be used by a service, but the service is not part of the plan.
      # In such cases, the parameter will be reported as unused, care should be take to understand whether it is really unused or not.
      get_unused_params:
        on-success: send_message
        publish:
          unused_params: <% let(plan_params => $.plan_params) ->  $.user_params.keys().where( not $plan_params.contains($) ) %>

      set_status_failed_get_flatten_data:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(get_flatten_data).result %>

      send_message:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.get_deprecated_parameters
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                deprecated: <% $.get('deprecated_result', []) %>
                unused: <% $.get('unused_params', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  publish_ui_logs_to_swift:
    description: >
      This workflow drains a zaqar queue, and publish its messages into a log
      file in swift.  This workflow is called by cron trigger.

    input:
      - logging_queue_name: tripleo-ui-logging
      - logging_container: tripleo-ui-logs

    tags:
      - tripleo-common-managed

    tasks:

      # We're using a NoOp action to start the workflow.  The recursive nature
      # of the workflow means that Mistral will refuse to execute it because it
      # doesn't know where to begin.
      start:
        on-success: get_messages

      get_messages:
        action: zaqar.claim_messages
        on-success:
          - format_messages: <% task().result.len() > 0 %>
        input:
          queue_name: <% $.logging_queue_name %>
          ttl: 60
          grace: 60
        publish:
          status: SUCCESS
          messages: <% task().result %>
          message_ids: <% task().result.select($._id) %>

      format_messages:
        action: tripleo.logging_to_swift.format_messages
        on-success: upload_to_swift
        input:
          messages: <% $.messages %>
        publish:
          status: SUCCESS
          formatted_messages: <% task().result %>

      upload_to_swift:
        action: tripleo.logging_to_swift.publish_ui_log_to_swift
        on-success: delete_messages
        input:
          logging_data: <% $.formatted_messages %>
          logging_container: <% $.logging_container %>
        publish:
          status: SUCCESS

      delete_messages:
        action: zaqar.delete_messages
        on-success: get_messages
        input:
          queue_name: <% $.logging_queue_name %>
          messages: <% $.message_ids %>
        publish:
          status: SUCCESS

  download_logs:
    description: Creates a tarball with logging data
    input:
      - queue_name: tripleo
      - logging_container: "tripleo-ui-logs"
      - downloads_container: "tripleo-ui-logs-downloads"
      - delete_after: 3600

    tags:
      - tripleo-common-managed

    tasks:

      publish_logs:
        workflow: tripleo.plan_management.v1.publish_ui_logs_to_swift
        on-success: prepare_log_download
        on-error: publish_logs_set_status_failed

      prepare_log_download:
        action: tripleo.logging_to_swift.prepare_log_download
        input:
          logging_container: <% $.logging_container %>
          downloads_container: <% $.downloads_container %>
          delete_after: <% $.delete_after %>
        on-success: create_tempurl
        on-error: download_logs_set_status_failed
        publish:
          filename: <% task().result %>

      create_tempurl:
        action: tripleo.swift.tempurl
        on-success: set_status_success
        on-error: create_tempurl_set_status_failed
        input:
          container: <% $.downloads_container %>
          obj: <% $.filename %>
          valid: 3600
        publish:
          tempurl: <% task().result %>

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task(create_tempurl).result %>
          tempurl: <% task(create_tempurl).result %>

      publish_logs_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(publish_logs).result %>

      download_logs_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(prepare_log_download).result %>

      create_tempurl_set_status_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(create_tempurl).result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.download_logs
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                tempurl: <% $.get('tempurl', '') %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  list_roles:
    description: Retrieve the roles_data.yaml and return a usable object

    input:
      - container: overcloud
      - roles_data_file: 'roles_data.yaml'
      - queue_name: tripleo

    output:
      roles_data: <% $.roles_data %>

    tags:
      - tripleo-common-managed

    tasks:
      get_roles_data:
        action: swift.get_object
        input:
          container: <% $.container %>
          obj: <% $.roles_data_file %>
        publish:
          roles_data: <% yaml_parse(task().result.last()) %>
          status: SUCCESS
        on-success: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.list_roles
              payload:
                status: <% $.status %>
                roles_data: <% $.get('roles_data', {}) %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  list_available_networks:
    input:
      - container
      - queue_name: tripleo

    output:
      available_networks: <% $.available_networks %>

    tags:
      - tripleo-common-managed

    tasks:
      get_network_file_names:
        action: swift.get_container
        input:
          container: <% $.container %>
        publish:
          network_names: <% task().result[1].where($.name.startsWith('networks/')).where($.name.endsWith('.yaml')).name %>
        on-success: get_network_files
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      get_network_files:
        with-items: network_name in <% $.network_names %>
        action: swift.get_object
        on-success: transform_output
        on-error: notify_zaqar
        input:
          container: <% $.container %>
          obj: <% $.network_name %>
        publish:
          status: SUCCESS
          available_yaml_networks: <% task().result.select($[1]) %>
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      transform_output:
        publish:
          status: SUCCESS
          available_networks: <% yaml_parse($.available_yaml_networks.join("\n")) %>
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-complete: notify_zaqar

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.list_available_networks
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                available_networks: <% $.get('available_networks', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  list_networks:
    input:
      - container: 'overcloud'
      - network_data_file: 'network_data.yaml'
      - queue_name: tripleo

    output:
      network_data: <% $.network_data %>

    tags:
      - tripleo-common-managed

    tasks:
      get_networks:
        action: swift.get_object
        input:
          container: <% $.container %>
          obj: <% $.network_data_file %>
        on-success: notify_zaqar
        publish:
          network_data: <% yaml_parse(task().result.last()) %>
          status: SUCCESS
          message: <% task().result %>
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      notify_zaqar:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.list_networks
              payload:
                status: <% $.status %>
                network_data: <% $.get('network_data', {}) %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  validate_network_files:
    description: Validate network files exist
    input:
      - container: overcloud
      - network_data
      - queue_name: tripleo

    output:
      network_data: <% $.network_data %>

    tags:
      - tripleo-common-managed

    tasks:
      get_network_names:
        publish:
          network_names_lower: <% $.network_data.where($.containsKey('name_lower')).name_lower %>
          network_names: <% $.network_data.where(not $.containsKey('name_lower')).name %>
        on-success: validate_networks

      validate_networks:
        with-items: network in <% $.network_names_lower.concat($.network_names) %>
        action: swift.head_object
        input:
          container: <% $.container %>
          obj: network/<% $.network.toLower() %>.yaml
        publish:
          status: SUCCESS
          message: <% task().result %>
        on-success: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      notify_zaqar:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.validate_network_files
              payload:
                status: <% $.status %>
                message: <% $.message %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  validate_networks:
    description: Validate network files were generated properly and exist
    input:
      - container: 'overcloud'
      - network_data_file: 'network_data.yaml'
      - queue_name: tripleo

    output:
      network_data: <% $.network_data %>

    tags:
      - tripleo-common-managed

    tasks:
      get_network_data:
        workflow: list_networks
        input:
          container: <% $.container %>
          network_data_file: <% $.network_data_file %>
          queue_name: <% $.queue_name %>
        publish:
          network_data: <% task().result.network_data %>
        on-success: validate_networks
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error:
          notify_zaqar

      validate_networks:
        workflow: validate_network_files
        input:
          container: <% $.container %>
          network_data: <% $.network_data %>
          queue_name: <% $.queue_name %>
        publish:
          status: SUCCESS
          message: <% task().result %>
        on-success: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.validate_networks
              payload:
                status: <% $.status %>
                network_data: <% $.get('network_data', {}) %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  validate_roles:
    description: Vaildate roles data exists and is parsable

    input:
      - container: overcloud
      - roles_data_file: 'roles_data.yaml'
      - queue_name: tripleo

    output:
      roles_data: <% $.roles_data %>

    tags:
      - tripleo-common-managed

    tasks:
      get_roles_data:
        workflow: list_roles
        input:
          container: <% $.container %>
          roles_data_file: <% $.roles_data_file %>
          queue_name: <% $.queue_name %>
        publish:
          roles_data: <% task().result.roles_data %>
          status: SUCCESS
        on-success: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error:
          notify_zaqar

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.validate_networks
              payload:
                status: <% $.status %>
                roles_data: <% $.get('roles_data', '') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  _validate_networks_from_roles:
    description: Internal workflow for validating a network exists from a role

    input:
      - container: overcloud
      - defined_networks
      - networks_in_roles
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      validate_network_in_network_data:
        publish:
          networks_found: <% $.networks_in_roles.toSet().intersect($.defined_networks.toSet()) %>
          networks_not_found: <% $.networks_in_roles.toSet().difference($.defined_networks.toSet()) %>
        on-success:
          - network_not_found: <% $.networks_not_found %>
          - notify_zaqar: <% not $.networks_not_found %>

      network_not_found:
        publish:
          message: <% "Some networks in roles are not defined, {0}".format($.networks_not_found.join(', ')) %>
          status: FAILED
        on-success: notify_zaqar

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1._validate_networks_from_role
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  validate_roles_and_networks:
    description: Vaidate that roles and network data are valid

    input:
      - container: overcloud
      - roles_data_file: 'roles_data.yaml'
      - network_data_file: 'network_data.yaml'
      - queue_name: tripleo

    output:
      roles_data: <% $.roles_data %>
      network_data: <% $.network_data %>

    tags:
      - tripleo-common-managed

    tasks:
      validate_network_data:
        workflow: validate_networks
        input:
          container: <% $.container %>
          network_data_file: <% $.network_data_file %>
          queue_name: <% $.queue_name %>
        publish:
          network_data: <% task().result.network_data %>
        on-success: validate_roles_data
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      validate_roles_data:
        workflow: validate_roles
        input:
          container: <% $.container %>
          roles_data_file: <% $.roles_data_file %>
          queue_name: <% $.queue_name %>
        publish:
          roles_data: <% task().result.roles_data %>
          role_networks_data: <% task().result.roles_data.networks %>
          networks_in_roles: <% task().result.roles_data.networks.flatten().distinct() %>
        on-success: validate_roles_and_networks
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      validate_roles_and_networks:
        workflow: _validate_networks_from_roles
        input:
          container: <% $.container %>
          defined_networks: <% $.network_data.name %>
          networks_in_roles: <% $.networks_in_roles %>
          queue_name: <% $.queue_name %>
        publish:
          status: SUCCESS
        on-success: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result.message %>
        on-error: notify_zaqar

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.validate_roles_and_networks
              payload:
                status: <% $.status %>
                roles_data: <% $.get('roles_data', {}) %>
                network_data: <% $.get('network_data', {}) %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  list_available_roles:
    input:
      - container: overcloud
      - queue_name: tripleo

    output:
      available_roles: <% $.available_roles %>

    tags:
      - tripleo-common-managed

    tasks:
      get_role_file_names:
        action: swift.get_container
        input:
          container: <% $.container %>
        publish:
          role_names: <% task().result[1].where($.name.startsWith('roles/')).where($.name.endsWith('.yaml')).name %>
        on-success: get_role_files
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      get_role_files:
        with-items: role_name in <% $.role_names %>
        action: swift.get_object
        on-success: transform_output
        on-error: notify_zaqar
        input:
          container: <% $.container %>
          obj: <% $.role_name %>
        publish:
          status: SUCCESS
          available_yaml_roles: <% task().result.select($[1]) %>
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      transform_output:
        publish:
          status: SUCCESS
          available_roles: <% yaml_parse($.available_yaml_roles.join("\n")) %>
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-complete: notify_zaqar

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.list_available_roles
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                available_roles: <% $.get('available_roles', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  update_roles:
    description: >
      takes data in json format validates its contents and persists them in
      roles_data.yaml, after successful update, templates are regenerated.
    input:
      - container
      - roles
      - roles_data_file: 'roles_data.yaml'
      - replace_all: false
      - queue_name: tripleo
    tags:
      - tripleo-common-managed
    tasks:
      get_available_roles:
        workflow: list_available_roles
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name%>
        publish:
          available_roles: <% task().result.available_roles %>
        on-success: validate_input
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      validate_input:
        description: >
          validate the format of input (verify that each role in input has the
          required attributes set. check README in roles directory in t-h-t),
          validate that roles in input exist in roles directory in t-h-t
        action: tripleo.plan.validate_roles
        input:
          container: <% $.container %>
          roles: <% $.roles %>
          available_roles: <% $.available_roles %>
        on-success: get_network_data
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      get_network_data:
        workflow: list_networks
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>
        publish:
          network_data: <% task().result.network_data %>
        on-success: validate_network_names
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      validate_network_names:
        description: >
          validate that Network names assigned to Role exist in
          network-data.yaml object in Swift container
        workflow: _validate_networks_from_roles
        input:
          container: <% $.container %>
          defined_networks: <% $.network_data.name %>
          networks_in_roles: <% $.roles.networks.flatten().distinct() %>
          queue_name: <% $.queue_name %>
        on-success: get_current_roles
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result.message %>

      get_current_roles:
        workflow: list_roles
        input:
          container: <% $.container %>
          roles_data_file: <% $.roles_data_file %>
          queue_name: <% $.queue_name %>
        publish:
            current_roles: <% task().result.roles_data %>
        on-success: update_roles_data
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      update_roles_data:
        description: >
          update roles_data.yaml object in Swift with roles from workflow input
        action: tripleo.plan.update_roles
        input:
          container: <% $.container %>
          roles: <% $.roles %>
          current_roles: <% $.current_roles %>
          replace_all: <% $.replace_all %>
        publish:
          updated_roles_data: <% task().result.roles %>
        on-success: update_roles_data_in_swift
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      update_roles_data_in_swift:
        description: >
          update roles_data.yaml object in Swift with data from workflow input
        action: swift.put_object
        input:
          container: <% $.container %>
          obj: <% $.roles_data_file %>
          contents: <% yaml_dump($.updated_roles_data) %>
        on-success: regenerate_templates
        publish-on-error:
          status: FAILED
          message: <% task().result %>
        on-error: notify_zaqar

      regenerate_templates:
        action: tripleo.templates.process container=<% $.container %>
        on-success: get_updated_roles
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      get_updated_roles:
        workflow: list_roles
        input:
          container: <% $.container %>
          roles_data_file:  <% $.roles_data_file %>
        publish:
            updated_roles: <% task().result.roles_data %>
            status: SUCCESS
        on-complete: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.roles.v1.update_roles
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                updated_roles: <% $.get('updated_roles', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  select_roles:
    description: >
      takes a list of role names as input and populates roles_data.yaml in
      container in Swift with respective roles from 'roles directory'
    input:
      - container
      - role_names
      - roles_data_file: 'roles_data.yaml'
      - replace_all: true
      - queue_name: tripleo
    tags:
      - tripleo-common-managed
    tasks:

      get_available_roles:
        workflow: list_available_roles
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>
        publish:
          available_roles: <% task().result.available_roles %>
        on-success: get_current_roles
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      get_current_roles:
        workflow: list_roles
        input:
          container: <% $.container %>
          roles_data_file: <% $.roles_data_file %>
          queue_name: <% $.queue_name %>
        publish:
          current_roles: <% task().result.roles_data %>
        on-success: gather_roles
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      gather_roles:
        description: >
          for each role name from the input, check if it exists in
          roles_data.yaml, if yes, use that role definition, if not, get the
          role definition from roles directory. Use the gathered roles
          definitions as input to updateRolesWorkflow - this ensures
          configuration of the roles which are already in roles_data.yaml
          will not get overridden by data from roles directory
        action: tripleo.plan.gather_roles
        input:
          role_names: <% $.role_names %>
          current_roles: <% $.current_roles %>
          available_roles: <% $.available_roles %>
        publish:
          gathered_roles: <% task().result.gathered_roles %>
        on-success: call_update_roles_workflow
        on-error: notify_zaqar
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      call_update_roles_workflow:
        workflow: update_roles
        input:
          container: <% $.container %>
          roles: <% $.gathered_roles %>
          roles_data_file: <% $.roles_data_file %>
          replace_all: <% $.replace_all %>
          queue_name: <% $.queue_name %>
        on-complete: notify_zaqar
        publish:
          selected_roles: <% task().result.updated_roles %>
          status: SUCCESS
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.plan_management.v1.select_roles
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                selected_roles: <% $.get('selected_roles', []) %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-04 06:26:16,499 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 47190
2018-12-04 06:26:16,540 DEBUG: RESP: [201] Content-Length: 47190 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:16 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.plan_management.v1\ndescription: TripleO Overcloud Deployment Workflows v1\n\nworkflows:\n\n  create_default_deployment_plan:\n    description: >\n      This workflow exists to maintain backwards compatibility in pike.  This\n      workflow will likely be removed in queens in favor of create_deployment_plan.\n    input:\n      - container\n      - queue_name: tripleo\n      - generate_passwords: true\n    tags:\n      - tripleo-common-managed\n    tasks:\n      call_create_deployment_plan:\n        workflow: tripleo.plan_management.v1.create_deployment_plan\n        on-success: set_status_success\n        on-error: call_create_deployment_plan_set_status_failed\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n          generate_passwords: <% $.generate_passwords %>\n          use_default_templates: true\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(call_create_deployment_plan).result %>\n\n      call_create_deployment_plan_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(call_create_deployment_plan).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.create_default_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  create_deployment_plan:\n    description: >\n      This workflow provides the capability to create a deployment plan using\n      the default heat templates provided in a standard TripleO undercloud\n      deployment, heat templates contained in an external git repository, or a\n      swift container that already contains templates.\n    input:\n      - container\n      - source_url: null\n      - queue_name: tripleo\n      - generate_passwords: true\n      - use_default_templates: false\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      container_required_check:\n        description: >\n          If using the default templates or importing templates from a git\n          repository, a new container needs to be created.  If using an existing\n          container containing templates, skip straight to create_plan.\n        on-success:\n          - verify_container_doesnt_exist: <% $.use_default_templates or $.source_url %>\n          - create_plan: <% $.use_default_templates = false and $.source_url = null %>\n\n      verify_container_doesnt_exist:\n        action: swift.head_container container=<% $.container %>\n        on-success: notify_zaqar\n        on-error: create_container\n        publish:\n          status: FAILED\n          message: \"Unable to create plan. The Swift container already exists\"\n\n      create_container:\n        action: tripleo.plan.create_container container=<% $.container %>\n        on-success: templates_source_check\n        on-error: create_container_set_status_failed\n\n      cleanup_temporary_files:\n        action: tripleo.git.clean container=<% $.container %>\n\n      templates_source_check:\n        on-success:\n          - upload_default_templates: <% $.use_default_templates = true %>\n          - clone_git_repo: <% $.source_url != null %>\n\n      clone_git_repo:\n        action: tripleo.git.clone container=<% $.container %> url=<% $.source_url %>\n        on-success: upload_templates_directory\n        on-error: clone_git_repo_set_status_failed\n\n      upload_templates_directory:\n        action: tripleo.templates.upload container=<% $.container %> templates_path=<% task(clone_git_repo).result %>\n        on-success: create_plan\n        on-complete: cleanup_temporary_files\n        on-error: upload_templates_directory_set_status_failed\n\n      upload_default_templates:\n        action: tripleo.templates.upload container=<% $.container %>\n        on-success: create_plan\n        on-error: upload_to_container_set_status_failed\n\n      create_plan:\n        on-success:\n          - ensure_passwords_exist: <% $.generate_passwords = true %>\n          - add_root_stack_name: <% $.generate_passwords != true %>\n\n      ensure_passwords_exist:\n        action: tripleo.parameters.generate_passwords container=<% $.container %>\n        on-success: add_root_stack_name\n        on-error: ensure_passwords_exist_set_status_failed\n\n      add_root_stack_name:\n        action: tripleo.parameters.update\n        input:\n          container: <% $.container %>\n          parameters:\n            RootStackName: <% $.container %>\n        on-success: container_images_prepare\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      container_images_prepare:\n        description: >\n          Populate all container image parameters with default values.\n        action: tripleo.container_images.prepare container=<% $.container %>\n        on-success: process_templates\n        on-error: container_images_prepare_set_status_failed\n\n      process_templates:\n        action: tripleo.templates.process container=<% $.container %>\n        on-success: set_status_success\n        on-error: process_templates_set_status_failed\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: 'Plan created.'\n\n      create_container_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_container).result %>\n\n      clone_git_repo_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(clone_git_repo).result %>\n\n      upload_templates_directory_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(upload_templates_directory).result %>\n\n      upload_to_container_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(upload_default_templates).result %>\n\n      ensure_passwords_exist_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(ensure_passwords_exist).result %>\n\n      process_templates_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(process_templates).result %>\n\n      container_images_prepare_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(container_images_prepare).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.create_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_deployment_plan:\n    input:\n      - container\n      - source_url: null\n      - queue_name: tripleo\n      - generate_passwords: true\n      - plan_environment: null\n    tags:\n      - tripleo-common-managed\n    tasks:\n      templates_source_check:\n        on-success:\n          - update_plan: <% $.source_url = null %>\n          - clone_git_repo: <% $.source_url != null %>\n\n      clone_git_repo:\n        action: tripleo.git.clone container=<% $.container %> url=<% $.source_url %>\n        on-success: upload_templates_directory\n        on-error: clone_git_repo_set_status_failed\n\n      upload_templates_directory:\n        action: tripleo.templates.upload container=<% $.container %> templates_path=<% task(clone_git_repo).result %>\n        on-success: create_swift_rings_backup_plan\n        on-complete: cleanup_temporary_files\n        on-error: upload_templates_directory_set_status_failed\n\n      cleanup_temporary_files:\n        action: tripleo.git.clean container=<% $.container %>\n\n      create_swift_rings_backup_plan:\n        workflow: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan\n        on-success: update_plan\n        on-error: create_swift_rings_backup_plan_set_status_failed\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n          use_default_templates: true\n\n      update_plan:\n        on-success:\n          - ensure_passwords_exist: <% $.generate_passwords = true %>\n          - container_images_prepare: <% $.generate_passwords != true %>\n\n      ensure_passwords_exist:\n        action: tripleo.parameters.generate_passwords container=<% $.container %>\n        on-success: container_images_prepare\n        on-error: ensure_passwords_exist_set_status_failed\n\n      container_images_prepare:\n        description: >\n          Populate all container image parameters with default values.\n        action: tripleo.container_images.prepare container=<% $.container %>\n        on-success: process_templates\n        on-error: container_images_prepare_set_status_failed\n\n      process_templates:\n        action: tripleo.templates.process container=<% $.container %>\n        on-success:\n          - set_status_success: <% $.plan_environment = null %>\n          - upload_plan_environment: <% $.plan_environment != null %>\n        on-error: process_templates_set_status_failed\n\n      upload_plan_environment:\n        action: tripleo.templates.upload_plan_environment container=<% $.container %> plan_environment=<% $.plan_environment %>\n        on-success: set_status_success\n        on-error: process_templates_set_status_failed\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: 'Plan updated.'\n\n      create_swift_rings_backup_plan_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_swift_rings_backup_plan).result %>\n\n      clone_git_repo_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(clone_git_repo).result %>\n\n      upload_templates_directory_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(upload_templates_directory).result %>\n\n      process_templates_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(process_templates).result %>\n\n      ensure_passwords_exist_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(ensure_passwords_exist).result %>\n\n      container_images_prepare_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(container_images_prepare).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.update_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  delete_deployment_plan:\n    description: >\n      Deletes a plan by deleting the container matching plan_name. It will\n      not delete the plan if a stack exists with the same name.\n\n    tags:\n      - tripleo-common-managed\n\n    input:\n      - container: overcloud\n      - queue_name: tripleo\n\n    tasks:\n      delete_plan:\n        action: tripleo.plan.delete container=<% $.container %>\n        on-complete: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.delete_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n\n  get_passwords:\n    description: Retrieves passwords for a given plan\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      verify_container_exists:\n        action: swift.head_container container=<% $.container %>\n        on-success: get_environment_passwords\n        on-error: verify_container_set_status_failed\n\n      get_environment_passwords:\n        action: tripleo.parameters.get_passwords container=<% $.container %>\n        on-success: get_passwords_set_status_success\n        on-error: get_passwords_set_status_failed\n\n      get_passwords_set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(get_environment_passwords).result %>\n\n      get_passwords_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(get_environment_passwords).result %>\n\n      verify_container_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(verify_container_exists).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.get_passwords\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  export_deployment_plan:\n    description: Creates an export tarball for a given plan\n    input:\n      - plan\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      export_plan:\n        action: tripleo.plan.export\n        input:\n          plan: <% $.plan %>\n          delete_after: 3600\n          exports_container: \"plan-exports\"\n        on-success: create_tempurl\n        on-error: export_plan_set_status_failed\n\n      create_tempurl:\n        action: tripleo.swift.tempurl\n        on-success: set_status_success\n        on-error: create_tempurl_set_status_failed\n        input:\n          container: \"plan-exports\"\n          obj: \"<% $.plan %>.tar.gz\"\n          valid: 3600\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(create_tempurl).result %>\n          tempurl: <% task(create_tempurl).result %>\n\n      export_plan_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(export_plan).result %>\n\n      create_tempurl_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_tempurl).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.export_deployment_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                tempurl: <% $.get('tempurl', '') %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  get_deprecated_parameters:\n    description: Gets the list of deprecated parameters in the whole of the plan including nested stack\n    input:\n      - container: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_flatten_data:\n        action: tripleo.parameters.get_flatten container=<% $.container %>\n        on-success: get_deprecated_params\n        on-error: set_status_failed_get_flatten_data\n        publish:\n          user_params: <% task().result.environment_parameters %>\n          plan_params: <% task().result.heat_resource_tree.parameters.keys() %>\n          parameter_groups: <% task().result.heat_resource_tree.resources.values().where( $.get('parameter_groups') ).select($.parameter_groups).flatten() %>\n\n      get_deprecated_params:\n        on-success: check_if_user_param_has_deprecated\n        publish:\n          deprecated_params: <% $.parameter_groups.where($.get('label') = 'deprecated').select($.parameters).flatten().distinct() %>\n\n      check_if_user_param_has_deprecated:\n        on-success: get_unused_params\n        publish:\n          deprecated_result: <% let(up => $.user_params) -> $.deprecated_params.select( dict('parameter' => $, 'deprecated' => true, 'user_defined' => $up.keys().contains($)) ) %>\n\n      # Get the list of parameters, which are defined by user via environment files's parameter_default, but not part of the plan definition\n      # It may be possible that the parameter will be used by a service, but the service is not part of the plan.\n      # In such cases, the parameter will be reported as unused, care should be take to understand whether it is really unused or not.\n      get_unused_params:\n        on-success: send_message\n        publish:\n          unused_params: <% let(plan_params => $.plan_params) ->  $.user_params.keys().where( not $plan_params.contains($) ) %>\n\n      set_status_failed_get_flatten_data:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(get_flatten_data).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.get_deprecated_parameters\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                deprecated: <% $.get('deprecated_result', []) %>\n                unused: <% $.get('unused_params', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  publish_ui_logs_to_swift:\n    description: >\n      This workflow drains a zaqar queue, and publish its messages into a log\n      file in swift.  This workflow is called by cron trigger.\n\n    input:\n      - logging_queue_name: tripleo-ui-logging\n      - logging_container: tripleo-ui-logs\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      # We're using a NoOp action to start the workflow.  The recursive nature\n      # of the workflow means that Mistral will refuse to execute it because it\n      # doesn't know where to begin.\n      start:\n        on-success: get_messages\n\n      get_messages:\n        action: zaqar.claim_messages\n        on-success:\n          - format_messages: <% task().result.len() > 0 %>\n        input:\n          queue_name: <% $.logging_queue_name %>\n          ttl: 60\n          grace: 60\n        publish:\n          status: SUCCESS\n          messages: <% task().result %>\n          message_ids: <% task().result.select($._id) %>\n\n      format_messages:\n        action: tripleo.logging_to_swift.format_messages\n        on-success: upload_to_swift\n        input:\n          messages: <% $.messages %>\n        publish:\n          status: SUCCESS\n          formatted_messages: <% task().result %>\n\n      upload_to_swift:\n        action: tripleo.logging_to_swift.publish_ui_log_to_swift\n        on-success: delete_messages\n        input:\n          logging_data: <% $.formatted_messages %>\n          logging_container: <% $.logging_container %>\n        publish:\n          status: SUCCESS\n\n      delete_messages:\n        action: zaqar.delete_messages\n        on-success: get_messages\n        input:\n          queue_name: <% $.logging_queue_name %>\n          messages: <% $.message_ids %>\n        publish:\n          status: SUCCESS\n\n  download_logs:\n    description: Creates a tarball with logging data\n    input:\n      - queue_name: tripleo\n      - logging_container: \"tripleo-ui-logs\"\n      - downloads_container: \"tripleo-ui-logs-downloads\"\n      - delete_after: 3600\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      publish_logs:\n        workflow: tripleo.plan_management.v1.publish_ui_logs_to_swift\n        on-success: prepare_log_download\n        on-error: publish_logs_set_status_failed\n\n      prepare_log_download:\n        action: tripleo.logging_to_swift.prepare_log_download\n        input:\n          logging_container: <% $.logging_container %>\n          downloads_container: <% $.downloads_container %>\n          delete_after: <% $.delete_after %>\n        on-success: create_tempurl\n        on-error: download_logs_set_status_failed\n        publish:\n          filename: <% task().result %>\n\n      create_tempurl:\n        action: tripleo.swift.tempurl\n        on-success: set_status_success\n        on-error: create_tempurl_set_status_failed\n        input:\n          container: <% $.downloads_container %>\n          obj: <% $.filename %>\n          valid: 3600\n        publish:\n          tempurl: <% task().result %>\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(create_tempurl).result %>\n          tempurl: <% task(create_tempurl).result %>\n\n      publish_logs_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(publish_logs).result %>\n\n      download_logs_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(prepare_log_download).result %>\n\n      create_tempurl_set_status_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_tempurl).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.download_logs\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                tempurl: <% $.get('tempurl', '') %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list_roles:\n    description: Retrieve the roles_data.yaml and return a usable object\n\n    input:\n      - container: overcloud\n      - roles_data_file: 'roles_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      roles_data: <% $.roles_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_roles_data:\n        action: swift.get_object\n        input:\n          container: <% $.container %>\n          obj: <% $.roles_data_file %>\n        publish:\n          roles_data: <% yaml_parse(task().result.last()) %>\n          status: SUCCESS\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.list_roles\n              payload:\n                status: <% $.status %>\n                roles_data: <% $.get('roles_data', {}) %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list_available_networks:\n    input:\n      - container\n      - queue_name: tripleo\n\n    output:\n      available_networks: <% $.available_networks %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_network_file_names:\n        action: swift.get_container\n        input:\n          container: <% $.container %>\n        publish:\n          network_names: <% task().result[1].where($.name.startsWith('networks/')).where($.name.endsWith('.yaml')).name %>\n        on-success: get_network_files\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_network_files:\n        with-items: network_name in <% $.network_names %>\n        action: swift.get_object\n        on-success: transform_output\n        on-error: notify_zaqar\n        input:\n          container: <% $.container %>\n          obj: <% $.network_name %>\n        publish:\n          status: SUCCESS\n          available_yaml_networks: <% task().result.select($[1]) %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      transform_output:\n        publish:\n          status: SUCCESS\n          available_networks: <% yaml_parse($.available_yaml_networks.join(\"\\n\")) %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-complete: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.list_available_networks\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                available_networks: <% $.get('available_networks', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list_networks:\n    input:\n      - container: 'overcloud'\n      - network_data_file: 'network_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_networks:\n        action: swift.get_object\n        input:\n          container: <% $.container %>\n          obj: <% $.network_data_file %>\n        on-success: notify_zaqar\n        publish:\n          network_data: <% yaml_parse(task().result.last()) %>\n          status: SUCCESS\n          message: <% task().result %>\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.list_networks\n              payload:\n                status: <% $.status %>\n                network_data: <% $.get('network_data', {}) %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_network_files:\n    description: Validate network files exist\n    input:\n      - container: overcloud\n      - network_data\n      - queue_name: tripleo\n\n    output:\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_network_names:\n        publish:\n          network_names_lower: <% $.network_data.where($.containsKey('name_lower')).name_lower %>\n          network_names: <% $.network_data.where(not $.containsKey('name_lower')).name %>\n        on-success: validate_networks\n\n      validate_networks:\n        with-items: network in <% $.network_names_lower.concat($.network_names) %>\n        action: swift.head_object\n        input:\n          container: <% $.container %>\n          obj: network/<% $.network.toLower() %>.yaml\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.validate_network_files\n              payload:\n                status: <% $.status %>\n                message: <% $.message %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_networks:\n    description: Validate network files were generated properly and exist\n    input:\n      - container: 'overcloud'\n      - network_data_file: 'network_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_network_data:\n        workflow: list_networks\n        input:\n          container: <% $.container %>\n          network_data_file: <% $.network_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().result.network_data %>\n        on-success: validate_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error:\n          notify_zaqar\n\n      validate_networks:\n        workflow: validate_network_files\n        input:\n          container: <% $.container %>\n          network_data: <% $.network_data %>\n          queue_name: <% $.queue_name %>\n        publish:\n          status: SUCCESS\n          message: <% task().result %>\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.validate_networks\n              payload:\n                status: <% $.status %>\n                network_data: <% $.get('network_data', {}) %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_roles:\n    description: Vaildate roles data exists and is parsable\n\n    input:\n      - container: overcloud\n      - roles_data_file: 'roles_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      roles_data: <% $.roles_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_roles_data:\n        workflow: list_roles\n        input:\n          container: <% $.container %>\n          roles_data_file: <% $.roles_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          roles_data: <% task().result.roles_data %>\n          status: SUCCESS\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error:\n          notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.validate_networks\n              payload:\n                status: <% $.status %>\n                roles_data: <% $.get('roles_data', '') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  _validate_networks_from_roles:\n    description: Internal workflow for validating a network exists from a role\n\n    input:\n      - container: overcloud\n      - defined_networks\n      - networks_in_roles\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      validate_network_in_network_data:\n        publish:\n          networks_found: <% $.networks_in_roles.toSet().intersect($.defined_networks.toSet()) %>\n          networks_not_found: <% $.networks_in_roles.toSet().difference($.defined_networks.toSet()) %>\n        on-success:\n          - network_not_found: <% $.networks_not_found %>\n          - notify_zaqar: <% not $.networks_not_found %>\n\n      network_not_found:\n        publish:\n          message: <% \"Some networks in roles are not defined, {0}\".format($.networks_not_found.join(', ')) %>\n          status: FAILED\n        on-success: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1._validate_networks_from_role\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  validate_roles_and_networks:\n    description: Vaidate that roles and network data are valid\n\n    input:\n      - container: overcloud\n      - roles_data_file: 'roles_data.yaml'\n      - network_data_file: 'network_data.yaml'\n      - queue_name: tripleo\n\n    output:\n      roles_data: <% $.roles_data %>\n      network_data: <% $.network_data %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      validate_network_data:\n        workflow: validate_networks\n        input:\n          container: <% $.container %>\n          network_data_file: <% $.network_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().result.network_data %>\n        on-success: validate_roles_data\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      validate_roles_data:\n        workflow: validate_roles\n        input:\n          container: <% $.container %>\n          roles_data_file: <% $.roles_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          roles_data: <% task().result.roles_data %>\n          role_networks_data: <% task().result.roles_data.networks %>\n          networks_in_roles: <% task().result.roles_data.networks.flatten().distinct() %>\n        on-success: validate_roles_and_networks\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      validate_roles_and_networks:\n        workflow: _validate_networks_from_roles\n        input:\n          container: <% $.container %>\n          defined_networks: <% $.network_data.name %>\n          networks_in_roles: <% $.networks_in_roles %>\n          queue_name: <% $.queue_name %>\n        publish:\n          status: SUCCESS\n        on-success: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result.message %>\n        on-error: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.validate_roles_and_networks\n              payload:\n                status: <% $.status %>\n                roles_data: <% $.get('roles_data', {}) %>\n                network_data: <% $.get('network_data', {}) %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list_available_roles:\n    input:\n      - container: overcloud\n      - queue_name: tripleo\n\n    output:\n      available_roles: <% $.available_roles %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_role_file_names:\n        action: swift.get_container\n        input:\n          container: <% $.container %>\n        publish:\n          role_names: <% task().result[1].where($.name.startsWith('roles/')).where($.name.endsWith('.yaml')).name %>\n        on-success: get_role_files\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_role_files:\n        with-items: role_name in <% $.role_names %>\n        action: swift.get_object\n        on-success: transform_output\n        on-error: notify_zaqar\n        input:\n          container: <% $.container %>\n          obj: <% $.role_name %>\n        publish:\n          status: SUCCESS\n          available_yaml_roles: <% task().result.select($[1]) %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      transform_output:\n        publish:\n          status: SUCCESS\n          available_roles: <% yaml_parse($.available_yaml_roles.join(\"\\n\")) %>\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-complete: notify_zaqar\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.list_available_roles\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                available_roles: <% $.get('available_roles', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  update_roles:\n    description: >\n      takes data in json format validates its contents and persists them in\n      roles_data.yaml, after successful update, templates are regenerated.\n    input:\n      - container\n      - roles\n      - roles_data_file: 'roles_data.yaml'\n      - replace_all: false\n      - queue_name: tripleo\n    tags:\n      - tripleo-common-managed\n    tasks:\n      get_available_roles:\n        workflow: list_available_roles\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name%>\n        publish:\n          available_roles: <% task().result.available_roles %>\n        on-success: validate_input\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      validate_input:\n        description: >\n          validate the format of input (verify that each role in input has the\n          required attributes set. check README in roles directory in t-h-t),\n          validate that roles in input exist in roles directory in t-h-t\n        action: tripleo.plan.validate_roles\n        input:\n          container: <% $.container %>\n          roles: <% $.roles %>\n          available_roles: <% $.available_roles %>\n        on-success: get_network_data\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_network_data:\n        workflow: list_networks\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n        publish:\n          network_data: <% task().result.network_data %>\n        on-success: validate_network_names\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      validate_network_names:\n        description: >\n          validate that Network names assigned to Role exist in\n          network-data.yaml object in Swift container\n        workflow: _validate_networks_from_roles\n        input:\n          container: <% $.container %>\n          defined_networks: <% $.network_data.name %>\n          networks_in_roles: <% $.roles.networks.flatten().distinct() %>\n          queue_name: <% $.queue_name %>\n        on-success: get_current_roles\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result.message %>\n\n      get_current_roles:\n        workflow: list_roles\n        input:\n          container: <% $.container %>\n          roles_data_file: <% $.roles_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n            current_roles: <% task().result.roles_data %>\n        on-success: update_roles_data\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      update_roles_data:\n        description: >\n          update roles_data.yaml object in Swift with roles from workflow input\n        action: tripleo.plan.update_roles\n        input:\n          container: <% $.container %>\n          roles: <% $.roles %>\n          current_roles: <% $.current_roles %>\n          replace_all: <% $.replace_all %>\n        publish:\n          updated_roles_data: <% task().result.roles %>\n        on-success: update_roles_data_in_swift\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      update_roles_data_in_swift:\n        description: >\n          update roles_data.yaml object in Swift with data from workflow input\n        action: swift.put_object\n        input:\n          container: <% $.container %>\n          obj: <% $.roles_data_file %>\n          contents: <% yaml_dump($.updated_roles_data) %>\n        on-success: regenerate_templates\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n        on-error: notify_zaqar\n\n      regenerate_templates:\n        action: tripleo.templates.process container=<% $.container %>\n        on-success: get_updated_roles\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_updated_roles:\n        workflow: list_roles\n        input:\n          container: <% $.container %>\n          roles_data_file:  <% $.roles_data_file %>\n        publish:\n            updated_roles: <% task().result.roles_data %>\n            status: SUCCESS\n        on-complete: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.roles.v1.update_roles\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                updated_roles: <% $.get('updated_roles', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  select_roles:\n    description: >\n      takes a list of role names as input and populates roles_data.yaml in\n      container in Swift with respective roles from 'roles directory'\n    input:\n      - container\n      - role_names\n      - roles_data_file: 'roles_data.yaml'\n      - replace_all: true\n      - queue_name: tripleo\n    tags:\n      - tripleo-common-managed\n    tasks:\n\n      get_available_roles:\n        workflow: list_available_roles\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n        publish:\n          available_roles: <% task().result.available_roles %>\n        on-success: get_current_roles\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      get_current_roles:\n        workflow: list_roles\n        input:\n          container: <% $.container %>\n          roles_data_file: <% $.roles_data_file %>\n          queue_name: <% $.queue_name %>\n        publish:\n          current_roles: <% task().result.roles_data %>\n        on-success: gather_roles\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      gather_roles:\n        description: >\n          for each role name from the input, check if it exists in\n          roles_data.yaml, if yes, use that role definition, if not, get the\n          role definition from roles directory. Use the gathered roles\n          definitions as input to updateRolesWorkflow - this ensures\n          configuration of the roles which are already in roles_data.yaml\n          will not get overridden by data from roles directory\n        action: tripleo.plan.gather_roles\n        input:\n          role_names: <% $.role_names %>\n          current_roles: <% $.current_roles %>\n          available_roles: <% $.available_roles %>\n        publish:\n          gathered_roles: <% task().result.gathered_roles %>\n        on-success: call_update_roles_workflow\n        on-error: notify_zaqar\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      call_update_roles_workflow:\n        workflow: update_roles\n        input:\n          container: <% $.container %>\n          roles: <% $.gathered_roles %>\n          roles_data_file: <% $.roles_data_file %>\n          replace_all: <% $.replace_all %>\n          queue_name: <% $.queue_name %>\n        on-complete: notify_zaqar\n        publish:\n          selected_roles: <% task().result.updated_roles %>\n          status: SUCCESS\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.plan_management.v1.select_roles\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                selected_roles: <% $.get('selected_roles', []) %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.plan_management.v1", "tags": [], "created_at": "2018-12-04 11:26:16", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "1d51b601-118c-4e05-9861-db2f0db0269e"}

2018-12-04 06:26:16,541 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:16,543 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.scale.v1
description: TripleO Overcloud Deployment Workflows v1

workflows:

  delete_node:
    description: deletes given overcloud nodes and updates the stack

    input:
      - container
      - nodes
      - timeout: 240
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      delete_node:
        action: tripleo.scale.delete_node nodes=<% $.nodes %> timeout=<% $.timeout %> container=<% $.container %>
        on-success: wait_for_stack_in_progress
        on-error: set_delete_node_failed

      set_delete_node_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(delete_node).result %>

      wait_for_stack_in_progress:
        workflow: tripleo.stack.v1.wait_for_stack_in_progress stack=<% $.container %>
        on-success: wait_for_stack_complete
        on-error: wait_for_stack_in_progress_failed

      wait_for_stack_in_progress_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(wait_for_stack_in_progress).result %>

      wait_for_stack_complete:
        workflow: tripleo.stack.v1.wait_for_stack_complete_or_failed stack=<% $.container %>
        on-success: send_message
        on-error: wait_for_stack_complete_failed

      wait_for_stack_complete_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(wait_for_stack_complete).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.scale.v1.delete_node
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-04 06:26:16,865 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 2258
2018-12-04 06:26:16,867 DEBUG: RESP: [201] Content-Length: 2258 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:16 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.scale.v1\ndescription: TripleO Overcloud Deployment Workflows v1\n\nworkflows:\n\n  delete_node:\n    description: deletes given overcloud nodes and updates the stack\n\n    input:\n      - container\n      - nodes\n      - timeout: 240\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      delete_node:\n        action: tripleo.scale.delete_node nodes=<% $.nodes %> timeout=<% $.timeout %> container=<% $.container %>\n        on-success: wait_for_stack_in_progress\n        on-error: set_delete_node_failed\n\n      set_delete_node_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(delete_node).result %>\n\n      wait_for_stack_in_progress:\n        workflow: tripleo.stack.v1.wait_for_stack_in_progress stack=<% $.container %>\n        on-success: wait_for_stack_complete\n        on-error: wait_for_stack_in_progress_failed\n\n      wait_for_stack_in_progress_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(wait_for_stack_in_progress).result %>\n\n      wait_for_stack_complete:\n        workflow: tripleo.stack.v1.wait_for_stack_complete_or_failed stack=<% $.container %>\n        on-success: send_message\n        on-error: wait_for_stack_complete_failed\n\n      wait_for_stack_complete_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(wait_for_stack_complete).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.scale.v1.delete_node\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.scale.v1", "tags": [], "created_at": "2018-12-04 11:26:16", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "74ff436e-2a87-48ee-a795-396a76e96d74"}

2018-12-04 06:26:16,867 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:16,868 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.skydive_ansible.v1
description: TripleO manages Skydive with skydive-ansible

workflows:
  skydive_install:
    # allows for additional extra_vars via workflow input
    input:
      - ansible_playbook_verbosity: 0
      - ansible_extra_env_variables:
          ANSIBLE_ROLES_PATH: /usr/share/skydive-ansible/roles/
          ANSIBLE_RETRY_FILES_ENABLED: 'False'
          ANSIBLE_LOG_PATH: /var/log/mistral/skydive-install-workflow.log
          ANSIBLE_HOST_KEY_CHECKING: 'False'
      - skydive_ansible_extra_vars: {}
      - skydive_ansible_playbook: /usr/share/skydive-ansible/playbook.yml.sample
    tags:
      - tripleo-common-managed
    tasks:
      set_blacklisted_ips:
        publish:
          blacklisted_ips: <% env().get('blacklisted_ip_addresses', []) %>
        on-success: set_ip_lists
      set_ip_lists:
        publish:
          agent_ips: <% let(root => $) -> env().get('service_ips', {}).get('skydive_agent_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
          analyzer_ips: <% let(root => $) -> env().get('service_ips', {}).get('skydive_analyzer_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>
        on-success: enable_ssh_admin
      enable_ssh_admin:
        workflow: tripleo.access.v1.enable_ssh_admin
        input:
          ssh_servers: <% ($.agent_ips + $.analyzer_ips).toSet() %>
        on-success: get_private_key
      get_private_key:
        action: tripleo.validations.get_privkey
        publish:
          private_key: <% task().result %>
        on-success: set_fork_count
      set_fork_count:
        publish: # unique list of all IPs: make each list a set, take unions and count
          fork_count: <% min($.agent_ips.toSet().union($.analyzer_ips.toSet()).count(), 100) %> # don't use >100 forks
        on-success: set_role_vars
      set_role_vars:
        publish:
          # NOTE(sbaubeau): collect role settings from all tht roles
          agent_vars: <% env().get('role_merged_configs', {}).values().select($.get('skydive_agent_ansible_vars', {})).aggregate($1 + $2) %>
          analyzer_vars: <% env().get('role_merged_configs', {}).values().select($.get('skydive_analyzer_ansible_vars', {})).aggregate($1 + $2) %>
        on-success: build_extra_vars
      build_extra_vars:
        publish:
          # NOTE(sbaubeau): merge vars from all ansible roles
          extra_vars: <% $.agent_vars + $.analyzer_vars + $.skydive_ansible_extra_vars %>
        on-success: skydive_install
      skydive_install:
        action: tripleo.ansible-playbook
        input:
          inventory:
            agents:
              hosts: <% $.agent_ips.toDict($, {}) %>
            analyzers:
              hosts: <% $.analyzer_ips.toDict($, {}) %>
          playbook: <% $.skydive_ansible_playbook %>
          remote_user: tripleo-admin
          become: true
          become_user: root
          verbosity: <% $.ansible_playbook_verbosity %>
          forks: <% $.fork_count %>
          ssh_private_key: <% $.private_key %>
          extra_env_variables: <% $.ansible_extra_env_variables %>
          extra_vars: <% $.extra_vars %>
        publish:
          output: <% task().result %>
'
2018-12-04 06:26:17,197 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 3507
2018-12-04 06:26:17,199 DEBUG: RESP: [201] Content-Length: 3507 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:17 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.skydive_ansible.v1\ndescription: TripleO manages Skydive with skydive-ansible\n\nworkflows:\n  skydive_install:\n    # allows for additional extra_vars via workflow input\n    input:\n      - ansible_playbook_verbosity: 0\n      - ansible_extra_env_variables:\n          ANSIBLE_ROLES_PATH: /usr/share/skydive-ansible/roles/\n          ANSIBLE_RETRY_FILES_ENABLED: 'False'\n          ANSIBLE_LOG_PATH: /var/log/mistral/skydive-install-workflow.log\n          ANSIBLE_HOST_KEY_CHECKING: 'False'\n      - skydive_ansible_extra_vars: {}\n      - skydive_ansible_playbook: /usr/share/skydive-ansible/playbook.yml.sample\n    tags:\n      - tripleo-common-managed\n    tasks:\n      set_blacklisted_ips:\n        publish:\n          blacklisted_ips: <% env().get('blacklisted_ip_addresses', []) %>\n        on-success: set_ip_lists\n      set_ip_lists:\n        publish:\n          agent_ips: <% let(root => $) -> env().get('service_ips', {}).get('skydive_agent_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n          analyzer_ips: <% let(root => $) -> env().get('service_ips', {}).get('skydive_analyzer_ctlplane_node_ips', []).where(not ($ in $root.blacklisted_ips)) %>\n        on-success: enable_ssh_admin\n      enable_ssh_admin:\n        workflow: tripleo.access.v1.enable_ssh_admin\n        input:\n          ssh_servers: <% ($.agent_ips + $.analyzer_ips).toSet() %>\n        on-success: get_private_key\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        publish:\n          private_key: <% task().result %>\n        on-success: set_fork_count\n      set_fork_count:\n        publish: # unique list of all IPs: make each list a set, take unions and count\n          fork_count: <% min($.agent_ips.toSet().union($.analyzer_ips.toSet()).count(), 100) %> # don't use >100 forks\n        on-success: set_role_vars\n      set_role_vars:\n        publish:\n          # NOTE(sbaubeau): collect role settings from all tht roles\n          agent_vars: <% env().get('role_merged_configs', {}).values().select($.get('skydive_agent_ansible_vars', {})).aggregate($1 + $2) %>\n          analyzer_vars: <% env().get('role_merged_configs', {}).values().select($.get('skydive_analyzer_ansible_vars', {})).aggregate($1 + $2) %>\n        on-success: build_extra_vars\n      build_extra_vars:\n        publish:\n          # NOTE(sbaubeau): merge vars from all ansible roles\n          extra_vars: <% $.agent_vars + $.analyzer_vars + $.skydive_ansible_extra_vars %>\n        on-success: skydive_install\n      skydive_install:\n        action: tripleo.ansible-playbook\n        input:\n          inventory:\n            agents:\n              hosts: <% $.agent_ips.toDict($, {}) %>\n            analyzers:\n              hosts: <% $.analyzer_ips.toDict($, {}) %>\n          playbook: <% $.skydive_ansible_playbook %>\n          remote_user: tripleo-admin\n          become: true\n          become_user: root\n          verbosity: <% $.ansible_playbook_verbosity %>\n          forks: <% $.fork_count %>\n          ssh_private_key: <% $.private_key %>\n          extra_env_variables: <% $.ansible_extra_env_variables %>\n          extra_vars: <% $.extra_vars %>\n        publish:\n          output: <% task().result %>\n", "name": "tripleo.skydive_ansible.v1", "tags": [], "created_at": "2018-12-04 11:26:17", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "bcd8099c-74cd-46ff-ad0e-69c490ee4138"}

2018-12-04 06:26:17,199 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:17,201 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.stack.v1
description: TripleO Stack Workflows

workflows:

  wait_for_stack_complete_or_failed:
    input:
      - stack
      - timeout: 14400 # 4 hours. Default timeout of stack deployment

    tags:
      - tripleo-common-managed

    tasks:

      wait_for_stack_status:
        action: heat.stacks_get stack_id=<% $.stack %>
        timeout: <% $.timeout %>
        retry:
          delay: 15
          count: <% $.timeout / 15 %>
          continue-on: <% task().result.stack_status in ['CREATE_IN_PROGRESS', 'UPDATE_IN_PROGRESS', 'DELETE_IN_PROGRESS'] %>

  wait_for_stack_in_progress:
    input:
      - stack
      - timeout: 600 # 10 minutes. Should not take much longer for a stack to transition to IN_PROGRESS

    tags:
      - tripleo-common-managed

    tasks:

      wait_for_stack_status:
        action: heat.stacks_get stack_id=<% $.stack %>
        timeout: <% $.timeout %>
        retry:
          delay: 15
          count: <% $.timeout / 15 %>
          continue-on: <% task().result.stack_status in ['CREATE_COMPLETE', 'CREATE_FAILED', 'UPDATE_COMPLETE', 'UPDATE_FAILED', 'DELETE_FAILED'] %>

  wait_for_stack_does_not_exist:
    input:
      - stack
      - timeout: 3600

    tags:
      - tripleo-common-managed

    tasks:
      wait_for_stack_does_not_exist:
        action: heat.stacks_list
        timeout: <% $.timeout %>
        retry:
          delay: 15
          count: <% $.timeout / 15 %>
          continue-on: <% $.stack in task(wait_for_stack_does_not_exist).result.select([$.stack_name, $.id]).flatten() %>

  delete_stack:
    input:
      - stack
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      delete_the_stack:
        action: heat.stacks_delete stack_id=<% $.stack %>
        on-success: wait_for_stack_does_not_exist
        on-error: delete_the_stack_failed

      delete_the_stack_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(delete_the_stack).result %>

      wait_for_stack_does_not_exist:
        workflow: tripleo.stack.v1.wait_for_stack_does_not_exist stack=<% $.stack %>
        on-success: send_message
        on-error: wait_for_stack_does_not_exist_failed

      wait_for_stack_does_not_exist_failed:
        on-success: send_message
        publish:
          status: FAILED
          message: <% task(wait_for_stack_does_not_exist).result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.scale.v1.delete_stack
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-04 06:26:17,614 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 3236
2018-12-04 06:26:17,615 DEBUG: RESP: [201] Content-Length: 3236 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:17 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.stack.v1\ndescription: TripleO Stack Workflows\n\nworkflows:\n\n  wait_for_stack_complete_or_failed:\n    input:\n      - stack\n      - timeout: 14400 # 4 hours. Default timeout of stack deployment\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      wait_for_stack_status:\n        action: heat.stacks_get stack_id=<% $.stack %>\n        timeout: <% $.timeout %>\n        retry:\n          delay: 15\n          count: <% $.timeout / 15 %>\n          continue-on: <% task().result.stack_status in ['CREATE_IN_PROGRESS', 'UPDATE_IN_PROGRESS', 'DELETE_IN_PROGRESS'] %>\n\n  wait_for_stack_in_progress:\n    input:\n      - stack\n      - timeout: 600 # 10 minutes. Should not take much longer for a stack to transition to IN_PROGRESS\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      wait_for_stack_status:\n        action: heat.stacks_get stack_id=<% $.stack %>\n        timeout: <% $.timeout %>\n        retry:\n          delay: 15\n          count: <% $.timeout / 15 %>\n          continue-on: <% task().result.stack_status in ['CREATE_COMPLETE', 'CREATE_FAILED', 'UPDATE_COMPLETE', 'UPDATE_FAILED', 'DELETE_FAILED'] %>\n\n  wait_for_stack_does_not_exist:\n    input:\n      - stack\n      - timeout: 3600\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      wait_for_stack_does_not_exist:\n        action: heat.stacks_list\n        timeout: <% $.timeout %>\n        retry:\n          delay: 15\n          count: <% $.timeout / 15 %>\n          continue-on: <% $.stack in task(wait_for_stack_does_not_exist).result.select([$.stack_name, $.id]).flatten() %>\n\n  delete_stack:\n    input:\n      - stack\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      delete_the_stack:\n        action: heat.stacks_delete stack_id=<% $.stack %>\n        on-success: wait_for_stack_does_not_exist\n        on-error: delete_the_stack_failed\n\n      delete_the_stack_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(delete_the_stack).result %>\n\n      wait_for_stack_does_not_exist:\n        workflow: tripleo.stack.v1.wait_for_stack_does_not_exist stack=<% $.stack %>\n        on-success: send_message\n        on-error: wait_for_stack_does_not_exist_failed\n\n      wait_for_stack_does_not_exist_failed:\n        on-success: send_message\n        publish:\n          status: FAILED\n          message: <% task(wait_for_stack_does_not_exist).result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.scale.v1.delete_stack\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.stack.v1", "tags": [], "created_at": "2018-12-04 11:26:17", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "eea51bd8-1b20-41cc-a8ee-ff7ef7c4e03a"}

2018-12-04 06:26:17,616 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:17,617 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.support.v1
description: TripleO support workflows

workflows:

  collect_logs:
    description: >
      This workflow runs sosreport on the servers where their names match the
      provided server_name input. The logs are stored in the provided sos_dir.
    input:
      - server_name
      - sos_dir: /var/tmp/tripleo-sos
      - sos_options: boot,cluster,hardware,kernel,memory,nfs,openstack,packagemanager,performance,services,storage,system,webserver,virt
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      collect_logs_on_servers:
        workflow: tripleo.deployment.v1.deploy_on_servers
        on-success: send_message
        on-error: set_collect_logs_on_servers_failed
        input:
          server_name: <% $.server_name %>
          config_name: 'run_sosreport'
          config: |
            #!/bin/bash
            mkdir -p <% $.sos_dir %>
            sosreport --batch \
            -p <% $.sos_options %> \
            --tmp-dir <% $.sos_dir %>

      set_collect_logs_on_servers_failed:
          on-complete:
           - send_message
          publish:
            type: tripleo.deployment.v1.fetch_logs
            status: FAILED
            message: <% task().result %>

      # status messaging
      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: <% $.get('type', 'tripleo.support.v1.collect_logs') %>
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = 'FAILED' %>

  upload_logs:
    description: >
      This workflow uploads the sosreport files stored in the provide sos_dir
      on the provided host (server_uuid) to a swift container on the undercloud
    input:
      - server_uuid
      - server_name
      - container
      - sos_dir: /var/tmp/tripleo-sos
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      # actions
      get_swift_information:
        action: tripleo.swift.swift_information
        on-success: do_log_upload
        on-error: set_get_swift_information_failed
        input:
          container: <% $.container %>
        publish:
          container_url: <% task().result.container_url %>
          auth_key: <% task().result.auth_key %>

      set_get_swift_information_failed:
        on-complete:
          - send_message
        publish:
          status: FAILED
          message: <% task(get_swift_information).result %>

      do_log_upload:
        action: tripleo.deployment.config
        on-success: send_message
        on-error: set_do_log_upload_failed
        input:
          server_id: <% $.server_uuid %>
          name: "upload_logs"
          config: |
            #!/bin/bash
            CONTAINER_URL="<% $.container_url %>"
            TOKEN="<% $.auth_key %>"
            SOS_DIR="<% $.sos_dir %>"
            for FILE in $(find $SOS_DIR -type f); do
              FILENAME=$(basename $FILE)
              curl -X PUT -i -H "X-Auth-Token: $TOKEN" -T $FILE $CONTAINER_URL/$FILENAME
              if [ $? -eq 0 ]; then
                rm -f $FILE
              fi
            done
          group: "script"
        publish:
          message: "Uploaded logs from <% $.server_name %>"

      set_do_log_upload_failed:
        on-complete:
          - send_message
        publish:
          status: FAILED
          message: <% tag(do_log_upload).result %>

      # status messaging
      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: <% $.get('type', 'tripleo.support.v1.upload_logs') %>
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = 'FAILED' %>

  create_container:
    description: >
      This work flow is used to check if the container exists and creates it
      if it does not exist.
    input:
      - container
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      check_container:
        action: swift.head_container container=<% $.container %>
        on-success: send_message
        on-error: create_container

      create_container:
        action: swift.put_container
        input:
          container: <% $.container %>
          headers:
            x-container-meta-usage-tripleo: support
        on-success: send_message
        on-error: set_create_container_failed

      set_create_container_failed:
        on-complete:
          - send_message
        publish:
          type: tripleo.support.v1.create_container.create_container
          status: FAILED
          message: <% task(create_container).result %>

      # status messaging
      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: <% $.get('type', 'tripleo.support.v1.create_container') %>
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = 'FAILED' %>

  delete_container:
    description: >
      This workflow deletes all the objects in a provided swift container and
      then removes the container itself from the undercloud.
    input:
      - container
      - concurrency: 5
      - timeout: 900
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      # actions
      check_container:
        action: swift.head_container container=<% $.container %>
        on-success: list_objects
        on-error: set_check_container_failure

      set_check_container_failure:
        on-complete: send_message
        publish:
          status: FAILED
          type: tripleo.support.v1.delete_container.check_container
          message: <% task(check_container).result %>

      list_objects:
        action: swift.get_container container=<% $.container %>
        on-success: delete_objects
        on-error: set_list_objects_failure
        publish:
          log_objects: <% task().result[1] %>

      set_list_objects_failure:
        on-complete: send_message
        publish:
          status: FAILED
          type: tripleo.support.v1.delete_container.list_objects
          message: <% task(list_objects).result %>

      delete_objects:
        action: swift.delete_object
        concurrency: <% $.concurrency %>
        timeout: <% $.timeout %>
        with-items: object in <% $.log_objects %>
        input:
          container: <% $.container %>
          obj: <% $.object.name %>
        on-success: remove_container
        on-error: set_delete_objects_failure

      set_delete_objects_failure:
        on-complete: send_message
        publish:
          status: FAILED
          type: tripleo.support.v1.delete_container.delete_objects
          message: <% task(delete_objects).result %>

      remove_container:
        action: swift.delete_container container=<% $.container %>
        on-success: send_message
        on-error: set_remove_container_failure

      set_remove_container_failure:
        on-complete: send_message
        publish:
          status: FAILED
          type: tripleo.support.v1.delete_container.remove_container
          message: <% task(remove_container).result %>

      # status messaging
      send_message:
        action: zaqar.queue_post
        wait-before: 5
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: <% $.get('type', 'tripleo.support.v1.delete_container') %>
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = 'FAILED' %>

  fetch_logs:
    description: >
      This workflow creates a container on the undercloud, executes the log
      collection on the servers whose names match the provided server_name, and
      executes the log upload process on all the servers to the container on
      the undercloud.
    input:
      - server_name
      - container
      - concurrency: 5
      - timeout: 1800
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      # actions
      create_container:
        workflow: tripleo.support.v1.create_container
        on-success: get_servers_matching
        on-error: set_create_container_failed
        input:
          container: <% $.container %>
          queue_name: <% $.queue_name %>

      set_create_container_failed:
          on-complete: send_message
          publish:
            type: tripleo.support.v1.fetch_logs.create_container
            status: FAILED
            message: <% task(create_container).result %>

      get_servers_matching:
        action: nova.servers_list
        on-success: collect_logs_on_servers
        publish:
          servers_with_name: <% task().result._info.where($.name.indexOf(execution().input.server_name) > -1) %>

      collect_logs_on_servers:
        workflow: tripleo.support.v1.collect_logs
        timeout: <% $.timeout %>
        on-success: upload_logs_on_servers
        on-error: set_collect_logs_on_servers_failed
        input:
          server_name: <% $.server_name %>
          queue_name: <% $.queue_name %>

      set_collect_logs_on_servers_failed:
          on-complete: send_message
          publish:
            type: tripleo.support.v1.fetch_logs.collect_logs_on_servers
            status: FAILED
            message: <% task(collect_logs_on_servers).result %>

      upload_logs_on_servers:
        on-success: send_message
        on-error: set_upload_logs_on_servers_failed
        with-items: server in <% $.servers_with_name %>
        concurrency: <% $.concurrency %>
        workflow: tripleo.support.v1.upload_logs
        input:
          server_name: <% $.server.name %>
          server_uuid: <% $.server.id %>
          container: <% $.container %>
          queue_name: <% $.queue_name %>

      set_upload_logs_on_servers_failed:
          on-complete: send_message
          publish:
            type: tripleo.support.v1.fetch_logs.upload_logs
            status: FAILED
            message: <% task(upload_logs_on_servers).result %>

      # status messaging
      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: <% $.get('type', 'tripleo.support.v1.fetch_logs') %>
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = 'FAILED' %>
'
2018-12-04 06:26:19,076 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 12000
2018-12-04 06:26:19,078 DEBUG: RESP: [201] Content-Length: 12000 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:19 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.support.v1\ndescription: TripleO support workflows\n\nworkflows:\n\n  collect_logs:\n    description: >\n      This workflow runs sosreport on the servers where their names match the\n      provided server_name input. The logs are stored in the provided sos_dir.\n    input:\n      - server_name\n      - sos_dir: /var/tmp/tripleo-sos\n      - sos_options: boot,cluster,hardware,kernel,memory,nfs,openstack,packagemanager,performance,services,storage,system,webserver,virt\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      collect_logs_on_servers:\n        workflow: tripleo.deployment.v1.deploy_on_servers\n        on-success: send_message\n        on-error: set_collect_logs_on_servers_failed\n        input:\n          server_name: <% $.server_name %>\n          config_name: 'run_sosreport'\n          config: |\n            #!/bin/bash\n            mkdir -p <% $.sos_dir %>\n            sosreport --batch \\\n            -p <% $.sos_options %> \\\n            --tmp-dir <% $.sos_dir %>\n\n      set_collect_logs_on_servers_failed:\n          on-complete:\n           - send_message\n          publish:\n            type: tripleo.deployment.v1.fetch_logs\n            status: FAILED\n            message: <% task().result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.collect_logs') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n  upload_logs:\n    description: >\n      This workflow uploads the sosreport files stored in the provide sos_dir\n      on the provided host (server_uuid) to a swift container on the undercloud\n    input:\n      - server_uuid\n      - server_name\n      - container\n      - sos_dir: /var/tmp/tripleo-sos\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      # actions\n      get_swift_information:\n        action: tripleo.swift.swift_information\n        on-success: do_log_upload\n        on-error: set_get_swift_information_failed\n        input:\n          container: <% $.container %>\n        publish:\n          container_url: <% task().result.container_url %>\n          auth_key: <% task().result.auth_key %>\n\n      set_get_swift_information_failed:\n        on-complete:\n          - send_message\n        publish:\n          status: FAILED\n          message: <% task(get_swift_information).result %>\n\n      do_log_upload:\n        action: tripleo.deployment.config\n        on-success: send_message\n        on-error: set_do_log_upload_failed\n        input:\n          server_id: <% $.server_uuid %>\n          name: \"upload_logs\"\n          config: |\n            #!/bin/bash\n            CONTAINER_URL=\"<% $.container_url %>\"\n            TOKEN=\"<% $.auth_key %>\"\n            SOS_DIR=\"<% $.sos_dir %>\"\n            for FILE in $(find $SOS_DIR -type f); do\n              FILENAME=$(basename $FILE)\n              curl -X PUT -i -H \"X-Auth-Token: $TOKEN\" -T $FILE $CONTAINER_URL/$FILENAME\n              if [ $? -eq 0 ]; then\n                rm -f $FILE\n              fi\n            done\n          group: \"script\"\n        publish:\n          message: \"Uploaded logs from <% $.server_name %>\"\n\n      set_do_log_upload_failed:\n        on-complete:\n          - send_message\n        publish:\n          status: FAILED\n          message: <% tag(do_log_upload).result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.upload_logs') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n  create_container:\n    description: >\n      This work flow is used to check if the container exists and creates it\n      if it does not exist.\n    input:\n      - container\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_container:\n        action: swift.head_container container=<% $.container %>\n        on-success: send_message\n        on-error: create_container\n\n      create_container:\n        action: swift.put_container\n        input:\n          container: <% $.container %>\n          headers:\n            x-container-meta-usage-tripleo: support\n        on-success: send_message\n        on-error: set_create_container_failed\n\n      set_create_container_failed:\n        on-complete:\n          - send_message\n        publish:\n          type: tripleo.support.v1.create_container.create_container\n          status: FAILED\n          message: <% task(create_container).result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.create_container') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n  delete_container:\n    description: >\n      This workflow deletes all the objects in a provided swift container and\n      then removes the container itself from the undercloud.\n    input:\n      - container\n      - concurrency: 5\n      - timeout: 900\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      # actions\n      check_container:\n        action: swift.head_container container=<% $.container %>\n        on-success: list_objects\n        on-error: set_check_container_failure\n\n      set_check_container_failure:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          type: tripleo.support.v1.delete_container.check_container\n          message: <% task(check_container).result %>\n\n      list_objects:\n        action: swift.get_container container=<% $.container %>\n        on-success: delete_objects\n        on-error: set_list_objects_failure\n        publish:\n          log_objects: <% task().result[1] %>\n\n      set_list_objects_failure:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          type: tripleo.support.v1.delete_container.list_objects\n          message: <% task(list_objects).result %>\n\n      delete_objects:\n        action: swift.delete_object\n        concurrency: <% $.concurrency %>\n        timeout: <% $.timeout %>\n        with-items: object in <% $.log_objects %>\n        input:\n          container: <% $.container %>\n          obj: <% $.object.name %>\n        on-success: remove_container\n        on-error: set_delete_objects_failure\n\n      set_delete_objects_failure:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          type: tripleo.support.v1.delete_container.delete_objects\n          message: <% task(delete_objects).result %>\n\n      remove_container:\n        action: swift.delete_container container=<% $.container %>\n        on-success: send_message\n        on-error: set_remove_container_failure\n\n      set_remove_container_failure:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          type: tripleo.support.v1.delete_container.remove_container\n          message: <% task(remove_container).result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        wait-before: 5\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.delete_container') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n\n  fetch_logs:\n    description: >\n      This workflow creates a container on the undercloud, executes the log\n      collection on the servers whose names match the provided server_name, and\n      executes the log upload process on all the servers to the container on\n      the undercloud.\n    input:\n      - server_name\n      - container\n      - concurrency: 5\n      - timeout: 1800\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      # actions\n      create_container:\n        workflow: tripleo.support.v1.create_container\n        on-success: get_servers_matching\n        on-error: set_create_container_failed\n        input:\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n\n      set_create_container_failed:\n          on-complete: send_message\n          publish:\n            type: tripleo.support.v1.fetch_logs.create_container\n            status: FAILED\n            message: <% task(create_container).result %>\n\n      get_servers_matching:\n        action: nova.servers_list\n        on-success: collect_logs_on_servers\n        publish:\n          servers_with_name: <% task().result._info.where($.name.indexOf(execution().input.server_name) > -1) %>\n\n      collect_logs_on_servers:\n        workflow: tripleo.support.v1.collect_logs\n        timeout: <% $.timeout %>\n        on-success: upload_logs_on_servers\n        on-error: set_collect_logs_on_servers_failed\n        input:\n          server_name: <% $.server_name %>\n          queue_name: <% $.queue_name %>\n\n      set_collect_logs_on_servers_failed:\n          on-complete: send_message\n          publish:\n            type: tripleo.support.v1.fetch_logs.collect_logs_on_servers\n            status: FAILED\n            message: <% task(collect_logs_on_servers).result %>\n\n      upload_logs_on_servers:\n        on-success: send_message\n        on-error: set_upload_logs_on_servers_failed\n        with-items: server in <% $.servers_with_name %>\n        concurrency: <% $.concurrency %>\n        workflow: tripleo.support.v1.upload_logs\n        input:\n          server_name: <% $.server.name %>\n          server_uuid: <% $.server.id %>\n          container: <% $.container %>\n          queue_name: <% $.queue_name %>\n\n      set_upload_logs_on_servers_failed:\n          on-complete: send_message\n          publish:\n            type: tripleo.support.v1.fetch_logs.upload_logs\n            status: FAILED\n            message: <% task(upload_logs_on_servers).result %>\n\n      # status messaging\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: <% $.get('type', 'tripleo.support.v1.fetch_logs') %>\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = 'FAILED' %>\n", "name": "tripleo.support.v1", "tags": [], "created_at": "2018-12-04 11:26:19", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "0ef63d6a-4c35-4e42-979a-3648455b99e0"}

2018-12-04 06:26:19,078 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:19,079 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.swift_ring.v1
description: Rebalance and distribute Swift rings using Ansible


workflows:
  rebalance:
    tags:
      - tripleo-common-managed

    tasks:
      get_private_key:
        action: tripleo.validations.get_privkey
        on-success: deploy_rings

      deploy_rings:
        action: tripleo.ansible-playbook
        publish:
          output: <% task().result %>
        input:
          ssh_private_key: <% task(get_private_key).result %>
          ssh_common_args: '-o StrictHostKeyChecking=no'
          ssh_extra_args: '-o UserKnownHostsFile=/dev/null'
          verbosity: 1
          remote_user: heat-admin
          become: true
          become_user: root
          playbook: /usr/share/tripleo-common/playbooks/swift_ring_rebalance.yaml
          inventory: /usr/bin/tripleo-ansible-inventory
          use_openstack_credentials: true
'
2018-12-04 06:26:19,188 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 1140
2018-12-04 06:26:19,189 DEBUG: RESP: [201] Content-Length: 1140 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:19 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.swift_ring.v1\ndescription: Rebalance and distribute Swift rings using Ansible\n\n\nworkflows:\n  rebalance:\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      get_private_key:\n        action: tripleo.validations.get_privkey\n        on-success: deploy_rings\n\n      deploy_rings:\n        action: tripleo.ansible-playbook\n        publish:\n          output: <% task().result %>\n        input:\n          ssh_private_key: <% task(get_private_key).result %>\n          ssh_common_args: '-o StrictHostKeyChecking=no'\n          ssh_extra_args: '-o UserKnownHostsFile=/dev/null'\n          verbosity: 1\n          remote_user: heat-admin\n          become: true\n          become_user: root\n          playbook: /usr/share/tripleo-common/playbooks/swift_ring_rebalance.yaml\n          inventory: /usr/bin/tripleo-ansible-inventory\n          use_openstack_credentials: true\n", "name": "tripleo.swift_ring.v1", "tags": [], "created_at": "2018-12-04 11:26:19", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "2814d72e-8f50-4eaa-84c2-b048f71fc90d"}

2018-12-04 06:26:19,189 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:19,190 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.swift_rings_backup.v1
description: TripleO Swift Rings backup container Deployment Workflow v1

workflows:

  create_swift_rings_backup_container_plan:
    description: >
      This plan ensures existence of container for Swift Rings backup.
    input:
      - container
      - queue_name: tripleo
    tags:
      - tripleo-common-managed
    tasks:

      swift_rings_container:
        publish:
          swift_rings_container: "<% $.container %>-swift-rings"
          swift_rings_tar: "swift-rings.tar.gz"
        on-complete: check_container

      check_container:
        action: swift.head_container container=<% $.swift_rings_container %>
        on-success: get_tempurl
        on-error: create_container

      create_container:
        action: swift.put_container container=<% $.swift_rings_container %>
        on-error: set_create_container_failed
        on-success: get_tempurl

      get_tempurl:
        action: tripleo.swift.tempurl
        on-success: set_get_tempurl
        input:
          container: <% $.swift_rings_container %>
          obj: <% $.swift_rings_tar %>

      set_get_tempurl:
        action: tripleo.parameters.update
        input:
          parameters:
            SwiftRingGetTempurl: <% task(get_tempurl).result %>
          container: <% $.container %>
        on-success: put_tempurl

      put_tempurl:
        action: tripleo.swift.tempurl
        on-success: set_put_tempurl
        input:
          container: <% $.swift_rings_container %>
          obj: <% $.swift_rings_tar %>
          method: "PUT"

      set_put_tempurl:
        action: tripleo.parameters.update
        input:
          parameters:
            SwiftRingPutTempurl: <% task(put_tempurl).result %>
          container: <% $.container %>
        on-success: set_status_success
        on-error: set_put_tempurl_failed

      set_status_success:
        on-success: notify_zaqar
        publish:
          status: SUCCESS
          message: <% task(set_put_tempurl).result %>

      set_put_tempurl_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(set_put_tempurl).result %>

      set_create_container_failed:
        on-success: notify_zaqar
        publish:
          status: FAILED
          message: <% task(create_container).result %>

      notify_zaqar:
        action: zaqar.queue_post
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan
              payload:
                status: <% $.status %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-04 06:26:19,636 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 3154
2018-12-04 06:26:19,637 DEBUG: RESP: [201] Content-Length: 3154 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:19 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.swift_rings_backup.v1\ndescription: TripleO Swift Rings backup container Deployment Workflow v1\n\nworkflows:\n\n  create_swift_rings_backup_container_plan:\n    description: >\n      This plan ensures existence of container for Swift Rings backup.\n    input:\n      - container\n      - queue_name: tripleo\n    tags:\n      - tripleo-common-managed\n    tasks:\n\n      swift_rings_container:\n        publish:\n          swift_rings_container: \"<% $.container %>-swift-rings\"\n          swift_rings_tar: \"swift-rings.tar.gz\"\n        on-complete: check_container\n\n      check_container:\n        action: swift.head_container container=<% $.swift_rings_container %>\n        on-success: get_tempurl\n        on-error: create_container\n\n      create_container:\n        action: swift.put_container container=<% $.swift_rings_container %>\n        on-error: set_create_container_failed\n        on-success: get_tempurl\n\n      get_tempurl:\n        action: tripleo.swift.tempurl\n        on-success: set_get_tempurl\n        input:\n          container: <% $.swift_rings_container %>\n          obj: <% $.swift_rings_tar %>\n\n      set_get_tempurl:\n        action: tripleo.parameters.update\n        input:\n          parameters:\n            SwiftRingGetTempurl: <% task(get_tempurl).result %>\n          container: <% $.container %>\n        on-success: put_tempurl\n\n      put_tempurl:\n        action: tripleo.swift.tempurl\n        on-success: set_put_tempurl\n        input:\n          container: <% $.swift_rings_container %>\n          obj: <% $.swift_rings_tar %>\n          method: \"PUT\"\n\n      set_put_tempurl:\n        action: tripleo.parameters.update\n        input:\n          parameters:\n            SwiftRingPutTempurl: <% task(put_tempurl).result %>\n          container: <% $.container %>\n        on-success: set_status_success\n        on-error: set_put_tempurl_failed\n\n      set_status_success:\n        on-success: notify_zaqar\n        publish:\n          status: SUCCESS\n          message: <% task(set_put_tempurl).result %>\n\n      set_put_tempurl_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(set_put_tempurl).result %>\n\n      set_create_container_failed:\n        on-success: notify_zaqar\n        publish:\n          status: FAILED\n          message: <% task(create_container).result %>\n\n      notify_zaqar:\n        action: zaqar.queue_post\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.swift_rings_backup.v1.create_swift_rings_backup_container_plan\n              payload:\n                status: <% $.status %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.swift_rings_backup.v1", "tags": [], "created_at": "2018-12-04 11:26:19", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "84b86453-a1ef-42de-b2f4-e48b62d12e5c"}

2018-12-04 06:26:19,638 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:19,639 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.undercloud_backup.v1
description: TripleO Undercloud backup workflows

workflows:

  backup:
    description: This workflow will launch the Undercloud backup
    tags:
      - tripleo-common-managed
    input:
      - sources_path: '/home/stack/'
      - queue_name: tripleo
    tasks:
      # Action to know if there is enough available space
      # to run the Undercloud backup
      get_free_space:
        action: tripleo.undercloud.get_free_space
        publish:
            status: SUCCESS
            message: <% task().result %>
            free_space: <% task().result %>
        on-success: create_backup_dir
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      # We create a temp directory to store the Undercloud
      # backup
      create_backup_dir:
        action: tripleo.undercloud.create_backup_dir
        publish:
            status: SUCCESS
            message: <% task().result %>
            backup_path: <% task().result %>
        on-success: get_database_credentials
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      # The Undercloud database password for the root
      # user is stored in a Mistral environment, we
      # need the password in order to run the database dump
      get_database_credentials:
        action: mistral.environments_get name='tripleo.undercloud-config'
        publish:
            status: SUCCESS
            message: <% task().result %>
            undercloud_db_password: <% task(get_database_credentials).result.variables.undercloud_db_password %>
        on-success: create_database_backup
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      # Run the DB dump of all the databases and store the result
      # in the temporary folder
      create_database_backup:
        input:
            path: <% $.backup_path.path %>
            dbuser: root
            dbpassword: <% $.undercloud_db_password %>
        action: tripleo.undercloud.create_database_backup
        publish:
            status: SUCCESS
            message: <% task().result %>
        on-success: create_fs_backup
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      # This action will run the fs backup
      create_fs_backup:
        input:
            sources_path: <% $.sources_path %>
            path: <% $.backup_path.path %>
        action: tripleo.undercloud.create_file_system_backup
        publish:
            status: SUCCESS
            message: <% task().result %>
        on-success: upload_backup
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      # This action will push the backup to swift
      upload_backup:
        input:
            backup_path: <% $.backup_path.path %>
        action: tripleo.undercloud.upload_backup_to_swift
        publish:
            status: SUCCESS
            message: <% task().result %>
        on-success: cleanup_backup
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      # This action will remove the backup temp folder
      cleanup_backup:
        input:
            path: <% $.backup_path.path %>
        action: tripleo.undercloud.remove_temp_dir
        publish:
            status: SUCCESS
            message: <% task().result %>
        on-success: send_message
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      # Sending a message to show that the backup finished
      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.undercloud_backup.v1.launch
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                execution: <% execution() %>
                message: <% $.get('message', '') %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-04 06:26:20,083 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 4669
2018-12-04 06:26:20,084 DEBUG: RESP: [201] Content-Length: 4669 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:20 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.undercloud_backup.v1\ndescription: TripleO Undercloud backup workflows\n\nworkflows:\n\n  backup:\n    description: This workflow will launch the Undercloud backup\n    tags:\n      - tripleo-common-managed\n    input:\n      - sources_path: '/home/stack/'\n      - queue_name: tripleo\n    tasks:\n      # Action to know if there is enough available space\n      # to run the Undercloud backup\n      get_free_space:\n        action: tripleo.undercloud.get_free_space\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n            free_space: <% task().result %>\n        on-success: create_backup_dir\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # We create a temp directory to store the Undercloud\n      # backup\n      create_backup_dir:\n        action: tripleo.undercloud.create_backup_dir\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n            backup_path: <% task().result %>\n        on-success: get_database_credentials\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # The Undercloud database password for the root\n      # user is stored in a Mistral environment, we\n      # need the password in order to run the database dump\n      get_database_credentials:\n        action: mistral.environments_get name='tripleo.undercloud-config'\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n            undercloud_db_password: <% task(get_database_credentials).result.variables.undercloud_db_password %>\n        on-success: create_database_backup\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # Run the DB dump of all the databases and store the result\n      # in the temporary folder\n      create_database_backup:\n        input:\n            path: <% $.backup_path.path %>\n            dbuser: root\n            dbpassword: <% $.undercloud_db_password %>\n        action: tripleo.undercloud.create_database_backup\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n        on-success: create_fs_backup\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # This action will run the fs backup\n      create_fs_backup:\n        input:\n            sources_path: <% $.sources_path %>\n            path: <% $.backup_path.path %>\n        action: tripleo.undercloud.create_file_system_backup\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n        on-success: upload_backup\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # This action will push the backup to swift\n      upload_backup:\n        input:\n            backup_path: <% $.backup_path.path %>\n        action: tripleo.undercloud.upload_backup_to_swift\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n        on-success: cleanup_backup\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # This action will remove the backup temp folder\n      cleanup_backup:\n        input:\n            path: <% $.backup_path.path %>\n        action: tripleo.undercloud.remove_temp_dir\n        publish:\n            status: SUCCESS\n            message: <% task().result %>\n        on-success: send_message\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      # Sending a message to show that the backup finished\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.undercloud_backup.v1.launch\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                execution: <% execution() %>\n                message: <% $.get('message', '') %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.undercloud_backup.v1", "tags": [], "created_at": "2018-12-04 11:26:20", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "ab8583dd-79f8-456f-adda-0bc0f0c32e6f"}

2018-12-04 06:26:20,084 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:20,086 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/workbooks -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: text/plain" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '---
version: '2.0'
name: tripleo.validations.v1
description: TripleO Validations Workflows v1

workflows:

  run_validation:
    input:
      - validation_name
      - plan: overcloud
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      notify_running:
        on-complete: run_validation
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.run_validation
              payload:
                validation_name: <% $.validation_name %>
                plan: <% $.plan %>
                status: RUNNING
                execution: <% execution() %>

      run_validation:
        on-success: send_message
        on-error: set_status_failed
        action: tripleo.validations.run_validation validation=<% $.validation_name %> plan=<% $.plan %>
        publish:
          status: SUCCESS
          stdout: <% task().result.stdout %>
          stderr: <% task().result.stderr %>

      set_status_failed:
        on-complete: send_message
        publish:
          status: FAILED
          stdout: <% task(run_validation).result.stdout %>
          stderr: <% task(run_validation).result.stderr %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.run_validation
              payload:
                validation_name: <% $.validation_name %>
                plan: <% $.plan %>
                status: <% $.get('status', 'SUCCESS') %>
                stdout: <% $.stdout %>
                stderr: <% $.stderr %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  run_validations:
    input:
      - validation_names: []
      - plan: overcloud
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      notify_running:
        on-complete: run_validations
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.run_validations
              payload:
                validation_names: <% $.validation_names %>
                plan: <% $.plan %>
                status: RUNNING
                execution: <% execution() %>

      run_validations:
        on-success: send_message
        on-error: set_status_failed
        workflow: tripleo.validations.v1.run_validation validation_name=<% $.validation %> plan=<% $.plan %> queue_name=<% $.queue_name %>
        with-items: validation in <% $.validation_names %>
        publish:
          status: SUCCESS

      set_status_failed:
        on-complete: send_message
        publish:
          status: FAILED

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.run_validations
              payload:
                validation_names: <% $.validation_names %>
                plan: <% $.plan %>
                status: <% $.get('status', 'SUCCESS') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  run_groups:
    input:
      - group_names: []
      - plan: overcloud
      - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:

      find_validations:
        on-success: notify_running
        action: tripleo.validations.list_validations groups=<% $.group_names %>
        publish:
          validations: <% task().result %>

      notify_running:
        on-complete: run_validation_group
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.run_validations
              payload:
                group_names: <% $.group_names %>
                validation_names: <% $.validations.id %>
                plan: <% $.plan %>
                status: RUNNING
                execution: <% execution() %>

      run_validation_group:
        on-success: send_message
        on-error: set_status_failed
        workflow: tripleo.validations.v1.run_validation validation_name=<% $.validation %> plan=<% $.plan %> queue_name=<% $.queue_name %>
        with-items: validation in <% $.validations.id %>
        publish:
          status: SUCCESS

      set_status_failed:
        on-complete: send_message
        publish:
          status: FAILED

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.run_groups
              payload:
                group_names: <% $.group_names %>
                validation_names: <% $.validations.id %>
                plan: <% $.plan %>
                status: <% $.get('status', 'SUCCESS') %>
                execution: <% execution() %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  list:
    input:
      - group_names: []
    tags:
      - tripleo-common-managed
    tasks:
      find_validations:
        action: tripleo.validations.list_validations groups=<% $.group_names %>

  list_groups:
    tags:
      - tripleo-common-managed
    tasks:
      find_groups:
        action: tripleo.validations.list_groups

  add_validation_ssh_key_parameter:
    input:
     - container
     - queue_name: tripleo

    tags:
      - tripleo-common-managed

    tasks:
      test_validations_enabled:
        action: tripleo.validations.enabled
        on-success: get_pubkey
        on-error: unset_validation_key_parameter

      get_pubkey:
        action: tripleo.validations.get_pubkey
        on-success: set_validation_key_parameter
        publish:
          pubkey: <% task().result %>

      set_validation_key_parameter:
        action: tripleo.parameters.update
        input:
          parameters:
            node_admin_extra_ssh_keys: <% $.pubkey %>
          container: <% $.container %>

      # NOTE(shadower): We need to clear keys from a previous deployment
      unset_validation_key_parameter:
        action: tripleo.parameters.update
        input:
          parameters:
            node_admin_extra_ssh_keys: ""
          container: <% $.container %>

  copy_ssh_key:
    input:
      # FIXME: we should stop using heat-admin as e.g. split-stack
      # environments (where Nova didn't create overcloud nodes) don't
      # have it present
      - overcloud_admin: heat-admin
      - queue_name: tripleo
    tags:
      - tripleo-common-managed
    tasks:
      get_servers:
        action: nova.servers_list
        on-success: get_pubkey
        publish:
          servers: <% task().result._info %>

      get_pubkey:
        action: tripleo.validations.get_pubkey
        on-success: deploy_ssh_key
        publish:
          pubkey: <% task().result %>

      deploy_ssh_key:
        workflow: tripleo.deployment.v1.deploy_on_server
        with-items: server in <% $.servers %>
        input:
          server_name: <% $.server.name %>
          server_uuid: <% $.server.id %>
          config: |
            #!/bin/bash
            if ! grep "<% $.pubkey %>" /home/<% $.overcloud_admin %>/.ssh/authorized_keys; then
              echo "<% $.pubkey %>" >> /home/<% $.overcloud_admin %>/.ssh/authorized_keys
            fi
          config_name: copy_ssh_key
          group: script
          queue_name: <% $.queue_name %>

  check_boot_images:
    input:
      - deploy_kernel_name: 'bm-deploy-kernel'
      - deploy_ramdisk_name: 'bm-deploy-ramdisk'
      - run_validations: true
      - queue_name: tripleo
    output:
      errors: <% $.errors %>
      warnings: <% $.warnings %>
      kernel_id: <% $.kernel_id %>
      ramdisk_id: <% $.ramdisk_id %>
    tags:
      - tripleo-common-managed
    tasks:
      check_run_validations:
        on-complete:
          - get_images: <% $.run_validations %>
          - send_message: <% not $.run_validations %>

      get_images:
        action: glance.images_list
        on-success: check_images
        publish:
          images: <% task().result %>

      check_images:
        action: tripleo.validations.check_boot_images
        input:
          images: <% $.images %>
          deploy_kernel_name: <% $.deploy_kernel_name %>
          deploy_ramdisk_name: <% $.deploy_ramdisk_name %>
        on-success: send_message
        publish:
          kernel_id: <% task().result.kernel_id %>
          ramdisk_id: <% task().result.ramdisk_id %>
          warnings: <% task().result.warnings %>
          errors: <% task().result.errors %>
        on-error: send_message
        publish-on-error:
          kernel_id: <% task().result.kernel_id %>
          ramdisk_id: <% task().result.ramdisk_id %>
          warnings: <% task().result.warnings %>
          errors: <% task().result.errors %>
          status: FAILED
          message: <% task().result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.check_boot_images
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                kernel_id: <% $.kernel_id %>
                ramdisk_id: <% $.ramdisk_id %>
                errors: <% $.errors %>
                warnings: <% $.warnings %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  collect_flavors:
    input:
      - roles_info: {}
      - run_validations: true
      - queue_name: tripleo
    output:
      errors: <% $.errors %>
      warnings: <% $.warnings %>
      flavors: <% $.flavors %>

    tags:
      - tripleo-common-managed

    tasks:
      check_run_validations:
        on-complete:
          - check_flavors: <% $.run_validations %>
          - send_message: <% not $.run_validations %>

      check_flavors:
        action: tripleo.validations.check_flavors
        input:
          roles_info: <% $.roles_info %>
        on-success: send_message
        publish:
          flavors: <% task().result.flavors %>
          errors: <% task().result.errors %>
          warnings: <% task().result.warnings %>
        on-error: send_message
        publish-on-error:
          flavors: {}
          errors: <% task().result.errors %>
          warnings: <% task().result.warnings %>
          status: FAILED
          message: <% task().result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.collect_flavors
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                flavors: <% $.flavors %>
                errors: <% $.errors %>
                warnings: <% $.warnings %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  check_ironic_boot_configuration:
    input:
      - kernel_id: null
      - ramdisk_id: null
      - run_validations: true
      - queue_name: tripleo
    output:
      errors: <% $.errors %>
      warnings: <% $.warnings %>

    tags:
      - tripleo-common-managed

    tasks:
      check_run_validations:
        on-complete:
          - get_ironic_nodes: <% $.run_validations %>
          - send_message: <% not $.run_validations %>

      get_ironic_nodes:
        action: ironic.node_list
        input:
          provision_state: available
          maintenance: false
          detail: true
        on-success: check_node_boot_configuration
        publish:
          nodes: <% task().result %>
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      check_node_boot_configuration:
        action: tripleo.validations.check_node_boot_configuration
        input:
          node: <% $.node %>
          kernel_id: <% $.kernel_id %>
          ramdisk_id: <% $.ramdisk_id %>
        with-items: node in <% $.nodes %>
        on-success: send_message
        publish:
          errors: <% task().result.errors.flatten() %>
          warnings: <% task().result.warnings.flatten() %>
        on-error: send_message
        publish-on-error:
          errors: <% task().result.errors.flatten() %>
          warnings: <% task().result.warnings.flatten() %>
          status: FAILED
          message: <% task().result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.check_ironic_boot_configuration
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                errors: <% $.errors %>
                warnings: <% $.warnings %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  verify_profiles:
    input:
      - flavors: []
      - run_validations: true
      - queue_name: tripleo
    output:
      errors: <% $.errors %>
      warnings: <% $.warnings %>

    tags:
      - tripleo-common-managed

    tasks:
      check_run_validations:
        on-complete:
          - get_ironic_nodes: <% $.run_validations %>
          - send_message: <% not $.run_validations %>

      get_ironic_nodes:
        action: ironic.node_list
        input:
          maintenance: false
          detail: true
        on-success: verify_profiles
        publish:
          nodes: <% task().result %>
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>

      verify_profiles:
        action: tripleo.validations.verify_profiles
        input:
          nodes: <% $.nodes %>
          flavors: <% $.flavors %>
        on-success: send_message
        publish:
          errors: <% task().result.errors %>
          warnings: <% task().result.warnings %>
        on-error: send_message
        publish-on-error:
          errors: <% task().result.errors %>
          warnings: <% task().result.warnings %>
          status: FAILED
          message: <% task().result %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.verify_profiles
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                errors: <% $.errors %>
                warnings: <% $.warnings %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  check_default_nodes_count:
    input:
      - stack_id: overcloud
      - parameters: {}
      - default_role_counts: {}
      - run_validations: true
      - queue_name: tripleo
    output:
      statistics: <% $.statistics %>
      errors: <% $.errors %>
      warnings: <% $.warnings %>

    tags:
      - tripleo-common-managed

    tasks:
      check_run_validations:
        on-complete:
          - get_hypervisor_statistics: <% $.run_validations %>
          - send_message: <% not $.run_validations %>

      get_hypervisor_statistics:
        action: nova.hypervisors_statistics
        on-success: get_stack
        publish:
          statistics: <% task().result %>
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>
          errors: []
          warnings: []
          statistics: null

      get_stack:
        action: heat.stacks_get
        input:
          stack_id: <% $.stack_id %>
        on-success: get_associated_nodes
        publish:
          stack: <% task().result %>
        on-error: get_associated_nodes
        publish-on-error:
          stack: null

      get_associated_nodes:
        action: ironic.node_list
        input:
          associated: true
        on-success: get_available_nodes
        publish:
          associated_nodes: <% task().result %>
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>
          errors: []
          warnings: []

      get_available_nodes:
        action: ironic.node_list
        input:
          provision_state: available
          associated: false
          maintenance: false
        on-success: check_nodes_count
        publish:
          available_nodes: <% task().result %>
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>
          errors: []
          warnings: []

      check_nodes_count:
        action: tripleo.validations.check_nodes_count
        input:
          statistics: <% $.statistics %>
          stack: <% $.stack %>
          associated_nodes: <% $.associated_nodes %>
          available_nodes: <% $.available_nodes %>
          parameters: <% $.parameters %>
          default_role_counts: <% $.default_role_counts %>
        on-success: send_message
        publish:
          errors: <% task().result.errors %>
          warnings: <% task().result.warnings %>
        on-error: send_message
        publish-on-error:
          status: FAILED
          message: <% task().result %>
          statistics: null
          errors: <% task().result.errors %>
          warnings: <% task().result.warnings %>

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.check_hypervisor_stats
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                statistics: <% $.statistics %>
                errors: <% $.errors %>
                warnings: <% $.warnings %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>

  check_pre_deployment_validations:
    input:
      - deploy_kernel_name: 'bm-deploy-kernel'
      - deploy_ramdisk_name: 'bm-deploy-ramdisk'
      - roles_info: {}
      - stack_id: overcloud
      - parameters: {}
      - default_role_counts: {}
      - run_validations: true
      - queue_name: tripleo

    output:
      errors: <% $.errors %>
      warnings: <% $.warnings %>
      kernel_id: <% $.kernel_id %>
      ramdisk_id: <% $.ramdisk_id %>
      flavors: <% $.flavors %>
      statistics: <% $.statistics %>
    tags:
      - tripleo-common-managed
    tasks:
      init_messages:
        on-success: check_boot_images
        publish:
          errors: []
          warnings: []

      check_boot_images:
        workflow: check_boot_images
        input:
          deploy_kernel_name: <% $.deploy_kernel_name %>
          deploy_ramdisk_name: <% $.deploy_ramdisk_name %>
          run_validations: <% $.run_validations %>
          queue_name: <% $.queue_name %>
        publish:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          kernel_id: <% task().result.get('kernel_id') %>
          ramdisk_id: <% task().result.get('ramdisk_id') %>
        publish-on-error:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          kernel_id: <% task().result.get('kernel_id') %>
          ramdisk_id: <% task().result.get('ramdisk_id') %>
          status: FAILED
        on-success: collect_flavors
        on-error: collect_flavors

      collect_flavors:
        workflow: collect_flavors
        input:
          roles_info: <% $.roles_info %>
          run_validations: <% $.run_validations %>
          queue_name: <% $.queue_name %>
        publish:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          flavors: <% task().result.get('flavors') %>
        publish-on-error:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          flavors: <% task().result.get('flavors') %>
          status: FAILED
        on-success: check_ironic_boot_configuration
        on-error: check_ironic_boot_configuration

      check_ironic_boot_configuration:
        workflow: check_ironic_boot_configuration
        input:
          kernel_id: <% $.kernel_id %>
          ramdisk_id: <% $.ramdisk_id %>
          run_validations: <% $.run_validations %>
          queue_name: <% $.queue_name %>
        publish:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
        publish-on-error:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          status: FAILED
        on-success: check_default_nodes_count
        on-error: check_default_nodes_count

      check_default_nodes_count:
        workflow: check_default_nodes_count
        # ironic-nova sync happens once in two minutes
        retry: count=12 delay=10
        input:
          stack_id: <% $.stack_id %>
          parameters: <% $.parameters %>
          default_role_counts: <% $.default_role_counts %>
          run_validations: <% $.run_validations %>
          queue_name: <% $.queue_name %>
        publish:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          statistics: <% task().result.get('statistics') %>
        publish-on-error:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          statistics: <% task().result.get('statistics') %>
          status: FAILED
        on-success: verify_profiles
        # Do not confuse user with info about profiles if the nodes
        # count is off in the first place. Skip directly to
        # send_message. (bug 1703942)
        on-error: send_message

      verify_profiles:
        workflow: verify_profiles
        input:
          flavors: <% $.flavors %>
          run_validations: <% $.run_validations %>
          queue_name: <% $.queue_name %>
        publish:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
        publish-on-error:
          errors: <% $.errors + task().result.get('errors', []) %>
          warnings: <% $.warnings + task().result.get('warnings', []) %>
          status: FAILED
        on-success: send_message
        on-error: send_message

      send_message:
        action: zaqar.queue_post
        retry: count=5 delay=1
        input:
          queue_name: <% $.queue_name %>
          messages:
            body:
              type: tripleo.validations.v1.check_hypervisor_stats
              payload:
                status: <% $.get('status', 'SUCCESS') %>
                message: <% $.get('message', '') %>
                execution: <% execution() %>
                kernel_id: <% $.kernel_id %>
                ramdisk_id: <% $.ramdisk_id %>
                flavors: <% $.flavors %>
                statistics: <% $.statistics %>
                errors: <% $.errors %>
                warnings: <% $.warnings %>
        on-success:
          - fail: <% $.get('status') = "FAILED" %>
'
2018-12-04 06:26:22,653 DEBUG: https://172.16.0.10:13989 "POST /v2/workbooks HTTP/1.1" 201 25434
2018-12-04 06:26:22,693 DEBUG: RESP: [201] Content-Length: 25434 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:22 GMT Connection: keep-alive 
RESP BODY: {"definition": "---\nversion: '2.0'\nname: tripleo.validations.v1\ndescription: TripleO Validations Workflows v1\n\nworkflows:\n\n  run_validation:\n    input:\n      - validation_name\n      - plan: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      notify_running:\n        on-complete: run_validation\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validation\n              payload:\n                validation_name: <% $.validation_name %>\n                plan: <% $.plan %>\n                status: RUNNING\n                execution: <% execution() %>\n\n      run_validation:\n        on-success: send_message\n        on-error: set_status_failed\n        action: tripleo.validations.run_validation validation=<% $.validation_name %> plan=<% $.plan %>\n        publish:\n          status: SUCCESS\n          stdout: <% task().result.stdout %>\n          stderr: <% task().result.stderr %>\n\n      set_status_failed:\n        on-complete: send_message\n        publish:\n          status: FAILED\n          stdout: <% task(run_validation).result.stdout %>\n          stderr: <% task(run_validation).result.stderr %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validation\n              payload:\n                validation_name: <% $.validation_name %>\n                plan: <% $.plan %>\n                status: <% $.get('status', 'SUCCESS') %>\n                stdout: <% $.stdout %>\n                stderr: <% $.stderr %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  run_validations:\n    input:\n      - validation_names: []\n      - plan: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      notify_running:\n        on-complete: run_validations\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validations\n              payload:\n                validation_names: <% $.validation_names %>\n                plan: <% $.plan %>\n                status: RUNNING\n                execution: <% execution() %>\n\n      run_validations:\n        on-success: send_message\n        on-error: set_status_failed\n        workflow: tripleo.validations.v1.run_validation validation_name=<% $.validation %> plan=<% $.plan %> queue_name=<% $.queue_name %>\n        with-items: validation in <% $.validation_names %>\n        publish:\n          status: SUCCESS\n\n      set_status_failed:\n        on-complete: send_message\n        publish:\n          status: FAILED\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validations\n              payload:\n                validation_names: <% $.validation_names %>\n                plan: <% $.plan %>\n                status: <% $.get('status', 'SUCCESS') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  run_groups:\n    input:\n      - group_names: []\n      - plan: overcloud\n      - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n\n      find_validations:\n        on-success: notify_running\n        action: tripleo.validations.list_validations groups=<% $.group_names %>\n        publish:\n          validations: <% task().result %>\n\n      notify_running:\n        on-complete: run_validation_group\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_validations\n              payload:\n                group_names: <% $.group_names %>\n                validation_names: <% $.validations.id %>\n                plan: <% $.plan %>\n                status: RUNNING\n                execution: <% execution() %>\n\n      run_validation_group:\n        on-success: send_message\n        on-error: set_status_failed\n        workflow: tripleo.validations.v1.run_validation validation_name=<% $.validation %> plan=<% $.plan %> queue_name=<% $.queue_name %>\n        with-items: validation in <% $.validations.id %>\n        publish:\n          status: SUCCESS\n\n      set_status_failed:\n        on-complete: send_message\n        publish:\n          status: FAILED\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.run_groups\n              payload:\n                group_names: <% $.group_names %>\n                validation_names: <% $.validations.id %>\n                plan: <% $.plan %>\n                status: <% $.get('status', 'SUCCESS') %>\n                execution: <% execution() %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  list:\n    input:\n      - group_names: []\n    tags:\n      - tripleo-common-managed\n    tasks:\n      find_validations:\n        action: tripleo.validations.list_validations groups=<% $.group_names %>\n\n  list_groups:\n    tags:\n      - tripleo-common-managed\n    tasks:\n      find_groups:\n        action: tripleo.validations.list_groups\n\n  add_validation_ssh_key_parameter:\n    input:\n     - container\n     - queue_name: tripleo\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      test_validations_enabled:\n        action: tripleo.validations.enabled\n        on-success: get_pubkey\n        on-error: unset_validation_key_parameter\n\n      get_pubkey:\n        action: tripleo.validations.get_pubkey\n        on-success: set_validation_key_parameter\n        publish:\n          pubkey: <% task().result %>\n\n      set_validation_key_parameter:\n        action: tripleo.parameters.update\n        input:\n          parameters:\n            node_admin_extra_ssh_keys: <% $.pubkey %>\n          container: <% $.container %>\n\n      # NOTE(shadower): We need to clear keys from a previous deployment\n      unset_validation_key_parameter:\n        action: tripleo.parameters.update\n        input:\n          parameters:\n            node_admin_extra_ssh_keys: \"\"\n          container: <% $.container %>\n\n  copy_ssh_key:\n    input:\n      # FIXME: we should stop using heat-admin as e.g. split-stack\n      # environments (where Nova didn't create overcloud nodes) don't\n      # have it present\n      - overcloud_admin: heat-admin\n      - queue_name: tripleo\n    tags:\n      - tripleo-common-managed\n    tasks:\n      get_servers:\n        action: nova.servers_list\n        on-success: get_pubkey\n        publish:\n          servers: <% task().result._info %>\n\n      get_pubkey:\n        action: tripleo.validations.get_pubkey\n        on-success: deploy_ssh_key\n        publish:\n          pubkey: <% task().result %>\n\n      deploy_ssh_key:\n        workflow: tripleo.deployment.v1.deploy_on_server\n        with-items: server in <% $.servers %>\n        input:\n          server_name: <% $.server.name %>\n          server_uuid: <% $.server.id %>\n          config: |\n            #!/bin/bash\n            if ! grep \"<% $.pubkey %>\" /home/<% $.overcloud_admin %>/.ssh/authorized_keys; then\n              echo \"<% $.pubkey %>\" >> /home/<% $.overcloud_admin %>/.ssh/authorized_keys\n            fi\n          config_name: copy_ssh_key\n          group: script\n          queue_name: <% $.queue_name %>\n\n  check_boot_images:\n    input:\n      - deploy_kernel_name: 'bm-deploy-kernel'\n      - deploy_ramdisk_name: 'bm-deploy-ramdisk'\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n      kernel_id: <% $.kernel_id %>\n      ramdisk_id: <% $.ramdisk_id %>\n    tags:\n      - tripleo-common-managed\n    tasks:\n      check_run_validations:\n        on-complete:\n          - get_images: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      get_images:\n        action: glance.images_list\n        on-success: check_images\n        publish:\n          images: <% task().result %>\n\n      check_images:\n        action: tripleo.validations.check_boot_images\n        input:\n          images: <% $.images %>\n          deploy_kernel_name: <% $.deploy_kernel_name %>\n          deploy_ramdisk_name: <% $.deploy_ramdisk_name %>\n        on-success: send_message\n        publish:\n          kernel_id: <% task().result.kernel_id %>\n          ramdisk_id: <% task().result.ramdisk_id %>\n          warnings: <% task().result.warnings %>\n          errors: <% task().result.errors %>\n        on-error: send_message\n        publish-on-error:\n          kernel_id: <% task().result.kernel_id %>\n          ramdisk_id: <% task().result.ramdisk_id %>\n          warnings: <% task().result.warnings %>\n          errors: <% task().result.errors %>\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.check_boot_images\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                kernel_id: <% $.kernel_id %>\n                ramdisk_id: <% $.ramdisk_id %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  collect_flavors:\n    input:\n      - roles_info: {}\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n      flavors: <% $.flavors %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_run_validations:\n        on-complete:\n          - check_flavors: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      check_flavors:\n        action: tripleo.validations.check_flavors\n        input:\n          roles_info: <% $.roles_info %>\n        on-success: send_message\n        publish:\n          flavors: <% task().result.flavors %>\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n        on-error: send_message\n        publish-on-error:\n          flavors: {}\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.collect_flavors\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                flavors: <% $.flavors %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  check_ironic_boot_configuration:\n    input:\n      - kernel_id: null\n      - ramdisk_id: null\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_run_validations:\n        on-complete:\n          - get_ironic_nodes: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      get_ironic_nodes:\n        action: ironic.node_list\n        input:\n          provision_state: available\n          maintenance: false\n          detail: true\n        on-success: check_node_boot_configuration\n        publish:\n          nodes: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      check_node_boot_configuration:\n        action: tripleo.validations.check_node_boot_configuration\n        input:\n          node: <% $.node %>\n          kernel_id: <% $.kernel_id %>\n          ramdisk_id: <% $.ramdisk_id %>\n        with-items: node in <% $.nodes %>\n        on-success: send_message\n        publish:\n          errors: <% task().result.errors.flatten() %>\n          warnings: <% task().result.warnings.flatten() %>\n        on-error: send_message\n        publish-on-error:\n          errors: <% task().result.errors.flatten() %>\n          warnings: <% task().result.warnings.flatten() %>\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.check_ironic_boot_configuration\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  verify_profiles:\n    input:\n      - flavors: []\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_run_validations:\n        on-complete:\n          - get_ironic_nodes: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      get_ironic_nodes:\n        action: ironic.node_list\n        input:\n          maintenance: false\n          detail: true\n        on-success: verify_profiles\n        publish:\n          nodes: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n\n      verify_profiles:\n        action: tripleo.validations.verify_profiles\n        input:\n          nodes: <% $.nodes %>\n          flavors: <% $.flavors %>\n        on-success: send_message\n        publish:\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n        on-error: send_message\n        publish-on-error:\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n          status: FAILED\n          message: <% task().result %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.verify_profiles\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  check_default_nodes_count:\n    input:\n      - stack_id: overcloud\n      - parameters: {}\n      - default_role_counts: {}\n      - run_validations: true\n      - queue_name: tripleo\n    output:\n      statistics: <% $.statistics %>\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n\n    tags:\n      - tripleo-common-managed\n\n    tasks:\n      check_run_validations:\n        on-complete:\n          - get_hypervisor_statistics: <% $.run_validations %>\n          - send_message: <% not $.run_validations %>\n\n      get_hypervisor_statistics:\n        action: nova.hypervisors_statistics\n        on-success: get_stack\n        publish:\n          statistics: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n          errors: []\n          warnings: []\n          statistics: null\n\n      get_stack:\n        action: heat.stacks_get\n        input:\n          stack_id: <% $.stack_id %>\n        on-success: get_associated_nodes\n        publish:\n          stack: <% task().result %>\n        on-error: get_associated_nodes\n        publish-on-error:\n          stack: null\n\n      get_associated_nodes:\n        action: ironic.node_list\n        input:\n          associated: true\n        on-success: get_available_nodes\n        publish:\n          associated_nodes: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n          errors: []\n          warnings: []\n\n      get_available_nodes:\n        action: ironic.node_list\n        input:\n          provision_state: available\n          associated: false\n          maintenance: false\n        on-success: check_nodes_count\n        publish:\n          available_nodes: <% task().result %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n          errors: []\n          warnings: []\n\n      check_nodes_count:\n        action: tripleo.validations.check_nodes_count\n        input:\n          statistics: <% $.statistics %>\n          stack: <% $.stack %>\n          associated_nodes: <% $.associated_nodes %>\n          available_nodes: <% $.available_nodes %>\n          parameters: <% $.parameters %>\n          default_role_counts: <% $.default_role_counts %>\n        on-success: send_message\n        publish:\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n        on-error: send_message\n        publish-on-error:\n          status: FAILED\n          message: <% task().result %>\n          statistics: null\n          errors: <% task().result.errors %>\n          warnings: <% task().result.warnings %>\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.check_hypervisor_stats\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                statistics: <% $.statistics %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n\n  check_pre_deployment_validations:\n    input:\n      - deploy_kernel_name: 'bm-deploy-kernel'\n      - deploy_ramdisk_name: 'bm-deploy-ramdisk'\n      - roles_info: {}\n      - stack_id: overcloud\n      - parameters: {}\n      - default_role_counts: {}\n      - run_validations: true\n      - queue_name: tripleo\n\n    output:\n      errors: <% $.errors %>\n      warnings: <% $.warnings %>\n      kernel_id: <% $.kernel_id %>\n      ramdisk_id: <% $.ramdisk_id %>\n      flavors: <% $.flavors %>\n      statistics: <% $.statistics %>\n    tags:\n      - tripleo-common-managed\n    tasks:\n      init_messages:\n        on-success: check_boot_images\n        publish:\n          errors: []\n          warnings: []\n\n      check_boot_images:\n        workflow: check_boot_images\n        input:\n          deploy_kernel_name: <% $.deploy_kernel_name %>\n          deploy_ramdisk_name: <% $.deploy_ramdisk_name %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          kernel_id: <% task().result.get('kernel_id') %>\n          ramdisk_id: <% task().result.get('ramdisk_id') %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          kernel_id: <% task().result.get('kernel_id') %>\n          ramdisk_id: <% task().result.get('ramdisk_id') %>\n          status: FAILED\n        on-success: collect_flavors\n        on-error: collect_flavors\n\n      collect_flavors:\n        workflow: collect_flavors\n        input:\n          roles_info: <% $.roles_info %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          flavors: <% task().result.get('flavors') %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          flavors: <% task().result.get('flavors') %>\n          status: FAILED\n        on-success: check_ironic_boot_configuration\n        on-error: check_ironic_boot_configuration\n\n      check_ironic_boot_configuration:\n        workflow: check_ironic_boot_configuration\n        input:\n          kernel_id: <% $.kernel_id %>\n          ramdisk_id: <% $.ramdisk_id %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          status: FAILED\n        on-success: check_default_nodes_count\n        on-error: check_default_nodes_count\n\n      check_default_nodes_count:\n        workflow: check_default_nodes_count\n        # ironic-nova sync happens once in two minutes\n        retry: count=12 delay=10\n        input:\n          stack_id: <% $.stack_id %>\n          parameters: <% $.parameters %>\n          default_role_counts: <% $.default_role_counts %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          statistics: <% task().result.get('statistics') %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          statistics: <% task().result.get('statistics') %>\n          status: FAILED\n        on-success: verify_profiles\n        # Do not confuse user with info about profiles if the nodes\n        # count is off in the first place. Skip directly to\n        # send_message. (bug 1703942)\n        on-error: send_message\n\n      verify_profiles:\n        workflow: verify_profiles\n        input:\n          flavors: <% $.flavors %>\n          run_validations: <% $.run_validations %>\n          queue_name: <% $.queue_name %>\n        publish:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n        publish-on-error:\n          errors: <% $.errors + task().result.get('errors', []) %>\n          warnings: <% $.warnings + task().result.get('warnings', []) %>\n          status: FAILED\n        on-success: send_message\n        on-error: send_message\n\n      send_message:\n        action: zaqar.queue_post\n        retry: count=5 delay=1\n        input:\n          queue_name: <% $.queue_name %>\n          messages:\n            body:\n              type: tripleo.validations.v1.check_hypervisor_stats\n              payload:\n                status: <% $.get('status', 'SUCCESS') %>\n                message: <% $.get('message', '') %>\n                execution: <% execution() %>\n                kernel_id: <% $.kernel_id %>\n                ramdisk_id: <% $.ramdisk_id %>\n                flavors: <% $.flavors %>\n                statistics: <% $.statistics %>\n                errors: <% $.errors %>\n                warnings: <% $.warnings %>\n        on-success:\n          - fail: <% $.get('status') = \"FAILED\" %>\n", "name": "tripleo.validations.v1", "tags": [], "created_at": "2018-12-04 11:26:22", "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "3268805a-5446-443f-b19c-2b78a2650b49"}

2018-12-04 06:26:22,694 DEBUG: HTTP POST https://172.16.0.10:13989/v2/workbooks 201
2018-12-04 06:26:22,694 INFO: Mistral workbooks configured successfully
2018-12-04 06:26:22,699 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-04 06:26:23,775 DEBUG: https://172.16.0.10:13808 "GET /v1/AUTH_f41e7348daff4254892e5cd759aa8806?format=json HTTP/1.1" 200 735
2018-12-04 06:26:23,776 DEBUG: REQ: curl -i https://172.16.0.10:13808/v1/AUTH_f41e7348daff4254892e5cd759aa8806?format=json -X GET -H "Accept-Encoding: gzip" -H "X-Auth-Token: gAAAAABcBmQwwGNG..."
2018-12-04 06:26:23,776 DEBUG: RESP STATUS: 200 OK
2018-12-04 06:26:23,777 DEBUG: RESP HEADERS: {u'Content-Length': u'735', u'X-Account-Object-Count': u'1032', u'x-account-project-domain-id': u'default', u'X-Openstack-Request-Id': u'tx0888ca1d33b8418c85c5c-005c06645e', u'X-Account-Storage-Policy-Policy-0-Bytes-Used': u'5798917', u'X-Account-Storage-Policy-Policy-0-Container-Count': u'6', u'X-Timestamp': u'1543835958.76726', u'X-Account-Storage-Policy-Policy-0-Object-Count': u'1032', u'X-Account-Meta-Temp-Url-Key': u'1955b6607d068296...', u'X-Trans-Id': u'tx0888ca1d33b8418c85c5c-005c06645e', u'Date': u'Tue, 04 Dec 2018 11:26:23 GMT', u'X-Account-Bytes-Used': u'5798917', u'X-Account-Container-Count': u'6', u'Content-Type': u'application/json; charset=utf-8', u'Accept-Ranges': u'bytes'}
2018-12-04 06:26:23,777 DEBUG: RESP BODY: [{"count": 0, "last_modified": "2018-12-03T11:20:25.600150", "bytes": 0, "name": "__cache__"}, {"count": 1, "last_modified": "2018-12-03T14:21:02.194620", "bytes": 984449, "name": "ov-53fb54as2xx-0-pxl45v6fn2ir-Controller-6mydglkorfml"}, {"count": 1, "last_modified": "2018-12-03T14:20:59.680220", "bytes": 294463, "name": "ov-ee6n3ncd-0-k4s7hrx6t4ms-ObjectStorage-bi5ddzje2hvk"}, {"count": 1, "last_modified": "2018-12-03T14:21:01.873760", "bytes": 328338, "name": "ov-yv2x7kabme-0-xgrid4qniw4t-NovaCompute-yseyqikklj2f"}, {"count": 1028, "last_modified": "2018-12-03T11:19:18.783150", "bytes": 4189431, "name": "overcloud"}, {"count": 1, "last_modified": "2018-12-03T14:11:17.146620", "bytes": 2236, "name": "overcloud-swift-rings"}]
2018-12-04 06:26:23,779 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13989/v2/environments/tripleo.undercloud-config -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e"
2018-12-04 06:26:23,800 DEBUG: https://172.16.0.10:13989 "GET /v2/environments/tripleo.undercloud-config HTTP/1.1" 200 443
2018-12-04 06:26:23,801 DEBUG: RESP: [200] Content-Length: 443 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:23 GMT Connection: keep-alive 
RESP BODY: {"created_at": "2018-12-03 11:19:17", "description": "Undercloud configuration parameters", "variables": "{\"undercloud_ceilometer_snmpd_password\": \"fb321565da9bc0ef27c8324166ac09e4d889dc5d\", \"undercloud_db_password\": \"a9a10b0e40e705f2cb9c9d14baf2f49e5ce94c04\"}", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "22e08377-2cd4-40c6-93f5-cd30deea7fc0", "name": "tripleo.undercloud-config"}

2018-12-04 06:26:23,801 DEBUG: HTTP GET https://172.16.0.10:13989/v2/environments/tripleo.undercloud-config 200
2018-12-04 06:26:23,802 DEBUG: REQ: curl -g -i -X PUT https://172.16.0.10:13989/v2/environments -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '{"variables": "{\"undercloud_ceilometer_snmpd_password\": \"fb321565da9bc0ef27c8324166ac09e4d889dc5d\", \"undercloud_db_password\": \"a9a10b0e40e705f2cb9c9d14baf2f49e5ce94c04\"}", "name": "tripleo.undercloud-config", "description": "Undercloud configuration parameters"}'
2018-12-04 06:26:23,822 DEBUG: https://172.16.0.10:13989 "PUT /v2/environments HTTP/1.1" 200 443
2018-12-04 06:26:23,824 DEBUG: RESP: [200] Content-Length: 443 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:23 GMT Connection: keep-alive 
RESP BODY: {"created_at": "2018-12-03 11:19:17", "description": "Undercloud configuration parameters", "variables": "{\"undercloud_ceilometer_snmpd_password\": \"fb321565da9bc0ef27c8324166ac09e4d889dc5d\", \"undercloud_db_password\": \"a9a10b0e40e705f2cb9c9d14baf2f49e5ce94c04\"}", "updated_at": null, "scope": "private", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "22e08377-2cd4-40c6-93f5-cd30deea7fc0", "name": "tripleo.undercloud-config"}

2018-12-04 06:26:23,824 DEBUG: HTTP PUT https://172.16.0.10:13989/v2/environments 200
2018-12-04 06:26:23,825 INFO: Not creating default plan "overcloud" because it already exists.
2018-12-04 06:26:23,825 INFO: Configuring an hourly cron trigger for tripleo-ui logging
2018-12-04 06:26:23,826 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/cron_triggers -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '{"pattern": "0 * * * *", "workflow_name": "tripleo.plan_management.v1.publish_ui_logs_to_swift", "first_execution_time": null, "name": "publish-ui-logs-hourly", "remaining_executions": null}'
2018-12-04 06:26:25,305 DEBUG: https://172.16.0.10:13989 "POST /v2/cron_triggers HTTP/1.1" 201 493
2018-12-04 06:26:25,307 DEBUG: RESP: [201] Content-Length: 493 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:25 GMT Connection: keep-alive 
RESP BODY: {"created_at": "2018-12-04 11:26:25", "project_id": "f41e7348daff4254892e5cd759aa8806", "name": "publish-ui-logs-hourly", "pattern": "0 * * * *", "workflow_name": "tripleo.plan_management.v1.publish_ui_logs_to_swift", "workflow_input": "{}", "workflow_id": "8e3fa708-0cdf-460c-8b90-8625e2542b0e", "first_execution_time": null, "remaining_executions": null, "scope": "private", "workflow_params": "{}", "id": "54d3574e-ced6-4c55-be43-8e24313cec51", "next_execution_time": "2018-12-04 12:00:00"}

2018-12-04 06:26:25,307 DEBUG: HTTP POST https://172.16.0.10:13989/v2/cron_triggers 201
2018-12-04 06:26:25,307 DEBUG: REQ: curl -g -i -X POST https://172.16.0.10:13989/v2/executions -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "content-type: application/json" -H "X-Auth-Token: {SHA1}7274e676f198451c71a4e4005fac64b5a16c259e" -d '{"workflow_name": "tripleo.validations.v1.copy_ssh_key", "description": ""}'
2018-12-04 06:26:25,587 DEBUG: https://172.16.0.10:13989 "POST /v2/executions HTTP/1.1" 201 563
2018-12-04 06:26:25,589 DEBUG: RESP: [201] Content-Length: 563 Content-Type: application/json Date: Tue, 04 Dec 2018 11:26:25 GMT Connection: keep-alive 
RESP BODY: {"root_execution_id": null, "state_info": null, "description": "", "state": "RUNNING", "workflow_name": "tripleo.validations.v1.copy_ssh_key", "task_execution_id": null, "updated_at": "2018-12-04 11:26:25", "workflow_id": "de35b251-16bf-4646-9edb-3959f480ed0d", "params": "{\"namespace\": \"\", \"env\": {}}", "workflow_namespace": "", "output": "{}", "input": "{\"overcloud_admin\": \"heat-admin\", \"queue_name\": \"tripleo\"}", "created_at": "2018-12-04 11:26:25", "project_id": "f41e7348daff4254892e5cd759aa8806", "id": "91e54740-5929-4040-b6ff-c6240ed06b32"}

2018-12-04 06:26:25,589 DEBUG: HTTP POST https://172.16.0.10:13989/v2/executions 201
2018-12-04 06:26:25,818 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13000/ -H "Accept: application/json" -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5"
2018-12-04 06:26:25,820 DEBUG: Starting new HTTPS connection (1): 172.16.0.10
2018-12-04 06:26:25,841 DEBUG: https://172.16.0.10:13000 "GET / HTTP/1.1" 300 601
2018-12-04 06:26:25,843 DEBUG: RESP: [300] Date: Tue, 04 Dec 2018 11:26:25 GMT Server: Apache Vary: X-Auth-Token Content-Length: 601 Content-Type: application/json 
RESP BODY: {"versions": {"values": [{"status": "stable", "updated": "2018-02-28T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v3+json"}], "id": "v3.10", "links": [{"href": "https://172.16.0.10:13000/v3/", "rel": "self"}]}, {"status": "deprecated", "updated": "2016-08-04T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v2.0+json"}], "id": "v2.0", "links": [{"href": "https://172.16.0.10:13000/v2.0/", "rel": "self"}, {"href": "https://docs.openstack.org/", "type": "text/html", "rel": "describedby"}]}]}}

2018-12-04 06:26:25,844 DEBUG: Making authentication request to https://172.16.0.10:13000/v3/auth/tokens
2018-12-04 06:26:26,541 DEBUG: https://172.16.0.10:13000 "POST /v3/auth/tokens HTTP/1.1" 201 8096
2018-12-04 06:26:26,543 DEBUG: {"token": {"is_domain": false, "methods": ["password"], "roles": [{"id": "faa7ae54793f4277a142cde9530e003f", "name": "admin"}], "expires_at": "2018-12-04T15:26:26.000000Z", "project": {"domain": {"id": "default", "name": "Default"}, "id": "f41e7348daff4254892e5cd759aa8806", "name": "admin"}, "catalog": [{"endpoints": [{"url": "http://172.16.0.11:6385", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "0790be2fdc3b49d08f54f8afdd4a13f5"}, {"url": "https://172.16.0.10:13385", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "4d1975c16140453f8778e1fb9ac95da8"}, {"url": "http://172.16.0.11:6385", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "6d2c8e36ee28476ba57a9055e744d319"}], "type": "baremetal", "id": "2631f10c56d84d278c6347ffd617c17e", "name": "ironic"}, {"endpoints": [{"url": "http://172.16.0.11:8778/placement", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "17ffbfdf60bb4b1fa47d54afdfd65841"}, {"url": "http://172.16.0.11:8778/placement", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "4f70d07307cc47f7bef0a5e3b8fe3a67"}, {"url": "https://172.16.0.10:13778/placement", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "89249f96d951451f922f786665cd8ac0"}], "type": "placement", "id": "33d36ef2b525461882431a456c280ca4", "name": "placement"}, {"endpoints": [{"url": "http://172.16.0.11:35357", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "339fd082d7a348e0883ed89ccaf17e64"}, {"url": "http://172.16.0.11:5000", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "38e31d810fa946de8ccfbf1f7c3046b8"}, {"url": "https://172.16.0.10:13000", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "760586442ad141a8bb0f1ebc452fbf23"}], "type": "identity", "id": "3ecdc56d07224eac9452c019cb2faf2c", "name": "keystone"}, {"endpoints": [{"url": "http://172.16.0.11:8004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "19b04fec944847e3ae21b20f16b49cd1"}, {"url": "https://172.16.0.10:13004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "1bd9e950b8f64c758f56e16c1e416eb9"}, {"url": "http://172.16.0.11:8004/v1/f41e7348daff4254892e5cd759aa8806", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "f207a1d68f524c00be7cd409f67eb602"}], "type": "orchestration", "id": "6b802215e15547b4933f6ad11ff28780", "name": "heat"}, {"endpoints": [{"url": "http://172.16.0.11:8774/v2.1", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "04bf5d0432f24b62b7db4d32eb6e9185"}, {"url": "https://172.16.0.10:13774/v2.1", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "8e337cbbb3504e5e9f93d010f2a920f4"}, {"url": "http://172.16.0.11:8774/v2.1", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "97a0c76942474f85b0fc3128fe3d349c"}], "type": "compute", "id": "6c68f42f71d5448eb11cc7cd8a8b2b30", "name": "nova"}, {"endpoints": [{"url": "http://172.16.0.11:9696", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "8e04bd214b8f46b993b9f8b13f8ec47f"}, {"url": "http://172.16.0.11:9696", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "a133c18dcc214604a3bb74d5dc2e5c5f"}, {"url": "https://172.16.0.10:13696", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "df06914815264173860e975b90e1569c"}], "type": "network", "id": "acf2ad1a66ec4ae09be00688d95b112c", "name": "neutron"}, {"endpoints": [{"url": "http://172.16.0.11:5050", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "9d61189438cb45e3ac270c69205221ca"}, {"url": "http://172.16.0.11:5050", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "c2d37cd497924ce59d5bac0690d9e64b"}, {"url": "https://172.16.0.10:13050", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "d0b3f48230e349edafeac286fdeb8252"}], "type": "baremetal-introspection", "id": "b7b8e51747704c76888dcd514b7addf9", "name": "ironic-inspector"}, {"endpoints": [{"url": "http://172.16.0.11:8000/v1/f41e7348daff4254892e5cd759aa8806", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "848bf35e6ba8481fb799cc941e4c33de"}, {"url": "http://172.16.0.11:8000/v1/f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "a160fbb2b9a046f6902860dbf9e7900c"}, {"url": "https://172.16.0.10:13800/v1/f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "a314c5a74b3b4ca0ad7294b66e726139"}], "type": "cloudformation", "id": "cb16845f1f7c483e9775a0080437bfe9", "name": "heat-cfn"}, {"endpoints": [{"url": "http://172.16.0.11:9292", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "3561bb34989645fea114a7e5b4e443c7"}, {"url": "http://172.16.0.11:9292", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "8d54c0c6f22d43658500ee4b14d0d925"}, {"url": "https://172.16.0.10:13292", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "ee991dbd122a46aa90bb402e88d2cc12"}], "type": "image", "id": "cbe7192c36d147ceb3a550dce9ff3586", "name": "glance"}, {"endpoints": [{"url": "http://172.16.0.11:8888", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "068c25c0200a4a8aba4b6f6f6947a0f5"}, {"url": "http://172.16.0.11:8888", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "98d3307f2ce44059ac228d580f163ca1"}, {"url": "https://172.16.0.10:13888", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "a3978de44cb84351874ab38de3b38c15"}], "type": "messaging", "id": "d90c70e049d14593b7b666788ee43b6e", "name": "zaqar"}, {"endpoints": [{"url": "wss://172.16.0.10:9000", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "2eabe4530b0f41a9a78684eaffc82974"}, {"url": "ws://172.16.0.11:9000", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "300aa4234cf04b44b3de4d8604e6a75d"}, {"url": "ws://172.16.0.11:9000", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "bdaf3488dbab44a1ba43364b5d974e83"}], "type": "messaging-websocket", "id": "dc2d191d5b0e4491a88b1e372b18dbfe", "name": "zaqar-websocket"}, {"endpoints": [{"url": "http://172.16.0.11:8080", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "1e4eab20558248289100f101cfcf4e1e"}, {"url": "http://172.16.0.11:8080/v1/AUTH_f41e7348daff4254892e5cd759aa8806", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "6ee57e5f8aec4f3fa81e3ffef1b6f261"}, {"url": "https://172.16.0.10:13808/v1/AUTH_f41e7348daff4254892e5cd759aa8806", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "e4c21afceb3f4a8da0f2d75ba050dcdd"}], "type": "object-store", "id": "ea9a151f0e6342dbb1aeba5c885e208f", "name": "swift"}, {"endpoints": [{"url": "https://172.16.0.10:13989/v2", "interface": "public", "region": "regionOne", "region_id": "regionOne", "id": "15a1660af0f2476cbda3fe163c56f368"}, {"url": "http://172.16.0.11:8989/v2", "interface": "admin", "region": "regionOne", "region_id": "regionOne", "id": "63fadaa6590b47a3bb91a7c789dce370"}, {"url": "http://172.16.0.11:8989/v2", "interface": "internal", "region": "regionOne", "region_id": "regionOne", "id": "b3b5817a37ef4565b92c2fc34b95f250"}], "type": "workflowv2", "id": "f7c8197dd44f4085baddd516aa6e7d7e", "name": "mistral"}], "user": {"domain": {"id": "default", "name": "Default"}, "password_expires_at": null, "name": "admin", "id": "b0c11cd6e04b4e08ac31e050977f22cf"}, "audit_ids": ["8JHWXARRQDuuQfhfWq_LNg"], "issued_at": "2018-12-04T11:26:26.000000Z"}}
2018-12-04 06:26:26,544 DEBUG: REQ: curl -g -i -X GET https://172.16.0.10:13000/ -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}3a3f358be14f86c8782191100c2ffa97a30f9112"
2018-12-04 06:26:26,551 DEBUG: https://172.16.0.10:13000 "GET / HTTP/1.1" 300 601
2018-12-04 06:26:26,552 DEBUG: RESP: [300] Date: Tue, 04 Dec 2018 11:26:26 GMT Server: Apache Vary: X-Auth-Token Content-Length: 601 Content-Type: application/json 
RESP BODY: {"versions": {"values": [{"status": "stable", "updated": "2018-02-28T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v3+json"}], "id": "v3.10", "links": [{"href": "https://172.16.0.10:13000/v3/", "rel": "self"}]}, {"status": "deprecated", "updated": "2016-08-04T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v2.0+json"}], "id": "v2.0", "links": [{"href": "https://172.16.0.10:13000/v2.0/", "rel": "self"}, {"href": "https://docs.openstack.org/", "type": "text/html", "rel": "describedby"}]}]}}

2018-12-04 06:26:26,556 DEBUG: REQ: curl -g -i -X GET http://172.16.0.11:35357 -H "Accept: application/json" -H "User-Agent: -c keystoneauth1/3.4.0 python-requests/2.14.2 CPython/2.7.5"
2018-12-04 06:26:26,557 DEBUG: Starting new HTTP connection (1): 172.16.0.11
2018-12-04 06:26:26,562 DEBUG: http://172.16.0.11:35357 "GET / HTTP/1.1" 300 599
2018-12-04 06:26:26,563 DEBUG: RESP: [300] Date: Tue, 04 Dec 2018 11:26:26 GMT Server: Apache Vary: X-Auth-Token Content-Length: 599 Content-Type: application/json 
RESP BODY: {"versions": {"values": [{"status": "stable", "updated": "2018-02-28T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v3+json"}], "id": "v3.10", "links": [{"href": "http://172.16.0.11:35357/v3/", "rel": "self"}]}, {"status": "deprecated", "updated": "2016-08-04T00:00:00Z", "media-types": [{"base": "application/json", "type": "application/vnd.openstack.identity-v2.0+json"}], "id": "v2.0", "links": [{"href": "http://172.16.0.11:35357/v2.0/", "rel": "self"}, {"href": "https://docs.openstack.org/", "type": "text/html", "rel": "describedby"}]}]}}

2018-12-04 06:26:26,564 DEBUG: REQ: curl -g -i -X GET http://172.16.0.11:35357/v3/roles? -H "User-Agent: python-keystoneclient" -H "Accept: application/json" -H "X-Auth-Token: {SHA1}3a3f358be14f86c8782191100c2ffa97a30f9112"
2018-12-04 06:26:26,728 DEBUG: http://172.16.0.11:35357 "GET /v3/roles HTTP/1.1" 200 289
2018-12-04 06:26:26,730 DEBUG: RESP: [200] Date: Tue, 04 Dec 2018 11:26:26 GMT Server: Apache Vary: X-Auth-Token,Accept-Encoding x-openstack-request-id: req-e6062791-da14-49cf-a4c6-d6f1b8ca7302 Content-Encoding: gzip Content-Length: 289 Content-Type: application/json 
RESP BODY: {"links": {"self": "https://172.16.0.10:13000/v3/roles", "previous": null, "next": null}, "roles": [{"domain_id": null, "id": "5998b5f829fc401d8c428c41e0e0c562", "links": {"self": "https://172.16.0.10:13000/v3/roles/5998b5f829fc401d8c428c41e0e0c562"}, "name": "ResellerAdmin"}, {"domain_id": null, "id": "90067790123e41c0ba405c375a7e5d47", "links": {"self": "https://172.16.0.10:13000/v3/roles/90067790123e41c0ba405c375a7e5d47"}, "name": "swiftoperator"}, {"domain_id": null, "id": "cdb2bc33124e4ee6859f75543f4a126e", "links": {"self": "https://172.16.0.10:13000/v3/roles/cdb2bc33124e4ee6859f75543f4a126e"}, "name": "heat_stack_user"}, {"domain_id": null, "id": "faa7ae54793f4277a142cde9530e003f", "links": {"self": "https://172.16.0.10:13000/v3/roles/faa7ae54793f4277a142cde9530e003f"}, "name": "admin"}]}

2018-12-04 06:26:26,730 DEBUG: GET call to identity for http://172.16.0.11:35357/v3/roles used request id req-e6062791-da14-49cf-a4c6-d6f1b8ca7302
2018-12-04 06:26:26,767 INFO: 
#############################################################################
Undercloud install complete.

The file containing this installation's passwords is at
/home/stack/undercloud-passwords.conf.

There is also a stackrc file at /home/stack/stackrc.

These files are needed to interact with the OpenStack services, and should be
secured.

#############################################################################

